{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "import ipdb\n",
    "from scipy.io import loadmat\n",
    "import networkx as nx\n",
    "import multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from copy import deepcopy\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_arti(labels, c_train_num):\n",
    "    #labels: n-dim Longtensor, each element in [0,...,m-1].\n",
    "    #cora: m=7\n",
    "    num_classes = len(set(labels.tolist()))\n",
    "    c_idxs = [] # class-wise index\n",
    "    train_idx = []\n",
    "    val_idx = []\n",
    "    test_idx = []\n",
    "    c_num_mat = np.zeros((num_classes,3)).astype(int)\n",
    "    c_num_mat[:,1] = 25\n",
    "    c_num_mat[:,2] = 55\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        c_idx = (labels==i).nonzero()[:,-1].tolist()\n",
    "        print('{:d}-th class sample number: {:d}'.format(i,len(c_idx)))\n",
    "        random.shuffle(c_idx)\n",
    "        c_idxs.append(c_idx)\n",
    "\n",
    "        train_idx = train_idx + c_idx[:c_train_num[i]]\n",
    "        c_num_mat[i,0] = c_train_num[i]\n",
    "\n",
    "        val_idx = val_idx + c_idx[c_train_num[i]:(c_train_num[i]+int(c_train_num[i]*.2))]\n",
    "        test_idx = test_idx + c_idx[int(c_train_num[i]+(c_train_num[i]*.2)):]\n",
    "\n",
    "    random.shuffle(train_idx)\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "\n",
    "    train_idx = torch.LongTensor(train_idx)\n",
    "    val_idx = torch.LongTensor(val_idx)\n",
    "    test_idx = torch.LongTensor(test_idx)\n",
    "    #c_num_mat = torch.LongTensor(c_num_mat)\n",
    "\n",
    "\n",
    "    return train_idx, val_idx, test_idx, c_num_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_upsample(adj,features,labels,idx_train, portion=1.0, im_class_num=3):\n",
    "    c_largest = labels.max().item()\n",
    "    adj_back = adj\n",
    "    chosen = None\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    avg_number = int(idx_train.shape[0]/(c_largest+1))\n",
    "\n",
    "    for i in range(im_class_num):\n",
    "        new_chosen = idx_train[(labels==(c_largest-i))[idx_train]]\n",
    "        if portion == 0:#refers to even distribution\n",
    "            c_portion = int(avg_number/new_chosen.shape[0])\n",
    "\n",
    "            for j in range(c_portion):\n",
    "                if chosen is None:\n",
    "                    chosen = new_chosen\n",
    "                else:\n",
    "                    chosen = torch.cat((chosen, new_chosen), 0)\n",
    "\n",
    "        else:\n",
    "            c_portion = int(portion)\n",
    "            portion_rest = portion-c_portion\n",
    "            for j in range(c_portion):\n",
    "                num = int(new_chosen.shape[0])\n",
    "                new_chosen = new_chosen[:num]\n",
    "\n",
    "                if chosen is None:\n",
    "                    chosen = new_chosen\n",
    "                else:\n",
    "                    chosen = torch.cat((chosen, new_chosen), 0)\n",
    "            \n",
    "            num = int(new_chosen.shape[0]*portion_rest)\n",
    "            new_chosen = new_chosen[:num]\n",
    "\n",
    "            if chosen is None:\n",
    "                chosen = new_chosen\n",
    "            else:\n",
    "                chosen = torch.cat((chosen, new_chosen), 0)\n",
    "            \n",
    "\n",
    "    add_num = chosen.shape[0]\n",
    "    new_adj = adj_back.new(torch.Size((adj_back.shape[0]+add_num, adj_back.shape[0]+add_num)))\n",
    "    new_adj[:adj_back.shape[0], :adj_back.shape[0]] = adj_back[:,:]\n",
    "    new_adj[adj_back.shape[0]:, :adj_back.shape[0]] = adj_back[chosen,:]\n",
    "    new_adj[:adj_back.shape[0], adj_back.shape[0]:] = adj_back[:,chosen]\n",
    "    new_adj[adj_back.shape[0]:, adj_back.shape[0]:] = adj_back[chosen,:][:,chosen]\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    features_append = deepcopy(features[chosen,:])\n",
    "    labels_append = deepcopy(labels[chosen])\n",
    "    idx_new = np.arange(adj_back.shape[0], adj_back.shape[0]+add_num)\n",
    "    idx_train_append = idx_train.new(idx_new)\n",
    "\n",
    "    features = torch.cat((features,features_append), 0)\n",
    "    labels = torch.cat((labels,labels_append), 0)\n",
    "    idx_train = torch.cat((idx_train,idx_train_append), 0)\n",
    "    adj = new_adj\n",
    "\n",
    "    return adj, features, labels, idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_smote(adj,features,labels,idx_train, portion=1.0, im_class_num=3):\n",
    "    c_largest = labels.max().item()\n",
    "    adj_back = adj\n",
    "    chosen = None\n",
    "    new_features = None\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    avg_number = int(idx_train.shape[0]/(c_largest+1))\n",
    "\n",
    "    for i in range(im_class_num):\n",
    "        new_chosen = idx_train[(labels==(c_largest-i))[idx_train]]\n",
    "        if portion == 0:#refers to even distribution\n",
    "            c_portion = int(avg_number/new_chosen.shape[0])\n",
    "\n",
    "            portion_rest = (avg_number/new_chosen.shape[0]) - c_portion\n",
    "\n",
    "        else:\n",
    "            c_portion = int(portion)\n",
    "            portion_rest = portion-c_portion\n",
    "            \n",
    "        for j in range(c_portion):\n",
    "            num = int(new_chosen.shape[0])\n",
    "            new_chosen = new_chosen[:num]\n",
    "\n",
    "            chosen_embed = features[new_chosen,:]\n",
    "            distance = squareform(pdist(chosen_embed.detach()))\n",
    "            np.fill_diagonal(distance,distance.max()+100)\n",
    "\n",
    "            idx_neighbor = distance.argmin(axis=-1)\n",
    "            \n",
    "            interp_place = random.random()\n",
    "            embed = chosen_embed + (chosen_embed[idx_neighbor,:]-chosen_embed)*interp_place\n",
    "\n",
    "            if chosen is None:\n",
    "                chosen = new_chosen\n",
    "                new_features = embed\n",
    "            else:\n",
    "                chosen = torch.cat((chosen, new_chosen), 0)\n",
    "                new_features = torch.cat((new_features, embed),0)\n",
    "            \n",
    "        num = int(new_chosen.shape[0]*portion_rest)\n",
    "        new_chosen = new_chosen[:num]\n",
    "\n",
    "        chosen_embed = features[new_chosen,:]\n",
    "        distance = squareform(pdist(chosen_embed.detach()))\n",
    "        np.fill_diagonal(distance,distance.max()+100)\n",
    "\n",
    "        idx_neighbor = distance.argmin(axis=-1)\n",
    "            \n",
    "        interp_place = random.random()\n",
    "        embed = chosen_embed + (chosen_embed[idx_neighbor,:]-chosen_embed)*interp_place\n",
    "\n",
    "        if chosen is None:\n",
    "            chosen = new_chosen\n",
    "            new_features = embed\n",
    "        else:\n",
    "            chosen = torch.cat((chosen, new_chosen), 0)\n",
    "            new_features = torch.cat((new_features, embed),0)\n",
    "            \n",
    "\n",
    "    add_num = chosen.shape[0]\n",
    "    new_adj = adj_back.new(torch.Size((adj_back.shape[0]+add_num, adj_back.shape[0]+add_num)))\n",
    "    new_adj[:adj_back.shape[0], :adj_back.shape[0]] = adj_back[:,:]\n",
    "    new_adj[adj_back.shape[0]:, :adj_back.shape[0]] = adj_back[chosen,:]\n",
    "    new_adj[:adj_back.shape[0], adj_back.shape[0]:] = adj_back[:,chosen]\n",
    "    new_adj[adj_back.shape[0]:, adj_back.shape[0]:] = adj_back[chosen,:][:,chosen]\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    features_append = deepcopy(new_features)\n",
    "    labels_append = deepcopy(labels[chosen])\n",
    "    idx_new = np.arange(adj_back.shape[0], adj_back.shape[0]+add_num)\n",
    "    idx_train_append = idx_train.new(idx_new)\n",
    "\n",
    "    features = torch.cat((features,features_append), 0)\n",
    "    labels = torch.cat((labels,labels_append), 0)\n",
    "    idx_train = torch.cat((idx_train,idx_train_append), 0)\n",
    "    adj = new_adj\n",
    "\n",
    "    return adj, features, labels, idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ffn(hidden_units, dropout_rate, name=None):\n",
    "    fnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(fnn_layers, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        #for 3_D batch, need a loop!!!\n",
    "\n",
    "\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "#Multihead attention layer\n",
    "class MultiHead(Module):#currently, allowed for only one sample each time. As no padding mask is required.\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        num_heads,\n",
    "        kdim=None,\n",
    "        vdim=None,\n",
    "        embed_dim = 128,#should equal num_heads*head dim\n",
    "        v_embed_dim = None,\n",
    "        dropout=0.1,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super(MultiHead, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.kdim = kdim if kdim is not None else input_dim\n",
    "        self.vdim = vdim if vdim is not None else input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.v_embed_dim = v_embed_dim if v_embed_dim is not None else embed_dim\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.bias = bias\n",
    "        assert (\n",
    "            self.head_dim * num_heads == self.embed_dim\n",
    "        ), \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        assert self.v_embed_dim % num_heads ==0, \"v_embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "\n",
    "\n",
    "        self.q_proj = nn.Linear(self.input_dim, self.embed_dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(self.kdim, self.embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(self.vdim, self.v_embed_dim, bias=bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.v_embed_dim, self.v_embed_dim//self.num_heads, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if True:\n",
    "            # Empirically observed the convergence to be much better with\n",
    "            # the scaled initialization\n",
    "            nn.init.normal_(self.k_proj.weight)\n",
    "            nn.init.normal_(self.v_proj.weight)\n",
    "            nn.init.normal_(self.q_proj.weight)\n",
    "        else:\n",
    "            nn.init.normal_(self.k_proj.weight)\n",
    "            nn.init.normal_(self.v_proj.weight)\n",
    "            nn.init.normal_(self.q_proj.weight)\n",
    "\n",
    "        nn.init.normal_(self.out_proj.weight)\n",
    "\n",
    "        if self.out_proj.bias is not None:\n",
    "            nn.init.constant_(self.out_proj.bias, 0.)\n",
    "\n",
    "        if self.bias:\n",
    "            nn.init.constant_(self.k_proj.bias, 0.)\n",
    "            nn.init.constant_(self.v_proj.bias, 0.)\n",
    "            nn.init.constant_(self.q_proj.bias, 0.)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        need_weights: bool = False,\n",
    "        need_head_weights: bool = False,\n",
    "    ):\n",
    "        \"\"\"Input shape: Time x Batch x Channel\n",
    "        Args:\n",
    "            need_weights (bool, optional): return the attention weights,\n",
    "                averaged over heads (default: False).\n",
    "            need_head_weights (bool, optional): return the attention\n",
    "                weights for each head. Implies *need_weights*. Default:\n",
    "                return the average attention weights over all heads.\n",
    "        \"\"\"\n",
    "        if need_head_weights:\n",
    "            need_weights = True\n",
    "\n",
    "        batch_num, node_num, input_dim = query.size()\n",
    "\n",
    "        assert key is not None and value is not None\n",
    "\n",
    "        #project input\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(key)\n",
    "        v = self.v_proj(value)\n",
    "        q = q * self.scaling\n",
    "\n",
    "        #compute attention\n",
    "        q = q.view(batch_num, node_num, self.num_heads, self.head_dim).transpose(-2,-3).contiguous().view(batch_num*self.num_heads, node_num, self.head_dim)\n",
    "        k = k.view(batch_num, node_num, self.num_heads, self.head_dim).transpose(-2,-3).contiguous().view(batch_num*self.num_heads, node_num, self.head_dim)\n",
    "        v = v.view(batch_num, node_num, self.num_heads, self.vdim).transpose(-2,-3).contiguous().view(batch_num*self.num_heads, node_num, self.vdim)\n",
    "        attn_output_weights = torch.bmm(q, k.transpose(-1,-2))\n",
    "        attn_output_weights = F.softmax(attn_output_weights, dim=-1)\n",
    "\n",
    "        #drop out\n",
    "        attn_output_weights = F.dropout(attn_output_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        #collect output\n",
    "        attn_output = torch.bmm(attn_output_weights, v)\n",
    "        attn_output = attn_output.view(batch_num, self.num_heads, node_num, self.vdim).transpose(-2,-3).contiguous().view(batch_num, node_num, self.v_embed_dim)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "\n",
    "        if need_weights:\n",
    "            attn_output_weights = attn_output_weights #view: (batch_num, num_heads, node_num, node_num)\n",
    "            return attn_output, attn_output_weights.sum(dim=1) / self.num_heads\n",
    "        else:\n",
    "            return attn_output\n",
    "\n",
    "\n",
    "#Graphsage layer\n",
    "class SageConv(Module):\n",
    "    \"\"\"\n",
    "    Simple Graphsage layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super(SageConv, self).__init__()\n",
    "\n",
    "        self.proj = nn.Linear(in_features*2, out_features, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "        #print(\"note: for dense graph in graphsage, require it normalized.\")\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        nn.init.normal_(self.proj.weight)\n",
    "\n",
    "        if self.proj.bias is not None:\n",
    "            nn.init.constant_(self.proj.bias, 0.)\n",
    "\n",
    "    def forward(self, features, adj):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            adj: can be sparse or dense matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        #fuse info from neighbors. to be added:\n",
    "        if not isinstance(adj, torch.sparse.FloatTensor):\n",
    "            if len(adj.shape) == 3:\n",
    "                neigh_feature = torch.bmm(adj, features) / (adj.sum(dim=1).reshape((adj.shape[0], adj.shape[1],-1))+1)\n",
    "            else:\n",
    "                neigh_feature = torch.mm(adj, features) / (adj.sum(dim=1).reshape(adj.shape[0], -1)+1)\n",
    "        else:\n",
    "            #print(\"spmm not implemented for batch training. Note!\")\n",
    "            \n",
    "            neigh_feature = torch.spmm(adj, features) / (adj.to_dense().sum(dim=1).reshape(adj.shape[0], -1)+1)\n",
    "\n",
    "        #perform conv\n",
    "        data = torch.cat([features,neigh_feature], dim=-1)\n",
    "        combined = self.proj(data)\n",
    "\n",
    "        return combined\n",
    "\n",
    "#GraphAT layers\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        if isinstance(adj, torch.sparse.FloatTensor):\n",
    "            adj = adj.to_dense()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        N = h.size()[0]\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b\n",
    "\n",
    "\n",
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)\n",
    "\n",
    "    \n",
    "class SpGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(SpGraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
    "                \n",
    "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm = SpecialSpmm()\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        dv = 'cuda' if input.is_cuda else 'cpu'\n",
    "\n",
    "        N = input.size()[0]\n",
    "        edge = adj.nonzero().t()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        # h: N x out\n",
    "        assert not torch.isnan(h).any()\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
    "        # edge: 2*D x E\n",
    "\n",
    "        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "\n",
    "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n",
    "        # e_rowsum: N x 1\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "        \n",
    "        h_prime = h_prime.div(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------\n",
    "### models ###\n",
    "#--------------\n",
    "\n",
    "#gcn_encode\n",
    "class GCN_En(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(GCN_En, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GCN_En2(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(GCN_En2, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc2(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "class GCN_Classifier(nn.Module):\n",
    "    def __init__(self, nembed, nhid, nclass, dropout):\n",
    "        super(GCN_Classifier, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nembed, nhid)\n",
    "        self.mlp = nn.Linear(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "#sage model\n",
    "\n",
    "class Sage_En(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(Sage_En, self).__init__()\n",
    "\n",
    "        self.sage1 = SageConv(nfeat, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.sage1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "class Sage_En2(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(Sage_En2, self).__init__()\n",
    "\n",
    "        self.sage1 = SageConv(nfeat, nhid)\n",
    "        self.sage2 = SageConv(nhid, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.sage1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.sage2(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Sage_Classifier(nn.Module):\n",
    "    def __init__(self, nembed, nhid, nclass, dropout):\n",
    "        super(Sage_Classifier, self).__init__()\n",
    "\n",
    "        self.sage1 = SageConv(nembed, nhid)\n",
    "        self.mlp = nn.Linear(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.sage1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "#GAT model\n",
    "\n",
    "class GAT_En(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout, alpha=0.2, nheads=8):\n",
    "        super(GAT_En, self).__init__()\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_proj = nn.Linear(nhid * nheads, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.out_proj.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_proj(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class GAT_En2(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout, alpha=0.2, nheads=8):\n",
    "        super(GAT_En2, self).__init__()\n",
    "\n",
    "        \n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_proj = nn.Linear(nhid * nheads, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions_2 = [GraphAttentionLayer(nembed, nembed, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions_2):\n",
    "            self.add_module('attention2_{}'.format(i), attention)\n",
    "\n",
    "        self.out_proj_2 = nn.Linear(nembed * nheads, nembed)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.out_proj.weight,std=0.05)\n",
    "        nn.init.normal_(self.out_proj_2.weight,std=0.05)\n",
    "\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_proj(x))\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions_2], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_proj_2(x))\n",
    "        return x\n",
    "\n",
    "class GAT_Classifier(nn.Module):\n",
    "    def __init__(self, nembed, nhid, nclass, dropout, alpha=0.2, nheads=8):\n",
    "        super(GAT_Classifier, self).__init__()\n",
    "\n",
    "        \n",
    "        self.attentions = [GraphAttentionLayer(nembed, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_proj = nn.Linear(nhid * nheads, nhid)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.mlp = nn.Linear(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp.weight,std=0.05)\n",
    "        nn.init.normal_(self.out_proj.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_proj(x))\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, nembed, nhid, nclass, dropout):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.mlp = nn.Linear(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(Module):\n",
    "    \"\"\"\n",
    "    Simple Graphsage layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nembed, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.de_weight = Parameter(torch.FloatTensor(nembed, nembed))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.de_weight.size(1))\n",
    "        self.de_weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "    def forward(self, node_embed):\n",
    "        \n",
    "        combine = F.linear(node_embed, self.de_weight)\n",
    "        adj_out = torch.softmax(torch.mm(combine, combine.transpose(-1,-2)), dim = 0)\n",
    "\n",
    "        return adj_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_genuine(labels):\n",
    "    #labels: n-dim Longtensor, each element in [0,...,m-1].\n",
    "    #cora: m=7\n",
    "    num_classes = len(set(labels.tolist()))\n",
    "    c_idxs = [] # class-wise index\n",
    "    train_idx = []\n",
    "    val_idx = []\n",
    "    test_idx = []\n",
    "    c_num_mat = np.zeros((num_classes,3)).astype(int)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        c_idx = (labels==i).nonzero()[:,-1].tolist()\n",
    "        c_num = len(c_idx)\n",
    "        print('{:d}-th class sample number: {:d}'.format(i,len(c_idx)))\n",
    "        random.shuffle(c_idx)\n",
    "        c_idxs.append(c_idx)\n",
    "\n",
    "        if c_num <4:\n",
    "            if c_num < 3:\n",
    "                print(\"too small class type\")\n",
    "                ipdb.set_trace()\n",
    "            c_num_mat[i,0] = 1\n",
    "            c_num_mat[i,1] = 1\n",
    "            c_num_mat[i,2] = 1\n",
    "        else:\n",
    "            c_num_mat[i,0] = int(c_num/4)\n",
    "            c_num_mat[i,1] = int(c_num/4)\n",
    "            c_num_mat[i,2] = int(c_num/2)\n",
    "\n",
    "\n",
    "        train_idx = train_idx + c_idx[:c_num_mat[i,0]]\n",
    "\n",
    "        val_idx = val_idx + c_idx[c_num_mat[i,0]:c_num_mat[i,0]+c_num_mat[i,1]]\n",
    "        test_idx = test_idx + c_idx[c_num_mat[i,0]+c_num_mat[i,1]:c_num_mat[i,0]+c_num_mat[i,1]+c_num_mat[i,2]]\n",
    "\n",
    "    random.shuffle(train_idx)\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "\n",
    "    train_idx = torch.LongTensor(train_idx)\n",
    "    val_idx = torch.LongTensor(val_idx)\n",
    "    test_idx = torch.LongTensor(test_idx)\n",
    "    #c_num_mat = torch.LongTensor(c_num_mat)\n",
    "\n",
    "\n",
    "    return train_idx, val_idx, test_idx, c_num_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"interactions_smote.csv\")\n",
    "data['edge']=data['Gene Symbol']+',' +data['Interactor Symbol']\n",
    "Graphtype = nx.Graph()\n",
    "data['edge']=data['edge'].astype(str)\n",
    "g = nx.parse_edgelist(data['edge'], delimiter=',', create_using=Graphtype,)\n",
    "adj=nx.adjacency_matrix(g,weight=None)\n",
    "adj=adj.toarray()\n",
    "node_features = np.loadtxt('node_features_smote.txt')\n",
    "#node_features =np.ones((adj.shape[0],1))\n",
    "labels = np.loadtxt('Multi-Labels.txt')\n",
    "#labels = np.loadtxt('Labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=torch.LongTensor(labels)\n",
    "features=torch.LongTensor(node_features)\n",
    "adj=torch.LongTensor(adj)\n",
    "class_sample_num = 4000\n",
    "c_train_num = []\n",
    "for i in range(labels.max().item() + 1):\n",
    "    if i > labels.max().item()-1: #only imbalance the last classes\n",
    "        c_train_num.append(int(class_sample_num))\n",
    "\n",
    "    else:\n",
    "        c_train_num.append(class_sample_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 214\n",
      "2-th class sample number: 530\n",
      "3-th class sample number: 69\n",
      "-----------------------------------\n",
      "1\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 428\n",
      "2-th class sample number: 1060\n",
      "3-th class sample number: 138\n",
      "-----------------------------------\n",
      "2\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 856\n",
      "2-th class sample number: 2120\n",
      "3-th class sample number: 276\n",
      "-----------------------------------\n",
      "3\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 1712\n",
      "2-th class sample number: 4240\n",
      "3-th class sample number: 552\n",
      "-----------------------------------\n",
      "4\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 3424\n",
      "2-th class sample number: 8240\n",
      "3-th class sample number: 1104\n",
      "-----------------------------------\n",
      "5\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 6848\n",
      "2-th class sample number: 12240\n",
      "3-th class sample number: 2208\n",
      "-----------------------------------\n",
      "6\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 10848\n",
      "2-th class sample number: 16240\n",
      "3-th class sample number: 4416\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#for i in range(4):\n",
    "for i in range(7):\n",
    "        print(i)\n",
    "        print(\"-----------------------------------\")\n",
    "        idx_train, idx_val, idx_test, class_num_mat= split_arti(labels, c_train_num)\n",
    "        adj,features,labels,idx_train = src_upsample(adj,features,labels,idx_train,portion=1, im_class_num=3)\n",
    "        #adj,features,labels,idx_train = src_upsample(adj,features,labels,idx_train,portion=1, im_class_num=1)\n",
    "        print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th class sample number: 11403\n",
      "1-th class sample number: 14848\n",
      "2-th class sample number: 20240\n",
      "3-th class sample number: 8416\n"
     ]
    }
   ],
   "source": [
    "idx_train, idx_val, idx_test, class_num_mat= split_arti(labels, c_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = GCN_En(nfeat=features.shape[1],\n",
    "        nhid=2048,\n",
    "        nembed=2048,\n",
    "        dropout=.1)\n",
    "classifier = GCN_Classifier(nembed=1, \n",
    "        nhid=2048, \n",
    "        nclass=labels.max().item() + 1, \n",
    "        dropout=.1)\n",
    "decoder = Decoder(nembed=2048,\n",
    "        dropout=.1)\n",
    "\n",
    "\n",
    "optimizer_en = optim.AdamW(encoder.parameters(),\n",
    "                       lr=0.001,weight_decay=5e-4,eps=1e-04)\n",
    "optimizer_cls = optim.AdamW(classifier.parameters(),\n",
    "                       lr=0.001,weight_decay=5e-4,eps=1e-04)\n",
    "optimizer_de = optim.AdamW(decoder.parameters(),\n",
    "                       lr=0.001,weight_decay=5e-4,eps=1e-04)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_upsample(embed, labels, idx_train, adj=None, portion=1.0, im_class_num=3):\n",
    "    c_largest = labels.max().item()\n",
    "    avg_number = int(idx_train.shape[0]/(c_largest+1))\n",
    "    #ipdb.set_trace()\n",
    "    adj_new = None\n",
    "\n",
    "    for i in range(im_class_num):\n",
    "        chosen = idx_train[(labels==(c_largest-i))[idx_train]]\n",
    "        num = int(chosen.shape[0]*portion)\n",
    "        if portion == 0:\n",
    "            c_portion = int(avg_number/chosen.shape[0])\n",
    "            num = chosen.shape[0]\n",
    "        else:\n",
    "            c_portion = 1\n",
    "\n",
    "        for j in range(c_portion):\n",
    "            chosen = chosen[:num]\n",
    "\n",
    "            chosen_embed = embed[chosen,:]\n",
    "            distance = squareform(pdist(chosen_embed.detach()))\n",
    "            np.fill_diagonal(distance,distance.max()+100)\n",
    "\n",
    "            idx_neighbor = distance.argmin(axis=-1)\n",
    "            \n",
    "            interp_place = random.random()\n",
    "            new_embed = embed[chosen,:] + (chosen_embed[idx_neighbor[:],:]-embed[chosen,:])*interp_place\n",
    "\n",
    "\n",
    "            new_labels = labels.new(torch.Size((chosen.shape[0],1))).reshape(-1).fill_(c_largest-i)\n",
    "            idx_new = np.arange(embed.shape[0], embed.shape[0]+chosen.shape[0])\n",
    "            idx_train_append = idx_train.new(idx_new)\n",
    "\n",
    "            embed = torch.cat((embed,new_embed), 0)\n",
    "            labels = torch.cat((labels,new_labels), 0)\n",
    "            idx_train = torch.cat((idx_train,idx_train_append), 0)\n",
    "\n",
    "            if adj is not None:\n",
    "                if adj_new is None:\n",
    "                    adj_new = adj.new(torch.clamp(adj[chosen,:] + adj[idx_neighbor,:], min=0.0, max = 1.0))\n",
    "                else:\n",
    "                    temp = adj.new(torch.clamp(adj[chosen,:] + adj[idx_neighbor,:], min=0.0, max = 1.0))\n",
    "                    adj_new = torch.cat((adj_new, temp), 0)\n",
    "\n",
    "    if adj is not None:\n",
    "        add_num = adj_new.shape[0]\n",
    "        new_adj = adj.new(torch.Size((adj.shape[0]+add_num, adj.shape[0]+add_num))).fill_(0.0)\n",
    "        new_adj[:adj.shape[0], :adj.shape[0]] = adj[:,:]\n",
    "        new_adj[adj.shape[0]:, :adj.shape[0]] = adj_new[:,:]\n",
    "        new_adj[:adj.shape[0], adj.shape[0]:] = torch.transpose(adj_new, 0, 1)[:,:]\n",
    "\n",
    "        return embed, labels, idx_train, new_adj.detach()\n",
    "\n",
    "    else:\n",
    "        return embed, labels, idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "    decoder.train()\n",
    "    optimizer_en.zero_grad()\n",
    "    optimizer_cls.zero_grad()\n",
    "    optimizer_de.zero_grad()\n",
    "\n",
    "    embed = encoder(features, adj)\n",
    "\n",
    "    #perform SMOTE in embedding space\n",
    "    labels_new = labels\n",
    "    idx_train_new = idx_train\n",
    "    adj_new = adj\n",
    "\n",
    "   \n",
    "    #ipdb.set_trace()\n",
    "    output = classifier(embed, adj_new)\n",
    "    output_soft=torch.nn.functional.log_softmax(output)\n",
    "    loss_train = F.cross_entropy(output_soft[idx_train_new], labels_new[idx_train_new])\n",
    "    \n",
    "    acc_train = accuracy(output_soft[idx_train], labels_new[idx_train])\n",
    "    loss = loss_train\n",
    "    loss_rec = loss_train\n",
    "    loss.backward()\n",
    "    optimizer_en.step()\n",
    "\n",
    "    optimizer_cls.step()\n",
    "    loss_val = F.cross_entropy(output_soft[idx_val], labels[idx_val])\n",
    "    \n",
    "    acc_val = accuracy(output_soft[idx_val], labels[idx_val])\n",
    "\n",
    "    print('Epoch: {:05d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'loss_rec: {:.4f}'.format(loss_rec.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test(epoch = 0):\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    decoder.eval()\n",
    "    embed = encoder(features, adj)\n",
    "    output = classifier(embed, adj)\n",
    "    output_soft=torch.nn.functional.log_softmax(output)\n",
    "\n",
    "    loss_test = F.cross_entropy(output_soft[idx_test], labels[idx_test])\n",
    "    \n",
    "    acc_test = accuracy(output_soft[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "  \n",
    "\n",
    "    '''\n",
    "    if epoch==40:\n",
    "        torch\n",
    "    '''\n",
    "    return output\n",
    "\n",
    "\n",
    "def save_model(epoch):\n",
    "    saved_content = {}\n",
    "\n",
    "    saved_content['encoder'] = encoder.state_dict()\n",
    "    saved_content['decoder'] = decoder.state_dict()\n",
    "    saved_content['classifier'] = classifier.state_dict()\n",
    "\n",
    "    torch.save(saved_content, 'model_checkpoint.pth')\n",
    "\n",
    "    return\n",
    "\n",
    "def load_model(filename):\n",
    "    loaded_content = torch.load('checkpoint/{}/{}.pth')\n",
    "\n",
    "    encoder.load_state_dict(loaded_content['encoder'])\n",
    "    decoder.load_state_dict(loaded_content['decoder'])\n",
    "    classifier.load_state_dict(loaded_content['classifier'])\n",
    "\n",
    "    print(\"successfully loaded: \"+ filename)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.cuda()\n",
    "classifier = classifier.cuda()\n",
    "decoder = decoder.cuda()\n",
    "labels = labels.cuda()\n",
    "idx_train = idx_train.cuda()\n",
    "idx_val = idx_val.cuda()\n",
    "idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kyle\\AppData\\Local\\Temp\\ipykernel_16548\\4161052455.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  adj = torch.tensor(adj, dtype=torch.float)\n",
      "C:\\Users\\Kyle\\AppData\\Local\\Temp\\ipykernel_16548\\4161052455.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  features = torch.tensor(features, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "adj = torch.tensor(adj, dtype=torch.float)\n",
    "features = torch.tensor(features, dtype=torch.float)\n",
    "features = features.cuda()\n",
    "adj = adj.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (54907x2048 and 1x2048)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m t_total \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3000\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      6\u001b[0m         output\u001b[38;5;241m=\u001b[39mtest(epoch)\n",
      "Cell \u001b[1;32mIn[16], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     15\u001b[0m adj_new \u001b[38;5;241m=\u001b[39m adj\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#ipdb.set_trace()\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m output_soft\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(output)\n\u001b[0;32m     21\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output_soft[idx_train_new], labels_new[idx_train_new])\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 383\u001b[0m, in \u001b[0;36mGCN_Classifier.forward\u001b[1;34m(self, x, adj)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, adj):\n\u001b[1;32m--> 383\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    384\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    385\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 24\u001b[0m, in \u001b[0;36mGraphConvolution.forward\u001b[1;34m(self, input, adj)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, adj):\n\u001b[1;32m---> 24\u001b[0m     support \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mspmm(adj, support)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m#for 3_D batch, need a loop!!!\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (54907x2048 and 1x2048)"
     ]
    }
   ],
   "source": [
    "\n",
    "t_total = time.time()\n",
    "for epoch in range(3000):\n",
    "    train(epoch)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        output=test(epoch)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        save_model(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "output=test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idx_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idx_train[(labels==3)[idx_train]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(output, 'GraphSage_Upsample_output.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = output.max(1)[1].type_as(labels)\n",
    "cout=preds.cpu().detach().numpy()\n",
    "clabels=labels.cpu().detach().numpy()\n",
    "cidx_test=idx_test.cpu().detach().numpy()\n",
    "cidx_valid=idx_val.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(true_negative,false_positive):\n",
    "    return str(round(true_negative/(true_negative+false_positive),2))\n",
    "def sensitivity(true_positive,false_negative):\n",
    "    return str(round(true_positive/(true_positive+false_negative),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "cnf_matrix_n = metrics.confusion_matrix(clabels[cidx_test], cout[cidx_test])\n",
    "#class_names=[\"Not_Related\", \"Related\"]\n",
    "class_names=[\"Not_Related\", \"1\",\"2\",\"3\"] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "sns.heatmap(cnf_matrix_n, annot=True, cmap=\"YlGnBu\" ,fmt='d', xticklabels=class_names, yticklabels=class_names)\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Graph Sage Upsample')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_n[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(clabels[cidx_test], cout[cidx_test], target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#True Positive\n",
    "True_Positive_0=cnf_matrix_n[0][0]\n",
    "True_Positive_1=cnf_matrix_n[1][1]\n",
    "True_Positive_2=cnf_matrix_n[2][2]\n",
    "True_Positive_3=cnf_matrix_n[3][3]\n",
    "#True Negative\n",
    "True_Negatives_0=cnf_matrix_n[1][1]+cnf_matrix_n[1][2]+cnf_matrix_n[1][3]+cnf_matrix_n[2][1]+cnf_matrix_n[2][2]+cnf_matrix_n[2][3]+cnf_matrix_n[3][1]+cnf_matrix_n[3][2]+cnf_matrix_n[3][3]\n",
    "True_Negatives_1=cnf_matrix_n[0][0]+cnf_matrix_n[0][2]+cnf_matrix_n[0][3]+cnf_matrix_n[2][0]+cnf_matrix_n[2][2]+cnf_matrix_n[2][3]+cnf_matrix_n[3][0]+cnf_matrix_n[3][2]+cnf_matrix_n[3][3]\n",
    "True_Negatives_2=cnf_matrix_n[0][0]+cnf_matrix_n[0][1]+cnf_matrix_n[0][3]+cnf_matrix_n[2][0]+cnf_matrix_n[2][1]+cnf_matrix_n[2][3]+cnf_matrix_n[3][0]+cnf_matrix_n[3][1]+cnf_matrix_n[3][3]\n",
    "True_Negatives_3=cnf_matrix_n[0][0]+cnf_matrix_n[0][1]+cnf_matrix_n[0][2]+cnf_matrix_n[1][0]+cnf_matrix_n[1][1]+cnf_matrix_n[1][2]+cnf_matrix_n[2][0]+cnf_matrix_n[2][1]+cnf_matrix_n[2][2]\n",
    "#False Positive\n",
    "False_Positive_0=cnf_matrix_n[1][0]+cnf_matrix_n[2][0]+cnf_matrix_n[3][0]\n",
    "False_Positive_1=cnf_matrix_n[0][1]+cnf_matrix_n[2][1]+cnf_matrix_n[3][1]\n",
    "False_Positive_2=cnf_matrix_n[0][2]+cnf_matrix_n[1][2]+cnf_matrix_n[3][2]\n",
    "False_Positive_3=cnf_matrix_n[0][3]+cnf_matrix_n[1][3]+cnf_matrix_n[2][3]\n",
    "#False Negative\n",
    "False_Negative_0=cnf_matrix_n[0][1]+cnf_matrix_n[0][2]+cnf_matrix_n[0][3]\n",
    "False_Negative_1=cnf_matrix_n[1][0]+cnf_matrix_n[1][2]+cnf_matrix_n[1][3]\n",
    "False_Negative_2=cnf_matrix_n[2][0]+cnf_matrix_n[2][1]+cnf_matrix_n[2][3]\n",
    "False_Negative_3=cnf_matrix_n[3][0]+cnf_matrix_n[3][1]+cnf_matrix_n[3][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"test_specificity_0/test_sensitivity_0: \" + specificity(True_Negatives_0,False_Positive_0) +\"/\" +sensitivity(True_Positive_0,False_Negative_0)+\n",
    "     \"   test_specificity_1/test_sensitivity_1: \" + specificity(True_Negatives_1,False_Positive_1) +\"/\" +sensitivity(True_Positive_1,False_Negative_1)+\n",
    "    \"   test_specificity_2/test_sensitivity_2: \" + specificity(True_Negatives_2,False_Positive_2) +\"/\" +sensitivity(True_Positive_2,False_Negative_2)+\n",
    "     \"   test_specificity_3/test_sensitivity_3: \" + specificity(True_Negatives_3,False_Positive_3) +\"/\" +sensitivity(True_Positive_1,False_Negative_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_names[0])\n",
    "print(\"Specificity: \"+ specificity(True_Negatives_0,False_Positive_0))\n",
    "print(\"Sensitivity: \"+sensitivity(True_Positive_0,False_Negative_0))\n",
    "\n",
    "print(class_names[1])\n",
    "print(\"Specificity: \"+specificity(True_Negatives_1,False_Positive_1))\n",
    "print(\"Sensitivity: \"+sensitivity(True_Positive_1,False_Negative_1))\n",
    "\n",
    "print(class_names[2])\n",
    "print(\"Specificity: \"+specificity(True_Negatives_2,False_Positive_2))\n",
    "print(\"Sensitivity: \"+sensitivity(True_Positive_2,False_Negative_2))\n",
    "\n",
    "print(class_names[3])\n",
    "print(\"Specificity: \"+specificity(True_Negatives_3,False_Positive_3))\n",
    "print(\"Sensitivity: \"+sensitivity(True_Positive_3,False_Negative_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "cnf_matrix_n = metrics.confusion_matrix(clabels[cidx_valid], cout[cidx_valid])\n",
    "#class_names=[\"Not_Related\", \"Related\"]\n",
    "class_names=[\"Not_Related\", \"1\",\"2\",\"3\"] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "sns.heatmap(cnf_matrix_n, annot=True, cmap=\"YlGnBu\" ,fmt='d', xticklabels=class_names, yticklabels=class_names)\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Graph Sage Upsample')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#True Positive\n",
    "True_Positive_0=cnf_matrix_n[0][0]\n",
    "True_Positive_1=cnf_matrix_n[1][1]\n",
    "True_Positive_2=cnf_matrix_n[2][2]\n",
    "True_Positive_3=cnf_matrix_n[3][3]\n",
    "#True Negative\n",
    "True_Negatives_0=cnf_matrix_n[1][1]+cnf_matrix_n[1][2]+cnf_matrix_n[1][3]+cnf_matrix_n[2][1]+cnf_matrix_n[2][2]+cnf_matrix_n[2][3]+cnf_matrix_n[3][1]+cnf_matrix_n[3][2]+cnf_matrix_n[3][3]\n",
    "True_Negatives_1=cnf_matrix_n[0][0]+cnf_matrix_n[0][2]+cnf_matrix_n[0][3]+cnf_matrix_n[2][0]+cnf_matrix_n[2][2]+cnf_matrix_n[2][3]+cnf_matrix_n[3][0]+cnf_matrix_n[3][2]+cnf_matrix_n[3][3]\n",
    "True_Negatives_2=cnf_matrix_n[0][0]+cnf_matrix_n[0][1]+cnf_matrix_n[0][3]+cnf_matrix_n[2][0]+cnf_matrix_n[2][1]+cnf_matrix_n[2][3]+cnf_matrix_n[3][0]+cnf_matrix_n[3][1]+cnf_matrix_n[3][3]\n",
    "True_Negatives_3=cnf_matrix_n[0][0]+cnf_matrix_n[0][1]+cnf_matrix_n[0][2]+cnf_matrix_n[1][0]+cnf_matrix_n[1][1]+cnf_matrix_n[1][2]+cnf_matrix_n[2][0]+cnf_matrix_n[2][1]+cnf_matrix_n[2][2]\n",
    "#False Positive\n",
    "False_Positive_0=cnf_matrix_n[1][0]+cnf_matrix_n[2][0]+cnf_matrix_n[3][0]\n",
    "False_Positive_1=cnf_matrix_n[0][1]+cnf_matrix_n[2][1]+cnf_matrix_n[3][1]\n",
    "False_Positive_2=cnf_matrix_n[0][2]+cnf_matrix_n[1][2]+cnf_matrix_n[3][2]\n",
    "False_Positive_3=cnf_matrix_n[0][3]+cnf_matrix_n[1][3]+cnf_matrix_n[2][3]\n",
    "#False Negative\n",
    "False_Negative_0=cnf_matrix_n[0][1]+cnf_matrix_n[0][2]+cnf_matrix_n[0][3]\n",
    "False_Negative_1=cnf_matrix_n[1][0]+cnf_matrix_n[1][2]+cnf_matrix_n[1][3]\n",
    "False_Negative_2=cnf_matrix_n[2][0]+cnf_matrix_n[2][1]+cnf_matrix_n[2][3]\n",
    "False_Negative_3=cnf_matrix_n[3][0]+cnf_matrix_n[3][1]+cnf_matrix_n[3][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_names[0])\n",
    "print(\"Specificity: \"+ specificity(True_Negatives_0,False_Positive_0))\n",
    "print(\"Sensitivity: \"+sensitivity(True_Positive_0,False_Negative_0))\n",
    "\n",
    "print(class_names[1])\n",
    "print(\"Specificity: \"+specificity(True_Negatives_1,False_Positive_1))\n",
    "print(\"Sensitivity: \"+sensitivity(True_Positive_1,False_Negative_1))\n",
    "\n",
    "print(class_names[2])\n",
    "print(\"Specificity: \"+specificity(True_Negatives_2,False_Positive_2))\n",
    "print(\"Sensitivity: \"+sensitivity(True_Positive_2,False_Negative_2))\n",
    "\n",
    "print(class_names[3])\n",
    "print(\"Specificity: \"+specificity(True_Negatives_3,False_Positive_3))\n",
    "print(\"Sensitivity: \"+sensitivity(True_Positive_3,False_Negative_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"valid_specificity_0/valid_sensitivity_0: \" + specificity(True_Negatives_0,False_Positive_0) +\"/\" +sensitivity(True_Positive_0,False_Negative_0)+\n",
    "     \"   valid_specificity_1/valid_sensitivity_1: \" + specificity(True_Negatives_1,False_Positive_1) +\"/\" +sensitivity(True_Positive_1,False_Negative_1)+\n",
    "    \"   valid_specificity_2/valid_sensitivity_2: \" + specificity(True_Negatives_2,False_Positive_2) +\"/\" +sensitivity(True_Positive_2,False_Negative_2)+\n",
    "     \"   valid_specificity_3/valid_sensitivity_3: \" + specificity(True_Negatives_3,False_Positive_3) +\"/\" +sensitivity(True_Positive_1,False_Negative_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels=clabels[cidx_test]\n",
    "test_output=cout[cidx_test]\n",
    "valid_labels=clabels[cidx_valid]\n",
    "valid_output=cout[cidx_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_enc=np.eye(4)[test_labels]\n",
    "test_output_enc=np.eye(4)[test_output]\n",
    "valid_labels_enc=np.eye(4)[valid_labels]\n",
    "valid_output_enc=np.eye(4)[valid_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(test_labels_enc, test_output_enc,multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(valid_labels_enc, valid_output_enc,multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
