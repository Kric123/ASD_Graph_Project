{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "import ipdb\n",
    "from scipy.io import loadmat\n",
    "import networkx as nx\n",
    "import multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from copy import deepcopy\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_arti(labels, c_train_num):\n",
    "    #labels: n-dim Longtensor, each element in [0,...,m-1].\n",
    "    #cora: m=7\n",
    "    num_classes = len(set(labels.tolist()))\n",
    "    c_idxs = [] # class-wise index\n",
    "    train_idx = []\n",
    "    val_idx = []\n",
    "    test_idx = []\n",
    "    c_num_mat = np.zeros((num_classes,3)).astype(int)\n",
    "    c_num_mat[:,1] = 25\n",
    "    c_num_mat[:,2] = 55\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        c_idx = (labels==i).nonzero()[:,-1].tolist()\n",
    "        print('{:d}-th class sample number: {:d}'.format(i,len(c_idx)))\n",
    "        random.shuffle(c_idx)\n",
    "        c_idxs.append(c_idx)\n",
    "\n",
    "        train_idx = train_idx + c_idx[:c_train_num[i]]\n",
    "        c_num_mat[i,0] = c_train_num[i]\n",
    "\n",
    "        val_idx = val_idx + c_idx[c_train_num[i]:(c_train_num[i]+int(c_train_num[i]*.2))]\n",
    "        test_idx = test_idx + c_idx[int(c_train_num[i]+(c_train_num[i]*.2)):]\n",
    "\n",
    "    random.shuffle(train_idx)\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "\n",
    "    train_idx = torch.LongTensor(train_idx)\n",
    "    val_idx = torch.LongTensor(val_idx)\n",
    "    test_idx = torch.LongTensor(test_idx)\n",
    "    #c_num_mat = torch.LongTensor(c_num_mat)\n",
    "\n",
    "\n",
    "    return train_idx, val_idx, test_idx, c_num_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_upsample(adj,features,labels,idx_train, portion=1.0, im_class_num=3):\n",
    "    c_largest = labels.max().item()\n",
    "    adj_back = adj\n",
    "    chosen = None\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    avg_number = int(idx_train.shape[0]/(c_largest+1))\n",
    "\n",
    "    for i in range(im_class_num):\n",
    "        new_chosen = idx_train[(labels==(c_largest-i))[idx_train]]\n",
    "        if portion == 0:#refers to even distribution\n",
    "            c_portion = int(avg_number/new_chosen.shape[0])\n",
    "\n",
    "            for j in range(c_portion):\n",
    "                if chosen is None:\n",
    "                    chosen = new_chosen\n",
    "                else:\n",
    "                    chosen = torch.cat((chosen, new_chosen), 0)\n",
    "\n",
    "        else:\n",
    "            c_portion = int(portion)\n",
    "            portion_rest = portion-c_portion\n",
    "            for j in range(c_portion):\n",
    "                num = int(new_chosen.shape[0])\n",
    "                new_chosen = new_chosen[:num]\n",
    "\n",
    "                if chosen is None:\n",
    "                    chosen = new_chosen\n",
    "                else:\n",
    "                    chosen = torch.cat((chosen, new_chosen), 0)\n",
    "            \n",
    "            num = int(new_chosen.shape[0]*portion_rest)\n",
    "            new_chosen = new_chosen[:num]\n",
    "\n",
    "            if chosen is None:\n",
    "                chosen = new_chosen\n",
    "            else:\n",
    "                chosen = torch.cat((chosen, new_chosen), 0)\n",
    "            \n",
    "\n",
    "    add_num = chosen.shape[0]\n",
    "    new_adj = adj_back.new(torch.Size((adj_back.shape[0]+add_num, adj_back.shape[0]+add_num)))\n",
    "    new_adj[:adj_back.shape[0], :adj_back.shape[0]] = adj_back[:,:]\n",
    "    new_adj[adj_back.shape[0]:, :adj_back.shape[0]] = adj_back[chosen,:]\n",
    "    new_adj[:adj_back.shape[0], adj_back.shape[0]:] = adj_back[:,chosen]\n",
    "    new_adj[adj_back.shape[0]:, adj_back.shape[0]:] = adj_back[chosen,:][:,chosen]\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    features_append = deepcopy(features[chosen,:])\n",
    "    labels_append = deepcopy(labels[chosen])\n",
    "    idx_new = np.arange(adj_back.shape[0], adj_back.shape[0]+add_num)\n",
    "    idx_train_append = idx_train.new(idx_new)\n",
    "\n",
    "    features = torch.cat((features,features_append), 0)\n",
    "    labels = torch.cat((labels,labels_append), 0)\n",
    "    idx_train = torch.cat((idx_train,idx_train_append), 0)\n",
    "    adj = new_adj\n",
    "\n",
    "    return adj, features, labels, idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_smote(adj,features,labels,idx_train, portion=1.0, im_class_num=3):\n",
    "    c_largest = labels.max().item()\n",
    "    adj_back = adj\n",
    "    chosen = None\n",
    "    new_features = None\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    avg_number = int(idx_train.shape[0]/(c_largest+1))\n",
    "\n",
    "    for i in range(im_class_num):\n",
    "        new_chosen = idx_train[(labels==(c_largest-i))[idx_train]]\n",
    "        if portion == 0:#refers to even distribution\n",
    "            c_portion = int(avg_number/new_chosen.shape[0])\n",
    "\n",
    "            portion_rest = (avg_number/new_chosen.shape[0]) - c_portion\n",
    "\n",
    "        else:\n",
    "            c_portion = int(portion)\n",
    "            portion_rest = portion-c_portion\n",
    "            \n",
    "        for j in range(c_portion):\n",
    "            num = int(new_chosen.shape[0])\n",
    "            new_chosen = new_chosen[:num]\n",
    "\n",
    "            chosen_embed = features[new_chosen,:]\n",
    "            distance = squareform(pdist(chosen_embed.detach()))\n",
    "            np.fill_diagonal(distance,distance.max()+100)\n",
    "\n",
    "            idx_neighbor = distance.argmin(axis=-1)\n",
    "            \n",
    "            interp_place = random.random()\n",
    "            embed = chosen_embed + (chosen_embed[idx_neighbor,:]-chosen_embed)*interp_place\n",
    "\n",
    "            if chosen is None:\n",
    "                chosen = new_chosen\n",
    "                new_features = embed\n",
    "            else:\n",
    "                chosen = torch.cat((chosen, new_chosen), 0)\n",
    "                new_features = torch.cat((new_features, embed),0)\n",
    "            \n",
    "        num = int(new_chosen.shape[0]*portion_rest)\n",
    "        new_chosen = new_chosen[:num]\n",
    "\n",
    "        chosen_embed = features[new_chosen,:]\n",
    "        distance = squareform(pdist(chosen_embed.detach()))\n",
    "        np.fill_diagonal(distance,distance.max()+100)\n",
    "\n",
    "        idx_neighbor = distance.argmin(axis=-1)\n",
    "            \n",
    "        interp_place = random.random()\n",
    "        embed = chosen_embed + (chosen_embed[idx_neighbor,:]-chosen_embed)*interp_place\n",
    "\n",
    "        if chosen is None:\n",
    "            chosen = new_chosen\n",
    "            new_features = embed\n",
    "        else:\n",
    "            chosen = torch.cat((chosen, new_chosen), 0)\n",
    "            new_features = torch.cat((new_features, embed),0)\n",
    "            \n",
    "\n",
    "    add_num = chosen.shape[0]\n",
    "    new_adj = adj_back.new(torch.Size((adj_back.shape[0]+add_num, adj_back.shape[0]+add_num)))\n",
    "    new_adj[:adj_back.shape[0], :adj_back.shape[0]] = adj_back[:,:]\n",
    "    new_adj[adj_back.shape[0]:, :adj_back.shape[0]] = adj_back[chosen,:]\n",
    "    new_adj[:adj_back.shape[0], adj_back.shape[0]:] = adj_back[:,chosen]\n",
    "    new_adj[adj_back.shape[0]:, adj_back.shape[0]:] = adj_back[chosen,:][:,chosen]\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    features_append = deepcopy(new_features)\n",
    "    labels_append = deepcopy(labels[chosen])\n",
    "    idx_new = np.arange(adj_back.shape[0], adj_back.shape[0]+add_num)\n",
    "    idx_train_append = idx_train.new(idx_new)\n",
    "\n",
    "    features = torch.cat((features,features_append), 0)\n",
    "    labels = torch.cat((labels,labels_append), 0)\n",
    "    idx_train = torch.cat((idx_train,idx_train_append), 0)\n",
    "    adj = new_adj\n",
    "\n",
    "    return adj, features, labels, idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ffn(hidden_units, dropout_rate, name=None):\n",
    "    fnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(fnn_layers, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        #for 3_D batch, need a loop!!!\n",
    "\n",
    "\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "#Multihead attention layer\n",
    "class MultiHead(Module):#currently, allowed for only one sample each time. As no padding mask is required.\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        num_heads,\n",
    "        kdim=None,\n",
    "        vdim=None,\n",
    "        embed_dim = 128,#should equal num_heads*head dim\n",
    "        v_embed_dim = None,\n",
    "        dropout=0.1,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super(MultiHead, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.kdim = kdim if kdim is not None else input_dim\n",
    "        self.vdim = vdim if vdim is not None else input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.v_embed_dim = v_embed_dim if v_embed_dim is not None else embed_dim\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.bias = bias\n",
    "        assert (\n",
    "            self.head_dim * num_heads == self.embed_dim\n",
    "        ), \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        assert self.v_embed_dim % num_heads ==0, \"v_embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "\n",
    "\n",
    "        self.q_proj = nn.Linear(self.input_dim, self.embed_dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(self.kdim, self.embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(self.vdim, self.v_embed_dim, bias=bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.v_embed_dim, self.v_embed_dim//self.num_heads, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if True:\n",
    "            # Empirically observed the convergence to be much better with\n",
    "            # the scaled initialization\n",
    "            nn.init.normal_(self.k_proj.weight)\n",
    "            nn.init.normal_(self.v_proj.weight)\n",
    "            nn.init.normal_(self.q_proj.weight)\n",
    "        else:\n",
    "            nn.init.normal_(self.k_proj.weight)\n",
    "            nn.init.normal_(self.v_proj.weight)\n",
    "            nn.init.normal_(self.q_proj.weight)\n",
    "\n",
    "        nn.init.normal_(self.out_proj.weight)\n",
    "\n",
    "        if self.out_proj.bias is not None:\n",
    "            nn.init.constant_(self.out_proj.bias, 0.)\n",
    "\n",
    "        if self.bias:\n",
    "            nn.init.constant_(self.k_proj.bias, 0.)\n",
    "            nn.init.constant_(self.v_proj.bias, 0.)\n",
    "            nn.init.constant_(self.q_proj.bias, 0.)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        need_weights: bool = False,\n",
    "        need_head_weights: bool = False,\n",
    "    ):\n",
    "        \"\"\"Input shape: Time x Batch x Channel\n",
    "        Args:\n",
    "            need_weights (bool, optional): return the attention weights,\n",
    "                averaged over heads (default: False).\n",
    "            need_head_weights (bool, optional): return the attention\n",
    "                weights for each head. Implies *need_weights*. Default:\n",
    "                return the average attention weights over all heads.\n",
    "        \"\"\"\n",
    "        if need_head_weights:\n",
    "            need_weights = True\n",
    "\n",
    "        batch_num, node_num, input_dim = query.size()\n",
    "\n",
    "        assert key is not None and value is not None\n",
    "\n",
    "        #project input\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(key)\n",
    "        v = self.v_proj(value)\n",
    "        q = q * self.scaling\n",
    "\n",
    "        #compute attention\n",
    "        q = q.view(batch_num, node_num, self.num_heads, self.head_dim).transpose(-2,-3).contiguous().view(batch_num*self.num_heads, node_num, self.head_dim)\n",
    "        k = k.view(batch_num, node_num, self.num_heads, self.head_dim).transpose(-2,-3).contiguous().view(batch_num*self.num_heads, node_num, self.head_dim)\n",
    "        v = v.view(batch_num, node_num, self.num_heads, self.vdim).transpose(-2,-3).contiguous().view(batch_num*self.num_heads, node_num, self.vdim)\n",
    "        attn_output_weights = torch.bmm(q, k.transpose(-1,-2))\n",
    "        attn_output_weights = F.softmax(attn_output_weights, dim=-1)\n",
    "\n",
    "        #drop out\n",
    "        attn_output_weights = F.dropout(attn_output_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        #collect output\n",
    "        attn_output = torch.bmm(attn_output_weights, v)\n",
    "        attn_output = attn_output.view(batch_num, self.num_heads, node_num, self.vdim).transpose(-2,-3).contiguous().view(batch_num, node_num, self.v_embed_dim)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "\n",
    "        if need_weights:\n",
    "            attn_output_weights = attn_output_weights #view: (batch_num, num_heads, node_num, node_num)\n",
    "            return attn_output, attn_output_weights.sum(dim=1) / self.num_heads\n",
    "        else:\n",
    "            return attn_output\n",
    "\n",
    "\n",
    "#Graphsage layer\n",
    "class SageConv(Module):\n",
    "    \"\"\"\n",
    "    Simple Graphsage layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super(SageConv, self).__init__()\n",
    "\n",
    "        self.proj = nn.Linear(in_features*2, out_features, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "        #print(\"note: for dense graph in graphsage, require it normalized.\")\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        nn.init.normal_(self.proj.weight)\n",
    "\n",
    "        if self.proj.bias is not None:\n",
    "            nn.init.constant_(self.proj.bias, 0.)\n",
    "\n",
    "    def forward(self, features, adj):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            adj: can be sparse or dense matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        #fuse info from neighbors. to be added:\n",
    "        if not isinstance(adj, torch.sparse.FloatTensor):\n",
    "            if len(adj.shape) == 3:\n",
    "                neigh_feature = torch.bmm(adj, features) / (adj.sum(dim=1).reshape((adj.shape[0], adj.shape[1],-1))+1)\n",
    "            else:\n",
    "                neigh_feature = torch.mm(adj, features) / (adj.sum(dim=1).reshape(adj.shape[0], -1)+1)\n",
    "        else:\n",
    "            #print(\"spmm not implemented for batch training. Note!\")\n",
    "            \n",
    "            neigh_feature = torch.spmm(adj, features) / (adj.to_dense().sum(dim=1).reshape(adj.shape[0], -1)+1)\n",
    "\n",
    "        #perform conv\n",
    "        data = torch.cat([features,neigh_feature], dim=-1)\n",
    "        combined = self.proj(data)\n",
    "\n",
    "        return combined\n",
    "\n",
    "#GraphAT layers\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        if isinstance(adj, torch.sparse.FloatTensor):\n",
    "            adj = adj.to_dense()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        N = h.size()[0]\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b\n",
    "\n",
    "\n",
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)\n",
    "\n",
    "    \n",
    "class SpGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(SpGraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
    "                \n",
    "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm = SpecialSpmm()\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        dv = 'cuda' if input.is_cuda else 'cpu'\n",
    "\n",
    "        N = input.size()[0]\n",
    "        edge = adj.nonzero().t()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        # h: N x out\n",
    "        assert not torch.isnan(h).any()\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
    "        # edge: 2*D x E\n",
    "\n",
    "        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "\n",
    "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n",
    "        # e_rowsum: N x 1\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "        \n",
    "        h_prime = h_prime.div(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------\n",
    "### models ###\n",
    "#--------------\n",
    "\n",
    "#gcn_encode\n",
    "class GCN_En(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(GCN_En, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GCN_En2(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(GCN_En2, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc2(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "class GCN_Classifier(nn.Module):\n",
    "    def __init__(self, nembed, nhid, nclass, dropout):\n",
    "        super(GCN_Classifier, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nembed, nhid)\n",
    "        self.mlp = nn.Linear(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "#sage model\n",
    "\n",
    "class Sage_En(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(Sage_En, self).__init__()\n",
    "\n",
    "        self.sage1 = SageConv(nfeat, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.sage1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "class Sage_En2(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(Sage_En2, self).__init__()\n",
    "\n",
    "        self.sage1 = SageConv(nfeat, nhid)\n",
    "        self.sage2 = SageConv(nhid, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.sage1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.sage2(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Sage_Classifier(nn.Module):\n",
    "    def __init__(self, nembed, nhid, nclass, dropout):\n",
    "        super(Sage_Classifier, self).__init__()\n",
    "\n",
    "        self.sage1 = SageConv(nembed, nhid)\n",
    "        self.mlp = nn.Linear(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.sage1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "#GAT model\n",
    "\n",
    "class GAT_En(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout, alpha=0.2, nheads=8):\n",
    "        super(GAT_En, self).__init__()\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_proj = nn.Linear(nhid * nheads, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.out_proj.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_proj(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class GAT_En2(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout, alpha=0.2, nheads=8):\n",
    "        super(GAT_En2, self).__init__()\n",
    "\n",
    "        \n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_proj = nn.Linear(nhid * nheads, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions_2 = [GraphAttentionLayer(nembed, nembed, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions_2):\n",
    "            self.add_module('attention2_{}'.format(i), attention)\n",
    "\n",
    "        self.out_proj_2 = nn.Linear(nembed * nheads, nembed)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.out_proj.weight,std=0.05)\n",
    "        nn.init.normal_(self.out_proj_2.weight,std=0.05)\n",
    "\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_proj(x))\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions_2], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_proj_2(x))\n",
    "        return x\n",
    "\n",
    "class GAT_Classifier(nn.Module):\n",
    "    def __init__(self, nembed, nhid, nclass, dropout, alpha=0.2, nheads=8):\n",
    "        super(GAT_Classifier, self).__init__()\n",
    "\n",
    "        \n",
    "        self.attentions = [GraphAttentionLayer(nembed, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_proj = nn.Linear(nhid * nheads, nhid)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.mlp = nn.Linear(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp.weight,std=0.05)\n",
    "        nn.init.normal_(self.out_proj.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_proj(x))\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MLP_En(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(MLP_En, self).__init__()\n",
    "\n",
    "        self.mlp1 = nn.Linear(nfeat, nembed)\n",
    "        self.dropout = dropout\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp1.weight,std=0.05)\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.mlp1(x))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, nembed, nhid, nclass, dropout):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.mlp = nn.Linear(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(Module):\n",
    "    \"\"\"\n",
    "    Simple Graphsage layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nembed, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.de_weight = Parameter(torch.FloatTensor(nembed, nembed))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.de_weight.size(1))\n",
    "        self.de_weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "    def forward(self, node_embed):\n",
    "        \n",
    "        combine = F.linear(node_embed, self.de_weight)\n",
    "        adj_out = torch.sigmoid(torch.mm(combine, combine.transpose(-1,-2)))\n",
    "\n",
    "        return adj_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_genuine(labels):\n",
    "    #labels: n-dim Longtensor, each element in [0,...,m-1].\n",
    "    #cora: m=7\n",
    "    num_classes = len(set(labels.tolist()))\n",
    "    c_idxs = [] # class-wise index\n",
    "    train_idx = []\n",
    "    val_idx = []\n",
    "    test_idx = []\n",
    "    c_num_mat = np.zeros((num_classes,3)).astype(int)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        c_idx = (labels==i).nonzero()[:,-1].tolist()\n",
    "        c_num = len(c_idx)\n",
    "        print('{:d}-th class sample number: {:d}'.format(i,len(c_idx)))\n",
    "        random.shuffle(c_idx)\n",
    "        c_idxs.append(c_idx)\n",
    "\n",
    "        if c_num <4:\n",
    "            if c_num < 3:\n",
    "                print(\"too small class type\")\n",
    "                ipdb.set_trace()\n",
    "            c_num_mat[i,0] = 1\n",
    "            c_num_mat[i,1] = 1\n",
    "            c_num_mat[i,2] = 1\n",
    "        else:\n",
    "            c_num_mat[i,0] = int(c_num/4)\n",
    "            c_num_mat[i,1] = int(c_num/4)\n",
    "            c_num_mat[i,2] = int(c_num/2)\n",
    "\n",
    "\n",
    "        train_idx = train_idx + c_idx[:c_num_mat[i,0]]\n",
    "\n",
    "        val_idx = val_idx + c_idx[c_num_mat[i,0]:c_num_mat[i,0]+c_num_mat[i,1]]\n",
    "        test_idx = test_idx + c_idx[c_num_mat[i,0]+c_num_mat[i,1]:c_num_mat[i,0]+c_num_mat[i,1]+c_num_mat[i,2]]\n",
    "\n",
    "    random.shuffle(train_idx)\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "\n",
    "    train_idx = torch.LongTensor(train_idx)\n",
    "    val_idx = torch.LongTensor(val_idx)\n",
    "    test_idx = torch.LongTensor(test_idx)\n",
    "    #c_num_mat = torch.LongTensor(c_num_mat)\n",
    "\n",
    "\n",
    "    return train_idx, val_idx, test_idx, c_num_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"interactions_smote.csv\")\n",
    "data['edge']=data['Gene Symbol']+',' +data['Interactor Symbol']\n",
    "Graphtype = nx.Graph()\n",
    "data['edge']=data['edge'].astype(str)\n",
    "g = nx.parse_edgelist(data['edge'], delimiter=',', create_using=Graphtype,)\n",
    "adj=nx.adjacency_matrix(g,weight=None)\n",
    "adj=adj.toarray()\n",
    "node_features = np.loadtxt('node_features_smote.txt')\n",
    "#labels = np.loadtxt('Multi-Labels.txt')\n",
    "#labels = np.loadtxt('Labels.txt')\n",
    "labels = np.loadtxt('syndromic-Labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=torch.LongTensor(labels)\n",
    "features=torch.LongTensor(node_features)\n",
    "adj=torch.LongTensor(adj)\n",
    "class_sample_num = 6000\n",
    "c_train_num = []\n",
    "for i in range(labels.max().item() + 1):\n",
    "    if i > labels.max().item()-1: #only imbalance the last classes\n",
    "        c_train_num.append(int(class_sample_num))\n",
    "\n",
    "    else:\n",
    "        c_train_num.append(class_sample_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-----------------------------------\n",
      "0-th class sample number: 12047\n",
      "1-th class sample number: 169\n",
      "-----------------------------------\n",
      "1\n",
      "-----------------------------------\n",
      "0-th class sample number: 12047\n",
      "1-th class sample number: 338\n",
      "-----------------------------------\n",
      "2\n",
      "-----------------------------------\n",
      "0-th class sample number: 12047\n",
      "1-th class sample number: 676\n",
      "-----------------------------------\n",
      "3\n",
      "-----------------------------------\n",
      "0-th class sample number: 12047\n",
      "1-th class sample number: 1352\n",
      "-----------------------------------\n",
      "4\n",
      "-----------------------------------\n",
      "0-th class sample number: 12047\n",
      "1-th class sample number: 2704\n",
      "-----------------------------------\n",
      "5\n",
      "-----------------------------------\n",
      "0-th class sample number: 12047\n",
      "1-th class sample number: 5408\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#for i in range(4):\n",
    "for i in range(6):\n",
    "#for i in range(7):\n",
    "        print(i)\n",
    "        print(\"-----------------------------------\")\n",
    "        idx_train, idx_val, idx_test, class_num_mat= split_arti(labels, c_train_num)\n",
    "        #adj,features,labels,idx_train = src_upsample(adj,features,labels,idx_train,portion=1, im_class_num=3)\n",
    "        adj,features,labels,idx_train = src_upsample(adj,features,labels,idx_train,portion=1, im_class_num=1)\n",
    "        print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th class sample number: 12047\n",
      "1-th class sample number: 10816\n"
     ]
    }
   ],
   "source": [
    "idx_train, idx_val, idx_test, class_num_mat= split_arti(labels, c_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MLP_En(nfeat=features.shape[1],\n",
    "        nhid=1,\n",
    "        nembed=1,\n",
    "        dropout=0)\n",
    "classifier = Classifier(nembed=1, \n",
    "        nhid=1, \n",
    "        nclass=labels.max().item() + 1, \n",
    "        dropout=0)\n",
    "decoder = Decoder(nembed=1,\n",
    "        dropout=0)\n",
    "\n",
    "\n",
    "optimizer_en = optim.AdamW(encoder.parameters(),\n",
    "                       lr=0.001,weight_decay=5e-4,eps=1e-04)\n",
    "optimizer_cls = optim.AdamW(classifier.parameters(),\n",
    "                       lr=0.001,weight_decay=5e-4,eps=1e-04)\n",
    "optimizer_de = optim.AdamW(decoder.parameters(),\n",
    "                       lr=0.001,weight_decay=5e-4,eps=1e-04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_upsample(embed, labels, idx_train, adj=None, portion=1.0, im_class_num=3):\n",
    "    c_largest = labels.max().item()\n",
    "    avg_number = int(idx_train.shape[0]/(c_largest+1))\n",
    "    #ipdb.set_trace()\n",
    "    adj_new = None\n",
    "\n",
    "    for i in range(im_class_num):\n",
    "        chosen = idx_train[(labels==(c_largest-i))[idx_train]]\n",
    "        num = int(chosen.shape[0]*portion)\n",
    "        if portion == 0:\n",
    "            c_portion = int(avg_number/chosen.shape[0])\n",
    "            num = chosen.shape[0]\n",
    "        else:\n",
    "            c_portion = 1\n",
    "\n",
    "        for j in range(c_portion):\n",
    "            chosen = chosen[:num]\n",
    "\n",
    "            chosen_embed = embed[chosen,:]\n",
    "            distance = squareform(pdist(chosen_embed.detach()))\n",
    "            np.fill_diagonal(distance,distance.max()+100)\n",
    "\n",
    "            idx_neighbor = distance.argmin(axis=-1)\n",
    "            \n",
    "            interp_place = random.random()\n",
    "            new_embed = embed[chosen,:] + (chosen_embed[idx_neighbor[:],:]-embed[chosen,:])*interp_place\n",
    "\n",
    "\n",
    "            new_labels = labels.new(torch.Size((chosen.shape[0],1))).reshape(-1).fill_(c_largest-i)\n",
    "            idx_new = np.arange(embed.shape[0], embed.shape[0]+chosen.shape[0])\n",
    "            idx_train_append = idx_train.new(idx_new)\n",
    "\n",
    "            embed = torch.cat((embed,new_embed), 0)\n",
    "            labels = torch.cat((labels,new_labels), 0)\n",
    "            idx_train = torch.cat((idx_train,idx_train_append), 0)\n",
    "\n",
    "            if adj is not None:\n",
    "                if adj_new is None:\n",
    "                    adj_new = adj.new(torch.clamp(adj[chosen,:] + adj[idx_neighbor,:], min=0.0, max = 1.0))\n",
    "                else:\n",
    "                    temp = adj.new(torch.clamp(adj[chosen,:] + adj[idx_neighbor,:], min=0.0, max = 1.0))\n",
    "                    adj_new = torch.cat((adj_new, temp), 0)\n",
    "\n",
    "    if adj is not None:\n",
    "        add_num = adj_new.shape[0]\n",
    "        new_adj = adj.new(torch.Size((adj.shape[0]+add_num, adj.shape[0]+add_num))).fill_(0.0)\n",
    "        new_adj[:adj.shape[0], :adj.shape[0]] = adj[:,:]\n",
    "        new_adj[adj.shape[0]:, :adj.shape[0]] = adj_new[:,:]\n",
    "        new_adj[:adj.shape[0], adj.shape[0]:] = torch.transpose(adj_new, 0, 1)[:,:]\n",
    "\n",
    "        return embed, labels, idx_train, new_adj.detach()\n",
    "\n",
    "    else:\n",
    "        return embed, labels, idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "    decoder.train()\n",
    "    optimizer_en.zero_grad()\n",
    "    optimizer_cls.zero_grad()\n",
    "    optimizer_de.zero_grad()\n",
    "\n",
    "    embed = encoder(features, adj)\n",
    "\n",
    "    #perform SMOTE in embedding space\n",
    "    labels_new = labels\n",
    "    idx_train_new = idx_train\n",
    "    adj_new = adj\n",
    "\n",
    "   \n",
    "    #ipdb.set_trace()\n",
    "    output = classifier(embed, adj_new)\n",
    "\n",
    "    loss_train = F.cross_entropy(output[idx_train_new], labels_new[idx_train_new])\n",
    "    \n",
    "    acc_train = accuracy(output[idx_train], labels_new[idx_train])\n",
    "    loss = loss_train\n",
    "    loss_rec = loss_train\n",
    "    loss.backward()\n",
    "    optimizer_en.step()\n",
    "\n",
    "    optimizer_cls.step()\n",
    "    loss_val = F.cross_entropy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "\n",
    "    print('Epoch: {:05d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'loss_rec: {:.4f}'.format(loss_rec.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test(epoch = 0):\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    decoder.eval()\n",
    "    embed = encoder(features, adj)\n",
    "    output = classifier(embed, adj)\n",
    "    loss_test = F.cross_entropy(output[idx_test], labels[idx_test])\n",
    "    \n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "  \n",
    "\n",
    "    '''\n",
    "    if epoch==40:\n",
    "        torch\n",
    "    '''\n",
    "    return output\n",
    "\n",
    "\n",
    "def save_model(epoch):\n",
    "    saved_content = {}\n",
    "\n",
    "    saved_content['encoder'] = encoder.state_dict()\n",
    "    saved_content['decoder'] = decoder.state_dict()\n",
    "    saved_content['classifier'] = classifier.state_dict()\n",
    "\n",
    "    torch.save(saved_content, 'model_checkpoint.pth')\n",
    "\n",
    "    return\n",
    "\n",
    "def load_model(filename):\n",
    "    loaded_content = torch.load('checkpoint/{}/{}.pth')\n",
    "\n",
    "    encoder.load_state_dict(loaded_content['encoder'])\n",
    "    decoder.load_state_dict(loaded_content['decoder'])\n",
    "    classifier.load_state_dict(loaded_content['classifier'])\n",
    "\n",
    "    print(\"successfully loaded: \"+ filename)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.cuda()\n",
    "classifier = classifier.cuda()\n",
    "decoder = decoder.cuda()\n",
    "labels = labels.cuda()\n",
    "idx_train = idx_train.cuda()\n",
    "idx_val = idx_val.cuda()\n",
    "idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kyle\\AppData\\Local\\Temp\\ipykernel_30816\\4161052455.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  adj = torch.tensor(adj, dtype=torch.float)\n",
      "C:\\Users\\Kyle\\AppData\\Local\\Temp\\ipykernel_30816\\4161052455.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  features = torch.tensor(features, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "adj = torch.tensor(adj, dtype=torch.float)\n",
    "features = torch.tensor(features, dtype=torch.float)\n",
    "features = features.cuda()\n",
    "adj = adj.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00001 loss_train: 0.7013 loss_rec: 0.7013 acc_train: 0.5000 loss_val: 0.7013 acc_val: 0.5000 time: 0.6203s\n",
      "Test set results: loss= 0.7194 accuracy= 0.4273\n",
      "Epoch: 00002 loss_train: 0.7011 loss_rec: 0.7011 acc_train: 0.5000 loss_val: 0.7011 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00003 loss_train: 0.7010 loss_rec: 0.7010 acc_train: 0.5000 loss_val: 0.7010 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00004 loss_train: 0.7009 loss_rec: 0.7009 acc_train: 0.5000 loss_val: 0.7009 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00005 loss_train: 0.7007 loss_rec: 0.7007 acc_train: 0.5000 loss_val: 0.7007 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00006 loss_train: 0.7006 loss_rec: 0.7006 acc_train: 0.5000 loss_val: 0.7006 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00007 loss_train: 0.7005 loss_rec: 0.7005 acc_train: 0.5000 loss_val: 0.7005 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00008 loss_train: 0.7003 loss_rec: 0.7003 acc_train: 0.5000 loss_val: 0.7003 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 00009 loss_train: 0.7002 loss_rec: 0.7002 acc_train: 0.5000 loss_val: 0.7002 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 00010 loss_train: 0.7001 loss_rec: 0.7001 acc_train: 0.5000 loss_val: 0.7001 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 00011 loss_train: 0.7000 loss_rec: 0.7000 acc_train: 0.5000 loss_val: 0.7000 acc_val: 0.5000 time: 0.0010s\n",
      "Test set results: loss= 0.7167 accuracy= 0.4273\n",
      "Epoch: 00012 loss_train: 0.6998 loss_rec: 0.6998 acc_train: 0.5000 loss_val: 0.6998 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00013 loss_train: 0.6997 loss_rec: 0.6997 acc_train: 0.5000 loss_val: 0.6997 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00014 loss_train: 0.6996 loss_rec: 0.6996 acc_train: 0.5000 loss_val: 0.6996 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00015 loss_train: 0.6995 loss_rec: 0.6995 acc_train: 0.5000 loss_val: 0.6995 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00016 loss_train: 0.6993 loss_rec: 0.6993 acc_train: 0.5000 loss_val: 0.6993 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00017 loss_train: 0.6992 loss_rec: 0.6992 acc_train: 0.5000 loss_val: 0.6992 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00018 loss_train: 0.6991 loss_rec: 0.6991 acc_train: 0.5000 loss_val: 0.6991 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00019 loss_train: 0.6990 loss_rec: 0.6990 acc_train: 0.5000 loss_val: 0.6990 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00020 loss_train: 0.6989 loss_rec: 0.6989 acc_train: 0.5000 loss_val: 0.6989 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00021 loss_train: 0.6987 loss_rec: 0.6987 acc_train: 0.5000 loss_val: 0.6987 acc_val: 0.5000 time: 0.0010s\n",
      "Test set results: loss= 0.7140 accuracy= 0.4273\n",
      "Epoch: 00022 loss_train: 0.6986 loss_rec: 0.6986 acc_train: 0.5000 loss_val: 0.6986 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00023 loss_train: 0.6985 loss_rec: 0.6985 acc_train: 0.5000 loss_val: 0.6985 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00024 loss_train: 0.6984 loss_rec: 0.6984 acc_train: 0.5000 loss_val: 0.6984 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00025 loss_train: 0.6983 loss_rec: 0.6983 acc_train: 0.5000 loss_val: 0.6983 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00026 loss_train: 0.6982 loss_rec: 0.6982 acc_train: 0.5000 loss_val: 0.6981 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00027 loss_train: 0.6980 loss_rec: 0.6980 acc_train: 0.5000 loss_val: 0.6980 acc_val: 0.5000 time: 0.0005s\n",
      "Epoch: 00028 loss_train: 0.6979 loss_rec: 0.6979 acc_train: 0.5000 loss_val: 0.6979 acc_val: 0.5000 time: 0.0005s\n",
      "Epoch: 00029 loss_train: 0.6978 loss_rec: 0.6978 acc_train: 0.5000 loss_val: 0.6978 acc_val: 0.5000 time: 0.0005s\n",
      "Epoch: 00030 loss_train: 0.6977 loss_rec: 0.6977 acc_train: 0.5000 loss_val: 0.6977 acc_val: 0.5000 time: 0.0005s\n",
      "Epoch: 00031 loss_train: 0.6976 loss_rec: 0.6976 acc_train: 0.5000 loss_val: 0.6976 acc_val: 0.5000 time: 0.0010s\n",
      "Test set results: loss= 0.7113 accuracy= 0.4273\n",
      "Epoch: 00032 loss_train: 0.6974 loss_rec: 0.6974 acc_train: 0.5000 loss_val: 0.6974 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00033 loss_train: 0.6973 loss_rec: 0.6973 acc_train: 0.5000 loss_val: 0.6973 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00034 loss_train: 0.6972 loss_rec: 0.6972 acc_train: 0.5000 loss_val: 0.6972 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00035 loss_train: 0.6971 loss_rec: 0.6971 acc_train: 0.5000 loss_val: 0.6971 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00036 loss_train: 0.6970 loss_rec: 0.6970 acc_train: 0.5000 loss_val: 0.6970 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00037 loss_train: 0.6969 loss_rec: 0.6969 acc_train: 0.5000 loss_val: 0.6968 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00038 loss_train: 0.6967 loss_rec: 0.6967 acc_train: 0.5000 loss_val: 0.6967 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00039 loss_train: 0.6966 loss_rec: 0.6966 acc_train: 0.5000 loss_val: 0.6966 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00040 loss_train: 0.6965 loss_rec: 0.6965 acc_train: 0.5000 loss_val: 0.6965 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 00041 loss_train: 0.6964 loss_rec: 0.6964 acc_train: 0.5000 loss_val: 0.6963 acc_val: 0.5000 time: 0.0015s\n",
      "Test set results: loss= 0.7086 accuracy= 0.4273\n",
      "Epoch: 00042 loss_train: 0.6962 loss_rec: 0.6962 acc_train: 0.5000 loss_val: 0.6962 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00043 loss_train: 0.6961 loss_rec: 0.6961 acc_train: 0.5000 loss_val: 0.6961 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00044 loss_train: 0.6960 loss_rec: 0.6960 acc_train: 0.5000 loss_val: 0.6960 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00045 loss_train: 0.6959 loss_rec: 0.6959 acc_train: 0.5000 loss_val: 0.6958 acc_val: 0.5000 time: 0.0005s\n",
      "Epoch: 00046 loss_train: 0.6957 loss_rec: 0.6957 acc_train: 0.5000 loss_val: 0.6957 acc_val: 0.5000 time: 0.0005s\n",
      "Epoch: 00047 loss_train: 0.6956 loss_rec: 0.6956 acc_train: 0.5000 loss_val: 0.6956 acc_val: 0.5000 time: 0.0005s\n",
      "Epoch: 00048 loss_train: 0.6955 loss_rec: 0.6955 acc_train: 0.5000 loss_val: 0.6954 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00049 loss_train: 0.6953 loss_rec: 0.6953 acc_train: 0.5000 loss_val: 0.6953 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00050 loss_train: 0.6952 loss_rec: 0.6952 acc_train: 0.5000 loss_val: 0.6952 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00051 loss_train: 0.6951 loss_rec: 0.6951 acc_train: 0.5000 loss_val: 0.6951 acc_val: 0.5000 time: 0.0010s\n",
      "Test set results: loss= 0.7058 accuracy= 0.4273\n",
      "Epoch: 00052 loss_train: 0.6950 loss_rec: 0.6950 acc_train: 0.5000 loss_val: 0.6949 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00053 loss_train: 0.6948 loss_rec: 0.6948 acc_train: 0.5000 loss_val: 0.6948 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00054 loss_train: 0.6947 loss_rec: 0.6947 acc_train: 0.5000 loss_val: 0.6947 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00055 loss_train: 0.6946 loss_rec: 0.6946 acc_train: 0.5000 loss_val: 0.6945 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00056 loss_train: 0.6944 loss_rec: 0.6944 acc_train: 0.5000 loss_val: 0.6944 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00057 loss_train: 0.6943 loss_rec: 0.6943 acc_train: 0.5000 loss_val: 0.6943 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00058 loss_train: 0.6941 loss_rec: 0.6941 acc_train: 0.5000 loss_val: 0.6941 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00059 loss_train: 0.6940 loss_rec: 0.6940 acc_train: 0.5000 loss_val: 0.6940 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00060 loss_train: 0.6939 loss_rec: 0.6939 acc_train: 0.5000 loss_val: 0.6938 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00061 loss_train: 0.6937 loss_rec: 0.6937 acc_train: 0.5000 loss_val: 0.6937 acc_val: 0.5000 time: 0.0010s\n",
      "Test set results: loss= 0.7029 accuracy= 0.4273\n",
      "Epoch: 00062 loss_train: 0.6936 loss_rec: 0.6936 acc_train: 0.5000 loss_val: 0.6936 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00063 loss_train: 0.6934 loss_rec: 0.6934 acc_train: 0.5000 loss_val: 0.6934 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00064 loss_train: 0.6933 loss_rec: 0.6933 acc_train: 0.5000 loss_val: 0.6933 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00065 loss_train: 0.6932 loss_rec: 0.6932 acc_train: 0.5000 loss_val: 0.6931 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00066 loss_train: 0.6930 loss_rec: 0.6930 acc_train: 0.5000 loss_val: 0.6930 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00067 loss_train: 0.6929 loss_rec: 0.6929 acc_train: 0.5000 loss_val: 0.6928 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00068 loss_train: 0.6927 loss_rec: 0.6927 acc_train: 0.5000 loss_val: 0.6927 acc_val: 0.5000 time: 0.0011s\n",
      "Epoch: 00069 loss_train: 0.6926 loss_rec: 0.6926 acc_train: 0.5000 loss_val: 0.6925 acc_val: 0.5000 time: 0.0009s\n",
      "Epoch: 00070 loss_train: 0.6924 loss_rec: 0.6924 acc_train: 0.5000 loss_val: 0.6924 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00071 loss_train: 0.6923 loss_rec: 0.6923 acc_train: 0.5000 loss_val: 0.6922 acc_val: 0.5000 time: 0.0010s\n",
      "Test set results: loss= 0.7000 accuracy= 0.4273\n",
      "Epoch: 00072 loss_train: 0.6921 loss_rec: 0.6921 acc_train: 0.5000 loss_val: 0.6921 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00073 loss_train: 0.6920 loss_rec: 0.6920 acc_train: 0.5000 loss_val: 0.6919 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00074 loss_train: 0.6918 loss_rec: 0.6918 acc_train: 0.5000 loss_val: 0.6918 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00075 loss_train: 0.6917 loss_rec: 0.6917 acc_train: 0.5000 loss_val: 0.6916 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00076 loss_train: 0.6915 loss_rec: 0.6915 acc_train: 0.5000 loss_val: 0.6915 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00077 loss_train: 0.6913 loss_rec: 0.6913 acc_train: 0.5000 loss_val: 0.6913 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00078 loss_train: 0.6912 loss_rec: 0.6912 acc_train: 0.5000 loss_val: 0.6911 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00079 loss_train: 0.6910 loss_rec: 0.6910 acc_train: 0.5000 loss_val: 0.6910 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00080 loss_train: 0.6909 loss_rec: 0.6909 acc_train: 0.5000 loss_val: 0.6908 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00081 loss_train: 0.6907 loss_rec: 0.6907 acc_train: 0.5000 loss_val: 0.6907 acc_val: 0.5000 time: 0.0010s\n",
      "Test set results: loss= 0.6971 accuracy= 0.4273\n",
      "Epoch: 00082 loss_train: 0.6905 loss_rec: 0.6905 acc_train: 0.5000 loss_val: 0.6905 acc_val: 0.5000 time: 0.0010s\n",
      "Epoch: 00083 loss_train: 0.6904 loss_rec: 0.6904 acc_train: 0.5000 loss_val: 0.6903 acc_val: 0.5004 time: 0.0010s\n",
      "Epoch: 00084 loss_train: 0.6902 loss_rec: 0.6902 acc_train: 0.5000 loss_val: 0.6902 acc_val: 0.5004 time: 0.0010s\n",
      "Epoch: 00085 loss_train: 0.6900 loss_rec: 0.6900 acc_train: 0.5002 loss_val: 0.6900 acc_val: 0.5004 time: 0.0010s\n",
      "Epoch: 00086 loss_train: 0.6898 loss_rec: 0.6898 acc_train: 0.5002 loss_val: 0.6898 acc_val: 0.5008 time: 0.0010s\n",
      "Epoch: 00087 loss_train: 0.6897 loss_rec: 0.6897 acc_train: 0.5007 loss_val: 0.6896 acc_val: 0.5008 time: 0.0010s\n",
      "Epoch: 00088 loss_train: 0.6895 loss_rec: 0.6895 acc_train: 0.5008 loss_val: 0.6895 acc_val: 0.5008 time: 0.0005s\n",
      "Epoch: 00089 loss_train: 0.6893 loss_rec: 0.6893 acc_train: 0.5012 loss_val: 0.6893 acc_val: 0.5008 time: 0.0005s\n",
      "Epoch: 00090 loss_train: 0.6892 loss_rec: 0.6892 acc_train: 0.5015 loss_val: 0.6891 acc_val: 0.5013 time: 0.0005s\n",
      "Epoch: 00091 loss_train: 0.6890 loss_rec: 0.6890 acc_train: 0.5018 loss_val: 0.6890 acc_val: 0.5013 time: 0.0010s\n",
      "Test set results: loss= 0.6941 accuracy= 0.4295\n",
      "Epoch: 00092 loss_train: 0.6888 loss_rec: 0.6888 acc_train: 0.5025 loss_val: 0.6888 acc_val: 0.5013 time: 0.0010s\n",
      "Epoch: 00093 loss_train: 0.6886 loss_rec: 0.6886 acc_train: 0.5049 loss_val: 0.6886 acc_val: 0.5033 time: 0.0010s\n",
      "Epoch: 00094 loss_train: 0.6884 loss_rec: 0.6884 acc_train: 0.5046 loss_val: 0.6884 acc_val: 0.5013 time: 0.0010s\n",
      "Epoch: 00095 loss_train: 0.6883 loss_rec: 0.6883 acc_train: 0.5052 loss_val: 0.6882 acc_val: 0.5029 time: 0.0010s\n",
      "Epoch: 00096 loss_train: 0.6881 loss_rec: 0.6881 acc_train: 0.5062 loss_val: 0.6881 acc_val: 0.5038 time: 0.0010s\n",
      "Epoch: 00097 loss_train: 0.6879 loss_rec: 0.6879 acc_train: 0.5076 loss_val: 0.6879 acc_val: 0.5046 time: 0.0010s\n",
      "Epoch: 00098 loss_train: 0.6877 loss_rec: 0.6877 acc_train: 0.5114 loss_val: 0.6877 acc_val: 0.5096 time: 0.0010s\n",
      "Epoch: 00099 loss_train: 0.6875 loss_rec: 0.6875 acc_train: 0.5302 loss_val: 0.6875 acc_val: 0.5292 time: 0.0010s\n",
      "Epoch: 00100 loss_train: 0.6873 loss_rec: 0.6873 acc_train: 0.5326 loss_val: 0.6873 acc_val: 0.5300 time: 0.0010s\n",
      "Epoch: 00101 loss_train: 0.6871 loss_rec: 0.6871 acc_train: 0.5341 loss_val: 0.6871 acc_val: 0.5317 time: 0.0010s\n",
      "Test set results: loss= 0.6910 accuracy= 0.4690\n",
      "Epoch: 00102 loss_train: 0.6870 loss_rec: 0.6870 acc_train: 0.5357 loss_val: 0.6869 acc_val: 0.5325 time: 0.0010s\n",
      "Epoch: 00103 loss_train: 0.6868 loss_rec: 0.6868 acc_train: 0.5344 loss_val: 0.6868 acc_val: 0.5312 time: 0.0010s\n",
      "Epoch: 00104 loss_train: 0.6866 loss_rec: 0.6866 acc_train: 0.5409 loss_val: 0.6866 acc_val: 0.5367 time: 0.0010s\n",
      "Epoch: 00105 loss_train: 0.6864 loss_rec: 0.6864 acc_train: 0.5466 loss_val: 0.6864 acc_val: 0.5400 time: 0.0010s\n",
      "Epoch: 00106 loss_train: 0.6862 loss_rec: 0.6862 acc_train: 0.5483 loss_val: 0.6862 acc_val: 0.5421 time: 0.0010s\n",
      "Epoch: 00107 loss_train: 0.6860 loss_rec: 0.6860 acc_train: 0.5513 loss_val: 0.6860 acc_val: 0.5446 time: 0.0010s\n",
      "Epoch: 00108 loss_train: 0.6858 loss_rec: 0.6858 acc_train: 0.5548 loss_val: 0.6858 acc_val: 0.5479 time: 0.0010s\n",
      "Epoch: 00109 loss_train: 0.6856 loss_rec: 0.6856 acc_train: 0.5577 loss_val: 0.6856 acc_val: 0.5525 time: 0.0010s\n",
      "Epoch: 00110 loss_train: 0.6854 loss_rec: 0.6854 acc_train: 0.5635 loss_val: 0.6854 acc_val: 0.5604 time: 0.0010s\n",
      "Epoch: 00111 loss_train: 0.6852 loss_rec: 0.6852 acc_train: 0.5816 loss_val: 0.6852 acc_val: 0.5792 time: 0.0010s\n",
      "Test set results: loss= 0.6879 accuracy= 0.5337\n",
      "Epoch: 00112 loss_train: 0.6850 loss_rec: 0.6850 acc_train: 0.5863 loss_val: 0.6850 acc_val: 0.5871 time: 0.0010s\n",
      "Epoch: 00113 loss_train: 0.6848 loss_rec: 0.6848 acc_train: 0.5927 loss_val: 0.6848 acc_val: 0.5946 time: 0.0010s\n",
      "Epoch: 00114 loss_train: 0.6846 loss_rec: 0.6846 acc_train: 0.5946 loss_val: 0.6846 acc_val: 0.5954 time: 0.0010s\n",
      "Epoch: 00115 loss_train: 0.6844 loss_rec: 0.6844 acc_train: 0.5989 loss_val: 0.6844 acc_val: 0.5992 time: 0.0010s\n",
      "Epoch: 00116 loss_train: 0.6842 loss_rec: 0.6842 acc_train: 0.6028 loss_val: 0.6842 acc_val: 0.6017 time: 0.0010s\n",
      "Epoch: 00117 loss_train: 0.6840 loss_rec: 0.6840 acc_train: 0.6111 loss_val: 0.6840 acc_val: 0.6088 time: 0.0010s\n",
      "Epoch: 00118 loss_train: 0.6838 loss_rec: 0.6838 acc_train: 0.6451 loss_val: 0.6838 acc_val: 0.6388 time: 0.0010s\n",
      "Epoch: 00119 loss_train: 0.6836 loss_rec: 0.6836 acc_train: 0.6482 loss_val: 0.6836 acc_val: 0.6421 time: 0.0010s\n",
      "Epoch: 00120 loss_train: 0.6834 loss_rec: 0.6834 acc_train: 0.6517 loss_val: 0.6834 acc_val: 0.6496 time: 0.0010s\n",
      "Epoch: 00121 loss_train: 0.6831 loss_rec: 0.6831 acc_train: 0.6505 loss_val: 0.6832 acc_val: 0.6492 time: 0.0010s\n",
      "Test set results: loss= 0.6848 accuracy= 0.6512\n",
      "Epoch: 00122 loss_train: 0.6829 loss_rec: 0.6829 acc_train: 0.6691 loss_val: 0.6830 acc_val: 0.6667 time: 0.0010s\n",
      "Epoch: 00123 loss_train: 0.6827 loss_rec: 0.6827 acc_train: 0.6695 loss_val: 0.6827 acc_val: 0.6683 time: 0.0010s\n",
      "Epoch: 00124 loss_train: 0.6825 loss_rec: 0.6825 acc_train: 0.6737 loss_val: 0.6825 acc_val: 0.6738 time: 0.0010s\n",
      "Epoch: 00125 loss_train: 0.6823 loss_rec: 0.6823 acc_train: 0.6765 loss_val: 0.6823 acc_val: 0.6754 time: 0.0010s\n",
      "Epoch: 00126 loss_train: 0.6821 loss_rec: 0.6821 acc_train: 0.6800 loss_val: 0.6821 acc_val: 0.6758 time: 0.0005s\n",
      "Epoch: 00127 loss_train: 0.6819 loss_rec: 0.6819 acc_train: 0.6830 loss_val: 0.6819 acc_val: 0.6804 time: 0.0005s\n",
      "Epoch: 00128 loss_train: 0.6817 loss_rec: 0.6817 acc_train: 0.6896 loss_val: 0.6817 acc_val: 0.6871 time: 0.0005s\n",
      "Epoch: 00129 loss_train: 0.6814 loss_rec: 0.6814 acc_train: 0.6939 loss_val: 0.6815 acc_val: 0.6900 time: 0.0010s\n",
      "Epoch: 00130 loss_train: 0.6812 loss_rec: 0.6812 acc_train: 0.6906 loss_val: 0.6812 acc_val: 0.6875 time: 0.0010s\n",
      "Epoch: 00131 loss_train: 0.6810 loss_rec: 0.6810 acc_train: 0.6857 loss_val: 0.6810 acc_val: 0.6838 time: 0.0010s\n",
      "Test set results: loss= 0.6817 accuracy= 0.6855\n",
      "Epoch: 00132 loss_train: 0.6808 loss_rec: 0.6808 acc_train: 0.6853 loss_val: 0.6808 acc_val: 0.6858 time: 0.0020s\n",
      "Epoch: 00133 loss_train: 0.6806 loss_rec: 0.6806 acc_train: 0.6881 loss_val: 0.6806 acc_val: 0.6879 time: 0.0010s\n",
      "Epoch: 00134 loss_train: 0.6803 loss_rec: 0.6803 acc_train: 0.6919 loss_val: 0.6804 acc_val: 0.6917 time: 0.0010s\n",
      "Epoch: 00135 loss_train: 0.6801 loss_rec: 0.6801 acc_train: 0.6939 loss_val: 0.6801 acc_val: 0.6958 time: 0.0010s\n",
      "Epoch: 00136 loss_train: 0.6799 loss_rec: 0.6799 acc_train: 0.6939 loss_val: 0.6799 acc_val: 0.6938 time: 0.0010s\n",
      "Epoch: 00137 loss_train: 0.6797 loss_rec: 0.6797 acc_train: 0.6887 loss_val: 0.6797 acc_val: 0.6875 time: 0.0010s\n",
      "Epoch: 00138 loss_train: 0.6794 loss_rec: 0.6794 acc_train: 0.6922 loss_val: 0.6795 acc_val: 0.6929 time: 0.0010s\n",
      "Epoch: 00139 loss_train: 0.6792 loss_rec: 0.6792 acc_train: 0.6937 loss_val: 0.6792 acc_val: 0.6958 time: 0.0010s\n",
      "Epoch: 00140 loss_train: 0.6790 loss_rec: 0.6790 acc_train: 0.6955 loss_val: 0.6790 acc_val: 0.6971 time: 0.0010s\n",
      "Epoch: 00141 loss_train: 0.6788 loss_rec: 0.6788 acc_train: 0.6996 loss_val: 0.6788 acc_val: 0.6996 time: 0.0010s\n",
      "Test set results: loss= 0.6786 accuracy= 0.7089\n",
      "Epoch: 00142 loss_train: 0.6785 loss_rec: 0.6785 acc_train: 0.7017 loss_val: 0.6786 acc_val: 0.7021 time: 0.0010s\n",
      "Epoch: 00143 loss_train: 0.6783 loss_rec: 0.6783 acc_train: 0.7037 loss_val: 0.6783 acc_val: 0.7033 time: 0.0010s\n",
      "Epoch: 00144 loss_train: 0.6781 loss_rec: 0.6781 acc_train: 0.7082 loss_val: 0.6781 acc_val: 0.7067 time: 0.0010s\n",
      "Epoch: 00145 loss_train: 0.6778 loss_rec: 0.6778 acc_train: 0.7093 loss_val: 0.6779 acc_val: 0.7092 time: 0.0010s\n",
      "Epoch: 00146 loss_train: 0.6776 loss_rec: 0.6776 acc_train: 0.7075 loss_val: 0.6776 acc_val: 0.7083 time: 0.0010s\n",
      "Epoch: 00147 loss_train: 0.6773 loss_rec: 0.6773 acc_train: 0.7087 loss_val: 0.6774 acc_val: 0.7100 time: 0.0010s\n",
      "Epoch: 00148 loss_train: 0.6771 loss_rec: 0.6771 acc_train: 0.7068 loss_val: 0.6771 acc_val: 0.7083 time: 0.0010s\n",
      "Epoch: 00149 loss_train: 0.6769 loss_rec: 0.6769 acc_train: 0.7072 loss_val: 0.6769 acc_val: 0.7092 time: 0.0010s\n",
      "Epoch: 00150 loss_train: 0.6766 loss_rec: 0.6766 acc_train: 0.7067 loss_val: 0.6767 acc_val: 0.7087 time: 0.0010s\n",
      "Epoch: 00151 loss_train: 0.6764 loss_rec: 0.6764 acc_train: 0.7081 loss_val: 0.6764 acc_val: 0.7113 time: 0.0010s\n",
      "Test set results: loss= 0.6755 accuracy= 0.7196\n",
      "Epoch: 00152 loss_train: 0.6761 loss_rec: 0.6761 acc_train: 0.7065 loss_val: 0.6762 acc_val: 0.7087 time: 0.0010s\n",
      "Epoch: 00153 loss_train: 0.6759 loss_rec: 0.6759 acc_train: 0.7073 loss_val: 0.6759 acc_val: 0.7087 time: 0.0010s\n",
      "Epoch: 00154 loss_train: 0.6756 loss_rec: 0.6756 acc_train: 0.7077 loss_val: 0.6757 acc_val: 0.7096 time: 0.0010s\n",
      "Epoch: 00155 loss_train: 0.6754 loss_rec: 0.6754 acc_train: 0.7084 loss_val: 0.6754 acc_val: 0.7096 time: 0.0010s\n",
      "Epoch: 00156 loss_train: 0.6751 loss_rec: 0.6751 acc_train: 0.7087 loss_val: 0.6752 acc_val: 0.7096 time: 0.0010s\n",
      "Epoch: 00157 loss_train: 0.6749 loss_rec: 0.6749 acc_train: 0.7087 loss_val: 0.6750 acc_val: 0.7100 time: 0.0010s\n",
      "Epoch: 00158 loss_train: 0.6746 loss_rec: 0.6746 acc_train: 0.7127 loss_val: 0.6747 acc_val: 0.7154 time: 0.0010s\n",
      "Epoch: 00159 loss_train: 0.6744 loss_rec: 0.6744 acc_train: 0.7132 loss_val: 0.6745 acc_val: 0.7158 time: 0.0010s\n",
      "Epoch: 00160 loss_train: 0.6741 loss_rec: 0.6741 acc_train: 0.7142 loss_val: 0.6742 acc_val: 0.7163 time: 0.0010s\n",
      "Epoch: 00161 loss_train: 0.6739 loss_rec: 0.6739 acc_train: 0.7144 loss_val: 0.6739 acc_val: 0.7167 time: 0.0010s\n",
      "Test set results: loss= 0.6724 accuracy= 0.7298\n",
      "Epoch: 00162 loss_train: 0.6736 loss_rec: 0.6736 acc_train: 0.7156 loss_val: 0.6737 acc_val: 0.7175 time: 0.0010s\n",
      "Epoch: 00163 loss_train: 0.6734 loss_rec: 0.6734 acc_train: 0.7157 loss_val: 0.6734 acc_val: 0.7179 time: 0.0020s\n",
      "Epoch: 00164 loss_train: 0.6731 loss_rec: 0.6731 acc_train: 0.7163 loss_val: 0.6732 acc_val: 0.7179 time: 0.0010s\n",
      "Epoch: 00165 loss_train: 0.6728 loss_rec: 0.6728 acc_train: 0.7168 loss_val: 0.6729 acc_val: 0.7188 time: 0.0010s\n",
      "Epoch: 00166 loss_train: 0.6726 loss_rec: 0.6726 acc_train: 0.7170 loss_val: 0.6727 acc_val: 0.7183 time: 0.0010s\n",
      "Epoch: 00167 loss_train: 0.6723 loss_rec: 0.6723 acc_train: 0.7171 loss_val: 0.6724 acc_val: 0.7188 time: 0.0010s\n",
      "Epoch: 00168 loss_train: 0.6720 loss_rec: 0.6720 acc_train: 0.7172 loss_val: 0.6721 acc_val: 0.7192 time: 0.0010s\n",
      "Epoch: 00169 loss_train: 0.6718 loss_rec: 0.6718 acc_train: 0.7212 loss_val: 0.6719 acc_val: 0.7217 time: 0.0010s\n",
      "Epoch: 00170 loss_train: 0.6715 loss_rec: 0.6715 acc_train: 0.7214 loss_val: 0.6716 acc_val: 0.7221 time: 0.0010s\n",
      "Epoch: 00171 loss_train: 0.6712 loss_rec: 0.6712 acc_train: 0.7391 loss_val: 0.6713 acc_val: 0.7421 time: 0.0010s\n",
      "Test set results: loss= 0.6693 accuracy= 0.7541\n",
      "Epoch: 00172 loss_train: 0.6710 loss_rec: 0.6710 acc_train: 0.7394 loss_val: 0.6711 acc_val: 0.7421 time: 0.0010s\n",
      "Epoch: 00173 loss_train: 0.6707 loss_rec: 0.6707 acc_train: 0.7397 loss_val: 0.6708 acc_val: 0.7433 time: 0.0010s\n",
      "Epoch: 00174 loss_train: 0.6704 loss_rec: 0.6704 acc_train: 0.7398 loss_val: 0.6705 acc_val: 0.7433 time: 0.0010s\n",
      "Epoch: 00175 loss_train: 0.6702 loss_rec: 0.6702 acc_train: 0.7397 loss_val: 0.6703 acc_val: 0.7433 time: 0.0010s\n",
      "Epoch: 00176 loss_train: 0.6699 loss_rec: 0.6699 acc_train: 0.7398 loss_val: 0.6700 acc_val: 0.7438 time: 0.0010s\n",
      "Epoch: 00177 loss_train: 0.6696 loss_rec: 0.6696 acc_train: 0.7399 loss_val: 0.6697 acc_val: 0.7438 time: 0.0010s\n",
      "Epoch: 00178 loss_train: 0.6693 loss_rec: 0.6693 acc_train: 0.7399 loss_val: 0.6695 acc_val: 0.7438 time: 0.0010s\n",
      "Epoch: 00179 loss_train: 0.6691 loss_rec: 0.6691 acc_train: 0.7401 loss_val: 0.6692 acc_val: 0.7438 time: 0.0010s\n",
      "Epoch: 00180 loss_train: 0.6688 loss_rec: 0.6688 acc_train: 0.7432 loss_val: 0.6689 acc_val: 0.7479 time: 0.0010s\n",
      "Epoch: 00181 loss_train: 0.6685 loss_rec: 0.6685 acc_train: 0.7433 loss_val: 0.6686 acc_val: 0.7479 time: 0.0010s\n",
      "Test set results: loss= 0.6662 accuracy= 0.7593\n",
      "Epoch: 00182 loss_train: 0.6682 loss_rec: 0.6682 acc_train: 0.7433 loss_val: 0.6683 acc_val: 0.7479 time: 0.0010s\n",
      "Epoch: 00183 loss_train: 0.6679 loss_rec: 0.6679 acc_train: 0.7437 loss_val: 0.6681 acc_val: 0.7479 time: 0.0005s\n",
      "Epoch: 00184 loss_train: 0.6676 loss_rec: 0.6676 acc_train: 0.7441 loss_val: 0.6678 acc_val: 0.7483 time: 0.0005s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00185 loss_train: 0.6673 loss_rec: 0.6673 acc_train: 0.7442 loss_val: 0.6675 acc_val: 0.7492 time: 0.0010s\n",
      "Epoch: 00186 loss_train: 0.6671 loss_rec: 0.6671 acc_train: 0.7443 loss_val: 0.6672 acc_val: 0.7492 time: 0.0010s\n",
      "Epoch: 00187 loss_train: 0.6668 loss_rec: 0.6668 acc_train: 0.7452 loss_val: 0.6669 acc_val: 0.7500 time: 0.0010s\n",
      "Epoch: 00188 loss_train: 0.6665 loss_rec: 0.6665 acc_train: 0.7454 loss_val: 0.6666 acc_val: 0.7500 time: 0.0010s\n",
      "Epoch: 00189 loss_train: 0.6662 loss_rec: 0.6662 acc_train: 0.7453 loss_val: 0.6664 acc_val: 0.7500 time: 0.0010s\n",
      "Epoch: 00190 loss_train: 0.6659 loss_rec: 0.6659 acc_train: 0.7452 loss_val: 0.6661 acc_val: 0.7500 time: 0.0010s\n",
      "Epoch: 00191 loss_train: 0.6656 loss_rec: 0.6656 acc_train: 0.7452 loss_val: 0.6658 acc_val: 0.7500 time: 0.0015s\n",
      "Test set results: loss= 0.6631 accuracy= 0.7613\n",
      "Epoch: 00192 loss_train: 0.6653 loss_rec: 0.6653 acc_train: 0.7452 loss_val: 0.6655 acc_val: 0.7504 time: 0.0010s\n",
      "Epoch: 00193 loss_train: 0.6650 loss_rec: 0.6650 acc_train: 0.7452 loss_val: 0.6652 acc_val: 0.7504 time: 0.0020s\n",
      "Epoch: 00194 loss_train: 0.6647 loss_rec: 0.6647 acc_train: 0.7453 loss_val: 0.6649 acc_val: 0.7504 time: 0.0020s\n",
      "Epoch: 00195 loss_train: 0.6644 loss_rec: 0.6644 acc_train: 0.7478 loss_val: 0.6646 acc_val: 0.7542 time: 0.0015s\n",
      "Epoch: 00196 loss_train: 0.6641 loss_rec: 0.6641 acc_train: 0.7481 loss_val: 0.6643 acc_val: 0.7542 time: 0.0010s\n",
      "Epoch: 00197 loss_train: 0.6638 loss_rec: 0.6638 acc_train: 0.7482 loss_val: 0.6640 acc_val: 0.7546 time: 0.0010s\n",
      "Epoch: 00198 loss_train: 0.6635 loss_rec: 0.6635 acc_train: 0.7481 loss_val: 0.6637 acc_val: 0.7546 time: 0.0010s\n",
      "Epoch: 00199 loss_train: 0.6632 loss_rec: 0.6632 acc_train: 0.7482 loss_val: 0.6634 acc_val: 0.7550 time: 0.0010s\n",
      "Epoch: 00200 loss_train: 0.6629 loss_rec: 0.6629 acc_train: 0.7482 loss_val: 0.6631 acc_val: 0.7546 time: 0.0010s\n",
      "Epoch: 00201 loss_train: 0.6626 loss_rec: 0.6626 acc_train: 0.7481 loss_val: 0.6628 acc_val: 0.7546 time: 0.0010s\n",
      "Test set results: loss= 0.6599 accuracy= 0.7653\n",
      "Epoch: 00202 loss_train: 0.6623 loss_rec: 0.6623 acc_train: 0.7482 loss_val: 0.6625 acc_val: 0.7546 time: 0.0010s\n",
      "Epoch: 00203 loss_train: 0.6620 loss_rec: 0.6620 acc_train: 0.7482 loss_val: 0.6622 acc_val: 0.7546 time: 0.0010s\n",
      "Epoch: 00204 loss_train: 0.6617 loss_rec: 0.6617 acc_train: 0.7478 loss_val: 0.6619 acc_val: 0.7538 time: 0.0010s\n",
      "Epoch: 00205 loss_train: 0.6614 loss_rec: 0.6614 acc_train: 0.7478 loss_val: 0.6616 acc_val: 0.7538 time: 0.0010s\n",
      "Epoch: 00206 loss_train: 0.6611 loss_rec: 0.6611 acc_train: 0.7483 loss_val: 0.6613 acc_val: 0.7542 time: 0.0010s\n",
      "Epoch: 00207 loss_train: 0.6608 loss_rec: 0.6608 acc_train: 0.7482 loss_val: 0.6610 acc_val: 0.7542 time: 0.0010s\n",
      "Epoch: 00208 loss_train: 0.6604 loss_rec: 0.6604 acc_train: 0.7483 loss_val: 0.6607 acc_val: 0.7542 time: 0.0010s\n",
      "Epoch: 00209 loss_train: 0.6601 loss_rec: 0.6601 acc_train: 0.7483 loss_val: 0.6604 acc_val: 0.7542 time: 0.0010s\n",
      "Epoch: 00210 loss_train: 0.6598 loss_rec: 0.6598 acc_train: 0.7483 loss_val: 0.6601 acc_val: 0.7542 time: 0.0010s\n",
      "Epoch: 00211 loss_train: 0.6595 loss_rec: 0.6595 acc_train: 0.7483 loss_val: 0.6597 acc_val: 0.7542 time: 0.0010s\n",
      "Test set results: loss= 0.6567 accuracy= 0.7647\n",
      "Epoch: 00212 loss_train: 0.6592 loss_rec: 0.6592 acc_train: 0.7480 loss_val: 0.6594 acc_val: 0.7538 time: 0.0010s\n",
      "Epoch: 00213 loss_train: 0.6589 loss_rec: 0.6589 acc_train: 0.7480 loss_val: 0.6591 acc_val: 0.7542 time: 0.0010s\n",
      "Epoch: 00214 loss_train: 0.6585 loss_rec: 0.6585 acc_train: 0.7479 loss_val: 0.6588 acc_val: 0.7542 time: 0.0010s\n",
      "Epoch: 00215 loss_train: 0.6582 loss_rec: 0.6582 acc_train: 0.7512 loss_val: 0.6585 acc_val: 0.7563 time: 0.0010s\n",
      "Epoch: 00216 loss_train: 0.6579 loss_rec: 0.6579 acc_train: 0.7539 loss_val: 0.6582 acc_val: 0.7592 time: 0.0010s\n",
      "Epoch: 00217 loss_train: 0.6576 loss_rec: 0.6576 acc_train: 0.7538 loss_val: 0.6578 acc_val: 0.7592 time: 0.0010s\n",
      "Epoch: 00218 loss_train: 0.6573 loss_rec: 0.6573 acc_train: 0.7536 loss_val: 0.6575 acc_val: 0.7583 time: 0.0005s\n",
      "Epoch: 00219 loss_train: 0.6569 loss_rec: 0.6569 acc_train: 0.7537 loss_val: 0.6572 acc_val: 0.7579 time: 0.0005s\n",
      "Epoch: 00220 loss_train: 0.6566 loss_rec: 0.6566 acc_train: 0.7536 loss_val: 0.6569 acc_val: 0.7579 time: 0.0010s\n",
      "Epoch: 00221 loss_train: 0.6563 loss_rec: 0.6563 acc_train: 0.7537 loss_val: 0.6566 acc_val: 0.7579 time: 0.0010s\n",
      "Test set results: loss= 0.6534 accuracy= 0.7682\n",
      "Epoch: 00222 loss_train: 0.6559 loss_rec: 0.6559 acc_train: 0.7536 loss_val: 0.6562 acc_val: 0.7579 time: 0.0010s\n",
      "Epoch: 00223 loss_train: 0.6556 loss_rec: 0.6556 acc_train: 0.7530 loss_val: 0.6559 acc_val: 0.7579 time: 0.0010s\n",
      "Epoch: 00224 loss_train: 0.6553 loss_rec: 0.6553 acc_train: 0.7529 loss_val: 0.6556 acc_val: 0.7579 time: 0.0010s\n",
      "Epoch: 00225 loss_train: 0.6550 loss_rec: 0.6550 acc_train: 0.7528 loss_val: 0.6553 acc_val: 0.7571 time: 0.0010s\n",
      "Epoch: 00226 loss_train: 0.6546 loss_rec: 0.6546 acc_train: 0.7529 loss_val: 0.6549 acc_val: 0.7571 time: 0.0010s\n",
      "Epoch: 00227 loss_train: 0.6543 loss_rec: 0.6543 acc_train: 0.7532 loss_val: 0.6546 acc_val: 0.7579 time: 0.0010s\n",
      "Epoch: 00228 loss_train: 0.6540 loss_rec: 0.6540 acc_train: 0.7558 loss_val: 0.6543 acc_val: 0.7583 time: 0.0010s\n",
      "Epoch: 00229 loss_train: 0.6536 loss_rec: 0.6536 acc_train: 0.7557 loss_val: 0.6540 acc_val: 0.7583 time: 0.0010s\n",
      "Epoch: 00230 loss_train: 0.6533 loss_rec: 0.6533 acc_train: 0.7550 loss_val: 0.6536 acc_val: 0.7575 time: 0.0010s\n",
      "Epoch: 00231 loss_train: 0.6530 loss_rec: 0.6530 acc_train: 0.7554 loss_val: 0.6533 acc_val: 0.7583 time: 0.0010s\n",
      "Test set results: loss= 0.6502 accuracy= 0.7678\n",
      "Epoch: 00232 loss_train: 0.6526 loss_rec: 0.6526 acc_train: 0.7553 loss_val: 0.6530 acc_val: 0.7583 time: 0.0010s\n",
      "Epoch: 00233 loss_train: 0.6523 loss_rec: 0.6523 acc_train: 0.7553 loss_val: 0.6526 acc_val: 0.7583 time: 0.0010s\n",
      "Epoch: 00234 loss_train: 0.6520 loss_rec: 0.6520 acc_train: 0.7552 loss_val: 0.6523 acc_val: 0.7583 time: 0.0010s\n",
      "Epoch: 00235 loss_train: 0.6516 loss_rec: 0.6516 acc_train: 0.7552 loss_val: 0.6520 acc_val: 0.7579 time: 0.0010s\n",
      "Epoch: 00236 loss_train: 0.6513 loss_rec: 0.6513 acc_train: 0.7551 loss_val: 0.6517 acc_val: 0.7579 time: 0.0010s\n",
      "Epoch: 00237 loss_train: 0.6510 loss_rec: 0.6510 acc_train: 0.7551 loss_val: 0.6513 acc_val: 0.7579 time: 0.0010s\n",
      "Epoch: 00238 loss_train: 0.6506 loss_rec: 0.6506 acc_train: 0.7542 loss_val: 0.6510 acc_val: 0.7571 time: 0.0010s\n",
      "Epoch: 00239 loss_train: 0.6503 loss_rec: 0.6503 acc_train: 0.7542 loss_val: 0.6507 acc_val: 0.7571 time: 0.0010s\n",
      "Epoch: 00240 loss_train: 0.6500 loss_rec: 0.6500 acc_train: 0.7542 loss_val: 0.6503 acc_val: 0.7571 time: 0.0010s\n",
      "Epoch: 00241 loss_train: 0.6496 loss_rec: 0.6496 acc_train: 0.7542 loss_val: 0.6500 acc_val: 0.7571 time: 0.0010s\n",
      "Test set results: loss= 0.6468 accuracy= 0.7663\n",
      "Epoch: 00242 loss_train: 0.6493 loss_rec: 0.6493 acc_train: 0.7542 loss_val: 0.6497 acc_val: 0.7571 time: 0.0015s\n",
      "Epoch: 00243 loss_train: 0.6489 loss_rec: 0.6489 acc_train: 0.7542 loss_val: 0.6493 acc_val: 0.7571 time: 0.0020s\n",
      "Epoch: 00244 loss_train: 0.6486 loss_rec: 0.6486 acc_train: 0.7542 loss_val: 0.6490 acc_val: 0.7571 time: 0.0020s\n",
      "Epoch: 00245 loss_train: 0.6483 loss_rec: 0.6483 acc_train: 0.7537 loss_val: 0.6486 acc_val: 0.7567 time: 0.0015s\n",
      "Epoch: 00246 loss_train: 0.6479 loss_rec: 0.6479 acc_train: 0.7538 loss_val: 0.6483 acc_val: 0.7567 time: 0.0020s\n",
      "Epoch: 00247 loss_train: 0.6476 loss_rec: 0.6476 acc_train: 0.7538 loss_val: 0.6480 acc_val: 0.7567 time: 0.0020s\n",
      "Epoch: 00248 loss_train: 0.6472 loss_rec: 0.6472 acc_train: 0.7537 loss_val: 0.6476 acc_val: 0.7567 time: 0.0020s\n",
      "Epoch: 00249 loss_train: 0.6469 loss_rec: 0.6469 acc_train: 0.7537 loss_val: 0.6473 acc_val: 0.7567 time: 0.0020s\n",
      "Epoch: 00250 loss_train: 0.6465 loss_rec: 0.6465 acc_train: 0.7537 loss_val: 0.6469 acc_val: 0.7567 time: 0.0020s\n",
      "Epoch: 00251 loss_train: 0.6462 loss_rec: 0.6462 acc_train: 0.7537 loss_val: 0.6466 acc_val: 0.7563 time: 0.0015s\n",
      "Test set results: loss= 0.6435 accuracy= 0.7654\n",
      "Epoch: 00252 loss_train: 0.6458 loss_rec: 0.6458 acc_train: 0.7536 loss_val: 0.6463 acc_val: 0.7563 time: 0.0015s\n",
      "Epoch: 00253 loss_train: 0.6455 loss_rec: 0.6455 acc_train: 0.7533 loss_val: 0.6459 acc_val: 0.7554 time: 0.0020s\n",
      "Epoch: 00254 loss_train: 0.6452 loss_rec: 0.6452 acc_train: 0.7531 loss_val: 0.6456 acc_val: 0.7554 time: 0.0015s\n",
      "Epoch: 00255 loss_train: 0.6448 loss_rec: 0.6448 acc_train: 0.7532 loss_val: 0.6452 acc_val: 0.7550 time: 0.0015s\n",
      "Epoch: 00256 loss_train: 0.6445 loss_rec: 0.6445 acc_train: 0.7532 loss_val: 0.6449 acc_val: 0.7550 time: 0.0020s\n",
      "Epoch: 00257 loss_train: 0.6441 loss_rec: 0.6441 acc_train: 0.7532 loss_val: 0.6445 acc_val: 0.7550 time: 0.0020s\n",
      "Epoch: 00258 loss_train: 0.6438 loss_rec: 0.6438 acc_train: 0.7532 loss_val: 0.6442 acc_val: 0.7550 time: 0.0015s\n",
      "Epoch: 00259 loss_train: 0.6434 loss_rec: 0.6434 acc_train: 0.7532 loss_val: 0.6439 acc_val: 0.7550 time: 0.0015s\n",
      "Epoch: 00260 loss_train: 0.6431 loss_rec: 0.6431 acc_train: 0.7531 loss_val: 0.6435 acc_val: 0.7550 time: 0.0020s\n",
      "Epoch: 00261 loss_train: 0.6427 loss_rec: 0.6427 acc_train: 0.7530 loss_val: 0.6432 acc_val: 0.7550 time: 0.0020s\n",
      "Test set results: loss= 0.6401 accuracy= 0.7639\n",
      "Epoch: 00262 loss_train: 0.6424 loss_rec: 0.6424 acc_train: 0.7530 loss_val: 0.6428 acc_val: 0.7550 time: 0.0020s\n",
      "Epoch: 00263 loss_train: 0.6420 loss_rec: 0.6420 acc_train: 0.7530 loss_val: 0.6425 acc_val: 0.7550 time: 0.0015s\n",
      "Epoch: 00264 loss_train: 0.6416 loss_rec: 0.6416 acc_train: 0.7530 loss_val: 0.6421 acc_val: 0.7550 time: 0.0020s\n",
      "Epoch: 00265 loss_train: 0.6413 loss_rec: 0.6413 acc_train: 0.7531 loss_val: 0.6418 acc_val: 0.7550 time: 0.0020s\n",
      "Epoch: 00266 loss_train: 0.6409 loss_rec: 0.6409 acc_train: 0.7531 loss_val: 0.6414 acc_val: 0.7550 time: 0.0020s\n",
      "Epoch: 00267 loss_train: 0.6406 loss_rec: 0.6406 acc_train: 0.7531 loss_val: 0.6411 acc_val: 0.7546 time: 0.0020s\n",
      "Epoch: 00268 loss_train: 0.6402 loss_rec: 0.6402 acc_train: 0.7531 loss_val: 0.6407 acc_val: 0.7546 time: 0.0015s\n",
      "Epoch: 00269 loss_train: 0.6399 loss_rec: 0.6399 acc_train: 0.7529 loss_val: 0.6404 acc_val: 0.7546 time: 0.0015s\n",
      "Epoch: 00270 loss_train: 0.6395 loss_rec: 0.6395 acc_train: 0.7555 loss_val: 0.6400 acc_val: 0.7579 time: 0.0020s\n",
      "Epoch: 00271 loss_train: 0.6392 loss_rec: 0.6392 acc_train: 0.7554 loss_val: 0.6397 acc_val: 0.7579 time: 0.0020s\n",
      "Test set results: loss= 0.6366 accuracy= 0.7690\n",
      "Epoch: 00272 loss_train: 0.6388 loss_rec: 0.6388 acc_train: 0.7582 loss_val: 0.6393 acc_val: 0.7600 time: 0.0020s\n",
      "Epoch: 00273 loss_train: 0.6384 loss_rec: 0.6384 acc_train: 0.7582 loss_val: 0.6390 acc_val: 0.7600 time: 0.0015s\n",
      "Epoch: 00274 loss_train: 0.6381 loss_rec: 0.6381 acc_train: 0.7580 loss_val: 0.6386 acc_val: 0.7600 time: 0.0015s\n",
      "Epoch: 00275 loss_train: 0.6377 loss_rec: 0.6377 acc_train: 0.7581 loss_val: 0.6383 acc_val: 0.7600 time: 0.0020s\n",
      "Epoch: 00276 loss_train: 0.6374 loss_rec: 0.6374 acc_train: 0.7580 loss_val: 0.6379 acc_val: 0.7600 time: 0.0020s\n",
      "Epoch: 00277 loss_train: 0.6370 loss_rec: 0.6370 acc_train: 0.7580 loss_val: 0.6375 acc_val: 0.7596 time: 0.0020s\n",
      "Epoch: 00278 loss_train: 0.6366 loss_rec: 0.6366 acc_train: 0.7580 loss_val: 0.6372 acc_val: 0.7592 time: 0.0020s\n",
      "Epoch: 00279 loss_train: 0.6363 loss_rec: 0.6363 acc_train: 0.7580 loss_val: 0.6368 acc_val: 0.7592 time: 0.0015s\n",
      "Epoch: 00280 loss_train: 0.6359 loss_rec: 0.6359 acc_train: 0.7580 loss_val: 0.6365 acc_val: 0.7592 time: 0.0020s\n",
      "Epoch: 00281 loss_train: 0.6355 loss_rec: 0.6355 acc_train: 0.7579 loss_val: 0.6361 acc_val: 0.7592 time: 0.0025s\n",
      "Test set results: loss= 0.6332 accuracy= 0.7683\n",
      "Epoch: 00282 loss_train: 0.6352 loss_rec: 0.6352 acc_train: 0.7579 loss_val: 0.6358 acc_val: 0.7592 time: 0.0020s\n",
      "Epoch: 00283 loss_train: 0.6349 loss_rec: 0.6349 acc_train: 0.7579 loss_val: 0.6354 acc_val: 0.7596 time: 0.0020s\n",
      "Epoch: 00284 loss_train: 0.6345 loss_rec: 0.6345 acc_train: 0.7578 loss_val: 0.6351 acc_val: 0.7596 time: 0.0020s\n",
      "Epoch: 00285 loss_train: 0.6341 loss_rec: 0.6341 acc_train: 0.7577 loss_val: 0.6347 acc_val: 0.7596 time: 0.0020s\n",
      "Epoch: 00286 loss_train: 0.6338 loss_rec: 0.6338 acc_train: 0.7577 loss_val: 0.6343 acc_val: 0.7596 time: 0.0015s\n",
      "Epoch: 00287 loss_train: 0.6334 loss_rec: 0.6334 acc_train: 0.7584 loss_val: 0.6340 acc_val: 0.7604 time: 0.0020s\n",
      "Epoch: 00288 loss_train: 0.6330 loss_rec: 0.6330 acc_train: 0.7584 loss_val: 0.6336 acc_val: 0.7604 time: 0.0020s\n",
      "Epoch: 00289 loss_train: 0.6327 loss_rec: 0.6327 acc_train: 0.7582 loss_val: 0.6333 acc_val: 0.7604 time: 0.0020s\n",
      "Epoch: 00290 loss_train: 0.6323 loss_rec: 0.6323 acc_train: 0.7582 loss_val: 0.6329 acc_val: 0.7604 time: 0.0015s\n",
      "Epoch: 00291 loss_train: 0.6319 loss_rec: 0.6319 acc_train: 0.7581 loss_val: 0.6325 acc_val: 0.7604 time: 0.0015s\n",
      "Test set results: loss= 0.6298 accuracy= 0.7685\n",
      "Epoch: 00292 loss_train: 0.6316 loss_rec: 0.6316 acc_train: 0.7579 loss_val: 0.6322 acc_val: 0.7604 time: 0.0015s\n",
      "Epoch: 00293 loss_train: 0.6312 loss_rec: 0.6312 acc_train: 0.7605 loss_val: 0.6318 acc_val: 0.7638 time: 0.0020s\n",
      "Epoch: 00294 loss_train: 0.6308 loss_rec: 0.6308 acc_train: 0.7605 loss_val: 0.6314 acc_val: 0.7638 time: 0.0020s\n",
      "Epoch: 00295 loss_train: 0.6305 loss_rec: 0.6305 acc_train: 0.7606 loss_val: 0.6311 acc_val: 0.7638 time: 0.0020s\n",
      "Epoch: 00296 loss_train: 0.6301 loss_rec: 0.6301 acc_train: 0.7606 loss_val: 0.6307 acc_val: 0.7638 time: 0.0015s\n",
      "Epoch: 00297 loss_train: 0.6297 loss_rec: 0.6297 acc_train: 0.7605 loss_val: 0.6304 acc_val: 0.7638 time: 0.0015s\n",
      "Epoch: 00298 loss_train: 0.6294 loss_rec: 0.6294 acc_train: 0.7605 loss_val: 0.6300 acc_val: 0.7638 time: 0.0020s\n",
      "Epoch: 00299 loss_train: 0.6290 loss_rec: 0.6290 acc_train: 0.7607 loss_val: 0.6296 acc_val: 0.7638 time: 0.0020s\n",
      "Epoch: 00300 loss_train: 0.6286 loss_rec: 0.6286 acc_train: 0.7607 loss_val: 0.6293 acc_val: 0.7638 time: 0.0025s\n",
      "Epoch: 00301 loss_train: 0.6283 loss_rec: 0.6283 acc_train: 0.7695 loss_val: 0.6289 acc_val: 0.7725 time: 0.0015s\n",
      "Test set results: loss= 0.6263 accuracy= 0.7776\n",
      "Epoch: 00302 loss_train: 0.6279 loss_rec: 0.6279 acc_train: 0.7694 loss_val: 0.6286 acc_val: 0.7725 time: 0.0020s\n",
      "Epoch: 00303 loss_train: 0.6275 loss_rec: 0.6275 acc_train: 0.7691 loss_val: 0.6282 acc_val: 0.7725 time: 0.0020s\n",
      "Epoch: 00304 loss_train: 0.6272 loss_rec: 0.6272 acc_train: 0.7726 loss_val: 0.6278 acc_val: 0.7750 time: 0.0020s\n",
      "Epoch: 00305 loss_train: 0.6268 loss_rec: 0.6268 acc_train: 0.7725 loss_val: 0.6275 acc_val: 0.7750 time: 0.0015s\n",
      "Epoch: 00306 loss_train: 0.6264 loss_rec: 0.6264 acc_train: 0.7723 loss_val: 0.6271 acc_val: 0.7750 time: 0.0020s\n",
      "Epoch: 00307 loss_train: 0.6260 loss_rec: 0.6260 acc_train: 0.7722 loss_val: 0.6267 acc_val: 0.7742 time: 0.0015s\n",
      "Epoch: 00308 loss_train: 0.6257 loss_rec: 0.6257 acc_train: 0.7722 loss_val: 0.6264 acc_val: 0.7742 time: 0.0020s\n",
      "Epoch: 00309 loss_train: 0.6253 loss_rec: 0.6253 acc_train: 0.7721 loss_val: 0.6260 acc_val: 0.7742 time: 0.0015s\n",
      "Epoch: 00310 loss_train: 0.6249 loss_rec: 0.6249 acc_train: 0.7720 loss_val: 0.6256 acc_val: 0.7742 time: 0.0020s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00311 loss_train: 0.6246 loss_rec: 0.6246 acc_train: 0.7720 loss_val: 0.6253 acc_val: 0.7742 time: 0.0020s\n",
      "Test set results: loss= 0.6228 accuracy= 0.7782\n",
      "Epoch: 00312 loss_train: 0.6242 loss_rec: 0.6242 acc_train: 0.7720 loss_val: 0.6249 acc_val: 0.7742 time: 0.0015s\n",
      "Epoch: 00313 loss_train: 0.6238 loss_rec: 0.6238 acc_train: 0.7719 loss_val: 0.6246 acc_val: 0.7742 time: 0.0020s\n",
      "Epoch: 00314 loss_train: 0.6234 loss_rec: 0.6234 acc_train: 0.7717 loss_val: 0.6242 acc_val: 0.7742 time: 0.0015s\n",
      "Epoch: 00315 loss_train: 0.6231 loss_rec: 0.6231 acc_train: 0.7717 loss_val: 0.6238 acc_val: 0.7742 time: 0.0020s\n",
      "Epoch: 00316 loss_train: 0.6227 loss_rec: 0.6227 acc_train: 0.7717 loss_val: 0.6234 acc_val: 0.7742 time: 0.0015s\n",
      "Epoch: 00317 loss_train: 0.6223 loss_rec: 0.6223 acc_train: 0.7716 loss_val: 0.6231 acc_val: 0.7742 time: 0.0020s\n",
      "Epoch: 00318 loss_train: 0.6220 loss_rec: 0.6220 acc_train: 0.7716 loss_val: 0.6227 acc_val: 0.7742 time: 0.0015s\n",
      "Epoch: 00319 loss_train: 0.6216 loss_rec: 0.6216 acc_train: 0.7716 loss_val: 0.6223 acc_val: 0.7742 time: 0.0020s\n",
      "Epoch: 00320 loss_train: 0.6212 loss_rec: 0.6212 acc_train: 0.7745 loss_val: 0.6220 acc_val: 0.7771 time: 0.0015s\n",
      "Epoch: 00321 loss_train: 0.6209 loss_rec: 0.6209 acc_train: 0.7743 loss_val: 0.6216 acc_val: 0.7767 time: 0.0020s\n",
      "Test set results: loss= 0.6194 accuracy= 0.7800\n",
      "Epoch: 00322 loss_train: 0.6205 loss_rec: 0.6205 acc_train: 0.7743 loss_val: 0.6212 acc_val: 0.7767 time: 0.0015s\n",
      "Epoch: 00323 loss_train: 0.6201 loss_rec: 0.6201 acc_train: 0.7741 loss_val: 0.6209 acc_val: 0.7762 time: 0.0020s\n",
      "Epoch: 00324 loss_train: 0.6197 loss_rec: 0.6197 acc_train: 0.7738 loss_val: 0.6205 acc_val: 0.7758 time: 0.0015s\n",
      "Epoch: 00325 loss_train: 0.6194 loss_rec: 0.6194 acc_train: 0.7738 loss_val: 0.6201 acc_val: 0.7754 time: 0.0015s\n",
      "Epoch: 00326 loss_train: 0.6190 loss_rec: 0.6190 acc_train: 0.7738 loss_val: 0.6198 acc_val: 0.7746 time: 0.0015s\n",
      "Epoch: 00327 loss_train: 0.6186 loss_rec: 0.6186 acc_train: 0.7740 loss_val: 0.6194 acc_val: 0.7746 time: 0.0015s\n",
      "Epoch: 00328 loss_train: 0.6183 loss_rec: 0.6183 acc_train: 0.7741 loss_val: 0.6190 acc_val: 0.7746 time: 0.0015s\n",
      "Epoch: 00329 loss_train: 0.6179 loss_rec: 0.6179 acc_train: 0.7742 loss_val: 0.6187 acc_val: 0.7750 time: 0.0015s\n",
      "Epoch: 00330 loss_train: 0.6175 loss_rec: 0.6175 acc_train: 0.7742 loss_val: 0.6183 acc_val: 0.7750 time: 0.0020s\n",
      "Epoch: 00331 loss_train: 0.6171 loss_rec: 0.6171 acc_train: 0.7740 loss_val: 0.6179 acc_val: 0.7738 time: 0.0015s\n",
      "Test set results: loss= 0.6159 accuracy= 0.7793\n",
      "Epoch: 00332 loss_train: 0.6168 loss_rec: 0.6168 acc_train: 0.7740 loss_val: 0.6176 acc_val: 0.7738 time: 0.0015s\n",
      "Epoch: 00333 loss_train: 0.6164 loss_rec: 0.6164 acc_train: 0.7740 loss_val: 0.6172 acc_val: 0.7738 time: 0.0020s\n",
      "Epoch: 00334 loss_train: 0.6160 loss_rec: 0.6160 acc_train: 0.7739 loss_val: 0.6168 acc_val: 0.7738 time: 0.0015s\n",
      "Epoch: 00335 loss_train: 0.6156 loss_rec: 0.6156 acc_train: 0.7739 loss_val: 0.6164 acc_val: 0.7738 time: 0.0020s\n",
      "Epoch: 00336 loss_train: 0.6153 loss_rec: 0.6153 acc_train: 0.7771 loss_val: 0.6161 acc_val: 0.7767 time: 0.0015s\n",
      "Epoch: 00337 loss_train: 0.6149 loss_rec: 0.6149 acc_train: 0.7770 loss_val: 0.6157 acc_val: 0.7767 time: 0.0015s\n",
      "Epoch: 00338 loss_train: 0.6145 loss_rec: 0.6145 acc_train: 0.7770 loss_val: 0.6153 acc_val: 0.7767 time: 0.0020s\n",
      "Epoch: 00339 loss_train: 0.6141 loss_rec: 0.6141 acc_train: 0.7770 loss_val: 0.6150 acc_val: 0.7767 time: 0.0015s\n",
      "Epoch: 00340 loss_train: 0.6138 loss_rec: 0.6138 acc_train: 0.7770 loss_val: 0.6146 acc_val: 0.7771 time: 0.0020s\n",
      "Epoch: 00341 loss_train: 0.6134 loss_rec: 0.6134 acc_train: 0.7770 loss_val: 0.6142 acc_val: 0.7771 time: 0.0015s\n",
      "Test set results: loss= 0.6123 accuracy= 0.7835\n",
      "Epoch: 00342 loss_train: 0.6130 loss_rec: 0.6130 acc_train: 0.7797 loss_val: 0.6138 acc_val: 0.7796 time: 0.0020s\n",
      "Epoch: 00343 loss_train: 0.6126 loss_rec: 0.6126 acc_train: 0.7797 loss_val: 0.6135 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00344 loss_train: 0.6123 loss_rec: 0.6123 acc_train: 0.7797 loss_val: 0.6131 acc_val: 0.7796 time: 0.0020s\n",
      "Epoch: 00345 loss_train: 0.6119 loss_rec: 0.6119 acc_train: 0.7797 loss_val: 0.6127 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00346 loss_train: 0.6115 loss_rec: 0.6115 acc_train: 0.7797 loss_val: 0.6124 acc_val: 0.7796 time: 0.0020s\n",
      "Epoch: 00347 loss_train: 0.6111 loss_rec: 0.6111 acc_train: 0.7798 loss_val: 0.6120 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00348 loss_train: 0.6108 loss_rec: 0.6108 acc_train: 0.7798 loss_val: 0.6116 acc_val: 0.7792 time: 0.0015s\n",
      "Epoch: 00349 loss_train: 0.6104 loss_rec: 0.6104 acc_train: 0.7799 loss_val: 0.6113 acc_val: 0.7792 time: 0.0020s\n",
      "Epoch: 00350 loss_train: 0.6100 loss_rec: 0.6100 acc_train: 0.7799 loss_val: 0.6109 acc_val: 0.7792 time: 0.0015s\n",
      "Epoch: 00351 loss_train: 0.6096 loss_rec: 0.6096 acc_train: 0.7799 loss_val: 0.6105 acc_val: 0.7792 time: 0.0020s\n",
      "Test set results: loss= 0.6089 accuracy= 0.7836\n",
      "Epoch: 00352 loss_train: 0.6093 loss_rec: 0.6093 acc_train: 0.7799 loss_val: 0.6101 acc_val: 0.7792 time: 0.0015s\n",
      "Epoch: 00353 loss_train: 0.6089 loss_rec: 0.6089 acc_train: 0.7799 loss_val: 0.6098 acc_val: 0.7788 time: 0.0020s\n",
      "Epoch: 00354 loss_train: 0.6085 loss_rec: 0.6085 acc_train: 0.7799 loss_val: 0.6094 acc_val: 0.7788 time: 0.0015s\n",
      "Epoch: 00355 loss_train: 0.6081 loss_rec: 0.6081 acc_train: 0.7799 loss_val: 0.6090 acc_val: 0.7788 time: 0.0020s\n",
      "Epoch: 00356 loss_train: 0.6078 loss_rec: 0.6078 acc_train: 0.7798 loss_val: 0.6087 acc_val: 0.7788 time: 0.0015s\n",
      "Epoch: 00357 loss_train: 0.6074 loss_rec: 0.6074 acc_train: 0.7798 loss_val: 0.6083 acc_val: 0.7788 time: 0.0020s\n",
      "Epoch: 00358 loss_train: 0.6070 loss_rec: 0.6070 acc_train: 0.7796 loss_val: 0.6079 acc_val: 0.7783 time: 0.0015s\n",
      "Epoch: 00359 loss_train: 0.6066 loss_rec: 0.6066 acc_train: 0.7797 loss_val: 0.6075 acc_val: 0.7783 time: 0.0020s\n",
      "Epoch: 00360 loss_train: 0.6063 loss_rec: 0.6063 acc_train: 0.7798 loss_val: 0.6072 acc_val: 0.7788 time: 0.0015s\n",
      "Epoch: 00361 loss_train: 0.6059 loss_rec: 0.6059 acc_train: 0.7797 loss_val: 0.6068 acc_val: 0.7783 time: 0.0020s\n",
      "Test set results: loss= 0.6054 accuracy= 0.7827\n",
      "Epoch: 00362 loss_train: 0.6055 loss_rec: 0.6055 acc_train: 0.7797 loss_val: 0.6064 acc_val: 0.7783 time: 0.0015s\n",
      "Epoch: 00363 loss_train: 0.6052 loss_rec: 0.6052 acc_train: 0.7799 loss_val: 0.6061 acc_val: 0.7783 time: 0.0020s\n",
      "Epoch: 00364 loss_train: 0.6047 loss_rec: 0.6047 acc_train: 0.7790 loss_val: 0.6057 acc_val: 0.7783 time: 0.0015s\n",
      "Epoch: 00365 loss_train: 0.6044 loss_rec: 0.6044 acc_train: 0.7790 loss_val: 0.6053 acc_val: 0.7783 time: 0.0020s\n",
      "Epoch: 00366 loss_train: 0.6040 loss_rec: 0.6040 acc_train: 0.7791 loss_val: 0.6049 acc_val: 0.7779 time: 0.0015s\n",
      "Epoch: 00367 loss_train: 0.6036 loss_rec: 0.6036 acc_train: 0.7791 loss_val: 0.6046 acc_val: 0.7779 time: 0.0020s\n",
      "Epoch: 00368 loss_train: 0.6032 loss_rec: 0.6032 acc_train: 0.7790 loss_val: 0.6042 acc_val: 0.7779 time: 0.0015s\n",
      "Epoch: 00369 loss_train: 0.6029 loss_rec: 0.6029 acc_train: 0.7790 loss_val: 0.6038 acc_val: 0.7775 time: 0.0020s\n",
      "Epoch: 00370 loss_train: 0.6025 loss_rec: 0.6025 acc_train: 0.7789 loss_val: 0.6035 acc_val: 0.7775 time: 0.0020s\n",
      "Epoch: 00371 loss_train: 0.6021 loss_rec: 0.6021 acc_train: 0.7789 loss_val: 0.6031 acc_val: 0.7775 time: 0.0015s\n",
      "Test set results: loss= 0.6019 accuracy= 0.7819\n",
      "Epoch: 00372 loss_train: 0.6018 loss_rec: 0.6018 acc_train: 0.7788 loss_val: 0.6027 acc_val: 0.7775 time: 0.0020s\n",
      "Epoch: 00373 loss_train: 0.6014 loss_rec: 0.6014 acc_train: 0.7788 loss_val: 0.6024 acc_val: 0.7775 time: 0.0015s\n",
      "Epoch: 00374 loss_train: 0.6010 loss_rec: 0.6010 acc_train: 0.7785 loss_val: 0.6020 acc_val: 0.7775 time: 0.0020s\n",
      "Epoch: 00375 loss_train: 0.6006 loss_rec: 0.6006 acc_train: 0.7784 loss_val: 0.6016 acc_val: 0.7775 time: 0.0015s\n",
      "Epoch: 00376 loss_train: 0.6003 loss_rec: 0.6003 acc_train: 0.7785 loss_val: 0.6013 acc_val: 0.7771 time: 0.0020s\n",
      "Epoch: 00377 loss_train: 0.5999 loss_rec: 0.5999 acc_train: 0.7784 loss_val: 0.6009 acc_val: 0.7771 time: 0.0015s\n",
      "Epoch: 00378 loss_train: 0.5995 loss_rec: 0.5995 acc_train: 0.7784 loss_val: 0.6005 acc_val: 0.7771 time: 0.0020s\n",
      "Epoch: 00379 loss_train: 0.5991 loss_rec: 0.5991 acc_train: 0.7787 loss_val: 0.6001 acc_val: 0.7775 time: 0.0015s\n",
      "Epoch: 00380 loss_train: 0.5988 loss_rec: 0.5988 acc_train: 0.7787 loss_val: 0.5998 acc_val: 0.7775 time: 0.0020s\n",
      "Epoch: 00381 loss_train: 0.5984 loss_rec: 0.5984 acc_train: 0.7787 loss_val: 0.5994 acc_val: 0.7779 time: 0.0015s\n",
      "Test set results: loss= 0.5985 accuracy= 0.7840\n",
      "Epoch: 00382 loss_train: 0.5980 loss_rec: 0.5980 acc_train: 0.7817 loss_val: 0.5990 acc_val: 0.7825 time: 0.0020s\n",
      "Epoch: 00383 loss_train: 0.5976 loss_rec: 0.5976 acc_train: 0.7817 loss_val: 0.5987 acc_val: 0.7825 time: 0.0015s\n",
      "Epoch: 00384 loss_train: 0.5972 loss_rec: 0.5972 acc_train: 0.7816 loss_val: 0.5983 acc_val: 0.7825 time: 0.0015s\n",
      "Epoch: 00385 loss_train: 0.5969 loss_rec: 0.5969 acc_train: 0.7817 loss_val: 0.5979 acc_val: 0.7825 time: 0.0015s\n",
      "Epoch: 00386 loss_train: 0.5965 loss_rec: 0.5965 acc_train: 0.7817 loss_val: 0.5976 acc_val: 0.7825 time: 0.0015s\n",
      "Epoch: 00387 loss_train: 0.5961 loss_rec: 0.5961 acc_train: 0.7814 loss_val: 0.5972 acc_val: 0.7825 time: 0.0020s\n",
      "Epoch: 00388 loss_train: 0.5957 loss_rec: 0.5957 acc_train: 0.7815 loss_val: 0.5968 acc_val: 0.7825 time: 0.0015s\n",
      "Epoch: 00389 loss_train: 0.5954 loss_rec: 0.5954 acc_train: 0.7815 loss_val: 0.5965 acc_val: 0.7825 time: 0.0020s\n",
      "Epoch: 00390 loss_train: 0.5950 loss_rec: 0.5950 acc_train: 0.7816 loss_val: 0.5961 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00391 loss_train: 0.5946 loss_rec: 0.5946 acc_train: 0.7817 loss_val: 0.5957 acc_val: 0.7817 time: 0.0015s\n",
      "Test set results: loss= 0.5950 accuracy= 0.7838\n",
      "Epoch: 00392 loss_train: 0.5942 loss_rec: 0.5942 acc_train: 0.7817 loss_val: 0.5953 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00393 loss_train: 0.5939 loss_rec: 0.5939 acc_train: 0.7817 loss_val: 0.5950 acc_val: 0.7821 time: 0.0020s\n",
      "Epoch: 00394 loss_train: 0.5935 loss_rec: 0.5935 acc_train: 0.7816 loss_val: 0.5946 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00395 loss_train: 0.5931 loss_rec: 0.5931 acc_train: 0.7816 loss_val: 0.5942 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00396 loss_train: 0.5927 loss_rec: 0.5927 acc_train: 0.7815 loss_val: 0.5939 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00397 loss_train: 0.5924 loss_rec: 0.5924 acc_train: 0.7814 loss_val: 0.5935 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00398 loss_train: 0.5920 loss_rec: 0.5920 acc_train: 0.7814 loss_val: 0.5931 acc_val: 0.7821 time: 0.0020s\n",
      "Epoch: 00399 loss_train: 0.5916 loss_rec: 0.5916 acc_train: 0.7814 loss_val: 0.5928 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00400 loss_train: 0.5913 loss_rec: 0.5913 acc_train: 0.7814 loss_val: 0.5924 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00401 loss_train: 0.5909 loss_rec: 0.5909 acc_train: 0.7815 loss_val: 0.5920 acc_val: 0.7817 time: 0.0015s\n",
      "Test set results: loss= 0.5916 accuracy= 0.7836\n",
      "Epoch: 00402 loss_train: 0.5905 loss_rec: 0.5905 acc_train: 0.7816 loss_val: 0.5917 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00403 loss_train: 0.5901 loss_rec: 0.5901 acc_train: 0.7816 loss_val: 0.5913 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00404 loss_train: 0.5898 loss_rec: 0.5898 acc_train: 0.7816 loss_val: 0.5909 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00405 loss_train: 0.5894 loss_rec: 0.5894 acc_train: 0.7817 loss_val: 0.5906 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00406 loss_train: 0.5890 loss_rec: 0.5890 acc_train: 0.7813 loss_val: 0.5903 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00407 loss_train: 0.5886 loss_rec: 0.5886 acc_train: 0.7813 loss_val: 0.5899 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00408 loss_train: 0.5883 loss_rec: 0.5883 acc_train: 0.7813 loss_val: 0.5895 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00409 loss_train: 0.5879 loss_rec: 0.5879 acc_train: 0.7812 loss_val: 0.5891 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00410 loss_train: 0.5875 loss_rec: 0.5875 acc_train: 0.7812 loss_val: 0.5888 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00411 loss_train: 0.5872 loss_rec: 0.5872 acc_train: 0.7812 loss_val: 0.5884 acc_val: 0.7817 time: 0.0015s\n",
      "Test set results: loss= 0.5882 accuracy= 0.7831\n",
      "Epoch: 00412 loss_train: 0.5868 loss_rec: 0.5868 acc_train: 0.7812 loss_val: 0.5880 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00413 loss_train: 0.5864 loss_rec: 0.5864 acc_train: 0.7812 loss_val: 0.5876 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00414 loss_train: 0.5861 loss_rec: 0.5861 acc_train: 0.7811 loss_val: 0.5873 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00415 loss_train: 0.5857 loss_rec: 0.5857 acc_train: 0.7810 loss_val: 0.5869 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00416 loss_train: 0.5853 loss_rec: 0.5853 acc_train: 0.7810 loss_val: 0.5866 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00417 loss_train: 0.5849 loss_rec: 0.5849 acc_train: 0.7810 loss_val: 0.5862 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00418 loss_train: 0.5846 loss_rec: 0.5846 acc_train: 0.7844 loss_val: 0.5858 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00419 loss_train: 0.5842 loss_rec: 0.5842 acc_train: 0.7844 loss_val: 0.5855 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00420 loss_train: 0.5838 loss_rec: 0.5838 acc_train: 0.7844 loss_val: 0.5851 acc_val: 0.7846 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00421 loss_train: 0.5835 loss_rec: 0.5835 acc_train: 0.7844 loss_val: 0.5848 acc_val: 0.7846 time: 0.0025s\n",
      "Test set results: loss= 0.5848 accuracy= 0.7844\n",
      "Epoch: 00422 loss_train: 0.5831 loss_rec: 0.5831 acc_train: 0.7845 loss_val: 0.5844 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00423 loss_train: 0.5827 loss_rec: 0.5827 acc_train: 0.7843 loss_val: 0.5840 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00424 loss_train: 0.5824 loss_rec: 0.5824 acc_train: 0.7844 loss_val: 0.5837 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00425 loss_train: 0.5820 loss_rec: 0.5820 acc_train: 0.7841 loss_val: 0.5833 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00426 loss_train: 0.5817 loss_rec: 0.5817 acc_train: 0.7841 loss_val: 0.5830 acc_val: 0.7842 time: 0.0020s\n",
      "Epoch: 00427 loss_train: 0.5813 loss_rec: 0.5813 acc_train: 0.7841 loss_val: 0.5826 acc_val: 0.7842 time: 0.0020s\n",
      "Epoch: 00428 loss_train: 0.5809 loss_rec: 0.5809 acc_train: 0.7843 loss_val: 0.5823 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00429 loss_train: 0.5806 loss_rec: 0.5806 acc_train: 0.7844 loss_val: 0.5819 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00430 loss_train: 0.5802 loss_rec: 0.5802 acc_train: 0.7844 loss_val: 0.5816 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00431 loss_train: 0.5798 loss_rec: 0.5798 acc_train: 0.7844 loss_val: 0.5812 acc_val: 0.7846 time: 0.0015s\n",
      "Test set results: loss= 0.5814 accuracy= 0.7841\n",
      "Epoch: 00432 loss_train: 0.5795 loss_rec: 0.5795 acc_train: 0.7844 loss_val: 0.5808 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00433 loss_train: 0.5791 loss_rec: 0.5791 acc_train: 0.7844 loss_val: 0.5805 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00434 loss_train: 0.5787 loss_rec: 0.5787 acc_train: 0.7845 loss_val: 0.5801 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00435 loss_train: 0.5784 loss_rec: 0.5784 acc_train: 0.7845 loss_val: 0.5798 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00436 loss_train: 0.5780 loss_rec: 0.5780 acc_train: 0.7844 loss_val: 0.5794 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00437 loss_train: 0.5776 loss_rec: 0.5776 acc_train: 0.7844 loss_val: 0.5790 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00438 loss_train: 0.5773 loss_rec: 0.5773 acc_train: 0.7845 loss_val: 0.5787 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00439 loss_train: 0.5769 loss_rec: 0.5769 acc_train: 0.7844 loss_val: 0.5783 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00440 loss_train: 0.5766 loss_rec: 0.5766 acc_train: 0.7846 loss_val: 0.5780 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00441 loss_train: 0.5762 loss_rec: 0.5762 acc_train: 0.7847 loss_val: 0.5776 acc_val: 0.7850 time: 0.0015s\n",
      "Test set results: loss= 0.5780 accuracy= 0.7845\n",
      "Epoch: 00442 loss_train: 0.5758 loss_rec: 0.5758 acc_train: 0.7846 loss_val: 0.5772 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00443 loss_train: 0.5754 loss_rec: 0.5754 acc_train: 0.7846 loss_val: 0.5769 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00444 loss_train: 0.5751 loss_rec: 0.5751 acc_train: 0.7846 loss_val: 0.5765 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00445 loss_train: 0.5747 loss_rec: 0.5747 acc_train: 0.7845 loss_val: 0.5762 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00446 loss_train: 0.5744 loss_rec: 0.5744 acc_train: 0.7845 loss_val: 0.5758 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00447 loss_train: 0.5740 loss_rec: 0.5740 acc_train: 0.7843 loss_val: 0.5755 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00448 loss_train: 0.5736 loss_rec: 0.5736 acc_train: 0.7843 loss_val: 0.5751 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00449 loss_train: 0.5733 loss_rec: 0.5733 acc_train: 0.7843 loss_val: 0.5748 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00450 loss_train: 0.5729 loss_rec: 0.5729 acc_train: 0.7843 loss_val: 0.5744 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00451 loss_train: 0.5726 loss_rec: 0.5726 acc_train: 0.7843 loss_val: 0.5741 acc_val: 0.7846 time: 0.0020s\n",
      "Test set results: loss= 0.5747 accuracy= 0.7844\n",
      "Epoch: 00452 loss_train: 0.5722 loss_rec: 0.5722 acc_train: 0.7841 loss_val: 0.5737 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00453 loss_train: 0.5719 loss_rec: 0.5719 acc_train: 0.7841 loss_val: 0.5734 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00454 loss_train: 0.5715 loss_rec: 0.5715 acc_train: 0.7840 loss_val: 0.5730 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00455 loss_train: 0.5711 loss_rec: 0.5711 acc_train: 0.7840 loss_val: 0.5726 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00456 loss_train: 0.5708 loss_rec: 0.5708 acc_train: 0.7840 loss_val: 0.5723 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00457 loss_train: 0.5704 loss_rec: 0.5704 acc_train: 0.7840 loss_val: 0.5719 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00458 loss_train: 0.5701 loss_rec: 0.5701 acc_train: 0.7840 loss_val: 0.5717 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00459 loss_train: 0.5697 loss_rec: 0.5697 acc_train: 0.7839 loss_val: 0.5712 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00460 loss_train: 0.5693 loss_rec: 0.5693 acc_train: 0.7837 loss_val: 0.5709 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00461 loss_train: 0.5690 loss_rec: 0.5690 acc_train: 0.7837 loss_val: 0.5706 acc_val: 0.7850 time: 0.0015s\n",
      "Test set results: loss= 0.5714 accuracy= 0.7826\n",
      "Epoch: 00462 loss_train: 0.5686 loss_rec: 0.5686 acc_train: 0.7829 loss_val: 0.5702 acc_val: 0.7829 time: 0.0020s\n",
      "Epoch: 00463 loss_train: 0.5683 loss_rec: 0.5683 acc_train: 0.7827 loss_val: 0.5699 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00464 loss_train: 0.5679 loss_rec: 0.5679 acc_train: 0.7826 loss_val: 0.5695 acc_val: 0.7825 time: 0.0020s\n",
      "Epoch: 00465 loss_train: 0.5676 loss_rec: 0.5676 acc_train: 0.7826 loss_val: 0.5692 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00466 loss_train: 0.5672 loss_rec: 0.5672 acc_train: 0.7825 loss_val: 0.5688 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00467 loss_train: 0.5669 loss_rec: 0.5669 acc_train: 0.7825 loss_val: 0.5685 acc_val: 0.7829 time: 0.0020s\n",
      "Epoch: 00468 loss_train: 0.5665 loss_rec: 0.5665 acc_train: 0.7824 loss_val: 0.5681 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00469 loss_train: 0.5662 loss_rec: 0.5662 acc_train: 0.7824 loss_val: 0.5678 acc_val: 0.7829 time: 0.0020s\n",
      "Epoch: 00470 loss_train: 0.5658 loss_rec: 0.5658 acc_train: 0.7824 loss_val: 0.5674 acc_val: 0.7825 time: 0.0015s\n",
      "Epoch: 00471 loss_train: 0.5655 loss_rec: 0.5655 acc_train: 0.7824 loss_val: 0.5671 acc_val: 0.7821 time: 0.0015s\n",
      "Test set results: loss= 0.5681 accuracy= 0.7825\n",
      "Epoch: 00472 loss_train: 0.5651 loss_rec: 0.5651 acc_train: 0.7824 loss_val: 0.5667 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00473 loss_train: 0.5648 loss_rec: 0.5648 acc_train: 0.7824 loss_val: 0.5664 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00474 loss_train: 0.5644 loss_rec: 0.5644 acc_train: 0.7824 loss_val: 0.5661 acc_val: 0.7821 time: 0.0020s\n",
      "Epoch: 00475 loss_train: 0.5641 loss_rec: 0.5641 acc_train: 0.7822 loss_val: 0.5657 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00476 loss_train: 0.5637 loss_rec: 0.5637 acc_train: 0.7821 loss_val: 0.5654 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00477 loss_train: 0.5634 loss_rec: 0.5634 acc_train: 0.7821 loss_val: 0.5650 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00478 loss_train: 0.5630 loss_rec: 0.5630 acc_train: 0.7822 loss_val: 0.5647 acc_val: 0.7821 time: 0.0020s\n",
      "Epoch: 00479 loss_train: 0.5627 loss_rec: 0.5627 acc_train: 0.7822 loss_val: 0.5644 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00480 loss_train: 0.5623 loss_rec: 0.5623 acc_train: 0.7823 loss_val: 0.5640 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00481 loss_train: 0.5620 loss_rec: 0.5620 acc_train: 0.7827 loss_val: 0.5637 acc_val: 0.7825 time: 0.0020s\n",
      "Test set results: loss= 0.5648 accuracy= 0.7822\n",
      "Epoch: 00482 loss_train: 0.5616 loss_rec: 0.5616 acc_train: 0.7827 loss_val: 0.5633 acc_val: 0.7825 time: 0.0015s\n",
      "Epoch: 00483 loss_train: 0.5613 loss_rec: 0.5613 acc_train: 0.7827 loss_val: 0.5630 acc_val: 0.7825 time: 0.0020s\n",
      "Epoch: 00484 loss_train: 0.5610 loss_rec: 0.5610 acc_train: 0.7827 loss_val: 0.5627 acc_val: 0.7825 time: 0.0015s\n",
      "Epoch: 00485 loss_train: 0.5606 loss_rec: 0.5606 acc_train: 0.7827 loss_val: 0.5623 acc_val: 0.7825 time: 0.0015s\n",
      "Epoch: 00486 loss_train: 0.5603 loss_rec: 0.5603 acc_train: 0.7827 loss_val: 0.5620 acc_val: 0.7825 time: 0.0020s\n",
      "Epoch: 00487 loss_train: 0.5599 loss_rec: 0.5599 acc_train: 0.7827 loss_val: 0.5616 acc_val: 0.7825 time: 0.0015s\n",
      "Epoch: 00488 loss_train: 0.5596 loss_rec: 0.5596 acc_train: 0.7827 loss_val: 0.5613 acc_val: 0.7825 time: 0.0015s\n",
      "Epoch: 00489 loss_train: 0.5592 loss_rec: 0.5592 acc_train: 0.7826 loss_val: 0.5610 acc_val: 0.7825 time: 0.0020s\n",
      "Epoch: 00490 loss_train: 0.5589 loss_rec: 0.5589 acc_train: 0.7826 loss_val: 0.5606 acc_val: 0.7825 time: 0.0015s\n",
      "Epoch: 00491 loss_train: 0.5585 loss_rec: 0.5585 acc_train: 0.7826 loss_val: 0.5603 acc_val: 0.7825 time: 0.0015s\n",
      "Test set results: loss= 0.5616 accuracy= 0.7827\n",
      "Epoch: 00492 loss_train: 0.5582 loss_rec: 0.5582 acc_train: 0.7827 loss_val: 0.5599 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00493 loss_train: 0.5579 loss_rec: 0.5579 acc_train: 0.7827 loss_val: 0.5596 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00494 loss_train: 0.5575 loss_rec: 0.5575 acc_train: 0.7828 loss_val: 0.5593 acc_val: 0.7829 time: 0.0020s\n",
      "Epoch: 00495 loss_train: 0.5572 loss_rec: 0.5572 acc_train: 0.7828 loss_val: 0.5590 acc_val: 0.7825 time: 0.0020s\n",
      "Epoch: 00496 loss_train: 0.5568 loss_rec: 0.5568 acc_train: 0.7829 loss_val: 0.5586 acc_val: 0.7825 time: 0.0015s\n",
      "Epoch: 00497 loss_train: 0.5565 loss_rec: 0.5565 acc_train: 0.7828 loss_val: 0.5583 acc_val: 0.7825 time: 0.0015s\n",
      "Epoch: 00498 loss_train: 0.5562 loss_rec: 0.5562 acc_train: 0.7829 loss_val: 0.5579 acc_val: 0.7825 time: 0.0015s\n",
      "Epoch: 00499 loss_train: 0.5558 loss_rec: 0.5558 acc_train: 0.7831 loss_val: 0.5576 acc_val: 0.7825 time: 0.0015s\n",
      "Epoch: 00500 loss_train: 0.5555 loss_rec: 0.5555 acc_train: 0.7830 loss_val: 0.5573 acc_val: 0.7821 time: 0.0020s\n",
      "Epoch: 00501 loss_train: 0.5551 loss_rec: 0.5551 acc_train: 0.7831 loss_val: 0.5569 acc_val: 0.7821 time: 0.0015s\n",
      "Test set results: loss= 0.5585 accuracy= 0.7829\n",
      "Epoch: 00502 loss_train: 0.5548 loss_rec: 0.5548 acc_train: 0.7831 loss_val: 0.5567 acc_val: 0.7821 time: 0.0020s\n",
      "Epoch: 00503 loss_train: 0.5545 loss_rec: 0.5545 acc_train: 0.7831 loss_val: 0.5563 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00504 loss_train: 0.5541 loss_rec: 0.5541 acc_train: 0.7833 loss_val: 0.5559 acc_val: 0.7821 time: 0.0020s\n",
      "Epoch: 00505 loss_train: 0.5538 loss_rec: 0.5538 acc_train: 0.7833 loss_val: 0.5556 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00506 loss_train: 0.5535 loss_rec: 0.5535 acc_train: 0.7833 loss_val: 0.5553 acc_val: 0.7821 time: 0.0020s\n",
      "Epoch: 00507 loss_train: 0.5531 loss_rec: 0.5531 acc_train: 0.7833 loss_val: 0.5550 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00508 loss_train: 0.5528 loss_rec: 0.5528 acc_train: 0.7834 loss_val: 0.5546 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00509 loss_train: 0.5524 loss_rec: 0.5524 acc_train: 0.7833 loss_val: 0.5543 acc_val: 0.7821 time: 0.0020s\n",
      "Epoch: 00510 loss_train: 0.5521 loss_rec: 0.5521 acc_train: 0.7833 loss_val: 0.5540 acc_val: 0.7821 time: 0.0020s\n",
      "Epoch: 00511 loss_train: 0.5518 loss_rec: 0.5518 acc_train: 0.7833 loss_val: 0.5536 acc_val: 0.7821 time: 0.0015s\n",
      "Test set results: loss= 0.5554 accuracy= 0.7832\n",
      "Epoch: 00512 loss_train: 0.5514 loss_rec: 0.5514 acc_train: 0.7833 loss_val: 0.5533 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00513 loss_train: 0.5511 loss_rec: 0.5511 acc_train: 0.7832 loss_val: 0.5530 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00514 loss_train: 0.5508 loss_rec: 0.5508 acc_train: 0.7831 loss_val: 0.5527 acc_val: 0.7821 time: 0.0020s\n",
      "Epoch: 00515 loss_train: 0.5504 loss_rec: 0.5504 acc_train: 0.7831 loss_val: 0.5523 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00516 loss_train: 0.5501 loss_rec: 0.5501 acc_train: 0.7831 loss_val: 0.5520 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00517 loss_train: 0.5498 loss_rec: 0.5498 acc_train: 0.7805 loss_val: 0.5517 acc_val: 0.7796 time: 0.0020s\n",
      "Epoch: 00518 loss_train: 0.5495 loss_rec: 0.5495 acc_train: 0.7805 loss_val: 0.5514 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00519 loss_train: 0.5491 loss_rec: 0.5491 acc_train: 0.7806 loss_val: 0.5510 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00520 loss_train: 0.5488 loss_rec: 0.5488 acc_train: 0.7807 loss_val: 0.5507 acc_val: 0.7796 time: 0.0020s\n",
      "Epoch: 00521 loss_train: 0.5485 loss_rec: 0.5485 acc_train: 0.7807 loss_val: 0.5504 acc_val: 0.7796 time: 0.0015s\n",
      "Test set results: loss= 0.5523 accuracy= 0.7799\n",
      "Epoch: 00522 loss_train: 0.5482 loss_rec: 0.5482 acc_train: 0.7807 loss_val: 0.5501 acc_val: 0.7796 time: 0.0020s\n",
      "Epoch: 00523 loss_train: 0.5478 loss_rec: 0.5478 acc_train: 0.7806 loss_val: 0.5498 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00524 loss_train: 0.5475 loss_rec: 0.5475 acc_train: 0.7806 loss_val: 0.5494 acc_val: 0.7796 time: 0.0020s\n",
      "Epoch: 00525 loss_train: 0.5472 loss_rec: 0.5472 acc_train: 0.7806 loss_val: 0.5491 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00526 loss_train: 0.5468 loss_rec: 0.5468 acc_train: 0.7806 loss_val: 0.5488 acc_val: 0.7796 time: 0.0020s\n",
      "Epoch: 00527 loss_train: 0.5465 loss_rec: 0.5465 acc_train: 0.7806 loss_val: 0.5485 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00528 loss_train: 0.5462 loss_rec: 0.5462 acc_train: 0.7806 loss_val: 0.5482 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00529 loss_train: 0.5459 loss_rec: 0.5459 acc_train: 0.7805 loss_val: 0.5478 acc_val: 0.7792 time: 0.0020s\n",
      "Epoch: 00530 loss_train: 0.5455 loss_rec: 0.5455 acc_train: 0.7805 loss_val: 0.5475 acc_val: 0.7792 time: 0.0020s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00531 loss_train: 0.5452 loss_rec: 0.5452 acc_train: 0.7805 loss_val: 0.5472 acc_val: 0.7792 time: 0.0020s\n",
      "Test set results: loss= 0.5493 accuracy= 0.7794\n",
      "Epoch: 00532 loss_train: 0.5449 loss_rec: 0.5449 acc_train: 0.7805 loss_val: 0.5469 acc_val: 0.7792 time: 0.0015s\n",
      "Epoch: 00533 loss_train: 0.5446 loss_rec: 0.5446 acc_train: 0.7804 loss_val: 0.5466 acc_val: 0.7792 time: 0.0020s\n",
      "Epoch: 00534 loss_train: 0.5443 loss_rec: 0.5443 acc_train: 0.7803 loss_val: 0.5463 acc_val: 0.7792 time: 0.0015s\n",
      "Epoch: 00535 loss_train: 0.5439 loss_rec: 0.5439 acc_train: 0.7803 loss_val: 0.5460 acc_val: 0.7792 time: 0.0015s\n",
      "Epoch: 00536 loss_train: 0.5436 loss_rec: 0.5436 acc_train: 0.7803 loss_val: 0.5456 acc_val: 0.7792 time: 0.0020s\n",
      "Epoch: 00537 loss_train: 0.5433 loss_rec: 0.5433 acc_train: 0.7804 loss_val: 0.5453 acc_val: 0.7796 time: 0.0020s\n",
      "Epoch: 00538 loss_train: 0.5430 loss_rec: 0.5430 acc_train: 0.7806 loss_val: 0.5450 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00539 loss_train: 0.5427 loss_rec: 0.5427 acc_train: 0.7807 loss_val: 0.5447 acc_val: 0.7796 time: 0.0020s\n",
      "Epoch: 00540 loss_train: 0.5423 loss_rec: 0.5423 acc_train: 0.7807 loss_val: 0.5444 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00541 loss_train: 0.5420 loss_rec: 0.5420 acc_train: 0.7807 loss_val: 0.5441 acc_val: 0.7796 time: 0.0015s\n",
      "Test set results: loss= 0.5463 accuracy= 0.7803\n",
      "Epoch: 00542 loss_train: 0.5417 loss_rec: 0.5417 acc_train: 0.7807 loss_val: 0.5437 acc_val: 0.7796 time: 0.0020s\n",
      "Epoch: 00543 loss_train: 0.5414 loss_rec: 0.5414 acc_train: 0.7807 loss_val: 0.5434 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00544 loss_train: 0.5411 loss_rec: 0.5411 acc_train: 0.7807 loss_val: 0.5431 acc_val: 0.7796 time: 0.0020s\n",
      "Epoch: 00545 loss_train: 0.5407 loss_rec: 0.5407 acc_train: 0.7807 loss_val: 0.5428 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00546 loss_train: 0.5404 loss_rec: 0.5404 acc_train: 0.7807 loss_val: 0.5425 acc_val: 0.7800 time: 0.0015s\n",
      "Epoch: 00547 loss_train: 0.5401 loss_rec: 0.5401 acc_train: 0.7806 loss_val: 0.5422 acc_val: 0.7800 time: 0.0020s\n",
      "Epoch: 00548 loss_train: 0.5398 loss_rec: 0.5398 acc_train: 0.7806 loss_val: 0.5419 acc_val: 0.7800 time: 0.0020s\n",
      "Epoch: 00549 loss_train: 0.5395 loss_rec: 0.5395 acc_train: 0.7807 loss_val: 0.5416 acc_val: 0.7800 time: 0.0015s\n",
      "Epoch: 00550 loss_train: 0.5392 loss_rec: 0.5392 acc_train: 0.7807 loss_val: 0.5413 acc_val: 0.7800 time: 0.0020s\n",
      "Epoch: 00551 loss_train: 0.5389 loss_rec: 0.5389 acc_train: 0.7807 loss_val: 0.5410 acc_val: 0.7796 time: 0.0015s\n",
      "Test set results: loss= 0.5433 accuracy= 0.7802\n",
      "Epoch: 00552 loss_train: 0.5386 loss_rec: 0.5386 acc_train: 0.7808 loss_val: 0.5407 acc_val: 0.7796 time: 0.0020s\n",
      "Epoch: 00553 loss_train: 0.5382 loss_rec: 0.5382 acc_train: 0.7808 loss_val: 0.5403 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00554 loss_train: 0.5379 loss_rec: 0.5379 acc_train: 0.7808 loss_val: 0.5400 acc_val: 0.7796 time: 0.0020s\n",
      "Epoch: 00555 loss_train: 0.5376 loss_rec: 0.5376 acc_train: 0.7808 loss_val: 0.5397 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00556 loss_train: 0.5373 loss_rec: 0.5373 acc_train: 0.7808 loss_val: 0.5394 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00557 loss_train: 0.5370 loss_rec: 0.5370 acc_train: 0.7808 loss_val: 0.5391 acc_val: 0.7792 time: 0.0020s\n",
      "Epoch: 00558 loss_train: 0.5367 loss_rec: 0.5367 acc_train: 0.7808 loss_val: 0.5388 acc_val: 0.7792 time: 0.0015s\n",
      "Epoch: 00559 loss_train: 0.5364 loss_rec: 0.5364 acc_train: 0.7808 loss_val: 0.5385 acc_val: 0.7792 time: 0.0015s\n",
      "Epoch: 00560 loss_train: 0.5361 loss_rec: 0.5361 acc_train: 0.7809 loss_val: 0.5382 acc_val: 0.7792 time: 0.0015s\n",
      "Epoch: 00561 loss_train: 0.5358 loss_rec: 0.5358 acc_train: 0.7808 loss_val: 0.5379 acc_val: 0.7792 time: 0.0015s\n",
      "Test set results: loss= 0.5404 accuracy= 0.7805\n",
      "Epoch: 00562 loss_train: 0.5355 loss_rec: 0.5355 acc_train: 0.7809 loss_val: 0.5376 acc_val: 0.7792 time: 0.0020s\n",
      "Epoch: 00563 loss_train: 0.5352 loss_rec: 0.5352 acc_train: 0.7809 loss_val: 0.5373 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00564 loss_train: 0.5348 loss_rec: 0.5348 acc_train: 0.7809 loss_val: 0.5370 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00565 loss_train: 0.5345 loss_rec: 0.5345 acc_train: 0.7809 loss_val: 0.5367 acc_val: 0.7796 time: 0.0020s\n",
      "Epoch: 00566 loss_train: 0.5342 loss_rec: 0.5342 acc_train: 0.7809 loss_val: 0.5364 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00567 loss_train: 0.5339 loss_rec: 0.5339 acc_train: 0.7809 loss_val: 0.5361 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00568 loss_train: 0.5336 loss_rec: 0.5336 acc_train: 0.7809 loss_val: 0.5358 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00569 loss_train: 0.5333 loss_rec: 0.5333 acc_train: 0.7808 loss_val: 0.5355 acc_val: 0.7796 time: 0.0020s\n",
      "Epoch: 00570 loss_train: 0.5330 loss_rec: 0.5330 acc_train: 0.7808 loss_val: 0.5352 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00571 loss_train: 0.5327 loss_rec: 0.5327 acc_train: 0.7808 loss_val: 0.5349 acc_val: 0.7796 time: 0.0015s\n",
      "Test set results: loss= 0.5376 accuracy= 0.7807\n",
      "Epoch: 00572 loss_train: 0.5324 loss_rec: 0.5324 acc_train: 0.7807 loss_val: 0.5346 acc_val: 0.7796 time: 0.0020s\n",
      "Epoch: 00573 loss_train: 0.5321 loss_rec: 0.5321 acc_train: 0.7807 loss_val: 0.5343 acc_val: 0.7796 time: 0.0015s\n",
      "Epoch: 00574 loss_train: 0.5318 loss_rec: 0.5318 acc_train: 0.7807 loss_val: 0.5340 acc_val: 0.7800 time: 0.0020s\n",
      "Epoch: 00575 loss_train: 0.5316 loss_rec: 0.5316 acc_train: 0.7807 loss_val: 0.5338 acc_val: 0.7800 time: 0.0015s\n",
      "Epoch: 00576 loss_train: 0.5312 loss_rec: 0.5312 acc_train: 0.7807 loss_val: 0.5334 acc_val: 0.7800 time: 0.0020s\n",
      "Epoch: 00577 loss_train: 0.5309 loss_rec: 0.5309 acc_train: 0.7807 loss_val: 0.5332 acc_val: 0.7800 time: 0.0015s\n",
      "Epoch: 00578 loss_train: 0.5306 loss_rec: 0.5306 acc_train: 0.7807 loss_val: 0.5329 acc_val: 0.7800 time: 0.0020s\n",
      "Epoch: 00579 loss_train: 0.5303 loss_rec: 0.5303 acc_train: 0.7807 loss_val: 0.5326 acc_val: 0.7800 time: 0.0015s\n",
      "Epoch: 00580 loss_train: 0.5300 loss_rec: 0.5300 acc_train: 0.7807 loss_val: 0.5323 acc_val: 0.7800 time: 0.0015s\n",
      "Epoch: 00581 loss_train: 0.5297 loss_rec: 0.5297 acc_train: 0.7807 loss_val: 0.5320 acc_val: 0.7800 time: 0.0020s\n",
      "Test set results: loss= 0.5349 accuracy= 0.7808\n",
      "Epoch: 00582 loss_train: 0.5294 loss_rec: 0.5294 acc_train: 0.7807 loss_val: 0.5317 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00583 loss_train: 0.5291 loss_rec: 0.5291 acc_train: 0.7807 loss_val: 0.5314 acc_val: 0.7804 time: 0.0020s\n",
      "Epoch: 00584 loss_train: 0.5289 loss_rec: 0.5289 acc_train: 0.7807 loss_val: 0.5311 acc_val: 0.7804 time: 0.0020s\n",
      "Epoch: 00585 loss_train: 0.5286 loss_rec: 0.5286 acc_train: 0.7807 loss_val: 0.5308 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00586 loss_train: 0.5283 loss_rec: 0.5283 acc_train: 0.7807 loss_val: 0.5305 acc_val: 0.7804 time: 0.0020s\n",
      "Epoch: 00587 loss_train: 0.5280 loss_rec: 0.5280 acc_train: 0.7807 loss_val: 0.5303 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00588 loss_train: 0.5277 loss_rec: 0.5277 acc_train: 0.7807 loss_val: 0.5300 acc_val: 0.7808 time: 0.0020s\n",
      "Epoch: 00589 loss_train: 0.5274 loss_rec: 0.5274 acc_train: 0.7807 loss_val: 0.5297 acc_val: 0.7808 time: 0.0015s\n",
      "Epoch: 00590 loss_train: 0.5271 loss_rec: 0.5271 acc_train: 0.7804 loss_val: 0.5294 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00591 loss_train: 0.5268 loss_rec: 0.5268 acc_train: 0.7804 loss_val: 0.5291 acc_val: 0.7804 time: 0.0020s\n",
      "Test set results: loss= 0.5321 accuracy= 0.7806\n",
      "Epoch: 00592 loss_train: 0.5265 loss_rec: 0.5265 acc_train: 0.7804 loss_val: 0.5288 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00593 loss_train: 0.5263 loss_rec: 0.5263 acc_train: 0.7804 loss_val: 0.5286 acc_val: 0.7804 time: 0.0020s\n",
      "Epoch: 00594 loss_train: 0.5260 loss_rec: 0.5260 acc_train: 0.7804 loss_val: 0.5283 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00595 loss_train: 0.5257 loss_rec: 0.5257 acc_train: 0.7804 loss_val: 0.5280 acc_val: 0.7804 time: 0.0020s\n",
      "Epoch: 00596 loss_train: 0.5254 loss_rec: 0.5254 acc_train: 0.7805 loss_val: 0.5277 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00597 loss_train: 0.5251 loss_rec: 0.5251 acc_train: 0.7805 loss_val: 0.5274 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00598 loss_train: 0.5248 loss_rec: 0.5248 acc_train: 0.7805 loss_val: 0.5271 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00599 loss_train: 0.5245 loss_rec: 0.5245 acc_train: 0.7805 loss_val: 0.5269 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00600 loss_train: 0.5243 loss_rec: 0.5243 acc_train: 0.7805 loss_val: 0.5266 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00601 loss_train: 0.5240 loss_rec: 0.5240 acc_train: 0.7805 loss_val: 0.5263 acc_val: 0.7804 time: 0.0015s\n",
      "Test set results: loss= 0.5294 accuracy= 0.7807\n",
      "Epoch: 00602 loss_train: 0.5237 loss_rec: 0.5237 acc_train: 0.7806 loss_val: 0.5260 acc_val: 0.7804 time: 0.0025s\n",
      "Epoch: 00603 loss_train: 0.5234 loss_rec: 0.5234 acc_train: 0.7807 loss_val: 0.5257 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00604 loss_train: 0.5231 loss_rec: 0.5231 acc_train: 0.7806 loss_val: 0.5255 acc_val: 0.7804 time: 0.0020s\n",
      "Epoch: 00605 loss_train: 0.5229 loss_rec: 0.5229 acc_train: 0.7806 loss_val: 0.5252 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00606 loss_train: 0.5226 loss_rec: 0.5226 acc_train: 0.7803 loss_val: 0.5249 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00607 loss_train: 0.5223 loss_rec: 0.5223 acc_train: 0.7803 loss_val: 0.5246 acc_val: 0.7804 time: 0.0020s\n",
      "Epoch: 00608 loss_train: 0.5220 loss_rec: 0.5220 acc_train: 0.7803 loss_val: 0.5244 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00609 loss_train: 0.5217 loss_rec: 0.5217 acc_train: 0.7803 loss_val: 0.5241 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00610 loss_train: 0.5215 loss_rec: 0.5215 acc_train: 0.7803 loss_val: 0.5238 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00611 loss_train: 0.5212 loss_rec: 0.5212 acc_train: 0.7803 loss_val: 0.5236 acc_val: 0.7804 time: 0.0015s\n",
      "Test set results: loss= 0.5268 accuracy= 0.7806\n",
      "Epoch: 00612 loss_train: 0.5209 loss_rec: 0.5209 acc_train: 0.7803 loss_val: 0.5233 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00613 loss_train: 0.5206 loss_rec: 0.5206 acc_train: 0.7803 loss_val: 0.5230 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00614 loss_train: 0.5204 loss_rec: 0.5204 acc_train: 0.7803 loss_val: 0.5227 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00615 loss_train: 0.5201 loss_rec: 0.5201 acc_train: 0.7803 loss_val: 0.5225 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00616 loss_train: 0.5198 loss_rec: 0.5198 acc_train: 0.7803 loss_val: 0.5222 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00617 loss_train: 0.5195 loss_rec: 0.5195 acc_train: 0.7802 loss_val: 0.5219 acc_val: 0.7808 time: 0.0015s\n",
      "Epoch: 00618 loss_train: 0.5193 loss_rec: 0.5193 acc_train: 0.7802 loss_val: 0.5217 acc_val: 0.7808 time: 0.0015s\n",
      "Epoch: 00619 loss_train: 0.5190 loss_rec: 0.5190 acc_train: 0.7802 loss_val: 0.5214 acc_val: 0.7808 time: 0.0015s\n",
      "Epoch: 00620 loss_train: 0.5187 loss_rec: 0.5187 acc_train: 0.7802 loss_val: 0.5211 acc_val: 0.7808 time: 0.0015s\n",
      "Epoch: 00621 loss_train: 0.5185 loss_rec: 0.5185 acc_train: 0.7802 loss_val: 0.5209 acc_val: 0.7808 time: 0.0020s\n",
      "Test set results: loss= 0.5242 accuracy= 0.7808\n",
      "Epoch: 00622 loss_train: 0.5182 loss_rec: 0.5182 acc_train: 0.7802 loss_val: 0.5206 acc_val: 0.7808 time: 0.0020s\n",
      "Epoch: 00623 loss_train: 0.5179 loss_rec: 0.5179 acc_train: 0.7801 loss_val: 0.5203 acc_val: 0.7808 time: 0.0015s\n",
      "Epoch: 00624 loss_train: 0.5177 loss_rec: 0.5177 acc_train: 0.7801 loss_val: 0.5201 acc_val: 0.7808 time: 0.0015s\n",
      "Epoch: 00625 loss_train: 0.5174 loss_rec: 0.5174 acc_train: 0.7800 loss_val: 0.5198 acc_val: 0.7808 time: 0.0015s\n",
      "Epoch: 00626 loss_train: 0.5171 loss_rec: 0.5171 acc_train: 0.7802 loss_val: 0.5195 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00627 loss_train: 0.5169 loss_rec: 0.5169 acc_train: 0.7802 loss_val: 0.5193 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00628 loss_train: 0.5166 loss_rec: 0.5166 acc_train: 0.7802 loss_val: 0.5190 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00629 loss_train: 0.5163 loss_rec: 0.5163 acc_train: 0.7802 loss_val: 0.5187 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00630 loss_train: 0.5161 loss_rec: 0.5161 acc_train: 0.7805 loss_val: 0.5185 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00631 loss_train: 0.5158 loss_rec: 0.5158 acc_train: 0.7805 loss_val: 0.5183 acc_val: 0.7817 time: 0.0020s\n",
      "Test set results: loss= 0.5217 accuracy= 0.7814\n",
      "Epoch: 00632 loss_train: 0.5155 loss_rec: 0.5155 acc_train: 0.7805 loss_val: 0.5180 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00633 loss_train: 0.5153 loss_rec: 0.5153 acc_train: 0.7805 loss_val: 0.5177 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00634 loss_train: 0.5150 loss_rec: 0.5150 acc_train: 0.7805 loss_val: 0.5175 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00635 loss_train: 0.5148 loss_rec: 0.5148 acc_train: 0.7806 loss_val: 0.5172 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00636 loss_train: 0.5145 loss_rec: 0.5145 acc_train: 0.7806 loss_val: 0.5169 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00637 loss_train: 0.5142 loss_rec: 0.5142 acc_train: 0.7807 loss_val: 0.5167 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00638 loss_train: 0.5140 loss_rec: 0.5140 acc_train: 0.7807 loss_val: 0.5164 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00639 loss_train: 0.5137 loss_rec: 0.5137 acc_train: 0.7807 loss_val: 0.5162 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00640 loss_train: 0.5134 loss_rec: 0.5134 acc_train: 0.7807 loss_val: 0.5159 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00641 loss_train: 0.5132 loss_rec: 0.5132 acc_train: 0.7807 loss_val: 0.5157 acc_val: 0.7817 time: 0.0020s\n",
      "Test set results: loss= 0.5193 accuracy= 0.7814\n",
      "Epoch: 00642 loss_train: 0.5129 loss_rec: 0.5129 acc_train: 0.7807 loss_val: 0.5154 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00643 loss_train: 0.5127 loss_rec: 0.5127 acc_train: 0.7807 loss_val: 0.5152 acc_val: 0.7817 time: 0.0020s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00644 loss_train: 0.5125 loss_rec: 0.5125 acc_train: 0.7807 loss_val: 0.5149 acc_val: 0.7817 time: 0.0025s\n",
      "Epoch: 00645 loss_train: 0.5122 loss_rec: 0.5122 acc_train: 0.7807 loss_val: 0.5147 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00646 loss_train: 0.5120 loss_rec: 0.5120 acc_train: 0.7807 loss_val: 0.5144 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00647 loss_train: 0.5117 loss_rec: 0.5117 acc_train: 0.7807 loss_val: 0.5142 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00648 loss_train: 0.5114 loss_rec: 0.5114 acc_train: 0.7807 loss_val: 0.5139 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00649 loss_train: 0.5112 loss_rec: 0.5112 acc_train: 0.7804 loss_val: 0.5137 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00650 loss_train: 0.5109 loss_rec: 0.5109 acc_train: 0.7805 loss_val: 0.5134 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00651 loss_train: 0.5107 loss_rec: 0.5107 acc_train: 0.7805 loss_val: 0.5132 acc_val: 0.7817 time: 0.0015s\n",
      "Test set results: loss= 0.5170 accuracy= 0.7802\n",
      "Epoch: 00652 loss_train: 0.5104 loss_rec: 0.5104 acc_train: 0.7805 loss_val: 0.5129 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00653 loss_train: 0.5102 loss_rec: 0.5102 acc_train: 0.7805 loss_val: 0.5127 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00654 loss_train: 0.5099 loss_rec: 0.5099 acc_train: 0.7805 loss_val: 0.5124 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00655 loss_train: 0.5097 loss_rec: 0.5097 acc_train: 0.7806 loss_val: 0.5122 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00656 loss_train: 0.5094 loss_rec: 0.5094 acc_train: 0.7807 loss_val: 0.5119 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00657 loss_train: 0.5092 loss_rec: 0.5092 acc_train: 0.7807 loss_val: 0.5117 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00658 loss_train: 0.5089 loss_rec: 0.5089 acc_train: 0.7807 loss_val: 0.5115 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00659 loss_train: 0.5087 loss_rec: 0.5087 acc_train: 0.7803 loss_val: 0.5112 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00660 loss_train: 0.5084 loss_rec: 0.5084 acc_train: 0.7807 loss_val: 0.5110 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00661 loss_train: 0.5082 loss_rec: 0.5082 acc_train: 0.7807 loss_val: 0.5107 acc_val: 0.7817 time: 0.0015s\n",
      "Test set results: loss= 0.5147 accuracy= 0.7805\n",
      "Epoch: 00662 loss_train: 0.5079 loss_rec: 0.5079 acc_train: 0.7807 loss_val: 0.5105 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00663 loss_train: 0.5077 loss_rec: 0.5077 acc_train: 0.7807 loss_val: 0.5102 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00664 loss_train: 0.5074 loss_rec: 0.5074 acc_train: 0.7807 loss_val: 0.5100 acc_val: 0.7812 time: 0.0020s\n",
      "Epoch: 00665 loss_train: 0.5072 loss_rec: 0.5072 acc_train: 0.7807 loss_val: 0.5097 acc_val: 0.7812 time: 0.0015s\n",
      "Epoch: 00666 loss_train: 0.5070 loss_rec: 0.5070 acc_train: 0.7807 loss_val: 0.5095 acc_val: 0.7808 time: 0.0015s\n",
      "Epoch: 00667 loss_train: 0.5067 loss_rec: 0.5067 acc_train: 0.7806 loss_val: 0.5093 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00668 loss_train: 0.5065 loss_rec: 0.5065 acc_train: 0.7807 loss_val: 0.5090 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00669 loss_train: 0.5062 loss_rec: 0.5062 acc_train: 0.7806 loss_val: 0.5088 acc_val: 0.7800 time: 0.0020s\n",
      "Epoch: 00670 loss_train: 0.5060 loss_rec: 0.5060 acc_train: 0.7806 loss_val: 0.5085 acc_val: 0.7800 time: 0.0015s\n",
      "Epoch: 00671 loss_train: 0.5058 loss_rec: 0.5058 acc_train: 0.7807 loss_val: 0.5083 acc_val: 0.7800 time: 0.0015s\n",
      "Test set results: loss= 0.5125 accuracy= 0.7802\n",
      "Epoch: 00672 loss_train: 0.5055 loss_rec: 0.5055 acc_train: 0.7807 loss_val: 0.5081 acc_val: 0.7800 time: 0.0015s\n",
      "Epoch: 00673 loss_train: 0.5053 loss_rec: 0.5053 acc_train: 0.7808 loss_val: 0.5078 acc_val: 0.7800 time: 0.0015s\n",
      "Epoch: 00674 loss_train: 0.5050 loss_rec: 0.5050 acc_train: 0.7808 loss_val: 0.5076 acc_val: 0.7800 time: 0.0020s\n",
      "Epoch: 00675 loss_train: 0.5048 loss_rec: 0.5048 acc_train: 0.7810 loss_val: 0.5074 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00676 loss_train: 0.5046 loss_rec: 0.5046 acc_train: 0.7811 loss_val: 0.5071 acc_val: 0.7804 time: 0.0020s\n",
      "Epoch: 00677 loss_train: 0.5043 loss_rec: 0.5043 acc_train: 0.7811 loss_val: 0.5069 acc_val: 0.7804 time: 0.0020s\n",
      "Epoch: 00678 loss_train: 0.5041 loss_rec: 0.5041 acc_train: 0.7811 loss_val: 0.5067 acc_val: 0.7804 time: 0.0020s\n",
      "Epoch: 00679 loss_train: 0.5039 loss_rec: 0.5039 acc_train: 0.7812 loss_val: 0.5064 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00680 loss_train: 0.5036 loss_rec: 0.5036 acc_train: 0.7811 loss_val: 0.5062 acc_val: 0.7804 time: 0.0020s\n",
      "Epoch: 00681 loss_train: 0.5034 loss_rec: 0.5034 acc_train: 0.7810 loss_val: 0.5060 acc_val: 0.7800 time: 0.0015s\n",
      "Test set results: loss= 0.5103 accuracy= 0.7815\n",
      "Epoch: 00682 loss_train: 0.5032 loss_rec: 0.5032 acc_train: 0.7817 loss_val: 0.5057 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00683 loss_train: 0.5029 loss_rec: 0.5029 acc_train: 0.7817 loss_val: 0.5055 acc_val: 0.7804 time: 0.0020s\n",
      "Epoch: 00684 loss_train: 0.5027 loss_rec: 0.5027 acc_train: 0.7819 loss_val: 0.5053 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00685 loss_train: 0.5025 loss_rec: 0.5025 acc_train: 0.7819 loss_val: 0.5051 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00686 loss_train: 0.5023 loss_rec: 0.5023 acc_train: 0.7819 loss_val: 0.5048 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00687 loss_train: 0.5020 loss_rec: 0.5020 acc_train: 0.7819 loss_val: 0.5046 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00688 loss_train: 0.5018 loss_rec: 0.5018 acc_train: 0.7819 loss_val: 0.5044 acc_val: 0.7804 time: 0.0020s\n",
      "Epoch: 00689 loss_train: 0.5016 loss_rec: 0.5016 acc_train: 0.7817 loss_val: 0.5041 acc_val: 0.7800 time: 0.0015s\n",
      "Epoch: 00690 loss_train: 0.5014 loss_rec: 0.5014 acc_train: 0.7817 loss_val: 0.5039 acc_val: 0.7800 time: 0.0015s\n",
      "Epoch: 00691 loss_train: 0.5011 loss_rec: 0.5011 acc_train: 0.7817 loss_val: 0.5037 acc_val: 0.7800 time: 0.0020s\n",
      "Test set results: loss= 0.5082 accuracy= 0.7813\n",
      "Epoch: 00692 loss_train: 0.5009 loss_rec: 0.5009 acc_train: 0.7817 loss_val: 0.5035 acc_val: 0.7800 time: 0.0015s\n",
      "Epoch: 00693 loss_train: 0.5007 loss_rec: 0.5007 acc_train: 0.7817 loss_val: 0.5032 acc_val: 0.7800 time: 0.0015s\n",
      "Epoch: 00694 loss_train: 0.5004 loss_rec: 0.5004 acc_train: 0.7817 loss_val: 0.5030 acc_val: 0.7800 time: 0.0015s\n",
      "Epoch: 00695 loss_train: 0.5002 loss_rec: 0.5002 acc_train: 0.7817 loss_val: 0.5028 acc_val: 0.7800 time: 0.0015s\n",
      "Epoch: 00696 loss_train: 0.5000 loss_rec: 0.5000 acc_train: 0.7817 loss_val: 0.5026 acc_val: 0.7800 time: 0.0015s\n",
      "Epoch: 00697 loss_train: 0.4998 loss_rec: 0.4998 acc_train: 0.7819 loss_val: 0.5023 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00698 loss_train: 0.4995 loss_rec: 0.4995 acc_train: 0.7819 loss_val: 0.5021 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00699 loss_train: 0.4993 loss_rec: 0.4993 acc_train: 0.7817 loss_val: 0.5019 acc_val: 0.7804 time: 0.0015s\n",
      "Epoch: 00700 loss_train: 0.4991 loss_rec: 0.4991 acc_train: 0.7817 loss_val: 0.5016 acc_val: 0.7804 time: 0.0020s\n",
      "Epoch: 00701 loss_train: 0.4989 loss_rec: 0.4989 acc_train: 0.7822 loss_val: 0.5015 acc_val: 0.7812 time: 0.0015s\n",
      "Test set results: loss= 0.5062 accuracy= 0.7816\n",
      "Epoch: 00702 loss_train: 0.4986 loss_rec: 0.4986 acc_train: 0.7821 loss_val: 0.5012 acc_val: 0.7812 time: 0.0025s\n",
      "Epoch: 00703 loss_train: 0.4984 loss_rec: 0.4984 acc_train: 0.7820 loss_val: 0.5010 acc_val: 0.7812 time: 0.0015s\n",
      "Epoch: 00704 loss_train: 0.4982 loss_rec: 0.4982 acc_train: 0.7820 loss_val: 0.5008 acc_val: 0.7812 time: 0.0015s\n",
      "Epoch: 00705 loss_train: 0.4980 loss_rec: 0.4980 acc_train: 0.7820 loss_val: 0.5006 acc_val: 0.7812 time: 0.0015s\n",
      "Epoch: 00706 loss_train: 0.4978 loss_rec: 0.4978 acc_train: 0.7820 loss_val: 0.5004 acc_val: 0.7812 time: 0.0015s\n",
      "Epoch: 00707 loss_train: 0.4976 loss_rec: 0.4976 acc_train: 0.7822 loss_val: 0.5001 acc_val: 0.7808 time: 0.0015s\n",
      "Epoch: 00708 loss_train: 0.4974 loss_rec: 0.4974 acc_train: 0.7822 loss_val: 0.4999 acc_val: 0.7808 time: 0.0015s\n",
      "Epoch: 00709 loss_train: 0.4971 loss_rec: 0.4971 acc_train: 0.7822 loss_val: 0.4997 acc_val: 0.7808 time: 0.0015s\n",
      "Epoch: 00710 loss_train: 0.4969 loss_rec: 0.4969 acc_train: 0.7822 loss_val: 0.4995 acc_val: 0.7808 time: 0.0020s\n",
      "Epoch: 00711 loss_train: 0.4967 loss_rec: 0.4967 acc_train: 0.7822 loss_val: 0.4993 acc_val: 0.7808 time: 0.0015s\n",
      "Test set results: loss= 0.5042 accuracy= 0.7815\n",
      "Epoch: 00712 loss_train: 0.4965 loss_rec: 0.4965 acc_train: 0.7820 loss_val: 0.4991 acc_val: 0.7808 time: 0.0015s\n",
      "Epoch: 00713 loss_train: 0.4963 loss_rec: 0.4963 acc_train: 0.7822 loss_val: 0.4988 acc_val: 0.7812 time: 0.0015s\n",
      "Epoch: 00714 loss_train: 0.4960 loss_rec: 0.4960 acc_train: 0.7823 loss_val: 0.4986 acc_val: 0.7812 time: 0.0015s\n",
      "Epoch: 00715 loss_train: 0.4958 loss_rec: 0.4958 acc_train: 0.7823 loss_val: 0.4984 acc_val: 0.7812 time: 0.0015s\n",
      "Epoch: 00716 loss_train: 0.4956 loss_rec: 0.4956 acc_train: 0.7823 loss_val: 0.4982 acc_val: 0.7812 time: 0.0015s\n",
      "Epoch: 00717 loss_train: 0.4954 loss_rec: 0.4954 acc_train: 0.7823 loss_val: 0.4980 acc_val: 0.7812 time: 0.0015s\n",
      "Epoch: 00718 loss_train: 0.4952 loss_rec: 0.4952 acc_train: 0.7823 loss_val: 0.4978 acc_val: 0.7812 time: 0.0015s\n",
      "Epoch: 00719 loss_train: 0.4950 loss_rec: 0.4950 acc_train: 0.7822 loss_val: 0.4976 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00720 loss_train: 0.4947 loss_rec: 0.4947 acc_train: 0.7822 loss_val: 0.4973 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00721 loss_train: 0.4945 loss_rec: 0.4945 acc_train: 0.7822 loss_val: 0.4971 acc_val: 0.7817 time: 0.0015s\n",
      "Test set results: loss= 0.5022 accuracy= 0.7816\n",
      "Epoch: 00722 loss_train: 0.4943 loss_rec: 0.4943 acc_train: 0.7821 loss_val: 0.4969 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00723 loss_train: 0.4941 loss_rec: 0.4941 acc_train: 0.7821 loss_val: 0.4967 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00724 loss_train: 0.4939 loss_rec: 0.4939 acc_train: 0.7821 loss_val: 0.4965 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00725 loss_train: 0.4937 loss_rec: 0.4937 acc_train: 0.7820 loss_val: 0.4963 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00726 loss_train: 0.4935 loss_rec: 0.4935 acc_train: 0.7821 loss_val: 0.4961 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00727 loss_train: 0.4933 loss_rec: 0.4933 acc_train: 0.7820 loss_val: 0.4959 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00728 loss_train: 0.4931 loss_rec: 0.4931 acc_train: 0.7820 loss_val: 0.4957 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00729 loss_train: 0.4929 loss_rec: 0.4929 acc_train: 0.7820 loss_val: 0.4955 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00730 loss_train: 0.4927 loss_rec: 0.4927 acc_train: 0.7820 loss_val: 0.4953 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00731 loss_train: 0.4925 loss_rec: 0.4925 acc_train: 0.7820 loss_val: 0.4951 acc_val: 0.7817 time: 0.0020s\n",
      "Test set results: loss= 0.5003 accuracy= 0.7810\n",
      "Epoch: 00732 loss_train: 0.4922 loss_rec: 0.4922 acc_train: 0.7820 loss_val: 0.4949 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00733 loss_train: 0.4920 loss_rec: 0.4920 acc_train: 0.7820 loss_val: 0.4947 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00734 loss_train: 0.4918 loss_rec: 0.4918 acc_train: 0.7820 loss_val: 0.4945 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00735 loss_train: 0.4916 loss_rec: 0.4916 acc_train: 0.7821 loss_val: 0.4943 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00736 loss_train: 0.4914 loss_rec: 0.4914 acc_train: 0.7821 loss_val: 0.4941 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00737 loss_train: 0.4912 loss_rec: 0.4912 acc_train: 0.7821 loss_val: 0.4939 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00738 loss_train: 0.4910 loss_rec: 0.4910 acc_train: 0.7821 loss_val: 0.4936 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00739 loss_train: 0.4908 loss_rec: 0.4908 acc_train: 0.7821 loss_val: 0.4934 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00740 loss_train: 0.4906 loss_rec: 0.4906 acc_train: 0.7821 loss_val: 0.4932 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00741 loss_train: 0.4904 loss_rec: 0.4904 acc_train: 0.7821 loss_val: 0.4931 acc_val: 0.7817 time: 0.0015s\n",
      "Test set results: loss= 0.4985 accuracy= 0.7810\n",
      "Epoch: 00742 loss_train: 0.4902 loss_rec: 0.4902 acc_train: 0.7821 loss_val: 0.4929 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00743 loss_train: 0.4900 loss_rec: 0.4900 acc_train: 0.7821 loss_val: 0.4926 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00744 loss_train: 0.4898 loss_rec: 0.4898 acc_train: 0.7821 loss_val: 0.4924 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00745 loss_train: 0.4896 loss_rec: 0.4896 acc_train: 0.7821 loss_val: 0.4923 acc_val: 0.7817 time: 0.0020s\n",
      "Epoch: 00746 loss_train: 0.4894 loss_rec: 0.4894 acc_train: 0.7822 loss_val: 0.4921 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00747 loss_train: 0.4892 loss_rec: 0.4892 acc_train: 0.7822 loss_val: 0.4919 acc_val: 0.7817 time: 0.0015s\n",
      "Epoch: 00748 loss_train: 0.4890 loss_rec: 0.4890 acc_train: 0.7822 loss_val: 0.4917 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00749 loss_train: 0.4888 loss_rec: 0.4888 acc_train: 0.7822 loss_val: 0.4915 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00750 loss_train: 0.4886 loss_rec: 0.4886 acc_train: 0.7821 loss_val: 0.4913 acc_val: 0.7821 time: 0.0020s\n",
      "Epoch: 00751 loss_train: 0.4885 loss_rec: 0.4885 acc_train: 0.7821 loss_val: 0.4911 acc_val: 0.7821 time: 0.0015s\n",
      "Test set results: loss= 0.4967 accuracy= 0.7808\n",
      "Epoch: 00752 loss_train: 0.4882 loss_rec: 0.4882 acc_train: 0.7821 loss_val: 0.4909 acc_val: 0.7821 time: 0.0020s\n",
      "Epoch: 00753 loss_train: 0.4880 loss_rec: 0.4880 acc_train: 0.7821 loss_val: 0.4907 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00754 loss_train: 0.4879 loss_rec: 0.4879 acc_train: 0.7821 loss_val: 0.4905 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00755 loss_train: 0.4877 loss_rec: 0.4877 acc_train: 0.7821 loss_val: 0.4903 acc_val: 0.7821 time: 0.0020s\n",
      "Epoch: 00756 loss_train: 0.4875 loss_rec: 0.4875 acc_train: 0.7821 loss_val: 0.4901 acc_val: 0.7821 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00757 loss_train: 0.4873 loss_rec: 0.4873 acc_train: 0.7821 loss_val: 0.4899 acc_val: 0.7821 time: 0.0020s\n",
      "Epoch: 00758 loss_train: 0.4871 loss_rec: 0.4871 acc_train: 0.7821 loss_val: 0.4897 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00759 loss_train: 0.4869 loss_rec: 0.4869 acc_train: 0.7821 loss_val: 0.4895 acc_val: 0.7821 time: 0.0020s\n",
      "Epoch: 00760 loss_train: 0.4867 loss_rec: 0.4867 acc_train: 0.7821 loss_val: 0.4893 acc_val: 0.7821 time: 0.0015s\n",
      "Epoch: 00761 loss_train: 0.4865 loss_rec: 0.4865 acc_train: 0.7851 loss_val: 0.4891 acc_val: 0.7846 time: 0.0020s\n",
      "Test set results: loss= 0.4950 accuracy= 0.7832\n",
      "Epoch: 00762 loss_train: 0.4863 loss_rec: 0.4863 acc_train: 0.7851 loss_val: 0.4890 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00763 loss_train: 0.4861 loss_rec: 0.4861 acc_train: 0.7851 loss_val: 0.4888 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00764 loss_train: 0.4859 loss_rec: 0.4859 acc_train: 0.7851 loss_val: 0.4886 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00765 loss_train: 0.4858 loss_rec: 0.4858 acc_train: 0.7851 loss_val: 0.4884 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00766 loss_train: 0.4856 loss_rec: 0.4856 acc_train: 0.7880 loss_val: 0.4882 acc_val: 0.7863 time: 0.0020s\n",
      "Epoch: 00767 loss_train: 0.4854 loss_rec: 0.4854 acc_train: 0.7880 loss_val: 0.4880 acc_val: 0.7863 time: 0.0015s\n",
      "Epoch: 00768 loss_train: 0.4852 loss_rec: 0.4852 acc_train: 0.7876 loss_val: 0.4878 acc_val: 0.7863 time: 0.0020s\n",
      "Epoch: 00769 loss_train: 0.4850 loss_rec: 0.4850 acc_train: 0.7875 loss_val: 0.4877 acc_val: 0.7863 time: 0.0020s\n",
      "Epoch: 00770 loss_train: 0.4848 loss_rec: 0.4848 acc_train: 0.7875 loss_val: 0.4875 acc_val: 0.7863 time: 0.0015s\n",
      "Epoch: 00771 loss_train: 0.4846 loss_rec: 0.4846 acc_train: 0.7875 loss_val: 0.4873 acc_val: 0.7863 time: 0.0020s\n",
      "Test set results: loss= 0.4933 accuracy= 0.7854\n",
      "Epoch: 00772 loss_train: 0.4844 loss_rec: 0.4844 acc_train: 0.7876 loss_val: 0.4871 acc_val: 0.7858 time: 0.0015s\n",
      "Epoch: 00773 loss_train: 0.4843 loss_rec: 0.4843 acc_train: 0.7876 loss_val: 0.4869 acc_val: 0.7858 time: 0.0020s\n",
      "Epoch: 00774 loss_train: 0.4841 loss_rec: 0.4841 acc_train: 0.7876 loss_val: 0.4867 acc_val: 0.7858 time: 0.0015s\n",
      "Epoch: 00775 loss_train: 0.4839 loss_rec: 0.4839 acc_train: 0.7876 loss_val: 0.4865 acc_val: 0.7858 time: 0.0015s\n",
      "Epoch: 00776 loss_train: 0.4837 loss_rec: 0.4837 acc_train: 0.7876 loss_val: 0.4864 acc_val: 0.7858 time: 0.0020s\n",
      "Epoch: 00777 loss_train: 0.4835 loss_rec: 0.4835 acc_train: 0.7876 loss_val: 0.4862 acc_val: 0.7858 time: 0.0015s\n",
      "Epoch: 00778 loss_train: 0.4833 loss_rec: 0.4833 acc_train: 0.7876 loss_val: 0.4860 acc_val: 0.7858 time: 0.0015s\n",
      "Epoch: 00779 loss_train: 0.4831 loss_rec: 0.4831 acc_train: 0.7876 loss_val: 0.4858 acc_val: 0.7858 time: 0.0020s\n",
      "Epoch: 00780 loss_train: 0.4830 loss_rec: 0.4830 acc_train: 0.7876 loss_val: 0.4856 acc_val: 0.7858 time: 0.0015s\n",
      "Epoch: 00781 loss_train: 0.4828 loss_rec: 0.4828 acc_train: 0.7876 loss_val: 0.4855 acc_val: 0.7858 time: 0.0015s\n",
      "Test set results: loss= 0.4916 accuracy= 0.7852\n",
      "Epoch: 00782 loss_train: 0.4826 loss_rec: 0.4826 acc_train: 0.7876 loss_val: 0.4853 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00783 loss_train: 0.4824 loss_rec: 0.4824 acc_train: 0.7876 loss_val: 0.4851 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00784 loss_train: 0.4822 loss_rec: 0.4822 acc_train: 0.7876 loss_val: 0.4849 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00785 loss_train: 0.4821 loss_rec: 0.4821 acc_train: 0.7876 loss_val: 0.4848 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00786 loss_train: 0.4819 loss_rec: 0.4819 acc_train: 0.7877 loss_val: 0.4846 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00787 loss_train: 0.4817 loss_rec: 0.4817 acc_train: 0.7876 loss_val: 0.4844 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00788 loss_train: 0.4815 loss_rec: 0.4815 acc_train: 0.7876 loss_val: 0.4842 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00789 loss_train: 0.4814 loss_rec: 0.4814 acc_train: 0.7877 loss_val: 0.4840 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00790 loss_train: 0.4812 loss_rec: 0.4812 acc_train: 0.7876 loss_val: 0.4839 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00791 loss_train: 0.4810 loss_rec: 0.4810 acc_train: 0.7876 loss_val: 0.4837 acc_val: 0.7854 time: 0.0020s\n",
      "Test set results: loss= 0.4900 accuracy= 0.7852\n",
      "Epoch: 00792 loss_train: 0.4808 loss_rec: 0.4808 acc_train: 0.7876 loss_val: 0.4835 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00793 loss_train: 0.4807 loss_rec: 0.4807 acc_train: 0.7876 loss_val: 0.4834 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00794 loss_train: 0.4805 loss_rec: 0.4805 acc_train: 0.7875 loss_val: 0.4832 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00795 loss_train: 0.4803 loss_rec: 0.4803 acc_train: 0.7875 loss_val: 0.4830 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00796 loss_train: 0.4801 loss_rec: 0.4801 acc_train: 0.7876 loss_val: 0.4829 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00797 loss_train: 0.4799 loss_rec: 0.4799 acc_train: 0.7876 loss_val: 0.4827 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00798 loss_train: 0.4798 loss_rec: 0.4798 acc_train: 0.7875 loss_val: 0.4825 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00799 loss_train: 0.4796 loss_rec: 0.4796 acc_train: 0.7874 loss_val: 0.4823 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00800 loss_train: 0.4794 loss_rec: 0.4794 acc_train: 0.7874 loss_val: 0.4822 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00801 loss_train: 0.4793 loss_rec: 0.4793 acc_train: 0.7874 loss_val: 0.4820 acc_val: 0.7854 time: 0.0020s\n",
      "Test set results: loss= 0.4884 accuracy= 0.7848\n",
      "Epoch: 00802 loss_train: 0.4791 loss_rec: 0.4791 acc_train: 0.7874 loss_val: 0.4818 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00803 loss_train: 0.4789 loss_rec: 0.4789 acc_train: 0.7874 loss_val: 0.4816 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00804 loss_train: 0.4788 loss_rec: 0.4788 acc_train: 0.7874 loss_val: 0.4815 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00805 loss_train: 0.4786 loss_rec: 0.4786 acc_train: 0.7874 loss_val: 0.4813 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00806 loss_train: 0.4784 loss_rec: 0.4784 acc_train: 0.7875 loss_val: 0.4811 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00807 loss_train: 0.4782 loss_rec: 0.4782 acc_train: 0.7875 loss_val: 0.4810 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00808 loss_train: 0.4781 loss_rec: 0.4781 acc_train: 0.7875 loss_val: 0.4808 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00809 loss_train: 0.4779 loss_rec: 0.4779 acc_train: 0.7875 loss_val: 0.4806 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00810 loss_train: 0.4777 loss_rec: 0.4777 acc_train: 0.7875 loss_val: 0.4805 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00811 loss_train: 0.4776 loss_rec: 0.4776 acc_train: 0.7875 loss_val: 0.4803 acc_val: 0.7850 time: 0.0020s\n",
      "Test set results: loss= 0.4869 accuracy= 0.7840\n",
      "Epoch: 00812 loss_train: 0.4774 loss_rec: 0.4774 acc_train: 0.7867 loss_val: 0.4802 acc_val: 0.7833 time: 0.0015s\n",
      "Epoch: 00813 loss_train: 0.4772 loss_rec: 0.4772 acc_train: 0.7867 loss_val: 0.4800 acc_val: 0.7833 time: 0.0020s\n",
      "Epoch: 00814 loss_train: 0.4771 loss_rec: 0.4771 acc_train: 0.7867 loss_val: 0.4798 acc_val: 0.7833 time: 0.0015s\n",
      "Epoch: 00815 loss_train: 0.4769 loss_rec: 0.4769 acc_train: 0.7867 loss_val: 0.4796 acc_val: 0.7833 time: 0.0015s\n",
      "Epoch: 00816 loss_train: 0.4767 loss_rec: 0.4767 acc_train: 0.7867 loss_val: 0.4795 acc_val: 0.7833 time: 0.0020s\n",
      "Epoch: 00817 loss_train: 0.4766 loss_rec: 0.4766 acc_train: 0.7867 loss_val: 0.4793 acc_val: 0.7833 time: 0.0015s\n",
      "Epoch: 00818 loss_train: 0.4764 loss_rec: 0.4764 acc_train: 0.7867 loss_val: 0.4792 acc_val: 0.7833 time: 0.0020s\n",
      "Epoch: 00819 loss_train: 0.4762 loss_rec: 0.4762 acc_train: 0.7867 loss_val: 0.4790 acc_val: 0.7833 time: 0.0015s\n",
      "Epoch: 00820 loss_train: 0.4761 loss_rec: 0.4761 acc_train: 0.7868 loss_val: 0.4788 acc_val: 0.7833 time: 0.0015s\n",
      "Epoch: 00821 loss_train: 0.4759 loss_rec: 0.4759 acc_train: 0.7867 loss_val: 0.4787 acc_val: 0.7833 time: 0.0020s\n",
      "Test set results: loss= 0.4854 accuracy= 0.7838\n",
      "Epoch: 00822 loss_train: 0.4757 loss_rec: 0.4757 acc_train: 0.7867 loss_val: 0.4785 acc_val: 0.7833 time: 0.0015s\n",
      "Epoch: 00823 loss_train: 0.4756 loss_rec: 0.4756 acc_train: 0.7867 loss_val: 0.4784 acc_val: 0.7833 time: 0.0020s\n",
      "Epoch: 00824 loss_train: 0.4754 loss_rec: 0.4754 acc_train: 0.7866 loss_val: 0.4782 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00825 loss_train: 0.4753 loss_rec: 0.4753 acc_train: 0.7866 loss_val: 0.4780 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00826 loss_train: 0.4751 loss_rec: 0.4751 acc_train: 0.7866 loss_val: 0.4779 acc_val: 0.7829 time: 0.0020s\n",
      "Epoch: 00827 loss_train: 0.4749 loss_rec: 0.4749 acc_train: 0.7867 loss_val: 0.4777 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00828 loss_train: 0.4748 loss_rec: 0.4748 acc_train: 0.7867 loss_val: 0.4776 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00829 loss_train: 0.4746 loss_rec: 0.4746 acc_train: 0.7867 loss_val: 0.4774 acc_val: 0.7829 time: 0.0020s\n",
      "Epoch: 00830 loss_train: 0.4745 loss_rec: 0.4745 acc_train: 0.7867 loss_val: 0.4772 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00831 loss_train: 0.4743 loss_rec: 0.4743 acc_train: 0.7867 loss_val: 0.4771 acc_val: 0.7829 time: 0.0015s\n",
      "Test set results: loss= 0.4840 accuracy= 0.7836\n",
      "Epoch: 00832 loss_train: 0.4741 loss_rec: 0.4741 acc_train: 0.7867 loss_val: 0.4769 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00833 loss_train: 0.4740 loss_rec: 0.4740 acc_train: 0.7866 loss_val: 0.4768 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00834 loss_train: 0.4738 loss_rec: 0.4738 acc_train: 0.7864 loss_val: 0.4766 acc_val: 0.7829 time: 0.0020s\n",
      "Epoch: 00835 loss_train: 0.4737 loss_rec: 0.4737 acc_train: 0.7864 loss_val: 0.4765 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00836 loss_train: 0.4735 loss_rec: 0.4735 acc_train: 0.7864 loss_val: 0.4763 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00837 loss_train: 0.4734 loss_rec: 0.4734 acc_train: 0.7864 loss_val: 0.4762 acc_val: 0.7833 time: 0.0020s\n",
      "Epoch: 00838 loss_train: 0.4732 loss_rec: 0.4732 acc_train: 0.7864 loss_val: 0.4760 acc_val: 0.7833 time: 0.0015s\n",
      "Epoch: 00839 loss_train: 0.4730 loss_rec: 0.4730 acc_train: 0.7864 loss_val: 0.4759 acc_val: 0.7833 time: 0.0015s\n",
      "Epoch: 00840 loss_train: 0.4729 loss_rec: 0.4729 acc_train: 0.7864 loss_val: 0.4757 acc_val: 0.7833 time: 0.0015s\n",
      "Epoch: 00841 loss_train: 0.4727 loss_rec: 0.4727 acc_train: 0.7864 loss_val: 0.4755 acc_val: 0.7833 time: 0.0015s\n",
      "Test set results: loss= 0.4826 accuracy= 0.7838\n",
      "Epoch: 00842 loss_train: 0.4726 loss_rec: 0.4726 acc_train: 0.7863 loss_val: 0.4754 acc_val: 0.7833 time: 0.0025s\n",
      "Epoch: 00843 loss_train: 0.4724 loss_rec: 0.4724 acc_train: 0.7863 loss_val: 0.4753 acc_val: 0.7833 time: 0.0020s\n",
      "Epoch: 00844 loss_train: 0.4723 loss_rec: 0.4723 acc_train: 0.7863 loss_val: 0.4751 acc_val: 0.7833 time: 0.0015s\n",
      "Epoch: 00845 loss_train: 0.4721 loss_rec: 0.4721 acc_train: 0.7863 loss_val: 0.4749 acc_val: 0.7833 time: 0.0020s\n",
      "Epoch: 00846 loss_train: 0.4720 loss_rec: 0.4720 acc_train: 0.7863 loss_val: 0.4748 acc_val: 0.7833 time: 0.0015s\n",
      "Epoch: 00847 loss_train: 0.4718 loss_rec: 0.4718 acc_train: 0.7863 loss_val: 0.4746 acc_val: 0.7833 time: 0.0015s\n",
      "Epoch: 00848 loss_train: 0.4717 loss_rec: 0.4717 acc_train: 0.7863 loss_val: 0.4745 acc_val: 0.7833 time: 0.0020s\n",
      "Epoch: 00849 loss_train: 0.4715 loss_rec: 0.4715 acc_train: 0.7864 loss_val: 0.4743 acc_val: 0.7833 time: 0.0015s\n",
      "Epoch: 00850 loss_train: 0.4713 loss_rec: 0.4713 acc_train: 0.7864 loss_val: 0.4742 acc_val: 0.7833 time: 0.0020s\n",
      "Epoch: 00851 loss_train: 0.4712 loss_rec: 0.4712 acc_train: 0.7864 loss_val: 0.4741 acc_val: 0.7833 time: 0.0015s\n",
      "Test set results: loss= 0.4812 accuracy= 0.7832\n",
      "Epoch: 00852 loss_train: 0.4710 loss_rec: 0.4710 acc_train: 0.7864 loss_val: 0.4739 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00853 loss_train: 0.4709 loss_rec: 0.4709 acc_train: 0.7864 loss_val: 0.4738 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00854 loss_train: 0.4707 loss_rec: 0.4707 acc_train: 0.7864 loss_val: 0.4736 acc_val: 0.7829 time: 0.0015s\n",
      "Epoch: 00855 loss_train: 0.4706 loss_rec: 0.4706 acc_train: 0.7863 loss_val: 0.4735 acc_val: 0.7829 time: 0.0020s\n",
      "Epoch: 00856 loss_train: 0.4704 loss_rec: 0.4704 acc_train: 0.7866 loss_val: 0.4733 acc_val: 0.7838 time: 0.0015s\n",
      "Epoch: 00857 loss_train: 0.4703 loss_rec: 0.4703 acc_train: 0.7866 loss_val: 0.4732 acc_val: 0.7838 time: 0.0015s\n",
      "Epoch: 00858 loss_train: 0.4701 loss_rec: 0.4701 acc_train: 0.7866 loss_val: 0.4730 acc_val: 0.7838 time: 0.0020s\n",
      "Epoch: 00859 loss_train: 0.4700 loss_rec: 0.4700 acc_train: 0.7866 loss_val: 0.4729 acc_val: 0.7838 time: 0.0015s\n",
      "Epoch: 00860 loss_train: 0.4698 loss_rec: 0.4698 acc_train: 0.7867 loss_val: 0.4727 acc_val: 0.7838 time: 0.0015s\n",
      "Epoch: 00861 loss_train: 0.4697 loss_rec: 0.4697 acc_train: 0.7867 loss_val: 0.4726 acc_val: 0.7838 time: 0.0020s\n",
      "Test set results: loss= 0.4799 accuracy= 0.7842\n",
      "Epoch: 00862 loss_train: 0.4695 loss_rec: 0.4695 acc_train: 0.7868 loss_val: 0.4724 acc_val: 0.7842 time: 0.0020s\n",
      "Epoch: 00863 loss_train: 0.4694 loss_rec: 0.4694 acc_train: 0.7867 loss_val: 0.4723 acc_val: 0.7842 time: 0.0015s\n",
      "Epoch: 00864 loss_train: 0.4693 loss_rec: 0.4693 acc_train: 0.7866 loss_val: 0.4722 acc_val: 0.7842 time: 0.0015s\n",
      "Epoch: 00865 loss_train: 0.4691 loss_rec: 0.4691 acc_train: 0.7866 loss_val: 0.4720 acc_val: 0.7838 time: 0.0020s\n",
      "Epoch: 00866 loss_train: 0.4690 loss_rec: 0.4690 acc_train: 0.7866 loss_val: 0.4719 acc_val: 0.7842 time: 0.0020s\n",
      "Epoch: 00867 loss_train: 0.4688 loss_rec: 0.4688 acc_train: 0.7871 loss_val: 0.4717 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00868 loss_train: 0.4687 loss_rec: 0.4687 acc_train: 0.7872 loss_val: 0.4716 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00869 loss_train: 0.4686 loss_rec: 0.4686 acc_train: 0.7873 loss_val: 0.4714 acc_val: 0.7846 time: 0.0020s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00870 loss_train: 0.4684 loss_rec: 0.4684 acc_train: 0.7872 loss_val: 0.4713 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00871 loss_train: 0.4683 loss_rec: 0.4683 acc_train: 0.7872 loss_val: 0.4712 acc_val: 0.7846 time: 0.0015s\n",
      "Test set results: loss= 0.4787 accuracy= 0.7840\n",
      "Epoch: 00872 loss_train: 0.4681 loss_rec: 0.4681 acc_train: 0.7872 loss_val: 0.4710 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00873 loss_train: 0.4680 loss_rec: 0.4680 acc_train: 0.7872 loss_val: 0.4709 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00874 loss_train: 0.4678 loss_rec: 0.4678 acc_train: 0.7872 loss_val: 0.4707 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00875 loss_train: 0.4677 loss_rec: 0.4677 acc_train: 0.7872 loss_val: 0.4706 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00876 loss_train: 0.4675 loss_rec: 0.4675 acc_train: 0.7872 loss_val: 0.4705 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00877 loss_train: 0.4674 loss_rec: 0.4674 acc_train: 0.7872 loss_val: 0.4703 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00878 loss_train: 0.4672 loss_rec: 0.4672 acc_train: 0.7872 loss_val: 0.4702 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00879 loss_train: 0.4671 loss_rec: 0.4671 acc_train: 0.7871 loss_val: 0.4701 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00880 loss_train: 0.4670 loss_rec: 0.4670 acc_train: 0.7871 loss_val: 0.4700 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00881 loss_train: 0.4668 loss_rec: 0.4668 acc_train: 0.7871 loss_val: 0.4698 acc_val: 0.7842 time: 0.0020s\n",
      "Test set results: loss= 0.4774 accuracy= 0.7841\n",
      "Epoch: 00882 loss_train: 0.4667 loss_rec: 0.4667 acc_train: 0.7872 loss_val: 0.4696 acc_val: 0.7842 time: 0.0015s\n",
      "Epoch: 00883 loss_train: 0.4665 loss_rec: 0.4665 acc_train: 0.7872 loss_val: 0.4695 acc_val: 0.7842 time: 0.0020s\n",
      "Epoch: 00884 loss_train: 0.4664 loss_rec: 0.4664 acc_train: 0.7872 loss_val: 0.4694 acc_val: 0.7842 time: 0.0015s\n",
      "Epoch: 00885 loss_train: 0.4663 loss_rec: 0.4663 acc_train: 0.7872 loss_val: 0.4692 acc_val: 0.7842 time: 0.0015s\n",
      "Epoch: 00886 loss_train: 0.4661 loss_rec: 0.4661 acc_train: 0.7874 loss_val: 0.4691 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00887 loss_train: 0.4660 loss_rec: 0.4660 acc_train: 0.7874 loss_val: 0.4690 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00888 loss_train: 0.4658 loss_rec: 0.4658 acc_train: 0.7874 loss_val: 0.4688 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00889 loss_train: 0.4657 loss_rec: 0.4657 acc_train: 0.7874 loss_val: 0.4687 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00890 loss_train: 0.4656 loss_rec: 0.4656 acc_train: 0.7874 loss_val: 0.4686 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00891 loss_train: 0.4654 loss_rec: 0.4654 acc_train: 0.7874 loss_val: 0.4684 acc_val: 0.7850 time: 0.0015s\n",
      "Test set results: loss= 0.4762 accuracy= 0.7846\n",
      "Epoch: 00892 loss_train: 0.4653 loss_rec: 0.4653 acc_train: 0.7874 loss_val: 0.4683 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00893 loss_train: 0.4652 loss_rec: 0.4652 acc_train: 0.7874 loss_val: 0.4682 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00894 loss_train: 0.4650 loss_rec: 0.4650 acc_train: 0.7874 loss_val: 0.4681 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00895 loss_train: 0.4649 loss_rec: 0.4649 acc_train: 0.7874 loss_val: 0.4679 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00896 loss_train: 0.4648 loss_rec: 0.4648 acc_train: 0.7875 loss_val: 0.4678 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00897 loss_train: 0.4646 loss_rec: 0.4646 acc_train: 0.7875 loss_val: 0.4677 acc_val: 0.7850 time: 0.0025s\n",
      "Epoch: 00898 loss_train: 0.4645 loss_rec: 0.4645 acc_train: 0.7873 loss_val: 0.4675 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00899 loss_train: 0.4644 loss_rec: 0.4644 acc_train: 0.7872 loss_val: 0.4674 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00900 loss_train: 0.4642 loss_rec: 0.4642 acc_train: 0.7872 loss_val: 0.4673 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00901 loss_train: 0.4641 loss_rec: 0.4641 acc_train: 0.7872 loss_val: 0.4671 acc_val: 0.7846 time: 0.0020s\n",
      "Test set results: loss= 0.4750 accuracy= 0.7847\n",
      "Epoch: 00902 loss_train: 0.4640 loss_rec: 0.4640 acc_train: 0.7872 loss_val: 0.4670 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00903 loss_train: 0.4639 loss_rec: 0.4639 acc_train: 0.7873 loss_val: 0.4669 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00904 loss_train: 0.4637 loss_rec: 0.4637 acc_train: 0.7873 loss_val: 0.4668 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00905 loss_train: 0.4636 loss_rec: 0.4636 acc_train: 0.7873 loss_val: 0.4666 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00906 loss_train: 0.4634 loss_rec: 0.4634 acc_train: 0.7875 loss_val: 0.4665 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00907 loss_train: 0.4633 loss_rec: 0.4633 acc_train: 0.7875 loss_val: 0.4664 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00908 loss_train: 0.4632 loss_rec: 0.4632 acc_train: 0.7875 loss_val: 0.4662 acc_val: 0.7850 time: 0.0016s\n",
      "Epoch: 00909 loss_train: 0.4630 loss_rec: 0.4630 acc_train: 0.7873 loss_val: 0.4661 acc_val: 0.7850 time: 0.0019s\n",
      "Epoch: 00910 loss_train: 0.4629 loss_rec: 0.4629 acc_train: 0.7873 loss_val: 0.4660 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00911 loss_train: 0.4628 loss_rec: 0.4628 acc_train: 0.7873 loss_val: 0.4659 acc_val: 0.7850 time: 0.0015s\n",
      "Test set results: loss= 0.4739 accuracy= 0.7853\n",
      "Epoch: 00912 loss_train: 0.4627 loss_rec: 0.4627 acc_train: 0.7873 loss_val: 0.4658 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00913 loss_train: 0.4625 loss_rec: 0.4625 acc_train: 0.7873 loss_val: 0.4656 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00914 loss_train: 0.4624 loss_rec: 0.4624 acc_train: 0.7873 loss_val: 0.4655 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00915 loss_train: 0.4623 loss_rec: 0.4623 acc_train: 0.7873 loss_val: 0.4654 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00916 loss_train: 0.4621 loss_rec: 0.4621 acc_train: 0.7873 loss_val: 0.4653 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00917 loss_train: 0.4620 loss_rec: 0.4620 acc_train: 0.7873 loss_val: 0.4651 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00918 loss_train: 0.4619 loss_rec: 0.4619 acc_train: 0.7873 loss_val: 0.4650 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00919 loss_train: 0.4617 loss_rec: 0.4617 acc_train: 0.7873 loss_val: 0.4649 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00920 loss_train: 0.4616 loss_rec: 0.4616 acc_train: 0.7873 loss_val: 0.4648 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00921 loss_train: 0.4615 loss_rec: 0.4615 acc_train: 0.7874 loss_val: 0.4646 acc_val: 0.7854 time: 0.0020s\n",
      "Test set results: loss= 0.4728 accuracy= 0.7851\n",
      "Epoch: 00922 loss_train: 0.4614 loss_rec: 0.4614 acc_train: 0.7874 loss_val: 0.4645 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00923 loss_train: 0.4612 loss_rec: 0.4612 acc_train: 0.7873 loss_val: 0.4644 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00924 loss_train: 0.4611 loss_rec: 0.4611 acc_train: 0.7871 loss_val: 0.4643 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00925 loss_train: 0.4610 loss_rec: 0.4610 acc_train: 0.7871 loss_val: 0.4642 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00926 loss_train: 0.4609 loss_rec: 0.4609 acc_train: 0.7872 loss_val: 0.4640 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00927 loss_train: 0.4607 loss_rec: 0.4607 acc_train: 0.7872 loss_val: 0.4639 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00928 loss_train: 0.4606 loss_rec: 0.4606 acc_train: 0.7872 loss_val: 0.4638 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00929 loss_train: 0.4605 loss_rec: 0.4605 acc_train: 0.7872 loss_val: 0.4637 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00930 loss_train: 0.4604 loss_rec: 0.4604 acc_train: 0.7872 loss_val: 0.4636 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00931 loss_train: 0.4602 loss_rec: 0.4602 acc_train: 0.7873 loss_val: 0.4634 acc_val: 0.7854 time: 0.0015s\n",
      "Test set results: loss= 0.4717 accuracy= 0.7851\n",
      "Epoch: 00932 loss_train: 0.4601 loss_rec: 0.4601 acc_train: 0.7873 loss_val: 0.4634 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00933 loss_train: 0.4600 loss_rec: 0.4600 acc_train: 0.7872 loss_val: 0.4632 acc_val: 0.7858 time: 0.0020s\n",
      "Epoch: 00934 loss_train: 0.4599 loss_rec: 0.4599 acc_train: 0.7872 loss_val: 0.4631 acc_val: 0.7858 time: 0.0015s\n",
      "Epoch: 00935 loss_train: 0.4597 loss_rec: 0.4597 acc_train: 0.7872 loss_val: 0.4630 acc_val: 0.7858 time: 0.0015s\n",
      "Epoch: 00936 loss_train: 0.4596 loss_rec: 0.4596 acc_train: 0.7872 loss_val: 0.4628 acc_val: 0.7858 time: 0.0020s\n",
      "Epoch: 00937 loss_train: 0.4595 loss_rec: 0.4595 acc_train: 0.7873 loss_val: 0.4627 acc_val: 0.7858 time: 0.0015s\n",
      "Epoch: 00938 loss_train: 0.4593 loss_rec: 0.4593 acc_train: 0.7873 loss_val: 0.4626 acc_val: 0.7858 time: 0.0015s\n",
      "Epoch: 00939 loss_train: 0.4592 loss_rec: 0.4592 acc_train: 0.7873 loss_val: 0.4625 acc_val: 0.7858 time: 0.0020s\n",
      "Epoch: 00940 loss_train: 0.4591 loss_rec: 0.4591 acc_train: 0.7907 loss_val: 0.4624 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 00941 loss_train: 0.4590 loss_rec: 0.4590 acc_train: 0.7907 loss_val: 0.4623 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4707 accuracy= 0.7888\n",
      "Epoch: 00942 loss_train: 0.4589 loss_rec: 0.4589 acc_train: 0.7907 loss_val: 0.4621 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 00943 loss_train: 0.4587 loss_rec: 0.4587 acc_train: 0.7907 loss_val: 0.4620 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 00944 loss_train: 0.4586 loss_rec: 0.4586 acc_train: 0.7907 loss_val: 0.4619 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 00945 loss_train: 0.4585 loss_rec: 0.4585 acc_train: 0.7905 loss_val: 0.4618 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 00946 loss_train: 0.4584 loss_rec: 0.4584 acc_train: 0.7905 loss_val: 0.4617 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 00947 loss_train: 0.4583 loss_rec: 0.4583 acc_train: 0.7905 loss_val: 0.4616 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 00948 loss_train: 0.4581 loss_rec: 0.4581 acc_train: 0.7905 loss_val: 0.4615 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 00949 loss_train: 0.4580 loss_rec: 0.4580 acc_train: 0.7905 loss_val: 0.4613 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 00950 loss_train: 0.4579 loss_rec: 0.4579 acc_train: 0.7905 loss_val: 0.4612 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 00951 loss_train: 0.4578 loss_rec: 0.4578 acc_train: 0.7905 loss_val: 0.4611 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4697 accuracy= 0.7888\n",
      "Epoch: 00952 loss_train: 0.4577 loss_rec: 0.4577 acc_train: 0.7905 loss_val: 0.4610 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 00953 loss_train: 0.4575 loss_rec: 0.4575 acc_train: 0.7903 loss_val: 0.4609 acc_val: 0.7858 time: 0.0015s\n",
      "Epoch: 00954 loss_train: 0.4574 loss_rec: 0.4574 acc_train: 0.7903 loss_val: 0.4608 acc_val: 0.7858 time: 0.0015s\n",
      "Epoch: 00955 loss_train: 0.4573 loss_rec: 0.4573 acc_train: 0.7903 loss_val: 0.4607 acc_val: 0.7858 time: 0.0015s\n",
      "Epoch: 00956 loss_train: 0.4572 loss_rec: 0.4572 acc_train: 0.7903 loss_val: 0.4605 acc_val: 0.7858 time: 0.0015s\n",
      "Epoch: 00957 loss_train: 0.4571 loss_rec: 0.4571 acc_train: 0.7900 loss_val: 0.4604 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00958 loss_train: 0.4569 loss_rec: 0.4569 acc_train: 0.7901 loss_val: 0.4603 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00959 loss_train: 0.4568 loss_rec: 0.4568 acc_train: 0.7902 loss_val: 0.4602 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00960 loss_train: 0.4567 loss_rec: 0.4567 acc_train: 0.7902 loss_val: 0.4601 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00961 loss_train: 0.4566 loss_rec: 0.4566 acc_train: 0.7902 loss_val: 0.4600 acc_val: 0.7854 time: 0.0015s\n",
      "Test set results: loss= 0.4686 accuracy= 0.7879\n",
      "Epoch: 00962 loss_train: 0.4565 loss_rec: 0.4565 acc_train: 0.7902 loss_val: 0.4599 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00963 loss_train: 0.4564 loss_rec: 0.4564 acc_train: 0.7904 loss_val: 0.4598 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00964 loss_train: 0.4562 loss_rec: 0.4562 acc_train: 0.7904 loss_val: 0.4597 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00965 loss_train: 0.4561 loss_rec: 0.4561 acc_train: 0.7904 loss_val: 0.4596 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00966 loss_train: 0.4560 loss_rec: 0.4560 acc_train: 0.7904 loss_val: 0.4595 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00967 loss_train: 0.4559 loss_rec: 0.4559 acc_train: 0.7904 loss_val: 0.4593 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 00968 loss_train: 0.4558 loss_rec: 0.4558 acc_train: 0.7904 loss_val: 0.4592 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 00969 loss_train: 0.4557 loss_rec: 0.4557 acc_train: 0.7904 loss_val: 0.4591 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00970 loss_train: 0.4556 loss_rec: 0.4556 acc_train: 0.7904 loss_val: 0.4590 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00971 loss_train: 0.4555 loss_rec: 0.4555 acc_train: 0.7904 loss_val: 0.4589 acc_val: 0.7850 time: 0.0020s\n",
      "Test set results: loss= 0.4677 accuracy= 0.7879\n",
      "Epoch: 00972 loss_train: 0.4553 loss_rec: 0.4553 acc_train: 0.7904 loss_val: 0.4588 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00973 loss_train: 0.4552 loss_rec: 0.4552 acc_train: 0.7902 loss_val: 0.4587 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00974 loss_train: 0.4551 loss_rec: 0.4551 acc_train: 0.7903 loss_val: 0.4586 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00975 loss_train: 0.4550 loss_rec: 0.4550 acc_train: 0.7903 loss_val: 0.4585 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00976 loss_train: 0.4549 loss_rec: 0.4549 acc_train: 0.7903 loss_val: 0.4584 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00977 loss_train: 0.4548 loss_rec: 0.4548 acc_train: 0.7903 loss_val: 0.4583 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00978 loss_train: 0.4547 loss_rec: 0.4547 acc_train: 0.7901 loss_val: 0.4582 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00979 loss_train: 0.4545 loss_rec: 0.4545 acc_train: 0.7901 loss_val: 0.4581 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 00980 loss_train: 0.4544 loss_rec: 0.4544 acc_train: 0.7901 loss_val: 0.4580 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00981 loss_train: 0.4543 loss_rec: 0.4543 acc_train: 0.7901 loss_val: 0.4579 acc_val: 0.7850 time: 0.0015s\n",
      "Test set results: loss= 0.4667 accuracy= 0.7873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00982 loss_train: 0.4542 loss_rec: 0.4542 acc_train: 0.7901 loss_val: 0.4578 acc_val: 0.7850 time: 0.0030s\n",
      "Epoch: 00983 loss_train: 0.4541 loss_rec: 0.4541 acc_train: 0.7901 loss_val: 0.4577 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00984 loss_train: 0.4540 loss_rec: 0.4540 acc_train: 0.7902 loss_val: 0.4575 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 00985 loss_train: 0.4539 loss_rec: 0.4539 acc_train: 0.7903 loss_val: 0.4574 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 00986 loss_train: 0.4538 loss_rec: 0.4538 acc_train: 0.7903 loss_val: 0.4574 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 00987 loss_train: 0.4537 loss_rec: 0.4537 acc_train: 0.7901 loss_val: 0.4573 acc_val: 0.7842 time: 0.0015s\n",
      "Epoch: 00988 loss_train: 0.4536 loss_rec: 0.4536 acc_train: 0.7901 loss_val: 0.4572 acc_val: 0.7842 time: 0.0030s\n",
      "Epoch: 00989 loss_train: 0.4535 loss_rec: 0.4535 acc_train: 0.7901 loss_val: 0.4571 acc_val: 0.7842 time: 0.0015s\n",
      "Epoch: 00990 loss_train: 0.4534 loss_rec: 0.4534 acc_train: 0.7901 loss_val: 0.4570 acc_val: 0.7842 time: 0.0020s\n",
      "Epoch: 00991 loss_train: 0.4532 loss_rec: 0.4532 acc_train: 0.7901 loss_val: 0.4569 acc_val: 0.7842 time: 0.0015s\n",
      "Test set results: loss= 0.4658 accuracy= 0.7873\n",
      "Epoch: 00992 loss_train: 0.4531 loss_rec: 0.4531 acc_train: 0.7901 loss_val: 0.4567 acc_val: 0.7842 time: 0.0020s\n",
      "Epoch: 00993 loss_train: 0.4530 loss_rec: 0.4530 acc_train: 0.7901 loss_val: 0.4567 acc_val: 0.7842 time: 0.0015s\n",
      "Epoch: 00994 loss_train: 0.4529 loss_rec: 0.4529 acc_train: 0.7901 loss_val: 0.4565 acc_val: 0.7842 time: 0.0015s\n",
      "Epoch: 00995 loss_train: 0.4528 loss_rec: 0.4528 acc_train: 0.7901 loss_val: 0.4564 acc_val: 0.7842 time: 0.0020s\n",
      "Epoch: 00996 loss_train: 0.4527 loss_rec: 0.4527 acc_train: 0.7901 loss_val: 0.4563 acc_val: 0.7842 time: 0.0015s\n",
      "Epoch: 00997 loss_train: 0.4526 loss_rec: 0.4526 acc_train: 0.7901 loss_val: 0.4562 acc_val: 0.7842 time: 0.0015s\n",
      "Epoch: 00998 loss_train: 0.4525 loss_rec: 0.4525 acc_train: 0.7901 loss_val: 0.4562 acc_val: 0.7842 time: 0.0020s\n",
      "Epoch: 00999 loss_train: 0.4524 loss_rec: 0.4524 acc_train: 0.7901 loss_val: 0.4561 acc_val: 0.7842 time: 0.0015s\n",
      "Epoch: 01000 loss_train: 0.4523 loss_rec: 0.4523 acc_train: 0.7901 loss_val: 0.4560 acc_val: 0.7842 time: 0.0020s\n",
      "Epoch: 01001 loss_train: 0.4522 loss_rec: 0.4522 acc_train: 0.7902 loss_val: 0.4559 acc_val: 0.7842 time: 0.0015s\n",
      "Test set results: loss= 0.4650 accuracy= 0.7868\n",
      "Epoch: 01002 loss_train: 0.4521 loss_rec: 0.4521 acc_train: 0.7902 loss_val: 0.4558 acc_val: 0.7842 time: 0.0020s\n",
      "Epoch: 01003 loss_train: 0.4520 loss_rec: 0.4520 acc_train: 0.7902 loss_val: 0.4557 acc_val: 0.7842 time: 0.0020s\n",
      "Epoch: 01004 loss_train: 0.4519 loss_rec: 0.4519 acc_train: 0.7902 loss_val: 0.4556 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 01005 loss_train: 0.4518 loss_rec: 0.4518 acc_train: 0.7902 loss_val: 0.4555 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 01006 loss_train: 0.4517 loss_rec: 0.4517 acc_train: 0.7902 loss_val: 0.4554 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 01007 loss_train: 0.4516 loss_rec: 0.4516 acc_train: 0.7902 loss_val: 0.4553 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 01008 loss_train: 0.4515 loss_rec: 0.4515 acc_train: 0.7902 loss_val: 0.4552 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 01009 loss_train: 0.4514 loss_rec: 0.4514 acc_train: 0.7902 loss_val: 0.4551 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 01010 loss_train: 0.4513 loss_rec: 0.4513 acc_train: 0.7901 loss_val: 0.4550 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 01011 loss_train: 0.4511 loss_rec: 0.4511 acc_train: 0.7900 loss_val: 0.4549 acc_val: 0.7846 time: 0.0015s\n",
      "Test set results: loss= 0.4641 accuracy= 0.7866\n",
      "Epoch: 01012 loss_train: 0.4510 loss_rec: 0.4510 acc_train: 0.7899 loss_val: 0.4548 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 01013 loss_train: 0.4510 loss_rec: 0.4510 acc_train: 0.7899 loss_val: 0.4547 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 01014 loss_train: 0.4508 loss_rec: 0.4508 acc_train: 0.7899 loss_val: 0.4546 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 01015 loss_train: 0.4507 loss_rec: 0.4507 acc_train: 0.7898 loss_val: 0.4545 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 01016 loss_train: 0.4506 loss_rec: 0.4506 acc_train: 0.7898 loss_val: 0.4544 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 01017 loss_train: 0.4505 loss_rec: 0.4505 acc_train: 0.7900 loss_val: 0.4543 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 01018 loss_train: 0.4504 loss_rec: 0.4504 acc_train: 0.7900 loss_val: 0.4542 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01019 loss_train: 0.4503 loss_rec: 0.4503 acc_train: 0.7900 loss_val: 0.4542 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01020 loss_train: 0.4502 loss_rec: 0.4502 acc_train: 0.7900 loss_val: 0.4541 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01021 loss_train: 0.4501 loss_rec: 0.4501 acc_train: 0.7900 loss_val: 0.4540 acc_val: 0.7850 time: 0.0015s\n",
      "Test set results: loss= 0.4633 accuracy= 0.7870\n",
      "Epoch: 01022 loss_train: 0.4500 loss_rec: 0.4500 acc_train: 0.7900 loss_val: 0.4539 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 01023 loss_train: 0.4499 loss_rec: 0.4499 acc_train: 0.7900 loss_val: 0.4538 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01024 loss_train: 0.4498 loss_rec: 0.4498 acc_train: 0.7900 loss_val: 0.4537 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 01025 loss_train: 0.4497 loss_rec: 0.4497 acc_train: 0.7900 loss_val: 0.4536 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01026 loss_train: 0.4496 loss_rec: 0.4496 acc_train: 0.7900 loss_val: 0.4535 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 01027 loss_train: 0.4496 loss_rec: 0.4496 acc_train: 0.7900 loss_val: 0.4534 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01028 loss_train: 0.4495 loss_rec: 0.4495 acc_train: 0.7900 loss_val: 0.4533 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01029 loss_train: 0.4494 loss_rec: 0.4494 acc_train: 0.7900 loss_val: 0.4532 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 01030 loss_train: 0.4493 loss_rec: 0.4493 acc_train: 0.7900 loss_val: 0.4532 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01031 loss_train: 0.4492 loss_rec: 0.4492 acc_train: 0.7900 loss_val: 0.4531 acc_val: 0.7850 time: 0.0015s\n",
      "Test set results: loss= 0.4625 accuracy= 0.7872\n",
      "Epoch: 01032 loss_train: 0.4491 loss_rec: 0.4491 acc_train: 0.7901 loss_val: 0.4530 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01033 loss_train: 0.4490 loss_rec: 0.4490 acc_train: 0.7901 loss_val: 0.4529 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01034 loss_train: 0.4489 loss_rec: 0.4489 acc_train: 0.7901 loss_val: 0.4528 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01035 loss_train: 0.4488 loss_rec: 0.4488 acc_train: 0.7901 loss_val: 0.4527 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 01036 loss_train: 0.4487 loss_rec: 0.4487 acc_train: 0.7902 loss_val: 0.4526 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01037 loss_train: 0.4486 loss_rec: 0.4486 acc_train: 0.7902 loss_val: 0.4525 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 01038 loss_train: 0.4485 loss_rec: 0.4485 acc_train: 0.7902 loss_val: 0.4525 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01039 loss_train: 0.4484 loss_rec: 0.4484 acc_train: 0.7902 loss_val: 0.4524 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01040 loss_train: 0.4483 loss_rec: 0.4483 acc_train: 0.7902 loss_val: 0.4523 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 01041 loss_train: 0.4482 loss_rec: 0.4482 acc_train: 0.7902 loss_val: 0.4522 acc_val: 0.7850 time: 0.0015s\n",
      "Test set results: loss= 0.4617 accuracy= 0.7873\n",
      "Epoch: 01042 loss_train: 0.4481 loss_rec: 0.4481 acc_train: 0.7902 loss_val: 0.4521 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 01043 loss_train: 0.4480 loss_rec: 0.4480 acc_train: 0.7902 loss_val: 0.4520 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 01044 loss_train: 0.4479 loss_rec: 0.4479 acc_train: 0.7902 loss_val: 0.4519 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 01045 loss_train: 0.4478 loss_rec: 0.4478 acc_train: 0.7902 loss_val: 0.4519 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01046 loss_train: 0.4477 loss_rec: 0.4477 acc_train: 0.7902 loss_val: 0.4518 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01047 loss_train: 0.4476 loss_rec: 0.4476 acc_train: 0.7902 loss_val: 0.4517 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 01048 loss_train: 0.4476 loss_rec: 0.4476 acc_train: 0.7903 loss_val: 0.4516 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01049 loss_train: 0.4475 loss_rec: 0.4475 acc_train: 0.7903 loss_val: 0.4515 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 01050 loss_train: 0.4474 loss_rec: 0.4474 acc_train: 0.7903 loss_val: 0.4514 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01051 loss_train: 0.4473 loss_rec: 0.4473 acc_train: 0.7903 loss_val: 0.4514 acc_val: 0.7854 time: 0.0015s\n",
      "Test set results: loss= 0.4610 accuracy= 0.7874\n",
      "Epoch: 01052 loss_train: 0.4472 loss_rec: 0.4472 acc_train: 0.7903 loss_val: 0.4513 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01053 loss_train: 0.4471 loss_rec: 0.4471 acc_train: 0.7903 loss_val: 0.4512 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01054 loss_train: 0.4470 loss_rec: 0.4470 acc_train: 0.7903 loss_val: 0.4511 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 01055 loss_train: 0.4469 loss_rec: 0.4469 acc_train: 0.7903 loss_val: 0.4510 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01056 loss_train: 0.4468 loss_rec: 0.4468 acc_train: 0.7903 loss_val: 0.4509 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01057 loss_train: 0.4467 loss_rec: 0.4467 acc_train: 0.7903 loss_val: 0.4508 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01058 loss_train: 0.4467 loss_rec: 0.4467 acc_train: 0.7904 loss_val: 0.4508 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 01059 loss_train: 0.4465 loss_rec: 0.4465 acc_train: 0.7904 loss_val: 0.4507 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01060 loss_train: 0.4465 loss_rec: 0.4465 acc_train: 0.7904 loss_val: 0.4506 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01061 loss_train: 0.4464 loss_rec: 0.4464 acc_train: 0.7904 loss_val: 0.4505 acc_val: 0.7854 time: 0.0020s\n",
      "Test set results: loss= 0.4602 accuracy= 0.7874\n",
      "Epoch: 01062 loss_train: 0.4463 loss_rec: 0.4463 acc_train: 0.7904 loss_val: 0.4504 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01063 loss_train: 0.4462 loss_rec: 0.4462 acc_train: 0.7904 loss_val: 0.4504 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 01064 loss_train: 0.4461 loss_rec: 0.4461 acc_train: 0.7904 loss_val: 0.4503 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 01065 loss_train: 0.4460 loss_rec: 0.4460 acc_train: 0.7903 loss_val: 0.4502 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 01066 loss_train: 0.4459 loss_rec: 0.4459 acc_train: 0.7898 loss_val: 0.4501 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01067 loss_train: 0.4458 loss_rec: 0.4458 acc_train: 0.7898 loss_val: 0.4500 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01068 loss_train: 0.4458 loss_rec: 0.4458 acc_train: 0.7898 loss_val: 0.4500 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 01069 loss_train: 0.4457 loss_rec: 0.4457 acc_train: 0.7898 loss_val: 0.4499 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01070 loss_train: 0.4456 loss_rec: 0.4456 acc_train: 0.7898 loss_val: 0.4498 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01071 loss_train: 0.4455 loss_rec: 0.4455 acc_train: 0.7898 loss_val: 0.4497 acc_val: 0.7854 time: 0.0015s\n",
      "Test set results: loss= 0.4596 accuracy= 0.7872\n",
      "Epoch: 01072 loss_train: 0.4454 loss_rec: 0.4454 acc_train: 0.7900 loss_val: 0.4497 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01073 loss_train: 0.4453 loss_rec: 0.4453 acc_train: 0.7900 loss_val: 0.4496 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 01074 loss_train: 0.4452 loss_rec: 0.4452 acc_train: 0.7900 loss_val: 0.4495 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01075 loss_train: 0.4452 loss_rec: 0.4452 acc_train: 0.7902 loss_val: 0.4494 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01076 loss_train: 0.4451 loss_rec: 0.4451 acc_train: 0.7902 loss_val: 0.4493 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01077 loss_train: 0.4450 loss_rec: 0.4450 acc_train: 0.7902 loss_val: 0.4493 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01078 loss_train: 0.4449 loss_rec: 0.4449 acc_train: 0.7901 loss_val: 0.4492 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 01079 loss_train: 0.4448 loss_rec: 0.4448 acc_train: 0.7901 loss_val: 0.4491 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01080 loss_train: 0.4447 loss_rec: 0.4447 acc_train: 0.7902 loss_val: 0.4490 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 01081 loss_train: 0.4446 loss_rec: 0.4446 acc_train: 0.7902 loss_val: 0.4490 acc_val: 0.7854 time: 0.0020s\n",
      "Test set results: loss= 0.4589 accuracy= 0.7872\n",
      "Epoch: 01082 loss_train: 0.4446 loss_rec: 0.4446 acc_train: 0.7902 loss_val: 0.4489 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01083 loss_train: 0.4445 loss_rec: 0.4445 acc_train: 0.7902 loss_val: 0.4488 acc_val: 0.7854 time: 0.0020s\n",
      "Epoch: 01084 loss_train: 0.4444 loss_rec: 0.4444 acc_train: 0.7902 loss_val: 0.4487 acc_val: 0.7854 time: 0.0015s\n",
      "Epoch: 01085 loss_train: 0.4443 loss_rec: 0.4443 acc_train: 0.7901 loss_val: 0.4487 acc_val: 0.7850 time: 0.0020s\n",
      "Epoch: 01086 loss_train: 0.4442 loss_rec: 0.4442 acc_train: 0.7901 loss_val: 0.4486 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01087 loss_train: 0.4441 loss_rec: 0.4441 acc_train: 0.7901 loss_val: 0.4485 acc_val: 0.7850 time: 0.0015s\n",
      "Epoch: 01088 loss_train: 0.4440 loss_rec: 0.4440 acc_train: 0.7901 loss_val: 0.4484 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 01089 loss_train: 0.4440 loss_rec: 0.4440 acc_train: 0.7900 loss_val: 0.4483 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 01090 loss_train: 0.4439 loss_rec: 0.4439 acc_train: 0.7900 loss_val: 0.4483 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 01091 loss_train: 0.4438 loss_rec: 0.4438 acc_train: 0.7900 loss_val: 0.4482 acc_val: 0.7846 time: 0.0015s\n",
      "Test set results: loss= 0.4582 accuracy= 0.7872\n",
      "Epoch: 01092 loss_train: 0.4437 loss_rec: 0.4437 acc_train: 0.7900 loss_val: 0.4481 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 01093 loss_train: 0.4436 loss_rec: 0.4436 acc_train: 0.7900 loss_val: 0.4480 acc_val: 0.7846 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01094 loss_train: 0.4436 loss_rec: 0.4436 acc_train: 0.7900 loss_val: 0.4480 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 01095 loss_train: 0.4435 loss_rec: 0.4435 acc_train: 0.7900 loss_val: 0.4479 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 01096 loss_train: 0.4434 loss_rec: 0.4434 acc_train: 0.7900 loss_val: 0.4478 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 01097 loss_train: 0.4433 loss_rec: 0.4433 acc_train: 0.7900 loss_val: 0.4478 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 01098 loss_train: 0.4432 loss_rec: 0.4432 acc_train: 0.7900 loss_val: 0.4477 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 01099 loss_train: 0.4431 loss_rec: 0.4431 acc_train: 0.7900 loss_val: 0.4476 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 01100 loss_train: 0.4431 loss_rec: 0.4431 acc_train: 0.7900 loss_val: 0.4475 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 01101 loss_train: 0.4430 loss_rec: 0.4430 acc_train: 0.7900 loss_val: 0.4475 acc_val: 0.7846 time: 0.0020s\n",
      "Test set results: loss= 0.4576 accuracy= 0.7871\n",
      "Epoch: 01102 loss_train: 0.4429 loss_rec: 0.4429 acc_train: 0.7900 loss_val: 0.4474 acc_val: 0.7846 time: 0.0015s\n",
      "Epoch: 01103 loss_train: 0.4428 loss_rec: 0.4428 acc_train: 0.7899 loss_val: 0.4473 acc_val: 0.7846 time: 0.0020s\n",
      "Epoch: 01104 loss_train: 0.4427 loss_rec: 0.4427 acc_train: 0.7927 loss_val: 0.4472 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01105 loss_train: 0.4427 loss_rec: 0.4427 acc_train: 0.7927 loss_val: 0.4472 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01106 loss_train: 0.4426 loss_rec: 0.4426 acc_train: 0.7927 loss_val: 0.4471 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01107 loss_train: 0.4425 loss_rec: 0.4425 acc_train: 0.7927 loss_val: 0.4470 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01108 loss_train: 0.4424 loss_rec: 0.4424 acc_train: 0.7927 loss_val: 0.4470 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01109 loss_train: 0.4423 loss_rec: 0.4423 acc_train: 0.7927 loss_val: 0.4469 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01110 loss_train: 0.4423 loss_rec: 0.4423 acc_train: 0.7927 loss_val: 0.4468 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01111 loss_train: 0.4422 loss_rec: 0.4422 acc_train: 0.7927 loss_val: 0.4467 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4570 accuracy= 0.7898\n",
      "Epoch: 01112 loss_train: 0.4421 loss_rec: 0.4421 acc_train: 0.7927 loss_val: 0.4467 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01113 loss_train: 0.4420 loss_rec: 0.4420 acc_train: 0.7923 loss_val: 0.4466 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01114 loss_train: 0.4419 loss_rec: 0.4419 acc_train: 0.7923 loss_val: 0.4465 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01115 loss_train: 0.4419 loss_rec: 0.4419 acc_train: 0.7923 loss_val: 0.4465 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01116 loss_train: 0.4418 loss_rec: 0.4418 acc_train: 0.7923 loss_val: 0.4464 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01117 loss_train: 0.4417 loss_rec: 0.4417 acc_train: 0.7924 loss_val: 0.4463 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01118 loss_train: 0.4416 loss_rec: 0.4416 acc_train: 0.7923 loss_val: 0.4463 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01119 loss_train: 0.4416 loss_rec: 0.4416 acc_train: 0.7923 loss_val: 0.4462 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01120 loss_train: 0.4415 loss_rec: 0.4415 acc_train: 0.7923 loss_val: 0.4462 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01121 loss_train: 0.4414 loss_rec: 0.4414 acc_train: 0.7923 loss_val: 0.4461 acc_val: 0.7871 time: 0.0015s\n",
      "Test set results: loss= 0.4564 accuracy= 0.7892\n",
      "Epoch: 01122 loss_train: 0.4413 loss_rec: 0.4413 acc_train: 0.7923 loss_val: 0.4460 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01123 loss_train: 0.4412 loss_rec: 0.4412 acc_train: 0.7923 loss_val: 0.4460 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01124 loss_train: 0.4412 loss_rec: 0.4412 acc_train: 0.7923 loss_val: 0.4459 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01125 loss_train: 0.4411 loss_rec: 0.4411 acc_train: 0.7922 loss_val: 0.4458 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01126 loss_train: 0.4410 loss_rec: 0.4410 acc_train: 0.7921 loss_val: 0.4458 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01127 loss_train: 0.4409 loss_rec: 0.4409 acc_train: 0.7921 loss_val: 0.4457 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01128 loss_train: 0.4409 loss_rec: 0.4409 acc_train: 0.7921 loss_val: 0.4456 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01129 loss_train: 0.4408 loss_rec: 0.4408 acc_train: 0.7921 loss_val: 0.4456 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01130 loss_train: 0.4407 loss_rec: 0.4407 acc_train: 0.7921 loss_val: 0.4455 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01131 loss_train: 0.4406 loss_rec: 0.4406 acc_train: 0.7921 loss_val: 0.4454 acc_val: 0.7871 time: 0.0015s\n",
      "Test set results: loss= 0.4558 accuracy= 0.7894\n",
      "Epoch: 01132 loss_train: 0.4406 loss_rec: 0.4406 acc_train: 0.7921 loss_val: 0.4454 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01133 loss_train: 0.4405 loss_rec: 0.4405 acc_train: 0.7921 loss_val: 0.4453 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01134 loss_train: 0.4404 loss_rec: 0.4404 acc_train: 0.7921 loss_val: 0.4452 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01135 loss_train: 0.4403 loss_rec: 0.4403 acc_train: 0.7921 loss_val: 0.4452 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01136 loss_train: 0.4403 loss_rec: 0.4403 acc_train: 0.7921 loss_val: 0.4451 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01137 loss_train: 0.4402 loss_rec: 0.4402 acc_train: 0.7921 loss_val: 0.4451 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01138 loss_train: 0.4401 loss_rec: 0.4401 acc_train: 0.7921 loss_val: 0.4450 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01139 loss_train: 0.4400 loss_rec: 0.4400 acc_train: 0.7921 loss_val: 0.4449 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01140 loss_train: 0.4400 loss_rec: 0.4400 acc_train: 0.7921 loss_val: 0.4449 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01141 loss_train: 0.4399 loss_rec: 0.4399 acc_train: 0.7921 loss_val: 0.4448 acc_val: 0.7871 time: 0.0015s\n",
      "Test set results: loss= 0.4552 accuracy= 0.7893\n",
      "Epoch: 01142 loss_train: 0.4398 loss_rec: 0.4398 acc_train: 0.7921 loss_val: 0.4448 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01143 loss_train: 0.4397 loss_rec: 0.4397 acc_train: 0.7921 loss_val: 0.4447 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01144 loss_train: 0.4397 loss_rec: 0.4397 acc_train: 0.7918 loss_val: 0.4447 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01145 loss_train: 0.4396 loss_rec: 0.4396 acc_train: 0.7918 loss_val: 0.4446 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01146 loss_train: 0.4395 loss_rec: 0.4395 acc_train: 0.7917 loss_val: 0.4445 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01147 loss_train: 0.4395 loss_rec: 0.4395 acc_train: 0.7920 loss_val: 0.4444 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01148 loss_train: 0.4394 loss_rec: 0.4394 acc_train: 0.7921 loss_val: 0.4444 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01149 loss_train: 0.4393 loss_rec: 0.4393 acc_train: 0.7921 loss_val: 0.4443 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01150 loss_train: 0.4392 loss_rec: 0.4392 acc_train: 0.7918 loss_val: 0.4443 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01151 loss_train: 0.4392 loss_rec: 0.4392 acc_train: 0.7918 loss_val: 0.4442 acc_val: 0.7871 time: 0.0020s\n",
      "Test set results: loss= 0.4547 accuracy= 0.7891\n",
      "Epoch: 01152 loss_train: 0.4391 loss_rec: 0.4391 acc_train: 0.7918 loss_val: 0.4441 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01153 loss_train: 0.4390 loss_rec: 0.4390 acc_train: 0.7918 loss_val: 0.4441 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01154 loss_train: 0.4389 loss_rec: 0.4389 acc_train: 0.7918 loss_val: 0.4440 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01155 loss_train: 0.4389 loss_rec: 0.4389 acc_train: 0.7920 loss_val: 0.4440 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01156 loss_train: 0.4388 loss_rec: 0.4388 acc_train: 0.7920 loss_val: 0.4439 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01157 loss_train: 0.4387 loss_rec: 0.4387 acc_train: 0.7920 loss_val: 0.4439 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01158 loss_train: 0.4387 loss_rec: 0.4387 acc_train: 0.7906 loss_val: 0.4438 acc_val: 0.7863 time: 0.0020s\n",
      "Epoch: 01159 loss_train: 0.4386 loss_rec: 0.4386 acc_train: 0.7906 loss_val: 0.4437 acc_val: 0.7863 time: 0.0015s\n",
      "Epoch: 01160 loss_train: 0.4385 loss_rec: 0.4385 acc_train: 0.7906 loss_val: 0.4437 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01161 loss_train: 0.4385 loss_rec: 0.4385 acc_train: 0.7906 loss_val: 0.4436 acc_val: 0.7867 time: 0.0015s\n",
      "Test set results: loss= 0.4542 accuracy= 0.7867\n",
      "Epoch: 01162 loss_train: 0.4384 loss_rec: 0.4384 acc_train: 0.7906 loss_val: 0.4436 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01163 loss_train: 0.4383 loss_rec: 0.4383 acc_train: 0.7906 loss_val: 0.4435 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01164 loss_train: 0.4382 loss_rec: 0.4382 acc_train: 0.7906 loss_val: 0.4434 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01165 loss_train: 0.4382 loss_rec: 0.4382 acc_train: 0.7906 loss_val: 0.4434 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01166 loss_train: 0.4381 loss_rec: 0.4381 acc_train: 0.7906 loss_val: 0.4433 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01167 loss_train: 0.4380 loss_rec: 0.4380 acc_train: 0.7906 loss_val: 0.4433 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01168 loss_train: 0.4380 loss_rec: 0.4380 acc_train: 0.7905 loss_val: 0.4432 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01169 loss_train: 0.4379 loss_rec: 0.4379 acc_train: 0.7905 loss_val: 0.4431 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01170 loss_train: 0.4378 loss_rec: 0.4378 acc_train: 0.7905 loss_val: 0.4431 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01171 loss_train: 0.4378 loss_rec: 0.4378 acc_train: 0.7905 loss_val: 0.4430 acc_val: 0.7867 time: 0.0015s\n",
      "Test set results: loss= 0.4537 accuracy= 0.7865\n",
      "Epoch: 01172 loss_train: 0.4377 loss_rec: 0.4377 acc_train: 0.7905 loss_val: 0.4430 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01173 loss_train: 0.4376 loss_rec: 0.4376 acc_train: 0.7905 loss_val: 0.4429 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01174 loss_train: 0.4376 loss_rec: 0.4376 acc_train: 0.7905 loss_val: 0.4429 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01175 loss_train: 0.4375 loss_rec: 0.4375 acc_train: 0.7903 loss_val: 0.4428 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01176 loss_train: 0.4374 loss_rec: 0.4374 acc_train: 0.7903 loss_val: 0.4427 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01177 loss_train: 0.4374 loss_rec: 0.4374 acc_train: 0.7903 loss_val: 0.4427 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01178 loss_train: 0.4373 loss_rec: 0.4373 acc_train: 0.7903 loss_val: 0.4426 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01179 loss_train: 0.4372 loss_rec: 0.4372 acc_train: 0.7903 loss_val: 0.4426 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01180 loss_train: 0.4372 loss_rec: 0.4372 acc_train: 0.7903 loss_val: 0.4425 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01181 loss_train: 0.4371 loss_rec: 0.4371 acc_train: 0.7903 loss_val: 0.4425 acc_val: 0.7875 time: 0.0020s\n",
      "Test set results: loss= 0.4532 accuracy= 0.7865\n",
      "Epoch: 01182 loss_train: 0.4370 loss_rec: 0.4370 acc_train: 0.7903 loss_val: 0.4424 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01183 loss_train: 0.4370 loss_rec: 0.4370 acc_train: 0.7904 loss_val: 0.4423 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01184 loss_train: 0.4369 loss_rec: 0.4369 acc_train: 0.7903 loss_val: 0.4423 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01185 loss_train: 0.4368 loss_rec: 0.4368 acc_train: 0.7903 loss_val: 0.4423 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01186 loss_train: 0.4368 loss_rec: 0.4368 acc_train: 0.7904 loss_val: 0.4422 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01187 loss_train: 0.4367 loss_rec: 0.4367 acc_train: 0.7904 loss_val: 0.4421 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01188 loss_train: 0.4366 loss_rec: 0.4366 acc_train: 0.7904 loss_val: 0.4421 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01189 loss_train: 0.4366 loss_rec: 0.4366 acc_train: 0.7904 loss_val: 0.4420 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01190 loss_train: 0.4365 loss_rec: 0.4365 acc_train: 0.7904 loss_val: 0.4420 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01191 loss_train: 0.4364 loss_rec: 0.4364 acc_train: 0.7905 loss_val: 0.4419 acc_val: 0.7871 time: 0.0020s\n",
      "Test set results: loss= 0.4527 accuracy= 0.7865\n",
      "Epoch: 01192 loss_train: 0.4364 loss_rec: 0.4364 acc_train: 0.7905 loss_val: 0.4419 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01193 loss_train: 0.4363 loss_rec: 0.4363 acc_train: 0.7905 loss_val: 0.4418 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01194 loss_train: 0.4362 loss_rec: 0.4362 acc_train: 0.7913 loss_val: 0.4417 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01195 loss_train: 0.4362 loss_rec: 0.4362 acc_train: 0.7913 loss_val: 0.4417 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01196 loss_train: 0.4361 loss_rec: 0.4361 acc_train: 0.7913 loss_val: 0.4416 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01197 loss_train: 0.4360 loss_rec: 0.4360 acc_train: 0.7913 loss_val: 0.4416 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01198 loss_train: 0.4360 loss_rec: 0.4360 acc_train: 0.7913 loss_val: 0.4416 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01199 loss_train: 0.4359 loss_rec: 0.4359 acc_train: 0.7913 loss_val: 0.4415 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01200 loss_train: 0.4359 loss_rec: 0.4359 acc_train: 0.7913 loss_val: 0.4414 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01201 loss_train: 0.4358 loss_rec: 0.4358 acc_train: 0.7913 loss_val: 0.4414 acc_val: 0.7883 time: 0.0020s\n",
      "Test set results: loss= 0.4523 accuracy= 0.7875\n",
      "Epoch: 01202 loss_train: 0.4357 loss_rec: 0.4357 acc_train: 0.7913 loss_val: 0.4413 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01203 loss_train: 0.4357 loss_rec: 0.4357 acc_train: 0.7913 loss_val: 0.4413 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01204 loss_train: 0.4356 loss_rec: 0.4356 acc_train: 0.7913 loss_val: 0.4412 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01205 loss_train: 0.4356 loss_rec: 0.4356 acc_train: 0.7913 loss_val: 0.4412 acc_val: 0.7883 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01206 loss_train: 0.4355 loss_rec: 0.4355 acc_train: 0.7913 loss_val: 0.4411 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01207 loss_train: 0.4354 loss_rec: 0.4354 acc_train: 0.7913 loss_val: 0.4411 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01208 loss_train: 0.4354 loss_rec: 0.4354 acc_train: 0.7913 loss_val: 0.4410 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01209 loss_train: 0.4353 loss_rec: 0.4353 acc_train: 0.7913 loss_val: 0.4410 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01210 loss_train: 0.4352 loss_rec: 0.4352 acc_train: 0.7913 loss_val: 0.4409 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01211 loss_train: 0.4352 loss_rec: 0.4352 acc_train: 0.7913 loss_val: 0.4409 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4518 accuracy= 0.7877\n",
      "Epoch: 01212 loss_train: 0.4351 loss_rec: 0.4351 acc_train: 0.7913 loss_val: 0.4408 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01213 loss_train: 0.4350 loss_rec: 0.4350 acc_train: 0.7913 loss_val: 0.4408 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01214 loss_train: 0.4350 loss_rec: 0.4350 acc_train: 0.7913 loss_val: 0.4407 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01215 loss_train: 0.4349 loss_rec: 0.4349 acc_train: 0.7913 loss_val: 0.4407 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01216 loss_train: 0.4349 loss_rec: 0.4349 acc_train: 0.7913 loss_val: 0.4406 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01217 loss_train: 0.4348 loss_rec: 0.4348 acc_train: 0.7913 loss_val: 0.4405 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01218 loss_train: 0.4347 loss_rec: 0.4347 acc_train: 0.7913 loss_val: 0.4405 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01219 loss_train: 0.4347 loss_rec: 0.4347 acc_train: 0.7913 loss_val: 0.4404 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01220 loss_train: 0.4346 loss_rec: 0.4346 acc_train: 0.7913 loss_val: 0.4404 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01221 loss_train: 0.4346 loss_rec: 0.4346 acc_train: 0.7913 loss_val: 0.4404 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4514 accuracy= 0.7877\n",
      "Epoch: 01222 loss_train: 0.4345 loss_rec: 0.4345 acc_train: 0.7913 loss_val: 0.4403 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01223 loss_train: 0.4344 loss_rec: 0.4344 acc_train: 0.7913 loss_val: 0.4403 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 01224 loss_train: 0.4344 loss_rec: 0.4344 acc_train: 0.7913 loss_val: 0.4402 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 01225 loss_train: 0.4343 loss_rec: 0.4343 acc_train: 0.7913 loss_val: 0.4402 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 01226 loss_train: 0.4343 loss_rec: 0.4343 acc_train: 0.7916 loss_val: 0.4401 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 01227 loss_train: 0.4342 loss_rec: 0.4342 acc_train: 0.7916 loss_val: 0.4401 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 01228 loss_train: 0.4342 loss_rec: 0.4342 acc_train: 0.7916 loss_val: 0.4400 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 01229 loss_train: 0.4341 loss_rec: 0.4341 acc_train: 0.7915 loss_val: 0.4400 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 01230 loss_train: 0.4340 loss_rec: 0.4340 acc_train: 0.7915 loss_val: 0.4399 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 01231 loss_train: 0.4340 loss_rec: 0.4340 acc_train: 0.7915 loss_val: 0.4399 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4510 accuracy= 0.7883\n",
      "Epoch: 01232 loss_train: 0.4339 loss_rec: 0.4339 acc_train: 0.7915 loss_val: 0.4398 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 01233 loss_train: 0.4339 loss_rec: 0.4339 acc_train: 0.7915 loss_val: 0.4398 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 01234 loss_train: 0.4338 loss_rec: 0.4338 acc_train: 0.7915 loss_val: 0.4397 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 01235 loss_train: 0.4337 loss_rec: 0.4337 acc_train: 0.7915 loss_val: 0.4397 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 01236 loss_train: 0.4337 loss_rec: 0.4337 acc_train: 0.7915 loss_val: 0.4396 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 01237 loss_train: 0.4336 loss_rec: 0.4336 acc_train: 0.7915 loss_val: 0.4396 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 01238 loss_train: 0.4336 loss_rec: 0.4336 acc_train: 0.7915 loss_val: 0.4395 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01239 loss_train: 0.4335 loss_rec: 0.4335 acc_train: 0.7916 loss_val: 0.4395 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01240 loss_train: 0.4335 loss_rec: 0.4335 acc_train: 0.7917 loss_val: 0.4394 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01241 loss_train: 0.4334 loss_rec: 0.4334 acc_train: 0.7917 loss_val: 0.4394 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4506 accuracy= 0.7879\n",
      "Epoch: 01242 loss_train: 0.4334 loss_rec: 0.4334 acc_train: 0.7917 loss_val: 0.4394 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01243 loss_train: 0.4333 loss_rec: 0.4333 acc_train: 0.7916 loss_val: 0.4393 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01244 loss_train: 0.4332 loss_rec: 0.4332 acc_train: 0.7911 loss_val: 0.4392 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01245 loss_train: 0.4332 loss_rec: 0.4332 acc_train: 0.7911 loss_val: 0.4392 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01246 loss_train: 0.4331 loss_rec: 0.4331 acc_train: 0.7911 loss_val: 0.4392 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01247 loss_train: 0.4331 loss_rec: 0.4331 acc_train: 0.7910 loss_val: 0.4391 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01248 loss_train: 0.4330 loss_rec: 0.4330 acc_train: 0.7909 loss_val: 0.4391 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01249 loss_train: 0.4330 loss_rec: 0.4330 acc_train: 0.7909 loss_val: 0.4391 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01250 loss_train: 0.4329 loss_rec: 0.4329 acc_train: 0.7910 loss_val: 0.4390 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01251 loss_train: 0.4328 loss_rec: 0.4328 acc_train: 0.7910 loss_val: 0.4389 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4502 accuracy= 0.7877\n",
      "Epoch: 01252 loss_train: 0.4328 loss_rec: 0.4328 acc_train: 0.7911 loss_val: 0.4389 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01253 loss_train: 0.4327 loss_rec: 0.4327 acc_train: 0.7912 loss_val: 0.4388 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01254 loss_train: 0.4327 loss_rec: 0.4327 acc_train: 0.7915 loss_val: 0.4388 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 01255 loss_train: 0.4326 loss_rec: 0.4326 acc_train: 0.7915 loss_val: 0.4388 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 01256 loss_train: 0.4326 loss_rec: 0.4326 acc_train: 0.7915 loss_val: 0.4387 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 01257 loss_train: 0.4325 loss_rec: 0.4325 acc_train: 0.7917 loss_val: 0.4387 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01258 loss_train: 0.4325 loss_rec: 0.4325 acc_train: 0.7917 loss_val: 0.4386 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01259 loss_train: 0.4324 loss_rec: 0.4324 acc_train: 0.7918 loss_val: 0.4386 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01260 loss_train: 0.4323 loss_rec: 0.4323 acc_train: 0.7918 loss_val: 0.4385 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01261 loss_train: 0.4323 loss_rec: 0.4323 acc_train: 0.7918 loss_val: 0.4385 acc_val: 0.7892 time: 0.0015s\n",
      "Test set results: loss= 0.4499 accuracy= 0.7888\n",
      "Epoch: 01262 loss_train: 0.4322 loss_rec: 0.4322 acc_train: 0.7918 loss_val: 0.4384 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01263 loss_train: 0.4322 loss_rec: 0.4322 acc_train: 0.7918 loss_val: 0.4384 acc_val: 0.7892 time: 0.0020s\n",
      "Epoch: 01264 loss_train: 0.4321 loss_rec: 0.4321 acc_train: 0.7918 loss_val: 0.4383 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01265 loss_train: 0.4321 loss_rec: 0.4321 acc_train: 0.7918 loss_val: 0.4383 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01266 loss_train: 0.4320 loss_rec: 0.4320 acc_train: 0.7918 loss_val: 0.4383 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01267 loss_train: 0.4320 loss_rec: 0.4320 acc_train: 0.7917 loss_val: 0.4382 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01268 loss_train: 0.4319 loss_rec: 0.4319 acc_train: 0.7917 loss_val: 0.4382 acc_val: 0.7892 time: 0.0020s\n",
      "Epoch: 01269 loss_train: 0.4319 loss_rec: 0.4319 acc_train: 0.7917 loss_val: 0.4381 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01270 loss_train: 0.4318 loss_rec: 0.4318 acc_train: 0.7917 loss_val: 0.4381 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01271 loss_train: 0.4318 loss_rec: 0.4318 acc_train: 0.7917 loss_val: 0.4381 acc_val: 0.7892 time: 0.0015s\n",
      "Test set results: loss= 0.4495 accuracy= 0.7883\n",
      "Epoch: 01272 loss_train: 0.4317 loss_rec: 0.4317 acc_train: 0.7917 loss_val: 0.4380 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01273 loss_train: 0.4317 loss_rec: 0.4317 acc_train: 0.7917 loss_val: 0.4380 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01274 loss_train: 0.4316 loss_rec: 0.4316 acc_train: 0.7917 loss_val: 0.4379 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01275 loss_train: 0.4315 loss_rec: 0.4315 acc_train: 0.7917 loss_val: 0.4379 acc_val: 0.7892 time: 0.0020s\n",
      "Epoch: 01276 loss_train: 0.4315 loss_rec: 0.4315 acc_train: 0.7917 loss_val: 0.4378 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01277 loss_train: 0.4314 loss_rec: 0.4314 acc_train: 0.7917 loss_val: 0.4378 acc_val: 0.7892 time: 0.0020s\n",
      "Epoch: 01278 loss_train: 0.4314 loss_rec: 0.4314 acc_train: 0.7917 loss_val: 0.4377 acc_val: 0.7892 time: 0.0020s\n",
      "Epoch: 01279 loss_train: 0.4313 loss_rec: 0.4313 acc_train: 0.7917 loss_val: 0.4377 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01280 loss_train: 0.4313 loss_rec: 0.4313 acc_train: 0.7917 loss_val: 0.4377 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01281 loss_train: 0.4312 loss_rec: 0.4312 acc_train: 0.7917 loss_val: 0.4376 acc_val: 0.7892 time: 0.0020s\n",
      "Test set results: loss= 0.4491 accuracy= 0.7883\n",
      "Epoch: 01282 loss_train: 0.4312 loss_rec: 0.4312 acc_train: 0.7917 loss_val: 0.4376 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01283 loss_train: 0.4311 loss_rec: 0.4311 acc_train: 0.7917 loss_val: 0.4375 acc_val: 0.7892 time: 0.0020s\n",
      "Epoch: 01284 loss_train: 0.4311 loss_rec: 0.4311 acc_train: 0.7917 loss_val: 0.4375 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01285 loss_train: 0.4310 loss_rec: 0.4310 acc_train: 0.7917 loss_val: 0.4374 acc_val: 0.7892 time: 0.0020s\n",
      "Epoch: 01286 loss_train: 0.4310 loss_rec: 0.4310 acc_train: 0.7917 loss_val: 0.4374 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01287 loss_train: 0.4309 loss_rec: 0.4309 acc_train: 0.7917 loss_val: 0.4374 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01288 loss_train: 0.4309 loss_rec: 0.4309 acc_train: 0.7917 loss_val: 0.4373 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01289 loss_train: 0.4308 loss_rec: 0.4308 acc_train: 0.7918 loss_val: 0.4373 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01290 loss_train: 0.4308 loss_rec: 0.4308 acc_train: 0.7918 loss_val: 0.4372 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01291 loss_train: 0.4307 loss_rec: 0.4307 acc_train: 0.7918 loss_val: 0.4372 acc_val: 0.7892 time: 0.0020s\n",
      "Test set results: loss= 0.4488 accuracy= 0.7883\n",
      "Epoch: 01292 loss_train: 0.4306 loss_rec: 0.4306 acc_train: 0.7918 loss_val: 0.4371 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01293 loss_train: 0.4306 loss_rec: 0.4306 acc_train: 0.7918 loss_val: 0.4371 acc_val: 0.7892 time: 0.0020s\n",
      "Epoch: 01294 loss_train: 0.4305 loss_rec: 0.4305 acc_train: 0.7922 loss_val: 0.4371 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01295 loss_train: 0.4305 loss_rec: 0.4305 acc_train: 0.7922 loss_val: 0.4370 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01296 loss_train: 0.4304 loss_rec: 0.4304 acc_train: 0.7922 loss_val: 0.4370 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01297 loss_train: 0.4304 loss_rec: 0.4304 acc_train: 0.7922 loss_val: 0.4369 acc_val: 0.7900 time: 0.0015s\n",
      "Epoch: 01298 loss_train: 0.4303 loss_rec: 0.4303 acc_train: 0.7922 loss_val: 0.4369 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01299 loss_train: 0.4303 loss_rec: 0.4303 acc_train: 0.7922 loss_val: 0.4369 acc_val: 0.7900 time: 0.0015s\n",
      "Epoch: 01300 loss_train: 0.4302 loss_rec: 0.4302 acc_train: 0.7922 loss_val: 0.4368 acc_val: 0.7900 time: 0.0015s\n",
      "Epoch: 01301 loss_train: 0.4302 loss_rec: 0.4302 acc_train: 0.7922 loss_val: 0.4368 acc_val: 0.7900 time: 0.0020s\n",
      "Test set results: loss= 0.4485 accuracy= 0.7887\n",
      "Epoch: 01302 loss_train: 0.4302 loss_rec: 0.4302 acc_train: 0.7923 loss_val: 0.4367 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01303 loss_train: 0.4301 loss_rec: 0.4301 acc_train: 0.7923 loss_val: 0.4367 acc_val: 0.7900 time: 0.0015s\n",
      "Epoch: 01304 loss_train: 0.4301 loss_rec: 0.4301 acc_train: 0.7923 loss_val: 0.4367 acc_val: 0.7900 time: 0.0015s\n",
      "Epoch: 01305 loss_train: 0.4300 loss_rec: 0.4300 acc_train: 0.7923 loss_val: 0.4366 acc_val: 0.7900 time: 0.0015s\n",
      "Epoch: 01306 loss_train: 0.4300 loss_rec: 0.4300 acc_train: 0.7923 loss_val: 0.4366 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01307 loss_train: 0.4299 loss_rec: 0.4299 acc_train: 0.7923 loss_val: 0.4365 acc_val: 0.7900 time: 0.0015s\n",
      "Epoch: 01308 loss_train: 0.4298 loss_rec: 0.4298 acc_train: 0.7923 loss_val: 0.4365 acc_val: 0.7900 time: 0.0015s\n",
      "Epoch: 01309 loss_train: 0.4298 loss_rec: 0.4298 acc_train: 0.7923 loss_val: 0.4365 acc_val: 0.7900 time: 0.0015s\n",
      "Epoch: 01310 loss_train: 0.4297 loss_rec: 0.4297 acc_train: 0.7923 loss_val: 0.4364 acc_val: 0.7900 time: 0.0015s\n",
      "Epoch: 01311 loss_train: 0.4297 loss_rec: 0.4297 acc_train: 0.7923 loss_val: 0.4364 acc_val: 0.7900 time: 0.0020s\n",
      "Test set results: loss= 0.4482 accuracy= 0.7887\n",
      "Epoch: 01312 loss_train: 0.4296 loss_rec: 0.4296 acc_train: 0.7923 loss_val: 0.4363 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01313 loss_train: 0.4296 loss_rec: 0.4296 acc_train: 0.7923 loss_val: 0.4363 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01314 loss_train: 0.4295 loss_rec: 0.4295 acc_train: 0.7923 loss_val: 0.4362 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01315 loss_train: 0.4295 loss_rec: 0.4295 acc_train: 0.7924 loss_val: 0.4362 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01316 loss_train: 0.4294 loss_rec: 0.4294 acc_train: 0.7924 loss_val: 0.4362 acc_val: 0.7904 time: 0.0020s\n",
      "Epoch: 01317 loss_train: 0.4294 loss_rec: 0.4294 acc_train: 0.7923 loss_val: 0.4361 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01318 loss_train: 0.4293 loss_rec: 0.4293 acc_train: 0.7923 loss_val: 0.4361 acc_val: 0.7904 time: 0.0020s\n",
      "Epoch: 01319 loss_train: 0.4293 loss_rec: 0.4293 acc_train: 0.7923 loss_val: 0.4361 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01320 loss_train: 0.4292 loss_rec: 0.4292 acc_train: 0.7923 loss_val: 0.4360 acc_val: 0.7908 time: 0.0020s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01321 loss_train: 0.4292 loss_rec: 0.4292 acc_train: 0.7922 loss_val: 0.4360 acc_val: 0.7908 time: 0.0020s\n",
      "Test set results: loss= 0.4478 accuracy= 0.7886\n",
      "Epoch: 01322 loss_train: 0.4291 loss_rec: 0.4291 acc_train: 0.7922 loss_val: 0.4359 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01323 loss_train: 0.4291 loss_rec: 0.4291 acc_train: 0.7922 loss_val: 0.4359 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01324 loss_train: 0.4291 loss_rec: 0.4291 acc_train: 0.7922 loss_val: 0.4359 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01325 loss_train: 0.4290 loss_rec: 0.4290 acc_train: 0.7921 loss_val: 0.4358 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01326 loss_train: 0.4290 loss_rec: 0.4290 acc_train: 0.7921 loss_val: 0.4358 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01327 loss_train: 0.4289 loss_rec: 0.4289 acc_train: 0.7921 loss_val: 0.4357 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01328 loss_train: 0.4289 loss_rec: 0.4289 acc_train: 0.7921 loss_val: 0.4357 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01329 loss_train: 0.4288 loss_rec: 0.4288 acc_train: 0.7922 loss_val: 0.4357 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01330 loss_train: 0.4288 loss_rec: 0.4288 acc_train: 0.7922 loss_val: 0.4356 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01331 loss_train: 0.4287 loss_rec: 0.4287 acc_train: 0.7922 loss_val: 0.4356 acc_val: 0.7913 time: 0.0015s\n",
      "Test set results: loss= 0.4476 accuracy= 0.7885\n",
      "Epoch: 01332 loss_train: 0.4287 loss_rec: 0.4287 acc_train: 0.7922 loss_val: 0.4355 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01333 loss_train: 0.4286 loss_rec: 0.4286 acc_train: 0.7922 loss_val: 0.4355 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01334 loss_train: 0.4286 loss_rec: 0.4286 acc_train: 0.7922 loss_val: 0.4355 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01335 loss_train: 0.4285 loss_rec: 0.4285 acc_train: 0.7922 loss_val: 0.4354 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01336 loss_train: 0.4285 loss_rec: 0.4285 acc_train: 0.7922 loss_val: 0.4354 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01337 loss_train: 0.4284 loss_rec: 0.4284 acc_train: 0.7922 loss_val: 0.4354 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01338 loss_train: 0.4284 loss_rec: 0.4284 acc_train: 0.7922 loss_val: 0.4353 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01339 loss_train: 0.4284 loss_rec: 0.4284 acc_train: 0.7922 loss_val: 0.4353 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01340 loss_train: 0.4283 loss_rec: 0.4283 acc_train: 0.7922 loss_val: 0.4352 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01341 loss_train: 0.4283 loss_rec: 0.4283 acc_train: 0.7922 loss_val: 0.4352 acc_val: 0.7908 time: 0.0015s\n",
      "Test set results: loss= 0.4473 accuracy= 0.7886\n",
      "Epoch: 01342 loss_train: 0.4282 loss_rec: 0.4282 acc_train: 0.7922 loss_val: 0.4352 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01343 loss_train: 0.4282 loss_rec: 0.4282 acc_train: 0.7922 loss_val: 0.4351 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01344 loss_train: 0.4281 loss_rec: 0.4281 acc_train: 0.7922 loss_val: 0.4351 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01345 loss_train: 0.4281 loss_rec: 0.4281 acc_train: 0.7922 loss_val: 0.4351 acc_val: 0.7904 time: 0.0020s\n",
      "Epoch: 01346 loss_train: 0.4280 loss_rec: 0.4280 acc_train: 0.7922 loss_val: 0.4350 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01347 loss_train: 0.4280 loss_rec: 0.4280 acc_train: 0.7922 loss_val: 0.4350 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01348 loss_train: 0.4279 loss_rec: 0.4279 acc_train: 0.7922 loss_val: 0.4350 acc_val: 0.7904 time: 0.0020s\n",
      "Epoch: 01349 loss_train: 0.4279 loss_rec: 0.4279 acc_train: 0.7922 loss_val: 0.4349 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01350 loss_train: 0.4278 loss_rec: 0.4278 acc_train: 0.7922 loss_val: 0.4349 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01351 loss_train: 0.4278 loss_rec: 0.4278 acc_train: 0.7922 loss_val: 0.4348 acc_val: 0.7904 time: 0.0020s\n",
      "Test set results: loss= 0.4470 accuracy= 0.7886\n",
      "Epoch: 01352 loss_train: 0.4278 loss_rec: 0.4278 acc_train: 0.7922 loss_val: 0.4348 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01353 loss_train: 0.4277 loss_rec: 0.4277 acc_train: 0.7922 loss_val: 0.4348 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01354 loss_train: 0.4277 loss_rec: 0.4277 acc_train: 0.7920 loss_val: 0.4347 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01355 loss_train: 0.4276 loss_rec: 0.4276 acc_train: 0.7921 loss_val: 0.4347 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01356 loss_train: 0.4276 loss_rec: 0.4276 acc_train: 0.7921 loss_val: 0.4347 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01357 loss_train: 0.4275 loss_rec: 0.4275 acc_train: 0.7921 loss_val: 0.4346 acc_val: 0.7904 time: 0.0015s\n",
      "Epoch: 01358 loss_train: 0.4275 loss_rec: 0.4275 acc_train: 0.7922 loss_val: 0.4346 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01359 loss_train: 0.4274 loss_rec: 0.4274 acc_train: 0.7922 loss_val: 0.4346 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01360 loss_train: 0.4274 loss_rec: 0.4274 acc_train: 0.7922 loss_val: 0.4345 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01361 loss_train: 0.4274 loss_rec: 0.4274 acc_train: 0.7922 loss_val: 0.4345 acc_val: 0.7908 time: 0.0015s\n",
      "Test set results: loss= 0.4468 accuracy= 0.7885\n",
      "Epoch: 01362 loss_train: 0.4273 loss_rec: 0.4273 acc_train: 0.7922 loss_val: 0.4345 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01363 loss_train: 0.4273 loss_rec: 0.4273 acc_train: 0.7923 loss_val: 0.4344 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01364 loss_train: 0.4272 loss_rec: 0.4272 acc_train: 0.7923 loss_val: 0.4344 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01365 loss_train: 0.4272 loss_rec: 0.4272 acc_train: 0.7922 loss_val: 0.4343 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01366 loss_train: 0.4271 loss_rec: 0.4271 acc_train: 0.7922 loss_val: 0.4343 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01367 loss_train: 0.4271 loss_rec: 0.4271 acc_train: 0.7922 loss_val: 0.4343 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01368 loss_train: 0.4270 loss_rec: 0.4270 acc_train: 0.7922 loss_val: 0.4342 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01369 loss_train: 0.4270 loss_rec: 0.4270 acc_train: 0.7923 loss_val: 0.4342 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01370 loss_train: 0.4270 loss_rec: 0.4270 acc_train: 0.7923 loss_val: 0.4342 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01371 loss_train: 0.4269 loss_rec: 0.4269 acc_train: 0.7923 loss_val: 0.4341 acc_val: 0.7908 time: 0.0015s\n",
      "Test set results: loss= 0.4465 accuracy= 0.7885\n",
      "Epoch: 01372 loss_train: 0.4269 loss_rec: 0.4269 acc_train: 0.7923 loss_val: 0.4341 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01373 loss_train: 0.4268 loss_rec: 0.4268 acc_train: 0.7923 loss_val: 0.4341 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01374 loss_train: 0.4268 loss_rec: 0.4268 acc_train: 0.7923 loss_val: 0.4340 acc_val: 0.7913 time: 0.0015s\n",
      "Epoch: 01375 loss_train: 0.4268 loss_rec: 0.4268 acc_train: 0.7924 loss_val: 0.4340 acc_val: 0.7913 time: 0.0020s\n",
      "Epoch: 01376 loss_train: 0.4267 loss_rec: 0.4267 acc_train: 0.7925 loss_val: 0.4340 acc_val: 0.7913 time: 0.0015s\n",
      "Epoch: 01377 loss_train: 0.4267 loss_rec: 0.4267 acc_train: 0.7925 loss_val: 0.4339 acc_val: 0.7913 time: 0.0020s\n",
      "Epoch: 01378 loss_train: 0.4266 loss_rec: 0.4266 acc_train: 0.7925 loss_val: 0.4339 acc_val: 0.7913 time: 0.0015s\n",
      "Epoch: 01379 loss_train: 0.4266 loss_rec: 0.4266 acc_train: 0.7925 loss_val: 0.4339 acc_val: 0.7913 time: 0.0020s\n",
      "Epoch: 01380 loss_train: 0.4265 loss_rec: 0.4265 acc_train: 0.7925 loss_val: 0.4338 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01381 loss_train: 0.4265 loss_rec: 0.4265 acc_train: 0.7925 loss_val: 0.4338 acc_val: 0.7908 time: 0.0015s\n",
      "Test set results: loss= 0.4463 accuracy= 0.7883\n",
      "Epoch: 01382 loss_train: 0.4265 loss_rec: 0.4265 acc_train: 0.7924 loss_val: 0.4338 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01383 loss_train: 0.4264 loss_rec: 0.4264 acc_train: 0.7924 loss_val: 0.4337 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01384 loss_train: 0.4264 loss_rec: 0.4264 acc_train: 0.7924 loss_val: 0.4337 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01385 loss_train: 0.4263 loss_rec: 0.4263 acc_train: 0.7924 loss_val: 0.4337 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01386 loss_train: 0.4263 loss_rec: 0.4263 acc_train: 0.7924 loss_val: 0.4336 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01387 loss_train: 0.4263 loss_rec: 0.4263 acc_train: 0.7924 loss_val: 0.4336 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01388 loss_train: 0.4262 loss_rec: 0.4262 acc_train: 0.7924 loss_val: 0.4336 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01389 loss_train: 0.4262 loss_rec: 0.4262 acc_train: 0.7924 loss_val: 0.4335 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01390 loss_train: 0.4261 loss_rec: 0.4261 acc_train: 0.7925 loss_val: 0.4335 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01391 loss_train: 0.4261 loss_rec: 0.4261 acc_train: 0.7926 loss_val: 0.4335 acc_val: 0.7908 time: 0.0015s\n",
      "Test set results: loss= 0.4460 accuracy= 0.7883\n",
      "Epoch: 01392 loss_train: 0.4260 loss_rec: 0.4260 acc_train: 0.7926 loss_val: 0.4334 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01393 loss_train: 0.4260 loss_rec: 0.4260 acc_train: 0.7926 loss_val: 0.4334 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01394 loss_train: 0.4260 loss_rec: 0.4260 acc_train: 0.7926 loss_val: 0.4334 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01395 loss_train: 0.4259 loss_rec: 0.4259 acc_train: 0.7926 loss_val: 0.4333 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01396 loss_train: 0.4259 loss_rec: 0.4259 acc_train: 0.7926 loss_val: 0.4333 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01397 loss_train: 0.4258 loss_rec: 0.4258 acc_train: 0.7926 loss_val: 0.4333 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01398 loss_train: 0.4258 loss_rec: 0.4258 acc_train: 0.7926 loss_val: 0.4333 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01399 loss_train: 0.4258 loss_rec: 0.4258 acc_train: 0.7926 loss_val: 0.4332 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01400 loss_train: 0.4257 loss_rec: 0.4257 acc_train: 0.7925 loss_val: 0.4332 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01401 loss_train: 0.4257 loss_rec: 0.4257 acc_train: 0.7926 loss_val: 0.4332 acc_val: 0.7908 time: 0.0015s\n",
      "Test set results: loss= 0.4458 accuracy= 0.7883\n",
      "Epoch: 01402 loss_train: 0.4256 loss_rec: 0.4256 acc_train: 0.7926 loss_val: 0.4331 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01403 loss_train: 0.4256 loss_rec: 0.4256 acc_train: 0.7926 loss_val: 0.4331 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01404 loss_train: 0.4256 loss_rec: 0.4256 acc_train: 0.7926 loss_val: 0.4331 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01405 loss_train: 0.4255 loss_rec: 0.4255 acc_train: 0.7927 loss_val: 0.4330 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01406 loss_train: 0.4255 loss_rec: 0.4255 acc_train: 0.7927 loss_val: 0.4330 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01407 loss_train: 0.4254 loss_rec: 0.4254 acc_train: 0.7927 loss_val: 0.4330 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01408 loss_train: 0.4254 loss_rec: 0.4254 acc_train: 0.7927 loss_val: 0.4329 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01409 loss_train: 0.4254 loss_rec: 0.4254 acc_train: 0.7927 loss_val: 0.4329 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01410 loss_train: 0.4253 loss_rec: 0.4253 acc_train: 0.7927 loss_val: 0.4329 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01411 loss_train: 0.4253 loss_rec: 0.4253 acc_train: 0.7925 loss_val: 0.4329 acc_val: 0.7908 time: 0.0015s\n",
      "Test set results: loss= 0.4456 accuracy= 0.7884\n",
      "Epoch: 01412 loss_train: 0.4252 loss_rec: 0.4252 acc_train: 0.7926 loss_val: 0.4328 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01413 loss_train: 0.4252 loss_rec: 0.4252 acc_train: 0.7927 loss_val: 0.4328 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01414 loss_train: 0.4252 loss_rec: 0.4252 acc_train: 0.7927 loss_val: 0.4328 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01415 loss_train: 0.4251 loss_rec: 0.4251 acc_train: 0.7926 loss_val: 0.4327 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01416 loss_train: 0.4251 loss_rec: 0.4251 acc_train: 0.7926 loss_val: 0.4327 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01417 loss_train: 0.4250 loss_rec: 0.4250 acc_train: 0.7926 loss_val: 0.4327 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01418 loss_train: 0.4250 loss_rec: 0.4250 acc_train: 0.7926 loss_val: 0.4326 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01419 loss_train: 0.4250 loss_rec: 0.4250 acc_train: 0.7926 loss_val: 0.4326 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01420 loss_train: 0.4249 loss_rec: 0.4249 acc_train: 0.7926 loss_val: 0.4326 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01421 loss_train: 0.4249 loss_rec: 0.4249 acc_train: 0.7926 loss_val: 0.4325 acc_val: 0.7908 time: 0.0015s\n",
      "Test set results: loss= 0.4454 accuracy= 0.7881\n",
      "Epoch: 01422 loss_train: 0.4248 loss_rec: 0.4248 acc_train: 0.7926 loss_val: 0.4325 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01423 loss_train: 0.4248 loss_rec: 0.4248 acc_train: 0.7926 loss_val: 0.4325 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01424 loss_train: 0.4248 loss_rec: 0.4248 acc_train: 0.7926 loss_val: 0.4325 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01425 loss_train: 0.4247 loss_rec: 0.4247 acc_train: 0.7926 loss_val: 0.4324 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01426 loss_train: 0.4247 loss_rec: 0.4247 acc_train: 0.7926 loss_val: 0.4324 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01427 loss_train: 0.4247 loss_rec: 0.4247 acc_train: 0.7926 loss_val: 0.4324 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01428 loss_train: 0.4246 loss_rec: 0.4246 acc_train: 0.7925 loss_val: 0.4323 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01429 loss_train: 0.4246 loss_rec: 0.4246 acc_train: 0.7925 loss_val: 0.4323 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01430 loss_train: 0.4245 loss_rec: 0.4245 acc_train: 0.7925 loss_val: 0.4323 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01431 loss_train: 0.4245 loss_rec: 0.4245 acc_train: 0.7925 loss_val: 0.4323 acc_val: 0.7908 time: 0.0020s\n",
      "Test set results: loss= 0.4452 accuracy= 0.7880\n",
      "Epoch: 01432 loss_train: 0.4245 loss_rec: 0.4245 acc_train: 0.7924 loss_val: 0.4322 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01433 loss_train: 0.4244 loss_rec: 0.4244 acc_train: 0.7924 loss_val: 0.4322 acc_val: 0.7908 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01434 loss_train: 0.4244 loss_rec: 0.4244 acc_train: 0.7924 loss_val: 0.4322 acc_val: 0.7908 time: 0.0025s\n",
      "Epoch: 01435 loss_train: 0.4244 loss_rec: 0.4244 acc_train: 0.7924 loss_val: 0.4321 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01436 loss_train: 0.4243 loss_rec: 0.4243 acc_train: 0.7925 loss_val: 0.4321 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01437 loss_train: 0.4243 loss_rec: 0.4243 acc_train: 0.7925 loss_val: 0.4321 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01438 loss_train: 0.4242 loss_rec: 0.4242 acc_train: 0.7925 loss_val: 0.4321 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01439 loss_train: 0.4242 loss_rec: 0.4242 acc_train: 0.7924 loss_val: 0.4320 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01440 loss_train: 0.4242 loss_rec: 0.4242 acc_train: 0.7924 loss_val: 0.4320 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01441 loss_train: 0.4241 loss_rec: 0.4241 acc_train: 0.7924 loss_val: 0.4320 acc_val: 0.7908 time: 0.0020s\n",
      "Test set results: loss= 0.4450 accuracy= 0.7879\n",
      "Epoch: 01442 loss_train: 0.4241 loss_rec: 0.4241 acc_train: 0.7924 loss_val: 0.4320 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01443 loss_train: 0.4241 loss_rec: 0.4241 acc_train: 0.7924 loss_val: 0.4319 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01444 loss_train: 0.4240 loss_rec: 0.4240 acc_train: 0.7924 loss_val: 0.4319 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01445 loss_train: 0.4240 loss_rec: 0.4240 acc_train: 0.7924 loss_val: 0.4319 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01446 loss_train: 0.4240 loss_rec: 0.4240 acc_train: 0.7924 loss_val: 0.4318 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01447 loss_train: 0.4239 loss_rec: 0.4239 acc_train: 0.7924 loss_val: 0.4318 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01448 loss_train: 0.4239 loss_rec: 0.4239 acc_train: 0.7924 loss_val: 0.4318 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01449 loss_train: 0.4239 loss_rec: 0.4239 acc_train: 0.7924 loss_val: 0.4318 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01450 loss_train: 0.4238 loss_rec: 0.4238 acc_train: 0.7924 loss_val: 0.4317 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01451 loss_train: 0.4238 loss_rec: 0.4238 acc_train: 0.7924 loss_val: 0.4317 acc_val: 0.7908 time: 0.0015s\n",
      "Test set results: loss= 0.4448 accuracy= 0.7879\n",
      "Epoch: 01452 loss_train: 0.4238 loss_rec: 0.4238 acc_train: 0.7924 loss_val: 0.4317 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01453 loss_train: 0.4237 loss_rec: 0.4237 acc_train: 0.7922 loss_val: 0.4317 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01454 loss_train: 0.4237 loss_rec: 0.4237 acc_train: 0.7923 loss_val: 0.4316 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01455 loss_train: 0.4236 loss_rec: 0.4236 acc_train: 0.7923 loss_val: 0.4316 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01456 loss_train: 0.4236 loss_rec: 0.4236 acc_train: 0.7924 loss_val: 0.4316 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01457 loss_train: 0.4236 loss_rec: 0.4236 acc_train: 0.7924 loss_val: 0.4315 acc_val: 0.7908 time: 0.0015s\n",
      "Epoch: 01458 loss_train: 0.4235 loss_rec: 0.4235 acc_train: 0.7924 loss_val: 0.4315 acc_val: 0.7908 time: 0.0020s\n",
      "Epoch: 01459 loss_train: 0.4235 loss_rec: 0.4235 acc_train: 0.7948 loss_val: 0.4315 acc_val: 0.7900 time: 0.0015s\n",
      "Epoch: 01460 loss_train: 0.4235 loss_rec: 0.4235 acc_train: 0.7948 loss_val: 0.4315 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01461 loss_train: 0.4234 loss_rec: 0.4234 acc_train: 0.7948 loss_val: 0.4314 acc_val: 0.7900 time: 0.0015s\n",
      "Test set results: loss= 0.4446 accuracy= 0.7883\n",
      "Epoch: 01462 loss_train: 0.4234 loss_rec: 0.4234 acc_train: 0.7948 loss_val: 0.4314 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01463 loss_train: 0.4234 loss_rec: 0.4234 acc_train: 0.7948 loss_val: 0.4314 acc_val: 0.7900 time: 0.0015s\n",
      "Epoch: 01464 loss_train: 0.4233 loss_rec: 0.4233 acc_train: 0.7948 loss_val: 0.4314 acc_val: 0.7900 time: 0.0015s\n",
      "Epoch: 01465 loss_train: 0.4233 loss_rec: 0.4233 acc_train: 0.7949 loss_val: 0.4313 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01466 loss_train: 0.4233 loss_rec: 0.4233 acc_train: 0.7948 loss_val: 0.4313 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01467 loss_train: 0.4232 loss_rec: 0.4232 acc_train: 0.7948 loss_val: 0.4313 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01468 loss_train: 0.4232 loss_rec: 0.4232 acc_train: 0.7948 loss_val: 0.4313 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01469 loss_train: 0.4232 loss_rec: 0.4232 acc_train: 0.7948 loss_val: 0.4312 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01470 loss_train: 0.4231 loss_rec: 0.4231 acc_train: 0.7948 loss_val: 0.4312 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01471 loss_train: 0.4231 loss_rec: 0.4231 acc_train: 0.7948 loss_val: 0.4312 acc_val: 0.7900 time: 0.0020s\n",
      "Test set results: loss= 0.4445 accuracy= 0.7883\n",
      "Epoch: 01472 loss_train: 0.4231 loss_rec: 0.4231 acc_train: 0.7948 loss_val: 0.4312 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01473 loss_train: 0.4230 loss_rec: 0.4230 acc_train: 0.7948 loss_val: 0.4311 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01474 loss_train: 0.4230 loss_rec: 0.4230 acc_train: 0.7948 loss_val: 0.4311 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01475 loss_train: 0.4230 loss_rec: 0.4230 acc_train: 0.7949 loss_val: 0.4311 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01476 loss_train: 0.4229 loss_rec: 0.4229 acc_train: 0.7949 loss_val: 0.4311 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01477 loss_train: 0.4229 loss_rec: 0.4229 acc_train: 0.7949 loss_val: 0.4310 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01478 loss_train: 0.4229 loss_rec: 0.4229 acc_train: 0.7943 loss_val: 0.4310 acc_val: 0.7892 time: 0.0020s\n",
      "Epoch: 01479 loss_train: 0.4228 loss_rec: 0.4228 acc_train: 0.7943 loss_val: 0.4310 acc_val: 0.7892 time: 0.0020s\n",
      "Epoch: 01480 loss_train: 0.4228 loss_rec: 0.4228 acc_train: 0.7949 loss_val: 0.4309 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01481 loss_train: 0.4228 loss_rec: 0.4228 acc_train: 0.7949 loss_val: 0.4309 acc_val: 0.7900 time: 0.0020s\n",
      "Test set results: loss= 0.4442 accuracy= 0.7880\n",
      "Epoch: 01482 loss_train: 0.4227 loss_rec: 0.4227 acc_train: 0.7949 loss_val: 0.4309 acc_val: 0.7900 time: 0.0015s\n",
      "Epoch: 01483 loss_train: 0.4227 loss_rec: 0.4227 acc_train: 0.7949 loss_val: 0.4309 acc_val: 0.7900 time: 0.0020s\n",
      "Epoch: 01484 loss_train: 0.4227 loss_rec: 0.4227 acc_train: 0.7944 loss_val: 0.4308 acc_val: 0.7892 time: 0.0020s\n",
      "Epoch: 01485 loss_train: 0.4226 loss_rec: 0.4226 acc_train: 0.7944 loss_val: 0.4308 acc_val: 0.7892 time: 0.0020s\n",
      "Epoch: 01486 loss_train: 0.4226 loss_rec: 0.4226 acc_train: 0.7944 loss_val: 0.4308 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 01487 loss_train: 0.4226 loss_rec: 0.4226 acc_train: 0.7944 loss_val: 0.4308 acc_val: 0.7892 time: 0.0020s\n",
      "Epoch: 01488 loss_train: 0.4225 loss_rec: 0.4225 acc_train: 0.7944 loss_val: 0.4308 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01489 loss_train: 0.4225 loss_rec: 0.4225 acc_train: 0.7943 loss_val: 0.4307 acc_val: 0.7896 time: 0.0020s\n",
      "Epoch: 01490 loss_train: 0.4225 loss_rec: 0.4225 acc_train: 0.7943 loss_val: 0.4307 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01491 loss_train: 0.4224 loss_rec: 0.4224 acc_train: 0.7943 loss_val: 0.4307 acc_val: 0.7896 time: 0.0020s\n",
      "Test set results: loss= 0.4441 accuracy= 0.7873\n",
      "Epoch: 01492 loss_train: 0.4224 loss_rec: 0.4224 acc_train: 0.7943 loss_val: 0.4307 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01493 loss_train: 0.4224 loss_rec: 0.4224 acc_train: 0.7944 loss_val: 0.4306 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01494 loss_train: 0.4223 loss_rec: 0.4223 acc_train: 0.7943 loss_val: 0.4306 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01495 loss_train: 0.4223 loss_rec: 0.4223 acc_train: 0.7943 loss_val: 0.4306 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01496 loss_train: 0.4223 loss_rec: 0.4223 acc_train: 0.7944 loss_val: 0.4306 acc_val: 0.7896 time: 0.0020s\n",
      "Epoch: 01497 loss_train: 0.4222 loss_rec: 0.4222 acc_train: 0.7944 loss_val: 0.4305 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01498 loss_train: 0.4222 loss_rec: 0.4222 acc_train: 0.7945 loss_val: 0.4305 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01499 loss_train: 0.4222 loss_rec: 0.4222 acc_train: 0.7944 loss_val: 0.4305 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01500 loss_train: 0.4221 loss_rec: 0.4221 acc_train: 0.7946 loss_val: 0.4305 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01501 loss_train: 0.4221 loss_rec: 0.4221 acc_train: 0.7946 loss_val: 0.4304 acc_val: 0.7896 time: 0.0015s\n",
      "Test set results: loss= 0.4439 accuracy= 0.7875\n",
      "Epoch: 01502 loss_train: 0.4221 loss_rec: 0.4221 acc_train: 0.7946 loss_val: 0.4304 acc_val: 0.7896 time: 0.0020s\n",
      "Epoch: 01503 loss_train: 0.4220 loss_rec: 0.4220 acc_train: 0.7946 loss_val: 0.4304 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01504 loss_train: 0.4220 loss_rec: 0.4220 acc_train: 0.7946 loss_val: 0.4304 acc_val: 0.7896 time: 0.0020s\n",
      "Epoch: 01505 loss_train: 0.4220 loss_rec: 0.4220 acc_train: 0.7953 loss_val: 0.4304 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01506 loss_train: 0.4220 loss_rec: 0.4220 acc_train: 0.7953 loss_val: 0.4303 acc_val: 0.7896 time: 0.0020s\n",
      "Epoch: 01507 loss_train: 0.4219 loss_rec: 0.4219 acc_train: 0.7953 loss_val: 0.4303 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01508 loss_train: 0.4219 loss_rec: 0.4219 acc_train: 0.7953 loss_val: 0.4303 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01509 loss_train: 0.4219 loss_rec: 0.4219 acc_train: 0.7953 loss_val: 0.4302 acc_val: 0.7896 time: 0.0020s\n",
      "Epoch: 01510 loss_train: 0.4218 loss_rec: 0.4218 acc_train: 0.7953 loss_val: 0.4302 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01511 loss_train: 0.4218 loss_rec: 0.4218 acc_train: 0.7952 loss_val: 0.4302 acc_val: 0.7896 time: 0.0020s\n",
      "Test set results: loss= 0.4438 accuracy= 0.7883\n",
      "Epoch: 01512 loss_train: 0.4218 loss_rec: 0.4218 acc_train: 0.7952 loss_val: 0.4302 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01513 loss_train: 0.4217 loss_rec: 0.4217 acc_train: 0.7952 loss_val: 0.4302 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01514 loss_train: 0.4217 loss_rec: 0.4217 acc_train: 0.7952 loss_val: 0.4302 acc_val: 0.7896 time: 0.0020s\n",
      "Epoch: 01515 loss_train: 0.4217 loss_rec: 0.4217 acc_train: 0.7952 loss_val: 0.4301 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01516 loss_train: 0.4216 loss_rec: 0.4216 acc_train: 0.7952 loss_val: 0.4301 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01517 loss_train: 0.4216 loss_rec: 0.4216 acc_train: 0.7952 loss_val: 0.4301 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01518 loss_train: 0.4216 loss_rec: 0.4216 acc_train: 0.7953 loss_val: 0.4301 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01519 loss_train: 0.4216 loss_rec: 0.4216 acc_train: 0.7953 loss_val: 0.4300 acc_val: 0.7896 time: 0.0020s\n",
      "Epoch: 01520 loss_train: 0.4215 loss_rec: 0.4215 acc_train: 0.7953 loss_val: 0.4300 acc_val: 0.7896 time: 0.0020s\n",
      "Epoch: 01521 loss_train: 0.4215 loss_rec: 0.4215 acc_train: 0.7953 loss_val: 0.4300 acc_val: 0.7896 time: 0.0015s\n",
      "Test set results: loss= 0.4437 accuracy= 0.7885\n",
      "Epoch: 01522 loss_train: 0.4215 loss_rec: 0.4215 acc_train: 0.7953 loss_val: 0.4300 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01523 loss_train: 0.4214 loss_rec: 0.4214 acc_train: 0.7953 loss_val: 0.4299 acc_val: 0.7896 time: 0.0015s\n",
      "Epoch: 01524 loss_train: 0.4214 loss_rec: 0.4214 acc_train: 0.7947 loss_val: 0.4299 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01525 loss_train: 0.4214 loss_rec: 0.4214 acc_train: 0.7947 loss_val: 0.4299 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01526 loss_train: 0.4214 loss_rec: 0.4214 acc_train: 0.7947 loss_val: 0.4299 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01527 loss_train: 0.4213 loss_rec: 0.4213 acc_train: 0.7947 loss_val: 0.4299 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01528 loss_train: 0.4213 loss_rec: 0.4213 acc_train: 0.7947 loss_val: 0.4298 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01529 loss_train: 0.4213 loss_rec: 0.4213 acc_train: 0.7947 loss_val: 0.4298 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01530 loss_train: 0.4212 loss_rec: 0.4212 acc_train: 0.7947 loss_val: 0.4298 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01531 loss_train: 0.4212 loss_rec: 0.4212 acc_train: 0.7953 loss_val: 0.4298 acc_val: 0.7896 time: 0.0015s\n",
      "Test set results: loss= 0.4435 accuracy= 0.7881\n",
      "Epoch: 01532 loss_train: 0.4212 loss_rec: 0.4212 acc_train: 0.7948 loss_val: 0.4297 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01533 loss_train: 0.4211 loss_rec: 0.4211 acc_train: 0.7948 loss_val: 0.4297 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01534 loss_train: 0.4211 loss_rec: 0.4211 acc_train: 0.7948 loss_val: 0.4297 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01535 loss_train: 0.4211 loss_rec: 0.4211 acc_train: 0.7948 loss_val: 0.4297 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01536 loss_train: 0.4210 loss_rec: 0.4210 acc_train: 0.7948 loss_val: 0.4297 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01537 loss_train: 0.4210 loss_rec: 0.4210 acc_train: 0.7948 loss_val: 0.4296 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01538 loss_train: 0.4210 loss_rec: 0.4210 acc_train: 0.7948 loss_val: 0.4296 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01539 loss_train: 0.4210 loss_rec: 0.4210 acc_train: 0.7948 loss_val: 0.4296 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01540 loss_train: 0.4209 loss_rec: 0.4209 acc_train: 0.7948 loss_val: 0.4296 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01541 loss_train: 0.4209 loss_rec: 0.4209 acc_train: 0.7948 loss_val: 0.4296 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4434 accuracy= 0.7881\n",
      "Epoch: 01542 loss_train: 0.4209 loss_rec: 0.4209 acc_train: 0.7948 loss_val: 0.4295 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01543 loss_train: 0.4208 loss_rec: 0.4208 acc_train: 0.7948 loss_val: 0.4295 acc_val: 0.7879 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01544 loss_train: 0.4208 loss_rec: 0.4208 acc_train: 0.7948 loss_val: 0.4295 acc_val: 0.7879 time: 0.0025s\n",
      "Epoch: 01545 loss_train: 0.4208 loss_rec: 0.4208 acc_train: 0.7948 loss_val: 0.4295 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01546 loss_train: 0.4207 loss_rec: 0.4207 acc_train: 0.7948 loss_val: 0.4295 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01547 loss_train: 0.4207 loss_rec: 0.4207 acc_train: 0.7948 loss_val: 0.4294 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01548 loss_train: 0.4207 loss_rec: 0.4207 acc_train: 0.7948 loss_val: 0.4294 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01549 loss_train: 0.4207 loss_rec: 0.4207 acc_train: 0.7948 loss_val: 0.4294 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01550 loss_train: 0.4206 loss_rec: 0.4206 acc_train: 0.7948 loss_val: 0.4294 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01551 loss_train: 0.4206 loss_rec: 0.4206 acc_train: 0.7950 loss_val: 0.4293 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4432 accuracy= 0.7881\n",
      "Epoch: 01552 loss_train: 0.4206 loss_rec: 0.4206 acc_train: 0.7950 loss_val: 0.4293 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01553 loss_train: 0.4205 loss_rec: 0.4205 acc_train: 0.7950 loss_val: 0.4293 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01554 loss_train: 0.4205 loss_rec: 0.4205 acc_train: 0.7950 loss_val: 0.4293 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01555 loss_train: 0.4205 loss_rec: 0.4205 acc_train: 0.7950 loss_val: 0.4293 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01556 loss_train: 0.4205 loss_rec: 0.4205 acc_train: 0.7950 loss_val: 0.4292 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01557 loss_train: 0.4204 loss_rec: 0.4204 acc_train: 0.7953 loss_val: 0.4292 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01558 loss_train: 0.4204 loss_rec: 0.4204 acc_train: 0.7953 loss_val: 0.4292 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01559 loss_train: 0.4204 loss_rec: 0.4204 acc_train: 0.7950 loss_val: 0.4292 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01560 loss_train: 0.4204 loss_rec: 0.4204 acc_train: 0.7950 loss_val: 0.4292 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01561 loss_train: 0.4203 loss_rec: 0.4203 acc_train: 0.7950 loss_val: 0.4291 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4431 accuracy= 0.7881\n",
      "Epoch: 01562 loss_train: 0.4203 loss_rec: 0.4203 acc_train: 0.7949 loss_val: 0.4291 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01563 loss_train: 0.4203 loss_rec: 0.4203 acc_train: 0.7948 loss_val: 0.4291 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01564 loss_train: 0.4202 loss_rec: 0.4202 acc_train: 0.7949 loss_val: 0.4291 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01565 loss_train: 0.4202 loss_rec: 0.4202 acc_train: 0.7949 loss_val: 0.4291 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01566 loss_train: 0.4202 loss_rec: 0.4202 acc_train: 0.7949 loss_val: 0.4291 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01567 loss_train: 0.4202 loss_rec: 0.4202 acc_train: 0.7948 loss_val: 0.4290 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01568 loss_train: 0.4201 loss_rec: 0.4201 acc_train: 0.7949 loss_val: 0.4290 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01569 loss_train: 0.4201 loss_rec: 0.4201 acc_train: 0.7949 loss_val: 0.4290 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01570 loss_train: 0.4201 loss_rec: 0.4201 acc_train: 0.7949 loss_val: 0.4290 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01571 loss_train: 0.4201 loss_rec: 0.4201 acc_train: 0.7949 loss_val: 0.4290 acc_val: 0.7875 time: 0.0020s\n",
      "Test set results: loss= 0.4430 accuracy= 0.7880\n",
      "Epoch: 01572 loss_train: 0.4200 loss_rec: 0.4200 acc_train: 0.7949 loss_val: 0.4289 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01573 loss_train: 0.4200 loss_rec: 0.4200 acc_train: 0.7949 loss_val: 0.4289 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01574 loss_train: 0.4200 loss_rec: 0.4200 acc_train: 0.7949 loss_val: 0.4289 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01575 loss_train: 0.4199 loss_rec: 0.4199 acc_train: 0.7949 loss_val: 0.4289 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01576 loss_train: 0.4199 loss_rec: 0.4199 acc_train: 0.7949 loss_val: 0.4289 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01577 loss_train: 0.4199 loss_rec: 0.4199 acc_train: 0.7949 loss_val: 0.4288 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01578 loss_train: 0.4199 loss_rec: 0.4199 acc_train: 0.7952 loss_val: 0.4288 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01579 loss_train: 0.4198 loss_rec: 0.4198 acc_train: 0.7952 loss_val: 0.4288 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01580 loss_train: 0.4198 loss_rec: 0.4198 acc_train: 0.7952 loss_val: 0.4288 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01581 loss_train: 0.4198 loss_rec: 0.4198 acc_train: 0.7952 loss_val: 0.4288 acc_val: 0.7875 time: 0.0020s\n",
      "Test set results: loss= 0.4428 accuracy= 0.7883\n",
      "Epoch: 01582 loss_train: 0.4197 loss_rec: 0.4197 acc_train: 0.7952 loss_val: 0.4287 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01583 loss_train: 0.4197 loss_rec: 0.4197 acc_train: 0.7952 loss_val: 0.4287 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01584 loss_train: 0.4197 loss_rec: 0.4197 acc_train: 0.7952 loss_val: 0.4287 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01585 loss_train: 0.4197 loss_rec: 0.4197 acc_train: 0.7952 loss_val: 0.4287 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01586 loss_train: 0.4196 loss_rec: 0.4196 acc_train: 0.7952 loss_val: 0.4287 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01587 loss_train: 0.4196 loss_rec: 0.4196 acc_train: 0.7952 loss_val: 0.4286 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01588 loss_train: 0.4196 loss_rec: 0.4196 acc_train: 0.7951 loss_val: 0.4286 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01589 loss_train: 0.4196 loss_rec: 0.4196 acc_train: 0.7952 loss_val: 0.4286 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01590 loss_train: 0.4195 loss_rec: 0.4195 acc_train: 0.7952 loss_val: 0.4286 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01591 loss_train: 0.4195 loss_rec: 0.4195 acc_train: 0.7952 loss_val: 0.4286 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4427 accuracy= 0.7884\n",
      "Epoch: 01592 loss_train: 0.4195 loss_rec: 0.4195 acc_train: 0.7952 loss_val: 0.4286 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01593 loss_train: 0.4195 loss_rec: 0.4195 acc_train: 0.7952 loss_val: 0.4285 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01594 loss_train: 0.4194 loss_rec: 0.4194 acc_train: 0.7952 loss_val: 0.4285 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01595 loss_train: 0.4194 loss_rec: 0.4194 acc_train: 0.7952 loss_val: 0.4285 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01596 loss_train: 0.4194 loss_rec: 0.4194 acc_train: 0.7952 loss_val: 0.4285 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01597 loss_train: 0.4193 loss_rec: 0.4193 acc_train: 0.7952 loss_val: 0.4285 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01598 loss_train: 0.4193 loss_rec: 0.4193 acc_train: 0.7952 loss_val: 0.4284 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01599 loss_train: 0.4193 loss_rec: 0.4193 acc_train: 0.7952 loss_val: 0.4285 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01600 loss_train: 0.4193 loss_rec: 0.4193 acc_train: 0.7952 loss_val: 0.4284 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01601 loss_train: 0.4192 loss_rec: 0.4192 acc_train: 0.7952 loss_val: 0.4284 acc_val: 0.7875 time: 0.0020s\n",
      "Test set results: loss= 0.4426 accuracy= 0.7883\n",
      "Epoch: 01602 loss_train: 0.4192 loss_rec: 0.4192 acc_train: 0.7952 loss_val: 0.4284 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01603 loss_train: 0.4192 loss_rec: 0.4192 acc_train: 0.7952 loss_val: 0.4284 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01604 loss_train: 0.4192 loss_rec: 0.4192 acc_train: 0.7952 loss_val: 0.4283 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01605 loss_train: 0.4191 loss_rec: 0.4191 acc_train: 0.7952 loss_val: 0.4283 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01606 loss_train: 0.4191 loss_rec: 0.4191 acc_train: 0.7952 loss_val: 0.4283 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01607 loss_train: 0.4191 loss_rec: 0.4191 acc_train: 0.7952 loss_val: 0.4283 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01608 loss_train: 0.4191 loss_rec: 0.4191 acc_train: 0.7952 loss_val: 0.4283 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01609 loss_train: 0.4190 loss_rec: 0.4190 acc_train: 0.7952 loss_val: 0.4282 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01610 loss_train: 0.4190 loss_rec: 0.4190 acc_train: 0.7952 loss_val: 0.4282 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01611 loss_train: 0.4190 loss_rec: 0.4190 acc_train: 0.7952 loss_val: 0.4282 acc_val: 0.7875 time: 0.0020s\n",
      "Test set results: loss= 0.4425 accuracy= 0.7885\n",
      "Epoch: 01612 loss_train: 0.4190 loss_rec: 0.4190 acc_train: 0.7952 loss_val: 0.4282 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01613 loss_train: 0.4189 loss_rec: 0.4189 acc_train: 0.7952 loss_val: 0.4282 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01614 loss_train: 0.4189 loss_rec: 0.4189 acc_train: 0.7952 loss_val: 0.4282 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01615 loss_train: 0.4189 loss_rec: 0.4189 acc_train: 0.7951 loss_val: 0.4281 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01616 loss_train: 0.4188 loss_rec: 0.4188 acc_train: 0.7951 loss_val: 0.4281 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01617 loss_train: 0.4188 loss_rec: 0.4188 acc_train: 0.7951 loss_val: 0.4281 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01618 loss_train: 0.4188 loss_rec: 0.4188 acc_train: 0.7951 loss_val: 0.4281 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01619 loss_train: 0.4188 loss_rec: 0.4188 acc_train: 0.7951 loss_val: 0.4281 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01620 loss_train: 0.4187 loss_rec: 0.4187 acc_train: 0.7951 loss_val: 0.4281 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01621 loss_train: 0.4187 loss_rec: 0.4187 acc_train: 0.7951 loss_val: 0.4280 acc_val: 0.7875 time: 0.0020s\n",
      "Test set results: loss= 0.4424 accuracy= 0.7883\n",
      "Epoch: 01622 loss_train: 0.4187 loss_rec: 0.4187 acc_train: 0.7951 loss_val: 0.4280 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01623 loss_train: 0.4187 loss_rec: 0.4187 acc_train: 0.7951 loss_val: 0.4280 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01624 loss_train: 0.4187 loss_rec: 0.4187 acc_train: 0.7951 loss_val: 0.4280 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01625 loss_train: 0.4186 loss_rec: 0.4186 acc_train: 0.7951 loss_val: 0.4280 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01626 loss_train: 0.4186 loss_rec: 0.4186 acc_train: 0.7951 loss_val: 0.4280 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01627 loss_train: 0.4186 loss_rec: 0.4186 acc_train: 0.7951 loss_val: 0.4279 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01628 loss_train: 0.4185 loss_rec: 0.4185 acc_train: 0.7951 loss_val: 0.4279 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01629 loss_train: 0.4185 loss_rec: 0.4185 acc_train: 0.7951 loss_val: 0.4279 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01630 loss_train: 0.4185 loss_rec: 0.4185 acc_train: 0.7951 loss_val: 0.4279 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01631 loss_train: 0.4185 loss_rec: 0.4185 acc_train: 0.7951 loss_val: 0.4279 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4423 accuracy= 0.7881\n",
      "Epoch: 01632 loss_train: 0.4184 loss_rec: 0.4184 acc_train: 0.7952 loss_val: 0.4279 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01633 loss_train: 0.4184 loss_rec: 0.4184 acc_train: 0.7952 loss_val: 0.4278 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01634 loss_train: 0.4184 loss_rec: 0.4184 acc_train: 0.7952 loss_val: 0.4278 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01635 loss_train: 0.4184 loss_rec: 0.4184 acc_train: 0.7952 loss_val: 0.4278 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01636 loss_train: 0.4183 loss_rec: 0.4183 acc_train: 0.7952 loss_val: 0.4278 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01637 loss_train: 0.4183 loss_rec: 0.4183 acc_train: 0.7952 loss_val: 0.4278 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01638 loss_train: 0.4183 loss_rec: 0.4183 acc_train: 0.7952 loss_val: 0.4278 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01639 loss_train: 0.4183 loss_rec: 0.4183 acc_train: 0.7952 loss_val: 0.4277 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01640 loss_train: 0.4183 loss_rec: 0.4183 acc_train: 0.7952 loss_val: 0.4277 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01641 loss_train: 0.4182 loss_rec: 0.4182 acc_train: 0.7952 loss_val: 0.4277 acc_val: 0.7875 time: 0.0020s\n",
      "Test set results: loss= 0.4422 accuracy= 0.7881\n",
      "Epoch: 01642 loss_train: 0.4182 loss_rec: 0.4182 acc_train: 0.7952 loss_val: 0.4277 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01643 loss_train: 0.4182 loss_rec: 0.4182 acc_train: 0.7952 loss_val: 0.4277 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01644 loss_train: 0.4182 loss_rec: 0.4182 acc_train: 0.7952 loss_val: 0.4277 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01645 loss_train: 0.4181 loss_rec: 0.4181 acc_train: 0.7952 loss_val: 0.4276 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01646 loss_train: 0.4181 loss_rec: 0.4181 acc_train: 0.7952 loss_val: 0.4276 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01647 loss_train: 0.4181 loss_rec: 0.4181 acc_train: 0.7952 loss_val: 0.4276 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01648 loss_train: 0.4181 loss_rec: 0.4181 acc_train: 0.7952 loss_val: 0.4276 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01649 loss_train: 0.4180 loss_rec: 0.4180 acc_train: 0.7952 loss_val: 0.4276 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01650 loss_train: 0.4180 loss_rec: 0.4180 acc_train: 0.7952 loss_val: 0.4276 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01651 loss_train: 0.4180 loss_rec: 0.4180 acc_train: 0.7952 loss_val: 0.4276 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4421 accuracy= 0.7881\n",
      "Epoch: 01652 loss_train: 0.4180 loss_rec: 0.4180 acc_train: 0.7952 loss_val: 0.4275 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01653 loss_train: 0.4179 loss_rec: 0.4179 acc_train: 0.7952 loss_val: 0.4275 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01654 loss_train: 0.4179 loss_rec: 0.4179 acc_train: 0.7952 loss_val: 0.4275 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01655 loss_train: 0.4179 loss_rec: 0.4179 acc_train: 0.7952 loss_val: 0.4275 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01656 loss_train: 0.4179 loss_rec: 0.4179 acc_train: 0.7952 loss_val: 0.4275 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01657 loss_train: 0.4178 loss_rec: 0.4178 acc_train: 0.7952 loss_val: 0.4275 acc_val: 0.7875 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01658 loss_train: 0.4178 loss_rec: 0.4178 acc_train: 0.7952 loss_val: 0.4274 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01659 loss_train: 0.4178 loss_rec: 0.4178 acc_train: 0.7952 loss_val: 0.4274 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01660 loss_train: 0.4178 loss_rec: 0.4178 acc_train: 0.7952 loss_val: 0.4274 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01661 loss_train: 0.4177 loss_rec: 0.4177 acc_train: 0.7952 loss_val: 0.4274 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4420 accuracy= 0.7883\n",
      "Epoch: 01662 loss_train: 0.4177 loss_rec: 0.4177 acc_train: 0.7952 loss_val: 0.4274 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01663 loss_train: 0.4177 loss_rec: 0.4177 acc_train: 0.7952 loss_val: 0.4274 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01664 loss_train: 0.4177 loss_rec: 0.4177 acc_train: 0.7952 loss_val: 0.4273 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01665 loss_train: 0.4177 loss_rec: 0.4177 acc_train: 0.7952 loss_val: 0.4273 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01666 loss_train: 0.4176 loss_rec: 0.4176 acc_train: 0.7952 loss_val: 0.4273 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01667 loss_train: 0.4176 loss_rec: 0.4176 acc_train: 0.7951 loss_val: 0.4273 acc_val: 0.7875 time: 0.0025s\n",
      "Epoch: 01668 loss_train: 0.4176 loss_rec: 0.4176 acc_train: 0.7951 loss_val: 0.4273 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01669 loss_train: 0.4176 loss_rec: 0.4176 acc_train: 0.7951 loss_val: 0.4273 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01670 loss_train: 0.4175 loss_rec: 0.4175 acc_train: 0.7953 loss_val: 0.4272 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01671 loss_train: 0.4175 loss_rec: 0.4175 acc_train: 0.7953 loss_val: 0.4272 acc_val: 0.7875 time: 0.0020s\n",
      "Test set results: loss= 0.4419 accuracy= 0.7884\n",
      "Epoch: 01672 loss_train: 0.4175 loss_rec: 0.4175 acc_train: 0.7953 loss_val: 0.4272 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01673 loss_train: 0.4175 loss_rec: 0.4175 acc_train: 0.7953 loss_val: 0.4272 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01674 loss_train: 0.4174 loss_rec: 0.4174 acc_train: 0.7953 loss_val: 0.4272 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01675 loss_train: 0.4174 loss_rec: 0.4174 acc_train: 0.7953 loss_val: 0.4272 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01676 loss_train: 0.4174 loss_rec: 0.4174 acc_train: 0.7953 loss_val: 0.4271 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01677 loss_train: 0.4174 loss_rec: 0.4174 acc_train: 0.7953 loss_val: 0.4271 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01678 loss_train: 0.4173 loss_rec: 0.4173 acc_train: 0.7953 loss_val: 0.4271 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01679 loss_train: 0.4173 loss_rec: 0.4173 acc_train: 0.7953 loss_val: 0.4271 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01680 loss_train: 0.4173 loss_rec: 0.4173 acc_train: 0.7953 loss_val: 0.4271 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01681 loss_train: 0.4173 loss_rec: 0.4173 acc_train: 0.7953 loss_val: 0.4271 acc_val: 0.7871 time: 0.0015s\n",
      "Test set results: loss= 0.4418 accuracy= 0.7884\n",
      "Epoch: 01682 loss_train: 0.4173 loss_rec: 0.4173 acc_train: 0.7953 loss_val: 0.4270 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01683 loss_train: 0.4172 loss_rec: 0.4172 acc_train: 0.7953 loss_val: 0.4270 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01684 loss_train: 0.4172 loss_rec: 0.4172 acc_train: 0.7953 loss_val: 0.4270 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01685 loss_train: 0.4172 loss_rec: 0.4172 acc_train: 0.7953 loss_val: 0.4270 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01686 loss_train: 0.4172 loss_rec: 0.4172 acc_train: 0.7953 loss_val: 0.4270 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01687 loss_train: 0.4171 loss_rec: 0.4171 acc_train: 0.7953 loss_val: 0.4270 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01688 loss_train: 0.4171 loss_rec: 0.4171 acc_train: 0.7953 loss_val: 0.4270 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01689 loss_train: 0.4171 loss_rec: 0.4171 acc_train: 0.7953 loss_val: 0.4269 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01690 loss_train: 0.4171 loss_rec: 0.4171 acc_train: 0.7953 loss_val: 0.4269 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01691 loss_train: 0.4170 loss_rec: 0.4170 acc_train: 0.7953 loss_val: 0.4269 acc_val: 0.7871 time: 0.0015s\n",
      "Test set results: loss= 0.4417 accuracy= 0.7884\n",
      "Epoch: 01692 loss_train: 0.4170 loss_rec: 0.4170 acc_train: 0.7953 loss_val: 0.4269 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01693 loss_train: 0.4170 loss_rec: 0.4170 acc_train: 0.7953 loss_val: 0.4269 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01694 loss_train: 0.4170 loss_rec: 0.4170 acc_train: 0.7953 loss_val: 0.4269 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01695 loss_train: 0.4170 loss_rec: 0.4170 acc_train: 0.7953 loss_val: 0.4268 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01696 loss_train: 0.4169 loss_rec: 0.4169 acc_train: 0.7953 loss_val: 0.4268 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01697 loss_train: 0.4169 loss_rec: 0.4169 acc_train: 0.7953 loss_val: 0.4268 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01698 loss_train: 0.4169 loss_rec: 0.4169 acc_train: 0.7953 loss_val: 0.4268 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01699 loss_train: 0.4169 loss_rec: 0.4169 acc_train: 0.7953 loss_val: 0.4268 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01700 loss_train: 0.4168 loss_rec: 0.4168 acc_train: 0.7953 loss_val: 0.4268 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01701 loss_train: 0.4168 loss_rec: 0.4168 acc_train: 0.7953 loss_val: 0.4268 acc_val: 0.7871 time: 0.0020s\n",
      "Test set results: loss= 0.4417 accuracy= 0.7881\n",
      "Epoch: 01702 loss_train: 0.4168 loss_rec: 0.4168 acc_train: 0.7954 loss_val: 0.4267 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01703 loss_train: 0.4168 loss_rec: 0.4168 acc_train: 0.7954 loss_val: 0.4267 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01704 loss_train: 0.4167 loss_rec: 0.4167 acc_train: 0.7954 loss_val: 0.4267 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01705 loss_train: 0.4167 loss_rec: 0.4167 acc_train: 0.7954 loss_val: 0.4267 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01706 loss_train: 0.4167 loss_rec: 0.4167 acc_train: 0.7954 loss_val: 0.4267 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01707 loss_train: 0.4167 loss_rec: 0.4167 acc_train: 0.7954 loss_val: 0.4267 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01708 loss_train: 0.4167 loss_rec: 0.4167 acc_train: 0.7955 loss_val: 0.4267 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01709 loss_train: 0.4166 loss_rec: 0.4166 acc_train: 0.7955 loss_val: 0.4267 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01710 loss_train: 0.4166 loss_rec: 0.4166 acc_train: 0.7955 loss_val: 0.4266 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01711 loss_train: 0.4166 loss_rec: 0.4166 acc_train: 0.7955 loss_val: 0.4266 acc_val: 0.7871 time: 0.0015s\n",
      "Test set results: loss= 0.4416 accuracy= 0.7881\n",
      "Epoch: 01712 loss_train: 0.4166 loss_rec: 0.4166 acc_train: 0.7955 loss_val: 0.4266 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01713 loss_train: 0.4165 loss_rec: 0.4165 acc_train: 0.7955 loss_val: 0.4266 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01714 loss_train: 0.4165 loss_rec: 0.4165 acc_train: 0.7955 loss_val: 0.4266 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01715 loss_train: 0.4165 loss_rec: 0.4165 acc_train: 0.7955 loss_val: 0.4266 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01716 loss_train: 0.4165 loss_rec: 0.4165 acc_train: 0.7955 loss_val: 0.4265 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01717 loss_train: 0.4164 loss_rec: 0.4164 acc_train: 0.7955 loss_val: 0.4265 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01718 loss_train: 0.4164 loss_rec: 0.4164 acc_train: 0.7955 loss_val: 0.4265 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01719 loss_train: 0.4164 loss_rec: 0.4164 acc_train: 0.7955 loss_val: 0.4265 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01720 loss_train: 0.4164 loss_rec: 0.4164 acc_train: 0.7955 loss_val: 0.4265 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01721 loss_train: 0.4164 loss_rec: 0.4164 acc_train: 0.7955 loss_val: 0.4265 acc_val: 0.7867 time: 0.0020s\n",
      "Test set results: loss= 0.4415 accuracy= 0.7881\n",
      "Epoch: 01722 loss_train: 0.4163 loss_rec: 0.4163 acc_train: 0.7955 loss_val: 0.4265 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01723 loss_train: 0.4163 loss_rec: 0.4163 acc_train: 0.7955 loss_val: 0.4264 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01724 loss_train: 0.4163 loss_rec: 0.4163 acc_train: 0.7955 loss_val: 0.4264 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01725 loss_train: 0.4163 loss_rec: 0.4163 acc_train: 0.7955 loss_val: 0.4264 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01726 loss_train: 0.4163 loss_rec: 0.4163 acc_train: 0.7955 loss_val: 0.4264 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01727 loss_train: 0.4162 loss_rec: 0.4162 acc_train: 0.7955 loss_val: 0.4264 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01728 loss_train: 0.4162 loss_rec: 0.4162 acc_train: 0.7955 loss_val: 0.4264 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01729 loss_train: 0.4162 loss_rec: 0.4162 acc_train: 0.7956 loss_val: 0.4264 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01730 loss_train: 0.4162 loss_rec: 0.4162 acc_train: 0.7957 loss_val: 0.4263 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01731 loss_train: 0.4161 loss_rec: 0.4161 acc_train: 0.7957 loss_val: 0.4263 acc_val: 0.7867 time: 0.0015s\n",
      "Test set results: loss= 0.4415 accuracy= 0.7883\n",
      "Epoch: 01732 loss_train: 0.4161 loss_rec: 0.4161 acc_train: 0.7957 loss_val: 0.4263 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01733 loss_train: 0.4161 loss_rec: 0.4161 acc_train: 0.7957 loss_val: 0.4263 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01734 loss_train: 0.4161 loss_rec: 0.4161 acc_train: 0.7957 loss_val: 0.4263 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01735 loss_train: 0.4161 loss_rec: 0.4161 acc_train: 0.7957 loss_val: 0.4263 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01736 loss_train: 0.4160 loss_rec: 0.4160 acc_train: 0.7957 loss_val: 0.4263 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01737 loss_train: 0.4160 loss_rec: 0.4160 acc_train: 0.7957 loss_val: 0.4262 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01738 loss_train: 0.4160 loss_rec: 0.4160 acc_train: 0.7957 loss_val: 0.4262 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01739 loss_train: 0.4160 loss_rec: 0.4160 acc_train: 0.7957 loss_val: 0.4262 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01740 loss_train: 0.4159 loss_rec: 0.4159 acc_train: 0.7957 loss_val: 0.4262 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01741 loss_train: 0.4159 loss_rec: 0.4159 acc_train: 0.7957 loss_val: 0.4262 acc_val: 0.7867 time: 0.0015s\n",
      "Test set results: loss= 0.4414 accuracy= 0.7883\n",
      "Epoch: 01742 loss_train: 0.4159 loss_rec: 0.4159 acc_train: 0.7957 loss_val: 0.4262 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01743 loss_train: 0.4159 loss_rec: 0.4159 acc_train: 0.7957 loss_val: 0.4261 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01744 loss_train: 0.4158 loss_rec: 0.4158 acc_train: 0.7957 loss_val: 0.4261 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01745 loss_train: 0.4158 loss_rec: 0.4158 acc_train: 0.7957 loss_val: 0.4261 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01746 loss_train: 0.4158 loss_rec: 0.4158 acc_train: 0.7957 loss_val: 0.4261 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01747 loss_train: 0.4158 loss_rec: 0.4158 acc_train: 0.7957 loss_val: 0.4261 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01748 loss_train: 0.4158 loss_rec: 0.4158 acc_train: 0.7957 loss_val: 0.4261 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01749 loss_train: 0.4157 loss_rec: 0.4157 acc_train: 0.7957 loss_val: 0.4260 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01750 loss_train: 0.4157 loss_rec: 0.4157 acc_train: 0.7957 loss_val: 0.4260 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01751 loss_train: 0.4157 loss_rec: 0.4157 acc_train: 0.7957 loss_val: 0.4260 acc_val: 0.7867 time: 0.0015s\n",
      "Test set results: loss= 0.4413 accuracy= 0.7880\n",
      "Epoch: 01752 loss_train: 0.4157 loss_rec: 0.4157 acc_train: 0.7957 loss_val: 0.4260 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01753 loss_train: 0.4156 loss_rec: 0.4156 acc_train: 0.7957 loss_val: 0.4260 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01754 loss_train: 0.4156 loss_rec: 0.4156 acc_train: 0.7957 loss_val: 0.4260 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01755 loss_train: 0.4156 loss_rec: 0.4156 acc_train: 0.7957 loss_val: 0.4260 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01756 loss_train: 0.4156 loss_rec: 0.4156 acc_train: 0.7957 loss_val: 0.4259 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01757 loss_train: 0.4156 loss_rec: 0.4156 acc_train: 0.7956 loss_val: 0.4259 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01758 loss_train: 0.4155 loss_rec: 0.4155 acc_train: 0.7956 loss_val: 0.4259 acc_val: 0.7867 time: 0.0025s\n",
      "Epoch: 01759 loss_train: 0.4155 loss_rec: 0.4155 acc_train: 0.7956 loss_val: 0.4259 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01760 loss_train: 0.4155 loss_rec: 0.4155 acc_train: 0.7956 loss_val: 0.4259 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01761 loss_train: 0.4155 loss_rec: 0.4155 acc_train: 0.7956 loss_val: 0.4259 acc_val: 0.7867 time: 0.0015s\n",
      "Test set results: loss= 0.4412 accuracy= 0.7880\n",
      "Epoch: 01762 loss_train: 0.4154 loss_rec: 0.4154 acc_train: 0.7956 loss_val: 0.4258 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01763 loss_train: 0.4154 loss_rec: 0.4154 acc_train: 0.7956 loss_val: 0.4258 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01764 loss_train: 0.4154 loss_rec: 0.4154 acc_train: 0.7956 loss_val: 0.4258 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01765 loss_train: 0.4154 loss_rec: 0.4154 acc_train: 0.7956 loss_val: 0.4258 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01766 loss_train: 0.4154 loss_rec: 0.4154 acc_train: 0.7956 loss_val: 0.4258 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01767 loss_train: 0.4153 loss_rec: 0.4153 acc_train: 0.7956 loss_val: 0.4258 acc_val: 0.7863 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01768 loss_train: 0.4153 loss_rec: 0.4153 acc_train: 0.7956 loss_val: 0.4258 acc_val: 0.7863 time: 0.0020s\n",
      "Epoch: 01769 loss_train: 0.4153 loss_rec: 0.4153 acc_train: 0.7956 loss_val: 0.4257 acc_val: 0.7863 time: 0.0015s\n",
      "Epoch: 01770 loss_train: 0.4153 loss_rec: 0.4153 acc_train: 0.7956 loss_val: 0.4257 acc_val: 0.7863 time: 0.0020s\n",
      "Epoch: 01771 loss_train: 0.4153 loss_rec: 0.4153 acc_train: 0.7956 loss_val: 0.4257 acc_val: 0.7863 time: 0.0015s\n",
      "Test set results: loss= 0.4412 accuracy= 0.7878\n",
      "Epoch: 01772 loss_train: 0.4152 loss_rec: 0.4152 acc_train: 0.7957 loss_val: 0.4257 acc_val: 0.7863 time: 0.0015s\n",
      "Epoch: 01773 loss_train: 0.4152 loss_rec: 0.4152 acc_train: 0.7957 loss_val: 0.4257 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01774 loss_train: 0.4152 loss_rec: 0.4152 acc_train: 0.7957 loss_val: 0.4257 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01775 loss_train: 0.4152 loss_rec: 0.4152 acc_train: 0.7957 loss_val: 0.4257 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01776 loss_train: 0.4151 loss_rec: 0.4151 acc_train: 0.7957 loss_val: 0.4256 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01777 loss_train: 0.4151 loss_rec: 0.4151 acc_train: 0.7957 loss_val: 0.4256 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01778 loss_train: 0.4151 loss_rec: 0.4151 acc_train: 0.7957 loss_val: 0.4256 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01779 loss_train: 0.4151 loss_rec: 0.4151 acc_train: 0.7957 loss_val: 0.4256 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01780 loss_train: 0.4151 loss_rec: 0.4151 acc_train: 0.7957 loss_val: 0.4256 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01781 loss_train: 0.4150 loss_rec: 0.4150 acc_train: 0.7957 loss_val: 0.4256 acc_val: 0.7867 time: 0.0020s\n",
      "Test set results: loss= 0.4411 accuracy= 0.7878\n",
      "Epoch: 01782 loss_train: 0.4150 loss_rec: 0.4150 acc_train: 0.7957 loss_val: 0.4256 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01783 loss_train: 0.4150 loss_rec: 0.4150 acc_train: 0.7957 loss_val: 0.4255 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01784 loss_train: 0.4150 loss_rec: 0.4150 acc_train: 0.7957 loss_val: 0.4255 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01785 loss_train: 0.4150 loss_rec: 0.4150 acc_train: 0.7957 loss_val: 0.4255 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01786 loss_train: 0.4149 loss_rec: 0.4149 acc_train: 0.7957 loss_val: 0.4255 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01787 loss_train: 0.4149 loss_rec: 0.4149 acc_train: 0.7957 loss_val: 0.4255 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01788 loss_train: 0.4149 loss_rec: 0.4149 acc_train: 0.7957 loss_val: 0.4255 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01789 loss_train: 0.4149 loss_rec: 0.4149 acc_train: 0.7957 loss_val: 0.4254 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01790 loss_train: 0.4149 loss_rec: 0.4149 acc_train: 0.7957 loss_val: 0.4254 acc_val: 0.7863 time: 0.0020s\n",
      "Epoch: 01791 loss_train: 0.4148 loss_rec: 0.4148 acc_train: 0.7957 loss_val: 0.4254 acc_val: 0.7863 time: 0.0015s\n",
      "Test set results: loss= 0.4410 accuracy= 0.7877\n",
      "Epoch: 01792 loss_train: 0.4148 loss_rec: 0.4148 acc_train: 0.7957 loss_val: 0.4254 acc_val: 0.7863 time: 0.0015s\n",
      "Epoch: 01793 loss_train: 0.4148 loss_rec: 0.4148 acc_train: 0.7957 loss_val: 0.4254 acc_val: 0.7863 time: 0.0020s\n",
      "Epoch: 01794 loss_train: 0.4148 loss_rec: 0.4148 acc_train: 0.7957 loss_val: 0.4254 acc_val: 0.7863 time: 0.0015s\n",
      "Epoch: 01795 loss_train: 0.4147 loss_rec: 0.4147 acc_train: 0.7957 loss_val: 0.4254 acc_val: 0.7863 time: 0.0020s\n",
      "Epoch: 01796 loss_train: 0.4147 loss_rec: 0.4147 acc_train: 0.7957 loss_val: 0.4254 acc_val: 0.7863 time: 0.0015s\n",
      "Epoch: 01797 loss_train: 0.4147 loss_rec: 0.4147 acc_train: 0.7959 loss_val: 0.4254 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01798 loss_train: 0.4147 loss_rec: 0.4147 acc_train: 0.7959 loss_val: 0.4253 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01799 loss_train: 0.4147 loss_rec: 0.4147 acc_train: 0.7959 loss_val: 0.4253 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01800 loss_train: 0.4147 loss_rec: 0.4147 acc_train: 0.7959 loss_val: 0.4253 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01801 loss_train: 0.4146 loss_rec: 0.4146 acc_train: 0.7959 loss_val: 0.4253 acc_val: 0.7867 time: 0.0020s\n",
      "Test set results: loss= 0.4410 accuracy= 0.7879\n",
      "Epoch: 01802 loss_train: 0.4146 loss_rec: 0.4146 acc_train: 0.7959 loss_val: 0.4253 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01803 loss_train: 0.4146 loss_rec: 0.4146 acc_train: 0.7959 loss_val: 0.4253 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01804 loss_train: 0.4146 loss_rec: 0.4146 acc_train: 0.7959 loss_val: 0.4252 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01805 loss_train: 0.4145 loss_rec: 0.4145 acc_train: 0.7959 loss_val: 0.4252 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01806 loss_train: 0.4145 loss_rec: 0.4145 acc_train: 0.7959 loss_val: 0.4252 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01807 loss_train: 0.4145 loss_rec: 0.4145 acc_train: 0.7959 loss_val: 0.4252 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01808 loss_train: 0.4145 loss_rec: 0.4145 acc_train: 0.7959 loss_val: 0.4252 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01809 loss_train: 0.4145 loss_rec: 0.4145 acc_train: 0.7959 loss_val: 0.4252 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01810 loss_train: 0.4144 loss_rec: 0.4144 acc_train: 0.7959 loss_val: 0.4252 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01811 loss_train: 0.4144 loss_rec: 0.4144 acc_train: 0.7959 loss_val: 0.4251 acc_val: 0.7867 time: 0.0020s\n",
      "Test set results: loss= 0.4409 accuracy= 0.7879\n",
      "Epoch: 01812 loss_train: 0.4144 loss_rec: 0.4144 acc_train: 0.7959 loss_val: 0.4252 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01813 loss_train: 0.4144 loss_rec: 0.4144 acc_train: 0.7959 loss_val: 0.4251 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01814 loss_train: 0.4144 loss_rec: 0.4144 acc_train: 0.7959 loss_val: 0.4251 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01815 loss_train: 0.4143 loss_rec: 0.4143 acc_train: 0.7959 loss_val: 0.4251 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01816 loss_train: 0.4143 loss_rec: 0.4143 acc_train: 0.7959 loss_val: 0.4251 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01817 loss_train: 0.4143 loss_rec: 0.4143 acc_train: 0.7959 loss_val: 0.4251 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01818 loss_train: 0.4143 loss_rec: 0.4143 acc_train: 0.7959 loss_val: 0.4251 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01819 loss_train: 0.4143 loss_rec: 0.4143 acc_train: 0.7959 loss_val: 0.4251 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01820 loss_train: 0.4143 loss_rec: 0.4143 acc_train: 0.7959 loss_val: 0.4250 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01821 loss_train: 0.4142 loss_rec: 0.4142 acc_train: 0.7959 loss_val: 0.4250 acc_val: 0.7867 time: 0.0020s\n",
      "Test set results: loss= 0.4409 accuracy= 0.7880\n",
      "Epoch: 01822 loss_train: 0.4142 loss_rec: 0.4142 acc_train: 0.7959 loss_val: 0.4250 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01823 loss_train: 0.4142 loss_rec: 0.4142 acc_train: 0.7959 loss_val: 0.4250 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01824 loss_train: 0.4142 loss_rec: 0.4142 acc_train: 0.7959 loss_val: 0.4250 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01825 loss_train: 0.4142 loss_rec: 0.4142 acc_train: 0.7959 loss_val: 0.4250 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01826 loss_train: 0.4141 loss_rec: 0.4141 acc_train: 0.7959 loss_val: 0.4250 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01827 loss_train: 0.4141 loss_rec: 0.4141 acc_train: 0.7959 loss_val: 0.4249 acc_val: 0.7867 time: 0.0020s\n",
      "Epoch: 01828 loss_train: 0.4141 loss_rec: 0.4141 acc_train: 0.7959 loss_val: 0.4249 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01829 loss_train: 0.4141 loss_rec: 0.4141 acc_train: 0.7959 loss_val: 0.4249 acc_val: 0.7867 time: 0.0025s\n",
      "Epoch: 01830 loss_train: 0.4141 loss_rec: 0.4141 acc_train: 0.7959 loss_val: 0.4249 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01831 loss_train: 0.4140 loss_rec: 0.4140 acc_train: 0.7959 loss_val: 0.4249 acc_val: 0.7867 time: 0.0020s\n",
      "Test set results: loss= 0.4408 accuracy= 0.7879\n",
      "Epoch: 01832 loss_train: 0.4140 loss_rec: 0.4140 acc_train: 0.7959 loss_val: 0.4249 acc_val: 0.7867 time: 0.0015s\n",
      "Epoch: 01833 loss_train: 0.4140 loss_rec: 0.4140 acc_train: 0.7959 loss_val: 0.4249 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01834 loss_train: 0.4140 loss_rec: 0.4140 acc_train: 0.7959 loss_val: 0.4249 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01835 loss_train: 0.4140 loss_rec: 0.4140 acc_train: 0.7959 loss_val: 0.4249 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01836 loss_train: 0.4139 loss_rec: 0.4139 acc_train: 0.7959 loss_val: 0.4248 acc_val: 0.7871 time: 0.0015s\n",
      "Epoch: 01837 loss_train: 0.4139 loss_rec: 0.4139 acc_train: 0.7959 loss_val: 0.4248 acc_val: 0.7871 time: 0.0020s\n",
      "Epoch: 01838 loss_train: 0.4139 loss_rec: 0.4139 acc_train: 0.7959 loss_val: 0.4248 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01839 loss_train: 0.4139 loss_rec: 0.4139 acc_train: 0.7960 loss_val: 0.4248 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01840 loss_train: 0.4139 loss_rec: 0.4139 acc_train: 0.7960 loss_val: 0.4248 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01841 loss_train: 0.4138 loss_rec: 0.4138 acc_train: 0.7960 loss_val: 0.4248 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4408 accuracy= 0.7878\n",
      "Epoch: 01842 loss_train: 0.4138 loss_rec: 0.4138 acc_train: 0.7960 loss_val: 0.4248 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01843 loss_train: 0.4138 loss_rec: 0.4138 acc_train: 0.7960 loss_val: 0.4248 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01844 loss_train: 0.4138 loss_rec: 0.4138 acc_train: 0.7960 loss_val: 0.4247 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01845 loss_train: 0.4138 loss_rec: 0.4138 acc_train: 0.7960 loss_val: 0.4247 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01846 loss_train: 0.4138 loss_rec: 0.4138 acc_train: 0.7960 loss_val: 0.4247 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01847 loss_train: 0.4137 loss_rec: 0.4137 acc_train: 0.7960 loss_val: 0.4247 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01848 loss_train: 0.4137 loss_rec: 0.4137 acc_train: 0.7961 loss_val: 0.4247 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01849 loss_train: 0.4137 loss_rec: 0.4137 acc_train: 0.7961 loss_val: 0.4247 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01850 loss_train: 0.4137 loss_rec: 0.4137 acc_train: 0.7961 loss_val: 0.4247 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01851 loss_train: 0.4136 loss_rec: 0.4136 acc_train: 0.7961 loss_val: 0.4247 acc_val: 0.7875 time: 0.0020s\n",
      "Test set results: loss= 0.4407 accuracy= 0.7875\n",
      "Epoch: 01852 loss_train: 0.4136 loss_rec: 0.4136 acc_train: 0.7961 loss_val: 0.4247 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01853 loss_train: 0.4136 loss_rec: 0.4136 acc_train: 0.7961 loss_val: 0.4246 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01854 loss_train: 0.4136 loss_rec: 0.4136 acc_train: 0.7961 loss_val: 0.4246 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01855 loss_train: 0.4136 loss_rec: 0.4136 acc_train: 0.7961 loss_val: 0.4246 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01856 loss_train: 0.4136 loss_rec: 0.4136 acc_train: 0.7961 loss_val: 0.4246 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01857 loss_train: 0.4135 loss_rec: 0.4135 acc_train: 0.7961 loss_val: 0.4246 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01858 loss_train: 0.4135 loss_rec: 0.4135 acc_train: 0.7961 loss_val: 0.4246 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01859 loss_train: 0.4135 loss_rec: 0.4135 acc_train: 0.7961 loss_val: 0.4246 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01860 loss_train: 0.4135 loss_rec: 0.4135 acc_train: 0.7961 loss_val: 0.4246 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01861 loss_train: 0.4135 loss_rec: 0.4135 acc_train: 0.7961 loss_val: 0.4245 acc_val: 0.7875 time: 0.0020s\n",
      "Test set results: loss= 0.4406 accuracy= 0.7874\n",
      "Epoch: 01862 loss_train: 0.4134 loss_rec: 0.4134 acc_train: 0.7961 loss_val: 0.4245 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01863 loss_train: 0.4134 loss_rec: 0.4134 acc_train: 0.7961 loss_val: 0.4245 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 01864 loss_train: 0.4134 loss_rec: 0.4134 acc_train: 0.7961 loss_val: 0.4245 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01865 loss_train: 0.4134 loss_rec: 0.4134 acc_train: 0.7962 loss_val: 0.4245 acc_val: 0.7875 time: 0.0025s\n",
      "Epoch: 01866 loss_train: 0.4134 loss_rec: 0.4134 acc_train: 0.7962 loss_val: 0.4245 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01867 loss_train: 0.4133 loss_rec: 0.4133 acc_train: 0.7961 loss_val: 0.4245 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 01868 loss_train: 0.4133 loss_rec: 0.4133 acc_train: 0.7961 loss_val: 0.4245 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01869 loss_train: 0.4133 loss_rec: 0.4133 acc_train: 0.7961 loss_val: 0.4245 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01870 loss_train: 0.4133 loss_rec: 0.4133 acc_train: 0.7961 loss_val: 0.4244 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01871 loss_train: 0.4133 loss_rec: 0.4133 acc_train: 0.7961 loss_val: 0.4244 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4406 accuracy= 0.7874\n",
      "Epoch: 01872 loss_train: 0.4133 loss_rec: 0.4133 acc_train: 0.7961 loss_val: 0.4244 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01873 loss_train: 0.4132 loss_rec: 0.4132 acc_train: 0.7961 loss_val: 0.4244 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01874 loss_train: 0.4132 loss_rec: 0.4132 acc_train: 0.7961 loss_val: 0.4244 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01875 loss_train: 0.4132 loss_rec: 0.4132 acc_train: 0.7961 loss_val: 0.4244 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01876 loss_train: 0.4132 loss_rec: 0.4132 acc_train: 0.7961 loss_val: 0.4244 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01877 loss_train: 0.4132 loss_rec: 0.4132 acc_train: 0.7961 loss_val: 0.4244 acc_val: 0.7879 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01878 loss_train: 0.4131 loss_rec: 0.4131 acc_train: 0.7963 loss_val: 0.4244 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01879 loss_train: 0.4131 loss_rec: 0.4131 acc_train: 0.7963 loss_val: 0.4244 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01880 loss_train: 0.4131 loss_rec: 0.4131 acc_train: 0.7963 loss_val: 0.4244 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01881 loss_train: 0.4131 loss_rec: 0.4131 acc_train: 0.7963 loss_val: 0.4243 acc_val: 0.7879 time: 0.0020s\n",
      "Test set results: loss= 0.4405 accuracy= 0.7873\n",
      "Epoch: 01882 loss_train: 0.4131 loss_rec: 0.4131 acc_train: 0.7963 loss_val: 0.4243 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01883 loss_train: 0.4131 loss_rec: 0.4131 acc_train: 0.7964 loss_val: 0.4243 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01884 loss_train: 0.4130 loss_rec: 0.4130 acc_train: 0.7964 loss_val: 0.4243 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01885 loss_train: 0.4130 loss_rec: 0.4130 acc_train: 0.7964 loss_val: 0.4243 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01886 loss_train: 0.4130 loss_rec: 0.4130 acc_train: 0.7964 loss_val: 0.4243 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01887 loss_train: 0.4130 loss_rec: 0.4130 acc_train: 0.7964 loss_val: 0.4243 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01888 loss_train: 0.4130 loss_rec: 0.4130 acc_train: 0.7964 loss_val: 0.4243 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01889 loss_train: 0.4130 loss_rec: 0.4130 acc_train: 0.7964 loss_val: 0.4243 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01890 loss_train: 0.4129 loss_rec: 0.4129 acc_train: 0.7964 loss_val: 0.4243 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01891 loss_train: 0.4129 loss_rec: 0.4129 acc_train: 0.7964 loss_val: 0.4243 acc_val: 0.7879 time: 0.0020s\n",
      "Test set results: loss= 0.4405 accuracy= 0.7873\n",
      "Epoch: 01892 loss_train: 0.4129 loss_rec: 0.4129 acc_train: 0.7964 loss_val: 0.4243 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01893 loss_train: 0.4129 loss_rec: 0.4129 acc_train: 0.7964 loss_val: 0.4242 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01894 loss_train: 0.4129 loss_rec: 0.4129 acc_train: 0.7964 loss_val: 0.4242 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01895 loss_train: 0.4128 loss_rec: 0.4128 acc_train: 0.7964 loss_val: 0.4242 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01896 loss_train: 0.4128 loss_rec: 0.4128 acc_train: 0.7964 loss_val: 0.4242 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01897 loss_train: 0.4128 loss_rec: 0.4128 acc_train: 0.7964 loss_val: 0.4242 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01898 loss_train: 0.4128 loss_rec: 0.4128 acc_train: 0.7964 loss_val: 0.4242 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01899 loss_train: 0.4128 loss_rec: 0.4128 acc_train: 0.7964 loss_val: 0.4242 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01900 loss_train: 0.4128 loss_rec: 0.4128 acc_train: 0.7964 loss_val: 0.4242 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01901 loss_train: 0.4127 loss_rec: 0.4127 acc_train: 0.7964 loss_val: 0.4242 acc_val: 0.7879 time: 0.0020s\n",
      "Test set results: loss= 0.4405 accuracy= 0.7874\n",
      "Epoch: 01902 loss_train: 0.4127 loss_rec: 0.4127 acc_train: 0.7964 loss_val: 0.4242 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01903 loss_train: 0.4127 loss_rec: 0.4127 acc_train: 0.7964 loss_val: 0.4241 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01904 loss_train: 0.4127 loss_rec: 0.4127 acc_train: 0.7964 loss_val: 0.4241 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01905 loss_train: 0.4127 loss_rec: 0.4127 acc_train: 0.7964 loss_val: 0.4241 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01906 loss_train: 0.4127 loss_rec: 0.4127 acc_train: 0.7964 loss_val: 0.4241 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01907 loss_train: 0.4126 loss_rec: 0.4126 acc_train: 0.7964 loss_val: 0.4241 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01908 loss_train: 0.4126 loss_rec: 0.4126 acc_train: 0.7964 loss_val: 0.4241 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01909 loss_train: 0.4126 loss_rec: 0.4126 acc_train: 0.7964 loss_val: 0.4241 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01910 loss_train: 0.4126 loss_rec: 0.4126 acc_train: 0.7964 loss_val: 0.4241 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01911 loss_train: 0.4126 loss_rec: 0.4126 acc_train: 0.7964 loss_val: 0.4241 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4404 accuracy= 0.7875\n",
      "Epoch: 01912 loss_train: 0.4125 loss_rec: 0.4125 acc_train: 0.7964 loss_val: 0.4241 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01913 loss_train: 0.4125 loss_rec: 0.4125 acc_train: 0.7964 loss_val: 0.4240 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01914 loss_train: 0.4125 loss_rec: 0.4125 acc_train: 0.7964 loss_val: 0.4240 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01915 loss_train: 0.4125 loss_rec: 0.4125 acc_train: 0.7964 loss_val: 0.4240 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01916 loss_train: 0.4125 loss_rec: 0.4125 acc_train: 0.7964 loss_val: 0.4240 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01917 loss_train: 0.4125 loss_rec: 0.4125 acc_train: 0.7964 loss_val: 0.4240 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01918 loss_train: 0.4125 loss_rec: 0.4125 acc_train: 0.7964 loss_val: 0.4240 acc_val: 0.7883 time: 0.0025s\n",
      "Epoch: 01919 loss_train: 0.4124 loss_rec: 0.4124 acc_train: 0.7965 loss_val: 0.4240 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01920 loss_train: 0.4124 loss_rec: 0.4124 acc_train: 0.7965 loss_val: 0.4240 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01921 loss_train: 0.4124 loss_rec: 0.4124 acc_train: 0.7965 loss_val: 0.4240 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4404 accuracy= 0.7875\n",
      "Epoch: 01922 loss_train: 0.4124 loss_rec: 0.4124 acc_train: 0.7965 loss_val: 0.4240 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01923 loss_train: 0.4124 loss_rec: 0.4124 acc_train: 0.7965 loss_val: 0.4240 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01924 loss_train: 0.4124 loss_rec: 0.4124 acc_train: 0.7965 loss_val: 0.4240 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01925 loss_train: 0.4123 loss_rec: 0.4123 acc_train: 0.7965 loss_val: 0.4239 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01926 loss_train: 0.4123 loss_rec: 0.4123 acc_train: 0.7966 loss_val: 0.4239 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01927 loss_train: 0.4123 loss_rec: 0.4123 acc_train: 0.7966 loss_val: 0.4239 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01928 loss_train: 0.4123 loss_rec: 0.4123 acc_train: 0.7965 loss_val: 0.4239 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01929 loss_train: 0.4123 loss_rec: 0.4123 acc_train: 0.7965 loss_val: 0.4239 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01930 loss_train: 0.4123 loss_rec: 0.4123 acc_train: 0.7965 loss_val: 0.4239 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01931 loss_train: 0.4122 loss_rec: 0.4122 acc_train: 0.7965 loss_val: 0.4239 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4404 accuracy= 0.7875\n",
      "Epoch: 01932 loss_train: 0.4122 loss_rec: 0.4122 acc_train: 0.7965 loss_val: 0.4239 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01933 loss_train: 0.4122 loss_rec: 0.4122 acc_train: 0.7965 loss_val: 0.4239 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01934 loss_train: 0.4122 loss_rec: 0.4122 acc_train: 0.7965 loss_val: 0.4239 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01935 loss_train: 0.4122 loss_rec: 0.4122 acc_train: 0.7965 loss_val: 0.4239 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01936 loss_train: 0.4122 loss_rec: 0.4122 acc_train: 0.7965 loss_val: 0.4239 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01937 loss_train: 0.4121 loss_rec: 0.4121 acc_train: 0.7965 loss_val: 0.4239 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01938 loss_train: 0.4121 loss_rec: 0.4121 acc_train: 0.7965 loss_val: 0.4239 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01939 loss_train: 0.4121 loss_rec: 0.4121 acc_train: 0.7965 loss_val: 0.4238 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01940 loss_train: 0.4121 loss_rec: 0.4121 acc_train: 0.7965 loss_val: 0.4238 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01941 loss_train: 0.4121 loss_rec: 0.4121 acc_train: 0.7965 loss_val: 0.4238 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4403 accuracy= 0.7875\n",
      "Epoch: 01942 loss_train: 0.4121 loss_rec: 0.4121 acc_train: 0.7966 loss_val: 0.4238 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01943 loss_train: 0.4120 loss_rec: 0.4120 acc_train: 0.7966 loss_val: 0.4238 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01944 loss_train: 0.4120 loss_rec: 0.4120 acc_train: 0.7966 loss_val: 0.4238 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01945 loss_train: 0.4120 loss_rec: 0.4120 acc_train: 0.7966 loss_val: 0.4238 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01946 loss_train: 0.4120 loss_rec: 0.4120 acc_train: 0.7965 loss_val: 0.4238 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01947 loss_train: 0.4120 loss_rec: 0.4120 acc_train: 0.7965 loss_val: 0.4238 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01948 loss_train: 0.4120 loss_rec: 0.4120 acc_train: 0.7965 loss_val: 0.4238 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01949 loss_train: 0.4119 loss_rec: 0.4119 acc_train: 0.7965 loss_val: 0.4238 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01950 loss_train: 0.4119 loss_rec: 0.4119 acc_train: 0.7965 loss_val: 0.4237 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01951 loss_train: 0.4119 loss_rec: 0.4119 acc_train: 0.7965 loss_val: 0.4237 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4403 accuracy= 0.7874\n",
      "Epoch: 01952 loss_train: 0.4119 loss_rec: 0.4119 acc_train: 0.7965 loss_val: 0.4237 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01953 loss_train: 0.4119 loss_rec: 0.4119 acc_train: 0.7965 loss_val: 0.4237 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01954 loss_train: 0.4119 loss_rec: 0.4119 acc_train: 0.7965 loss_val: 0.4237 acc_val: 0.7883 time: 0.0025s\n",
      "Epoch: 01955 loss_train: 0.4119 loss_rec: 0.4119 acc_train: 0.7965 loss_val: 0.4237 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01956 loss_train: 0.4118 loss_rec: 0.4118 acc_train: 0.7965 loss_val: 0.4237 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01957 loss_train: 0.4118 loss_rec: 0.4118 acc_train: 0.7965 loss_val: 0.4237 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01958 loss_train: 0.4118 loss_rec: 0.4118 acc_train: 0.7965 loss_val: 0.4237 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01959 loss_train: 0.4118 loss_rec: 0.4118 acc_train: 0.7966 loss_val: 0.4237 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01960 loss_train: 0.4118 loss_rec: 0.4118 acc_train: 0.7966 loss_val: 0.4237 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01961 loss_train: 0.4118 loss_rec: 0.4118 acc_train: 0.7966 loss_val: 0.4237 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4403 accuracy= 0.7874\n",
      "Epoch: 01962 loss_train: 0.4117 loss_rec: 0.4117 acc_train: 0.7966 loss_val: 0.4237 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01963 loss_train: 0.4117 loss_rec: 0.4117 acc_train: 0.7966 loss_val: 0.4237 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01964 loss_train: 0.4117 loss_rec: 0.4117 acc_train: 0.7966 loss_val: 0.4236 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01965 loss_train: 0.4117 loss_rec: 0.4117 acc_train: 0.7966 loss_val: 0.4236 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01966 loss_train: 0.4117 loss_rec: 0.4117 acc_train: 0.7967 loss_val: 0.4236 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01967 loss_train: 0.4117 loss_rec: 0.4117 acc_train: 0.7966 loss_val: 0.4236 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01968 loss_train: 0.4117 loss_rec: 0.4117 acc_train: 0.7966 loss_val: 0.4236 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01969 loss_train: 0.4116 loss_rec: 0.4116 acc_train: 0.7967 loss_val: 0.4236 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01970 loss_train: 0.4116 loss_rec: 0.4116 acc_train: 0.7967 loss_val: 0.4236 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01971 loss_train: 0.4116 loss_rec: 0.4116 acc_train: 0.7967 loss_val: 0.4236 acc_val: 0.7883 time: 0.0020s\n",
      "Test set results: loss= 0.4403 accuracy= 0.7874\n",
      "Epoch: 01972 loss_train: 0.4116 loss_rec: 0.4116 acc_train: 0.7967 loss_val: 0.4236 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01973 loss_train: 0.4116 loss_rec: 0.4116 acc_train: 0.7967 loss_val: 0.4236 acc_val: 0.7883 time: 0.0016s\n",
      "Epoch: 01974 loss_train: 0.4116 loss_rec: 0.4116 acc_train: 0.7967 loss_val: 0.4236 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01975 loss_train: 0.4115 loss_rec: 0.4115 acc_train: 0.7967 loss_val: 0.4236 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01976 loss_train: 0.4115 loss_rec: 0.4115 acc_train: 0.7967 loss_val: 0.4236 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01977 loss_train: 0.4115 loss_rec: 0.4115 acc_train: 0.7967 loss_val: 0.4236 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01978 loss_train: 0.4115 loss_rec: 0.4115 acc_train: 0.7967 loss_val: 0.4236 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01979 loss_train: 0.4115 loss_rec: 0.4115 acc_train: 0.7967 loss_val: 0.4236 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01980 loss_train: 0.4115 loss_rec: 0.4115 acc_train: 0.7967 loss_val: 0.4235 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01981 loss_train: 0.4115 loss_rec: 0.4115 acc_train: 0.7967 loss_val: 0.4235 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4402 accuracy= 0.7875\n",
      "Epoch: 01982 loss_train: 0.4114 loss_rec: 0.4114 acc_train: 0.7967 loss_val: 0.4235 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01983 loss_train: 0.4114 loss_rec: 0.4114 acc_train: 0.7967 loss_val: 0.4235 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01984 loss_train: 0.4114 loss_rec: 0.4114 acc_train: 0.7967 loss_val: 0.4235 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01985 loss_train: 0.4114 loss_rec: 0.4114 acc_train: 0.7969 loss_val: 0.4235 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01986 loss_train: 0.4114 loss_rec: 0.4114 acc_train: 0.7969 loss_val: 0.4235 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 01987 loss_train: 0.4114 loss_rec: 0.4114 acc_train: 0.7969 loss_val: 0.4235 acc_val: 0.7883 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01988 loss_train: 0.4113 loss_rec: 0.4113 acc_train: 0.7969 loss_val: 0.4235 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01989 loss_train: 0.4113 loss_rec: 0.4113 acc_train: 0.7969 loss_val: 0.4235 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 01990 loss_train: 0.4113 loss_rec: 0.4113 acc_train: 0.7969 loss_val: 0.4235 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01991 loss_train: 0.4113 loss_rec: 0.4113 acc_train: 0.7969 loss_val: 0.4235 acc_val: 0.7879 time: 0.0025s\n",
      "Test set results: loss= 0.4403 accuracy= 0.7877\n",
      "Epoch: 01992 loss_train: 0.4113 loss_rec: 0.4113 acc_train: 0.7969 loss_val: 0.4234 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01993 loss_train: 0.4113 loss_rec: 0.4113 acc_train: 0.7969 loss_val: 0.4234 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01994 loss_train: 0.4113 loss_rec: 0.4113 acc_train: 0.7970 loss_val: 0.4234 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01995 loss_train: 0.4112 loss_rec: 0.4112 acc_train: 0.7970 loss_val: 0.4234 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01996 loss_train: 0.4112 loss_rec: 0.4112 acc_train: 0.7970 loss_val: 0.4234 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01997 loss_train: 0.4112 loss_rec: 0.4112 acc_train: 0.7970 loss_val: 0.4234 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 01998 loss_train: 0.4112 loss_rec: 0.4112 acc_train: 0.7970 loss_val: 0.4234 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 01999 loss_train: 0.4112 loss_rec: 0.4112 acc_train: 0.7970 loss_val: 0.4234 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02000 loss_train: 0.4112 loss_rec: 0.4112 acc_train: 0.7970 loss_val: 0.4234 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02001 loss_train: 0.4112 loss_rec: 0.4112 acc_train: 0.7970 loss_val: 0.4234 acc_val: 0.7879 time: 0.0020s\n",
      "Test set results: loss= 0.4402 accuracy= 0.7878\n",
      "Epoch: 02002 loss_train: 0.4112 loss_rec: 0.4112 acc_train: 0.7970 loss_val: 0.4234 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02003 loss_train: 0.4111 loss_rec: 0.4111 acc_train: 0.7970 loss_val: 0.4234 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02004 loss_train: 0.4111 loss_rec: 0.4111 acc_train: 0.7970 loss_val: 0.4234 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02005 loss_train: 0.4111 loss_rec: 0.4111 acc_train: 0.7970 loss_val: 0.4234 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02006 loss_train: 0.4111 loss_rec: 0.4111 acc_train: 0.7970 loss_val: 0.4234 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02007 loss_train: 0.4111 loss_rec: 0.4111 acc_train: 0.7971 loss_val: 0.4234 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02008 loss_train: 0.4111 loss_rec: 0.4111 acc_train: 0.7971 loss_val: 0.4234 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02009 loss_train: 0.4110 loss_rec: 0.4110 acc_train: 0.7971 loss_val: 0.4234 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02010 loss_train: 0.4110 loss_rec: 0.4110 acc_train: 0.7971 loss_val: 0.4233 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02011 loss_train: 0.4110 loss_rec: 0.4110 acc_train: 0.7971 loss_val: 0.4233 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4402 accuracy= 0.7878\n",
      "Epoch: 02012 loss_train: 0.4110 loss_rec: 0.4110 acc_train: 0.7971 loss_val: 0.4233 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02013 loss_train: 0.4110 loss_rec: 0.4110 acc_train: 0.7971 loss_val: 0.4233 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02014 loss_train: 0.4110 loss_rec: 0.4110 acc_train: 0.7971 loss_val: 0.4233 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02015 loss_train: 0.4110 loss_rec: 0.4110 acc_train: 0.7971 loss_val: 0.4233 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02016 loss_train: 0.4109 loss_rec: 0.4109 acc_train: 0.7971 loss_val: 0.4233 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02017 loss_train: 0.4109 loss_rec: 0.4109 acc_train: 0.7971 loss_val: 0.4233 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02018 loss_train: 0.4109 loss_rec: 0.4109 acc_train: 0.7971 loss_val: 0.4233 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02019 loss_train: 0.4109 loss_rec: 0.4109 acc_train: 0.7971 loss_val: 0.4233 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02020 loss_train: 0.4109 loss_rec: 0.4109 acc_train: 0.7971 loss_val: 0.4233 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02021 loss_train: 0.4109 loss_rec: 0.4109 acc_train: 0.7971 loss_val: 0.4233 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4402 accuracy= 0.7879\n",
      "Epoch: 02022 loss_train: 0.4109 loss_rec: 0.4109 acc_train: 0.7972 loss_val: 0.4233 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02023 loss_train: 0.4108 loss_rec: 0.4108 acc_train: 0.7973 loss_val: 0.4233 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02024 loss_train: 0.4108 loss_rec: 0.4108 acc_train: 0.7973 loss_val: 0.4233 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02025 loss_train: 0.4108 loss_rec: 0.4108 acc_train: 0.7973 loss_val: 0.4233 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02026 loss_train: 0.4108 loss_rec: 0.4108 acc_train: 0.7973 loss_val: 0.4233 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02027 loss_train: 0.4108 loss_rec: 0.4108 acc_train: 0.7973 loss_val: 0.4233 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02028 loss_train: 0.4108 loss_rec: 0.4108 acc_train: 0.7973 loss_val: 0.4233 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02029 loss_train: 0.4108 loss_rec: 0.4108 acc_train: 0.7973 loss_val: 0.4233 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02030 loss_train: 0.4107 loss_rec: 0.4107 acc_train: 0.7973 loss_val: 0.4233 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02031 loss_train: 0.4107 loss_rec: 0.4107 acc_train: 0.7973 loss_val: 0.4232 acc_val: 0.7879 time: 0.0020s\n",
      "Test set results: loss= 0.4402 accuracy= 0.7879\n",
      "Epoch: 02032 loss_train: 0.4107 loss_rec: 0.4107 acc_train: 0.7973 loss_val: 0.4232 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02033 loss_train: 0.4107 loss_rec: 0.4107 acc_train: 0.7973 loss_val: 0.4232 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02034 loss_train: 0.4107 loss_rec: 0.4107 acc_train: 0.7973 loss_val: 0.4232 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02035 loss_train: 0.4107 loss_rec: 0.4107 acc_train: 0.7973 loss_val: 0.4232 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02036 loss_train: 0.4107 loss_rec: 0.4107 acc_train: 0.7973 loss_val: 0.4232 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02037 loss_train: 0.4106 loss_rec: 0.4106 acc_train: 0.7973 loss_val: 0.4232 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02038 loss_train: 0.4106 loss_rec: 0.4106 acc_train: 0.7973 loss_val: 0.4232 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02039 loss_train: 0.4106 loss_rec: 0.4106 acc_train: 0.7973 loss_val: 0.4232 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02040 loss_train: 0.4106 loss_rec: 0.4106 acc_train: 0.7973 loss_val: 0.4232 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02041 loss_train: 0.4106 loss_rec: 0.4106 acc_train: 0.7973 loss_val: 0.4232 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4402 accuracy= 0.7878\n",
      "Epoch: 02042 loss_train: 0.4106 loss_rec: 0.4106 acc_train: 0.7973 loss_val: 0.4232 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02043 loss_train: 0.4106 loss_rec: 0.4106 acc_train: 0.7973 loss_val: 0.4232 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02044 loss_train: 0.4105 loss_rec: 0.4105 acc_train: 0.7973 loss_val: 0.4232 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02045 loss_train: 0.4105 loss_rec: 0.4105 acc_train: 0.7973 loss_val: 0.4232 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02046 loss_train: 0.4105 loss_rec: 0.4105 acc_train: 0.7973 loss_val: 0.4231 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02047 loss_train: 0.4105 loss_rec: 0.4105 acc_train: 0.7973 loss_val: 0.4231 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02048 loss_train: 0.4105 loss_rec: 0.4105 acc_train: 0.7973 loss_val: 0.4231 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02049 loss_train: 0.4105 loss_rec: 0.4105 acc_train: 0.7974 loss_val: 0.4231 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02050 loss_train: 0.4105 loss_rec: 0.4105 acc_train: 0.7973 loss_val: 0.4231 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02051 loss_train: 0.4104 loss_rec: 0.4104 acc_train: 0.7974 loss_val: 0.4231 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4401 accuracy= 0.7879\n",
      "Epoch: 02052 loss_train: 0.4104 loss_rec: 0.4104 acc_train: 0.7974 loss_val: 0.4231 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02053 loss_train: 0.4104 loss_rec: 0.4104 acc_train: 0.7974 loss_val: 0.4231 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02054 loss_train: 0.4104 loss_rec: 0.4104 acc_train: 0.7974 loss_val: 0.4231 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02055 loss_train: 0.4104 loss_rec: 0.4104 acc_train: 0.7974 loss_val: 0.4231 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02056 loss_train: 0.4104 loss_rec: 0.4104 acc_train: 0.7974 loss_val: 0.4231 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02057 loss_train: 0.4104 loss_rec: 0.4104 acc_train: 0.7974 loss_val: 0.4231 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02058 loss_train: 0.4104 loss_rec: 0.4104 acc_train: 0.7974 loss_val: 0.4231 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02059 loss_train: 0.4103 loss_rec: 0.4103 acc_train: 0.7974 loss_val: 0.4231 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02060 loss_train: 0.4103 loss_rec: 0.4103 acc_train: 0.7974 loss_val: 0.4231 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02061 loss_train: 0.4103 loss_rec: 0.4103 acc_train: 0.7975 loss_val: 0.4231 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4402 accuracy= 0.7880\n",
      "Epoch: 02062 loss_train: 0.4103 loss_rec: 0.4103 acc_train: 0.7975 loss_val: 0.4231 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02063 loss_train: 0.4103 loss_rec: 0.4103 acc_train: 0.7975 loss_val: 0.4231 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02064 loss_train: 0.4103 loss_rec: 0.4103 acc_train: 0.7975 loss_val: 0.4231 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02065 loss_train: 0.4103 loss_rec: 0.4103 acc_train: 0.7976 loss_val: 0.4230 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02066 loss_train: 0.4103 loss_rec: 0.4103 acc_train: 0.7976 loss_val: 0.4230 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02067 loss_train: 0.4102 loss_rec: 0.4102 acc_train: 0.7976 loss_val: 0.4230 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02068 loss_train: 0.4102 loss_rec: 0.4102 acc_train: 0.7976 loss_val: 0.4230 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02069 loss_train: 0.4102 loss_rec: 0.4102 acc_train: 0.7976 loss_val: 0.4230 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02070 loss_train: 0.4102 loss_rec: 0.4102 acc_train: 0.7977 loss_val: 0.4230 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02071 loss_train: 0.4102 loss_rec: 0.4102 acc_train: 0.7977 loss_val: 0.4230 acc_val: 0.7879 time: 0.0020s\n",
      "Test set results: loss= 0.4402 accuracy= 0.7881\n",
      "Epoch: 02072 loss_train: 0.4102 loss_rec: 0.4102 acc_train: 0.7976 loss_val: 0.4230 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02073 loss_train: 0.4102 loss_rec: 0.4102 acc_train: 0.7976 loss_val: 0.4230 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02074 loss_train: 0.4101 loss_rec: 0.4101 acc_train: 0.7976 loss_val: 0.4230 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02075 loss_train: 0.4101 loss_rec: 0.4101 acc_train: 0.7976 loss_val: 0.4230 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02076 loss_train: 0.4101 loss_rec: 0.4101 acc_train: 0.7976 loss_val: 0.4230 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02077 loss_train: 0.4101 loss_rec: 0.4101 acc_train: 0.7976 loss_val: 0.4230 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02078 loss_train: 0.4101 loss_rec: 0.4101 acc_train: 0.7976 loss_val: 0.4230 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02079 loss_train: 0.4101 loss_rec: 0.4101 acc_train: 0.7977 loss_val: 0.4230 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02080 loss_train: 0.4101 loss_rec: 0.4101 acc_train: 0.7976 loss_val: 0.4230 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02081 loss_train: 0.4100 loss_rec: 0.4100 acc_train: 0.7973 loss_val: 0.4230 acc_val: 0.7879 time: 0.0020s\n",
      "Test set results: loss= 0.4402 accuracy= 0.7877\n",
      "Epoch: 02082 loss_train: 0.4100 loss_rec: 0.4100 acc_train: 0.7973 loss_val: 0.4230 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02083 loss_train: 0.4100 loss_rec: 0.4100 acc_train: 0.7973 loss_val: 0.4230 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02084 loss_train: 0.4100 loss_rec: 0.4100 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02085 loss_train: 0.4100 loss_rec: 0.4100 acc_train: 0.7972 loss_val: 0.4229 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02086 loss_train: 0.4100 loss_rec: 0.4100 acc_train: 0.7972 loss_val: 0.4229 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02087 loss_train: 0.4100 loss_rec: 0.4100 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02088 loss_train: 0.4100 loss_rec: 0.4100 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02089 loss_train: 0.4099 loss_rec: 0.4099 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02090 loss_train: 0.4099 loss_rec: 0.4099 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02091 loss_train: 0.4099 loss_rec: 0.4099 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4401 accuracy= 0.7878\n",
      "Epoch: 02092 loss_train: 0.4099 loss_rec: 0.4099 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02093 loss_train: 0.4099 loss_rec: 0.4099 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02094 loss_train: 0.4099 loss_rec: 0.4099 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02095 loss_train: 0.4099 loss_rec: 0.4099 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02096 loss_train: 0.4099 loss_rec: 0.4099 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02097 loss_train: 0.4098 loss_rec: 0.4098 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02098 loss_train: 0.4098 loss_rec: 0.4098 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02099 loss_train: 0.4098 loss_rec: 0.4098 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02100 loss_train: 0.4098 loss_rec: 0.4098 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02101 loss_train: 0.4098 loss_rec: 0.4098 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0020s\n",
      "Test set results: loss= 0.4402 accuracy= 0.7878\n",
      "Epoch: 02102 loss_train: 0.4098 loss_rec: 0.4098 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02103 loss_train: 0.4098 loss_rec: 0.4098 acc_train: 0.7973 loss_val: 0.4229 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02104 loss_train: 0.4098 loss_rec: 0.4098 acc_train: 0.7973 loss_val: 0.4228 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02105 loss_train: 0.4097 loss_rec: 0.4097 acc_train: 0.7973 loss_val: 0.4228 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02106 loss_train: 0.4097 loss_rec: 0.4097 acc_train: 0.7973 loss_val: 0.4228 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02107 loss_train: 0.4097 loss_rec: 0.4097 acc_train: 0.7973 loss_val: 0.4228 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02108 loss_train: 0.4097 loss_rec: 0.4097 acc_train: 0.7973 loss_val: 0.4228 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02109 loss_train: 0.4097 loss_rec: 0.4097 acc_train: 0.7974 loss_val: 0.4228 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02110 loss_train: 0.4097 loss_rec: 0.4097 acc_train: 0.7974 loss_val: 0.4228 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02111 loss_train: 0.4097 loss_rec: 0.4097 acc_train: 0.7974 loss_val: 0.4228 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4402 accuracy= 0.7878\n",
      "Epoch: 02112 loss_train: 0.4097 loss_rec: 0.4097 acc_train: 0.7974 loss_val: 0.4228 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02113 loss_train: 0.4096 loss_rec: 0.4096 acc_train: 0.7975 loss_val: 0.4228 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02114 loss_train: 0.4096 loss_rec: 0.4096 acc_train: 0.7975 loss_val: 0.4228 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02115 loss_train: 0.4096 loss_rec: 0.4096 acc_train: 0.7975 loss_val: 0.4228 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02116 loss_train: 0.4096 loss_rec: 0.4096 acc_train: 0.7975 loss_val: 0.4228 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02117 loss_train: 0.4096 loss_rec: 0.4096 acc_train: 0.7975 loss_val: 0.4228 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02118 loss_train: 0.4096 loss_rec: 0.4096 acc_train: 0.7975 loss_val: 0.4228 acc_val: 0.7879 time: 0.0025s\n",
      "Epoch: 02119 loss_train: 0.4096 loss_rec: 0.4096 acc_train: 0.7975 loss_val: 0.4228 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02120 loss_train: 0.4096 loss_rec: 0.4096 acc_train: 0.7975 loss_val: 0.4228 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02121 loss_train: 0.4095 loss_rec: 0.4095 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0020s\n",
      "Test set results: loss= 0.4402 accuracy= 0.7878\n",
      "Epoch: 02122 loss_train: 0.4095 loss_rec: 0.4095 acc_train: 0.7975 loss_val: 0.4228 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02123 loss_train: 0.4095 loss_rec: 0.4095 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02124 loss_train: 0.4095 loss_rec: 0.4095 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02125 loss_train: 0.4095 loss_rec: 0.4095 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02126 loss_train: 0.4095 loss_rec: 0.4095 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02127 loss_train: 0.4095 loss_rec: 0.4095 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02128 loss_train: 0.4094 loss_rec: 0.4094 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02129 loss_train: 0.4094 loss_rec: 0.4094 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02130 loss_train: 0.4094 loss_rec: 0.4094 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02131 loss_train: 0.4094 loss_rec: 0.4094 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4402 accuracy= 0.7878\n",
      "Epoch: 02132 loss_train: 0.4094 loss_rec: 0.4094 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02133 loss_train: 0.4094 loss_rec: 0.4094 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02134 loss_train: 0.4094 loss_rec: 0.4094 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02135 loss_train: 0.4094 loss_rec: 0.4094 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02136 loss_train: 0.4094 loss_rec: 0.4094 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02137 loss_train: 0.4093 loss_rec: 0.4093 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02138 loss_train: 0.4093 loss_rec: 0.4093 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02139 loss_train: 0.4093 loss_rec: 0.4093 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02140 loss_train: 0.4093 loss_rec: 0.4093 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02141 loss_train: 0.4093 loss_rec: 0.4093 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4402 accuracy= 0.7878\n",
      "Epoch: 02142 loss_train: 0.4093 loss_rec: 0.4093 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02143 loss_train: 0.4093 loss_rec: 0.4093 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02144 loss_train: 0.4093 loss_rec: 0.4093 acc_train: 0.7975 loss_val: 0.4226 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02145 loss_train: 0.4092 loss_rec: 0.4092 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02146 loss_train: 0.4092 loss_rec: 0.4092 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02147 loss_train: 0.4092 loss_rec: 0.4092 acc_train: 0.7975 loss_val: 0.4226 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02148 loss_train: 0.4092 loss_rec: 0.4092 acc_train: 0.7975 loss_val: 0.4227 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02149 loss_train: 0.4092 loss_rec: 0.4092 acc_train: 0.7975 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02150 loss_train: 0.4092 loss_rec: 0.4092 acc_train: 0.7973 loss_val: 0.4226 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02151 loss_train: 0.4092 loss_rec: 0.4092 acc_train: 0.7975 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4402 accuracy= 0.7879\n",
      "Epoch: 02152 loss_train: 0.4092 loss_rec: 0.4092 acc_train: 0.7975 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02153 loss_train: 0.4092 loss_rec: 0.4092 acc_train: 0.7976 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02154 loss_train: 0.4091 loss_rec: 0.4091 acc_train: 0.7976 loss_val: 0.4226 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02155 loss_train: 0.4091 loss_rec: 0.4091 acc_train: 0.7976 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02156 loss_train: 0.4091 loss_rec: 0.4091 acc_train: 0.7976 loss_val: 0.4226 acc_val: 0.7879 time: 0.0025s\n",
      "Epoch: 02157 loss_train: 0.4091 loss_rec: 0.4091 acc_train: 0.7976 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02158 loss_train: 0.4091 loss_rec: 0.4091 acc_train: 0.7976 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02159 loss_train: 0.4091 loss_rec: 0.4091 acc_train: 0.7974 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02160 loss_train: 0.4091 loss_rec: 0.4091 acc_train: 0.7974 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02161 loss_train: 0.4091 loss_rec: 0.4091 acc_train: 0.7974 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4403 accuracy= 0.7878\n",
      "Epoch: 02162 loss_train: 0.4090 loss_rec: 0.4090 acc_train: 0.7974 loss_val: 0.4226 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02163 loss_train: 0.4090 loss_rec: 0.4090 acc_train: 0.7974 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02164 loss_train: 0.4090 loss_rec: 0.4090 acc_train: 0.7974 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02165 loss_train: 0.4090 loss_rec: 0.4090 acc_train: 0.7974 loss_val: 0.4226 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02166 loss_train: 0.4090 loss_rec: 0.4090 acc_train: 0.7974 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02167 loss_train: 0.4090 loss_rec: 0.4090 acc_train: 0.7976 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02168 loss_train: 0.4090 loss_rec: 0.4090 acc_train: 0.7976 loss_val: 0.4226 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02169 loss_train: 0.4090 loss_rec: 0.4090 acc_train: 0.7976 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02170 loss_train: 0.4090 loss_rec: 0.4090 acc_train: 0.7975 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02171 loss_train: 0.4089 loss_rec: 0.4089 acc_train: 0.7975 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4403 accuracy= 0.7875\n",
      "Epoch: 02172 loss_train: 0.4089 loss_rec: 0.4089 acc_train: 0.7973 loss_val: 0.4226 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02173 loss_train: 0.4089 loss_rec: 0.4089 acc_train: 0.7975 loss_val: 0.4226 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02174 loss_train: 0.4089 loss_rec: 0.4089 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02175 loss_train: 0.4089 loss_rec: 0.4089 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7879 time: 0.0025s\n",
      "Epoch: 02176 loss_train: 0.4089 loss_rec: 0.4089 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02177 loss_train: 0.4089 loss_rec: 0.4089 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02178 loss_train: 0.4089 loss_rec: 0.4089 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02179 loss_train: 0.4089 loss_rec: 0.4089 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02180 loss_train: 0.4088 loss_rec: 0.4088 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02181 loss_train: 0.4088 loss_rec: 0.4088 acc_train: 0.7975 loss_val: 0.4225 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4403 accuracy= 0.7877\n",
      "Epoch: 02182 loss_train: 0.4088 loss_rec: 0.4088 acc_train: 0.7975 loss_val: 0.4225 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02183 loss_train: 0.4088 loss_rec: 0.4088 acc_train: 0.7975 loss_val: 0.4225 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02184 loss_train: 0.4088 loss_rec: 0.4088 acc_train: 0.7975 loss_val: 0.4225 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02185 loss_train: 0.4088 loss_rec: 0.4088 acc_train: 0.7975 loss_val: 0.4225 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02186 loss_train: 0.4088 loss_rec: 0.4088 acc_train: 0.7975 loss_val: 0.4225 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02187 loss_train: 0.4088 loss_rec: 0.4088 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02188 loss_train: 0.4087 loss_rec: 0.4087 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02189 loss_train: 0.4087 loss_rec: 0.4087 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02190 loss_train: 0.4087 loss_rec: 0.4087 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02191 loss_train: 0.4087 loss_rec: 0.4087 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4404 accuracy= 0.7875\n",
      "Epoch: 02192 loss_train: 0.4087 loss_rec: 0.4087 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02193 loss_train: 0.4087 loss_rec: 0.4087 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02194 loss_train: 0.4087 loss_rec: 0.4087 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02195 loss_train: 0.4087 loss_rec: 0.4087 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02196 loss_train: 0.4087 loss_rec: 0.4087 acc_train: 0.7975 loss_val: 0.4225 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02197 loss_train: 0.4086 loss_rec: 0.4086 acc_train: 0.7975 loss_val: 0.4225 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02198 loss_train: 0.4086 loss_rec: 0.4086 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02199 loss_train: 0.4086 loss_rec: 0.4086 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02200 loss_train: 0.4086 loss_rec: 0.4086 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02201 loss_train: 0.4086 loss_rec: 0.4086 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7875 time: 0.0020s\n",
      "Test set results: loss= 0.4404 accuracy= 0.7875\n",
      "Epoch: 02202 loss_train: 0.4086 loss_rec: 0.4086 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02203 loss_train: 0.4086 loss_rec: 0.4086 acc_train: 0.7975 loss_val: 0.4225 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02204 loss_train: 0.4086 loss_rec: 0.4086 acc_train: 0.7975 loss_val: 0.4224 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02205 loss_train: 0.4086 loss_rec: 0.4086 acc_train: 0.7975 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02206 loss_train: 0.4085 loss_rec: 0.4085 acc_train: 0.7975 loss_val: 0.4225 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02207 loss_train: 0.4085 loss_rec: 0.4085 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02208 loss_train: 0.4085 loss_rec: 0.4085 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02209 loss_train: 0.4085 loss_rec: 0.4085 acc_train: 0.7975 loss_val: 0.4225 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02210 loss_train: 0.4085 loss_rec: 0.4085 acc_train: 0.7975 loss_val: 0.4225 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02211 loss_train: 0.4085 loss_rec: 0.4085 acc_train: 0.7975 loss_val: 0.4224 acc_val: 0.7875 time: 0.0025s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.4404 accuracy= 0.7875\n",
      "Epoch: 02212 loss_train: 0.4085 loss_rec: 0.4085 acc_train: 0.7973 loss_val: 0.4224 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02213 loss_train: 0.4085 loss_rec: 0.4085 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02214 loss_train: 0.4085 loss_rec: 0.4085 acc_train: 0.7973 loss_val: 0.4225 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02215 loss_train: 0.4085 loss_rec: 0.4085 acc_train: 0.7973 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02216 loss_train: 0.4084 loss_rec: 0.4084 acc_train: 0.7975 loss_val: 0.4224 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02217 loss_train: 0.4084 loss_rec: 0.4084 acc_train: 0.7975 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02218 loss_train: 0.4084 loss_rec: 0.4084 acc_train: 0.7975 loss_val: 0.4224 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02219 loss_train: 0.4084 loss_rec: 0.4084 acc_train: 0.7975 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02220 loss_train: 0.4084 loss_rec: 0.4084 acc_train: 0.7973 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02221 loss_train: 0.4084 loss_rec: 0.4084 acc_train: 0.7973 loss_val: 0.4224 acc_val: 0.7875 time: 0.0020s\n",
      "Test set results: loss= 0.4405 accuracy= 0.7874\n",
      "Epoch: 02222 loss_train: 0.4084 loss_rec: 0.4084 acc_train: 0.7973 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02223 loss_train: 0.4084 loss_rec: 0.4084 acc_train: 0.7973 loss_val: 0.4224 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02224 loss_train: 0.4084 loss_rec: 0.4084 acc_train: 0.7973 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02225 loss_train: 0.4083 loss_rec: 0.4083 acc_train: 0.7973 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02226 loss_train: 0.4083 loss_rec: 0.4083 acc_train: 0.7975 loss_val: 0.4224 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02227 loss_train: 0.4083 loss_rec: 0.4083 acc_train: 0.7975 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02228 loss_train: 0.4083 loss_rec: 0.4083 acc_train: 0.7975 loss_val: 0.4224 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02229 loss_train: 0.4083 loss_rec: 0.4083 acc_train: 0.7975 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02230 loss_train: 0.4083 loss_rec: 0.4083 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02231 loss_train: 0.4083 loss_rec: 0.4083 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0020s\n",
      "Test set results: loss= 0.4405 accuracy= 0.7873\n",
      "Epoch: 02232 loss_train: 0.4083 loss_rec: 0.4083 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02233 loss_train: 0.4083 loss_rec: 0.4083 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02234 loss_train: 0.4083 loss_rec: 0.4083 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02235 loss_train: 0.4082 loss_rec: 0.4082 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02236 loss_train: 0.4082 loss_rec: 0.4082 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02237 loss_train: 0.4082 loss_rec: 0.4082 acc_train: 0.7974 loss_val: 0.4224 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02238 loss_train: 0.4082 loss_rec: 0.4082 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02239 loss_train: 0.4082 loss_rec: 0.4082 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02240 loss_train: 0.4082 loss_rec: 0.4082 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02241 loss_train: 0.4082 loss_rec: 0.4082 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4405 accuracy= 0.7873\n",
      "Epoch: 02242 loss_train: 0.4082 loss_rec: 0.4082 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02243 loss_train: 0.4082 loss_rec: 0.4082 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02244 loss_train: 0.4081 loss_rec: 0.4081 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02245 loss_train: 0.4081 loss_rec: 0.4081 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02246 loss_train: 0.4081 loss_rec: 0.4081 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02247 loss_train: 0.4081 loss_rec: 0.4081 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02248 loss_train: 0.4081 loss_rec: 0.4081 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02249 loss_train: 0.4081 loss_rec: 0.4081 acc_train: 0.7976 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02250 loss_train: 0.4081 loss_rec: 0.4081 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02251 loss_train: 0.4081 loss_rec: 0.4081 acc_train: 0.7974 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4405 accuracy= 0.7873\n",
      "Epoch: 02252 loss_train: 0.4081 loss_rec: 0.4081 acc_train: 0.7974 loss_val: 0.4224 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02253 loss_train: 0.4081 loss_rec: 0.4081 acc_train: 0.7974 loss_val: 0.4224 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02254 loss_train: 0.4081 loss_rec: 0.4081 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02255 loss_train: 0.4080 loss_rec: 0.4080 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02256 loss_train: 0.4080 loss_rec: 0.4080 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02257 loss_train: 0.4080 loss_rec: 0.4080 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02258 loss_train: 0.4080 loss_rec: 0.4080 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02259 loss_train: 0.4080 loss_rec: 0.4080 acc_train: 0.7974 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02260 loss_train: 0.4080 loss_rec: 0.4080 acc_train: 0.7974 loss_val: 0.4223 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02261 loss_train: 0.4080 loss_rec: 0.4080 acc_train: 0.7974 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4405 accuracy= 0.7873\n",
      "Epoch: 02262 loss_train: 0.4080 loss_rec: 0.4080 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02263 loss_train: 0.4080 loss_rec: 0.4080 acc_train: 0.7977 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02264 loss_train: 0.4079 loss_rec: 0.4079 acc_train: 0.7977 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02265 loss_train: 0.4079 loss_rec: 0.4079 acc_train: 0.7977 loss_val: 0.4223 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02266 loss_train: 0.4079 loss_rec: 0.4079 acc_train: 0.7975 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02267 loss_train: 0.4079 loss_rec: 0.4079 acc_train: 0.7975 loss_val: 0.4223 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02268 loss_train: 0.4079 loss_rec: 0.4079 acc_train: 0.7975 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02269 loss_train: 0.4079 loss_rec: 0.4079 acc_train: 0.7974 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02270 loss_train: 0.4079 loss_rec: 0.4079 acc_train: 0.7975 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02271 loss_train: 0.4079 loss_rec: 0.4079 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0020s\n",
      "Test set results: loss= 0.4405 accuracy= 0.7873\n",
      "Epoch: 02272 loss_train: 0.4079 loss_rec: 0.4079 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02273 loss_train: 0.4079 loss_rec: 0.4079 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02274 loss_train: 0.4078 loss_rec: 0.4078 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02275 loss_train: 0.4078 loss_rec: 0.4078 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02276 loss_train: 0.4078 loss_rec: 0.4078 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02277 loss_train: 0.4078 loss_rec: 0.4078 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02278 loss_train: 0.4078 loss_rec: 0.4078 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02279 loss_train: 0.4078 loss_rec: 0.4078 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02280 loss_train: 0.4078 loss_rec: 0.4078 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02281 loss_train: 0.4078 loss_rec: 0.4078 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0020s\n",
      "Test set results: loss= 0.4406 accuracy= 0.7873\n",
      "Epoch: 02282 loss_train: 0.4078 loss_rec: 0.4078 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02283 loss_train: 0.4078 loss_rec: 0.4078 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02284 loss_train: 0.4077 loss_rec: 0.4077 acc_train: 0.7976 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02285 loss_train: 0.4077 loss_rec: 0.4077 acc_train: 0.7977 loss_val: 0.4223 acc_val: 0.7875 time: 0.0025s\n",
      "Epoch: 02286 loss_train: 0.4077 loss_rec: 0.4077 acc_train: 0.7978 loss_val: 0.4223 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02287 loss_train: 0.4077 loss_rec: 0.4077 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02288 loss_train: 0.4077 loss_rec: 0.4077 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02289 loss_train: 0.4077 loss_rec: 0.4077 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02290 loss_train: 0.4077 loss_rec: 0.4077 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02291 loss_train: 0.4077 loss_rec: 0.4077 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4406 accuracy= 0.7872\n",
      "Epoch: 02292 loss_train: 0.4077 loss_rec: 0.4077 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02293 loss_train: 0.4077 loss_rec: 0.4077 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02294 loss_train: 0.4076 loss_rec: 0.4076 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02295 loss_train: 0.4076 loss_rec: 0.4076 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02296 loss_train: 0.4076 loss_rec: 0.4076 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02297 loss_train: 0.4076 loss_rec: 0.4076 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02298 loss_train: 0.4076 loss_rec: 0.4076 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02299 loss_train: 0.4076 loss_rec: 0.4076 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02300 loss_train: 0.4076 loss_rec: 0.4076 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02301 loss_train: 0.4076 loss_rec: 0.4076 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4406 accuracy= 0.7872\n",
      "Epoch: 02302 loss_train: 0.4076 loss_rec: 0.4076 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02303 loss_train: 0.4076 loss_rec: 0.4076 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02304 loss_train: 0.4075 loss_rec: 0.4075 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02305 loss_train: 0.4075 loss_rec: 0.4075 acc_train: 0.7978 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02306 loss_train: 0.4075 loss_rec: 0.4075 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02307 loss_train: 0.4075 loss_rec: 0.4075 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02308 loss_train: 0.4075 loss_rec: 0.4075 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02309 loss_train: 0.4075 loss_rec: 0.4075 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02310 loss_train: 0.4075 loss_rec: 0.4075 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02311 loss_train: 0.4075 loss_rec: 0.4075 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4406 accuracy= 0.7872\n",
      "Epoch: 02312 loss_train: 0.4075 loss_rec: 0.4075 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02313 loss_train: 0.4075 loss_rec: 0.4075 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02314 loss_train: 0.4074 loss_rec: 0.4074 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02315 loss_train: 0.4074 loss_rec: 0.4074 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02316 loss_train: 0.4074 loss_rec: 0.4074 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02317 loss_train: 0.4074 loss_rec: 0.4074 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02318 loss_train: 0.4074 loss_rec: 0.4074 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02319 loss_train: 0.4074 loss_rec: 0.4074 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02320 loss_train: 0.4074 loss_rec: 0.4074 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02321 loss_train: 0.4074 loss_rec: 0.4074 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4407 accuracy= 0.7872\n",
      "Epoch: 02322 loss_train: 0.4074 loss_rec: 0.4074 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02323 loss_train: 0.4074 loss_rec: 0.4074 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02324 loss_train: 0.4074 loss_rec: 0.4074 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02325 loss_train: 0.4073 loss_rec: 0.4073 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02326 loss_train: 0.4073 loss_rec: 0.4073 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02327 loss_train: 0.4073 loss_rec: 0.4073 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02328 loss_train: 0.4073 loss_rec: 0.4073 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02329 loss_train: 0.4073 loss_rec: 0.4073 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02330 loss_train: 0.4073 loss_rec: 0.4073 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02331 loss_train: 0.4073 loss_rec: 0.4073 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4408 accuracy= 0.7871\n",
      "Epoch: 02332 loss_train: 0.4073 loss_rec: 0.4073 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02333 loss_train: 0.4073 loss_rec: 0.4073 acc_train: 0.7979 loss_val: 0.4222 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02334 loss_train: 0.4073 loss_rec: 0.4073 acc_train: 0.7979 loss_val: 0.4221 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02335 loss_train: 0.4073 loss_rec: 0.4073 acc_train: 0.7979 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02336 loss_train: 0.4072 loss_rec: 0.4072 acc_train: 0.7979 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02337 loss_train: 0.4072 loss_rec: 0.4072 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02338 loss_train: 0.4072 loss_rec: 0.4072 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02339 loss_train: 0.4072 loss_rec: 0.4072 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02340 loss_train: 0.4072 loss_rec: 0.4072 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02341 loss_train: 0.4072 loss_rec: 0.4072 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4407 accuracy= 0.7872\n",
      "Epoch: 02342 loss_train: 0.4072 loss_rec: 0.4072 acc_train: 0.7979 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02343 loss_train: 0.4072 loss_rec: 0.4072 acc_train: 0.7979 loss_val: 0.4221 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02344 loss_train: 0.4072 loss_rec: 0.4072 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02345 loss_train: 0.4072 loss_rec: 0.4072 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02346 loss_train: 0.4071 loss_rec: 0.4071 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02347 loss_train: 0.4071 loss_rec: 0.4071 acc_train: 0.7979 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02348 loss_train: 0.4071 loss_rec: 0.4071 acc_train: 0.7979 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02349 loss_train: 0.4071 loss_rec: 0.4071 acc_train: 0.7979 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02350 loss_train: 0.4071 loss_rec: 0.4071 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02351 loss_train: 0.4071 loss_rec: 0.4071 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0020s\n",
      "Test set results: loss= 0.4408 accuracy= 0.7870\n",
      "Epoch: 02352 loss_train: 0.4071 loss_rec: 0.4071 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02353 loss_train: 0.4071 loss_rec: 0.4071 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02354 loss_train: 0.4071 loss_rec: 0.4071 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02355 loss_train: 0.4071 loss_rec: 0.4071 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02356 loss_train: 0.4071 loss_rec: 0.4071 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02357 loss_train: 0.4070 loss_rec: 0.4070 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02358 loss_train: 0.4070 loss_rec: 0.4070 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02359 loss_train: 0.4070 loss_rec: 0.4070 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02360 loss_train: 0.4070 loss_rec: 0.4070 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02361 loss_train: 0.4070 loss_rec: 0.4070 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Test set results: loss= 0.4408 accuracy= 0.7871\n",
      "Epoch: 02362 loss_train: 0.4070 loss_rec: 0.4070 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02363 loss_train: 0.4070 loss_rec: 0.4070 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02364 loss_train: 0.4070 loss_rec: 0.4070 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0020s\n",
      "Epoch: 02365 loss_train: 0.4070 loss_rec: 0.4070 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7875 time: 0.0015s\n",
      "Epoch: 02366 loss_train: 0.4070 loss_rec: 0.4070 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02367 loss_train: 0.4070 loss_rec: 0.4070 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02368 loss_train: 0.4070 loss_rec: 0.4070 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02369 loss_train: 0.4069 loss_rec: 0.4069 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02370 loss_train: 0.4069 loss_rec: 0.4069 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02371 loss_train: 0.4069 loss_rec: 0.4069 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4409 accuracy= 0.7867\n",
      "Epoch: 02372 loss_train: 0.4069 loss_rec: 0.4069 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02373 loss_train: 0.4069 loss_rec: 0.4069 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02374 loss_train: 0.4069 loss_rec: 0.4069 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02375 loss_train: 0.4069 loss_rec: 0.4069 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02376 loss_train: 0.4069 loss_rec: 0.4069 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02377 loss_train: 0.4069 loss_rec: 0.4069 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02378 loss_train: 0.4069 loss_rec: 0.4069 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02379 loss_train: 0.4069 loss_rec: 0.4069 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02380 loss_train: 0.4068 loss_rec: 0.4068 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02381 loss_train: 0.4068 loss_rec: 0.4068 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4409 accuracy= 0.7868\n",
      "Epoch: 02382 loss_train: 0.4068 loss_rec: 0.4068 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02383 loss_train: 0.4068 loss_rec: 0.4068 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02384 loss_train: 0.4068 loss_rec: 0.4068 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02385 loss_train: 0.4068 loss_rec: 0.4068 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02386 loss_train: 0.4068 loss_rec: 0.4068 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02387 loss_train: 0.4068 loss_rec: 0.4068 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02388 loss_train: 0.4068 loss_rec: 0.4068 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02389 loss_train: 0.4068 loss_rec: 0.4068 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02390 loss_train: 0.4068 loss_rec: 0.4068 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02391 loss_train: 0.4068 loss_rec: 0.4068 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4410 accuracy= 0.7868\n",
      "Epoch: 02392 loss_train: 0.4067 loss_rec: 0.4067 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02393 loss_train: 0.4067 loss_rec: 0.4067 acc_train: 0.7978 loss_val: 0.4221 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02394 loss_train: 0.4067 loss_rec: 0.4067 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02395 loss_train: 0.4067 loss_rec: 0.4067 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0025s\n",
      "Epoch: 02396 loss_train: 0.4067 loss_rec: 0.4067 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02397 loss_train: 0.4067 loss_rec: 0.4067 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02398 loss_train: 0.4067 loss_rec: 0.4067 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02399 loss_train: 0.4067 loss_rec: 0.4067 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02400 loss_train: 0.4067 loss_rec: 0.4067 acc_train: 0.7977 loss_val: 0.4221 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02401 loss_train: 0.4067 loss_rec: 0.4067 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4410 accuracy= 0.7868\n",
      "Epoch: 02402 loss_train: 0.4067 loss_rec: 0.4067 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02403 loss_train: 0.4066 loss_rec: 0.4066 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02404 loss_train: 0.4066 loss_rec: 0.4066 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02405 loss_train: 0.4066 loss_rec: 0.4066 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02406 loss_train: 0.4066 loss_rec: 0.4066 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02407 loss_train: 0.4066 loss_rec: 0.4066 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02408 loss_train: 0.4066 loss_rec: 0.4066 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02409 loss_train: 0.4066 loss_rec: 0.4066 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02410 loss_train: 0.4066 loss_rec: 0.4066 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02411 loss_train: 0.4066 loss_rec: 0.4066 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Test set results: loss= 0.4410 accuracy= 0.7868\n",
      "Epoch: 02412 loss_train: 0.4066 loss_rec: 0.4066 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02413 loss_train: 0.4066 loss_rec: 0.4066 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02414 loss_train: 0.4066 loss_rec: 0.4066 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02415 loss_train: 0.4065 loss_rec: 0.4065 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02416 loss_train: 0.4065 loss_rec: 0.4065 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02417 loss_train: 0.4065 loss_rec: 0.4065 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02418 loss_train: 0.4065 loss_rec: 0.4065 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02419 loss_train: 0.4065 loss_rec: 0.4065 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02420 loss_train: 0.4065 loss_rec: 0.4065 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02421 loss_train: 0.4065 loss_rec: 0.4065 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Test set results: loss= 0.4411 accuracy= 0.7867\n",
      "Epoch: 02422 loss_train: 0.4065 loss_rec: 0.4065 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02423 loss_train: 0.4065 loss_rec: 0.4065 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02424 loss_train: 0.4065 loss_rec: 0.4065 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02425 loss_train: 0.4065 loss_rec: 0.4065 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02426 loss_train: 0.4065 loss_rec: 0.4065 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02427 loss_train: 0.4065 loss_rec: 0.4065 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02428 loss_train: 0.4064 loss_rec: 0.4064 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02429 loss_train: 0.4064 loss_rec: 0.4064 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02430 loss_train: 0.4064 loss_rec: 0.4064 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02431 loss_train: 0.4064 loss_rec: 0.4064 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Test set results: loss= 0.4411 accuracy= 0.7867\n",
      "Epoch: 02432 loss_train: 0.4064 loss_rec: 0.4064 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02433 loss_train: 0.4064 loss_rec: 0.4064 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02434 loss_train: 0.4064 loss_rec: 0.4064 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02435 loss_train: 0.4064 loss_rec: 0.4064 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02436 loss_train: 0.4064 loss_rec: 0.4064 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0025s\n",
      "Epoch: 02437 loss_train: 0.4064 loss_rec: 0.4064 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02438 loss_train: 0.4064 loss_rec: 0.4064 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0025s\n",
      "Epoch: 02439 loss_train: 0.4064 loss_rec: 0.4064 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02440 loss_train: 0.4063 loss_rec: 0.4063 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02441 loss_train: 0.4063 loss_rec: 0.4063 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4411 accuracy= 0.7867\n",
      "Epoch: 02442 loss_train: 0.4063 loss_rec: 0.4063 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02443 loss_train: 0.4063 loss_rec: 0.4063 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02444 loss_train: 0.4063 loss_rec: 0.4063 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02445 loss_train: 0.4063 loss_rec: 0.4063 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02446 loss_train: 0.4063 loss_rec: 0.4063 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02447 loss_train: 0.4063 loss_rec: 0.4063 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02448 loss_train: 0.4063 loss_rec: 0.4063 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02449 loss_train: 0.4063 loss_rec: 0.4063 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0025s\n",
      "Epoch: 02450 loss_train: 0.4063 loss_rec: 0.4063 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02451 loss_train: 0.4063 loss_rec: 0.4063 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4412 accuracy= 0.7867\n",
      "Epoch: 02452 loss_train: 0.4062 loss_rec: 0.4062 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02453 loss_train: 0.4062 loss_rec: 0.4062 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02454 loss_train: 0.4062 loss_rec: 0.4062 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02455 loss_train: 0.4062 loss_rec: 0.4062 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02456 loss_train: 0.4062 loss_rec: 0.4062 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02457 loss_train: 0.4062 loss_rec: 0.4062 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02458 loss_train: 0.4062 loss_rec: 0.4062 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02459 loss_train: 0.4062 loss_rec: 0.4062 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02460 loss_train: 0.4062 loss_rec: 0.4062 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02461 loss_train: 0.4062 loss_rec: 0.4062 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4413 accuracy= 0.7867\n",
      "Epoch: 02462 loss_train: 0.4062 loss_rec: 0.4062 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02463 loss_train: 0.4062 loss_rec: 0.4062 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02464 loss_train: 0.4062 loss_rec: 0.4062 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02465 loss_train: 0.4061 loss_rec: 0.4061 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02466 loss_train: 0.4061 loss_rec: 0.4061 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02467 loss_train: 0.4061 loss_rec: 0.4061 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02468 loss_train: 0.4061 loss_rec: 0.4061 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02469 loss_train: 0.4061 loss_rec: 0.4061 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02470 loss_train: 0.4061 loss_rec: 0.4061 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0017s\n",
      "Epoch: 02471 loss_train: 0.4061 loss_rec: 0.4061 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4413 accuracy= 0.7867\n",
      "Epoch: 02472 loss_train: 0.4061 loss_rec: 0.4061 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02473 loss_train: 0.4061 loss_rec: 0.4061 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02474 loss_train: 0.4061 loss_rec: 0.4061 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02475 loss_train: 0.4061 loss_rec: 0.4061 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02476 loss_train: 0.4061 loss_rec: 0.4061 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02477 loss_train: 0.4061 loss_rec: 0.4061 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02478 loss_train: 0.4061 loss_rec: 0.4061 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02479 loss_train: 0.4060 loss_rec: 0.4060 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02480 loss_train: 0.4060 loss_rec: 0.4060 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02481 loss_train: 0.4060 loss_rec: 0.4060 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Test set results: loss= 0.4414 accuracy= 0.7867\n",
      "Epoch: 02482 loss_train: 0.4060 loss_rec: 0.4060 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02483 loss_train: 0.4060 loss_rec: 0.4060 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02484 loss_train: 0.4060 loss_rec: 0.4060 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02485 loss_train: 0.4060 loss_rec: 0.4060 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02486 loss_train: 0.4060 loss_rec: 0.4060 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0025s\n",
      "Epoch: 02487 loss_train: 0.4060 loss_rec: 0.4060 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02488 loss_train: 0.4060 loss_rec: 0.4060 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02489 loss_train: 0.4060 loss_rec: 0.4060 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02490 loss_train: 0.4060 loss_rec: 0.4060 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02491 loss_train: 0.4059 loss_rec: 0.4059 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Test set results: loss= 0.4414 accuracy= 0.7867\n",
      "Epoch: 02492 loss_train: 0.4059 loss_rec: 0.4059 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02493 loss_train: 0.4059 loss_rec: 0.4059 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02494 loss_train: 0.4059 loss_rec: 0.4059 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02495 loss_train: 0.4059 loss_rec: 0.4059 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02496 loss_train: 0.4059 loss_rec: 0.4059 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02497 loss_train: 0.4059 loss_rec: 0.4059 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02498 loss_train: 0.4059 loss_rec: 0.4059 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02499 loss_train: 0.4059 loss_rec: 0.4059 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02500 loss_train: 0.4059 loss_rec: 0.4059 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02501 loss_train: 0.4059 loss_rec: 0.4059 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4414 accuracy= 0.7867\n",
      "Epoch: 02502 loss_train: 0.4059 loss_rec: 0.4059 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02503 loss_train: 0.4059 loss_rec: 0.4059 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02504 loss_train: 0.4058 loss_rec: 0.4058 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02505 loss_train: 0.4058 loss_rec: 0.4058 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02506 loss_train: 0.4058 loss_rec: 0.4058 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02507 loss_train: 0.4058 loss_rec: 0.4058 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02508 loss_train: 0.4058 loss_rec: 0.4058 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02509 loss_train: 0.4058 loss_rec: 0.4058 acc_train: 0.7977 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02510 loss_train: 0.4058 loss_rec: 0.4058 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02511 loss_train: 0.4058 loss_rec: 0.4058 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4415 accuracy= 0.7867\n",
      "Epoch: 02512 loss_train: 0.4058 loss_rec: 0.4058 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02513 loss_train: 0.4058 loss_rec: 0.4058 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02514 loss_train: 0.4058 loss_rec: 0.4058 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02515 loss_train: 0.4058 loss_rec: 0.4058 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02516 loss_train: 0.4058 loss_rec: 0.4058 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02517 loss_train: 0.4058 loss_rec: 0.4058 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02518 loss_train: 0.4057 loss_rec: 0.4057 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02519 loss_train: 0.4057 loss_rec: 0.4057 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02520 loss_train: 0.4057 loss_rec: 0.4057 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02521 loss_train: 0.4057 loss_rec: 0.4057 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4416 accuracy= 0.7867\n",
      "Epoch: 02522 loss_train: 0.4057 loss_rec: 0.4057 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02523 loss_train: 0.4057 loss_rec: 0.4057 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02524 loss_train: 0.4057 loss_rec: 0.4057 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02525 loss_train: 0.4057 loss_rec: 0.4057 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02526 loss_train: 0.4057 loss_rec: 0.4057 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02527 loss_train: 0.4057 loss_rec: 0.4057 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02528 loss_train: 0.4057 loss_rec: 0.4057 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02529 loss_train: 0.4057 loss_rec: 0.4057 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02530 loss_train: 0.4057 loss_rec: 0.4057 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02531 loss_train: 0.4056 loss_rec: 0.4056 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4416 accuracy= 0.7866\n",
      "Epoch: 02532 loss_train: 0.4056 loss_rec: 0.4056 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02533 loss_train: 0.4056 loss_rec: 0.4056 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02534 loss_train: 0.4056 loss_rec: 0.4056 acc_train: 0.7978 loss_val: 0.4220 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02535 loss_train: 0.4056 loss_rec: 0.4056 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02536 loss_train: 0.4056 loss_rec: 0.4056 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02537 loss_train: 0.4056 loss_rec: 0.4056 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02538 loss_train: 0.4056 loss_rec: 0.4056 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02539 loss_train: 0.4056 loss_rec: 0.4056 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02540 loss_train: 0.4056 loss_rec: 0.4056 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02541 loss_train: 0.4056 loss_rec: 0.4056 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0015s\n",
      "Test set results: loss= 0.4416 accuracy= 0.7866\n",
      "Epoch: 02542 loss_train: 0.4056 loss_rec: 0.4056 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02543 loss_train: 0.4056 loss_rec: 0.4056 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02544 loss_train: 0.4055 loss_rec: 0.4055 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02545 loss_train: 0.4055 loss_rec: 0.4055 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02546 loss_train: 0.4055 loss_rec: 0.4055 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02547 loss_train: 0.4055 loss_rec: 0.4055 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02548 loss_train: 0.4055 loss_rec: 0.4055 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7883 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02549 loss_train: 0.4055 loss_rec: 0.4055 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02550 loss_train: 0.4055 loss_rec: 0.4055 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02551 loss_train: 0.4055 loss_rec: 0.4055 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0020s\n",
      "Test set results: loss= 0.4417 accuracy= 0.7866\n",
      "Epoch: 02552 loss_train: 0.4055 loss_rec: 0.4055 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02553 loss_train: 0.4055 loss_rec: 0.4055 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02554 loss_train: 0.4055 loss_rec: 0.4055 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02555 loss_train: 0.4055 loss_rec: 0.4055 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02556 loss_train: 0.4055 loss_rec: 0.4055 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02557 loss_train: 0.4054 loss_rec: 0.4054 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02558 loss_train: 0.4054 loss_rec: 0.4054 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02559 loss_train: 0.4054 loss_rec: 0.4054 acc_train: 0.7978 loss_val: 0.4219 acc_val: 0.7879 time: 0.0020s\n",
      "Epoch: 02560 loss_train: 0.4054 loss_rec: 0.4054 acc_train: 0.7979 loss_val: 0.4219 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02561 loss_train: 0.4054 loss_rec: 0.4054 acc_train: 0.7979 loss_val: 0.4219 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4417 accuracy= 0.7865\n",
      "Epoch: 02562 loss_train: 0.4054 loss_rec: 0.4054 acc_train: 0.7979 loss_val: 0.4219 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02563 loss_train: 0.4054 loss_rec: 0.4054 acc_train: 0.7979 loss_val: 0.4219 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02564 loss_train: 0.4054 loss_rec: 0.4054 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02565 loss_train: 0.4054 loss_rec: 0.4054 acc_train: 0.7979 loss_val: 0.4219 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02566 loss_train: 0.4054 loss_rec: 0.4054 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02567 loss_train: 0.4054 loss_rec: 0.4054 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02568 loss_train: 0.4054 loss_rec: 0.4054 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02569 loss_train: 0.4053 loss_rec: 0.4053 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02570 loss_train: 0.4053 loss_rec: 0.4053 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02571 loss_train: 0.4053 loss_rec: 0.4053 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4418 accuracy= 0.7865\n",
      "Epoch: 02572 loss_train: 0.4053 loss_rec: 0.4053 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02573 loss_train: 0.4053 loss_rec: 0.4053 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02574 loss_train: 0.4053 loss_rec: 0.4053 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02575 loss_train: 0.4053 loss_rec: 0.4053 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02576 loss_train: 0.4053 loss_rec: 0.4053 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02577 loss_train: 0.4053 loss_rec: 0.4053 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02578 loss_train: 0.4053 loss_rec: 0.4053 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02579 loss_train: 0.4053 loss_rec: 0.4053 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02580 loss_train: 0.4052 loss_rec: 0.4052 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02581 loss_train: 0.4052 loss_rec: 0.4052 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4418 accuracy= 0.7865\n",
      "Epoch: 02582 loss_train: 0.4052 loss_rec: 0.4052 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02583 loss_train: 0.4052 loss_rec: 0.4052 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02584 loss_train: 0.4052 loss_rec: 0.4052 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02585 loss_train: 0.4052 loss_rec: 0.4052 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02586 loss_train: 0.4052 loss_rec: 0.4052 acc_train: 0.7979 loss_val: 0.4218 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02587 loss_train: 0.4052 loss_rec: 0.4052 acc_train: 0.7980 loss_val: 0.4217 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02588 loss_train: 0.4052 loss_rec: 0.4052 acc_train: 0.7980 loss_val: 0.4217 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02589 loss_train: 0.4052 loss_rec: 0.4052 acc_train: 0.7980 loss_val: 0.4218 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02590 loss_train: 0.4052 loss_rec: 0.4052 acc_train: 0.7980 loss_val: 0.4217 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02591 loss_train: 0.4052 loss_rec: 0.4052 acc_train: 0.7980 loss_val: 0.4217 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4418 accuracy= 0.7865\n",
      "Epoch: 02592 loss_train: 0.4051 loss_rec: 0.4051 acc_train: 0.7980 loss_val: 0.4217 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02593 loss_train: 0.4051 loss_rec: 0.4051 acc_train: 0.7980 loss_val: 0.4217 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02594 loss_train: 0.4051 loss_rec: 0.4051 acc_train: 0.7980 loss_val: 0.4217 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02595 loss_train: 0.4051 loss_rec: 0.4051 acc_train: 0.7980 loss_val: 0.4217 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02596 loss_train: 0.4051 loss_rec: 0.4051 acc_train: 0.7980 loss_val: 0.4217 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02597 loss_train: 0.4051 loss_rec: 0.4051 acc_train: 0.7979 loss_val: 0.4217 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02598 loss_train: 0.4051 loss_rec: 0.4051 acc_train: 0.7979 loss_val: 0.4217 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02599 loss_train: 0.4051 loss_rec: 0.4051 acc_train: 0.7979 loss_val: 0.4217 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02600 loss_train: 0.4051 loss_rec: 0.4051 acc_train: 0.7979 loss_val: 0.4217 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02601 loss_train: 0.4051 loss_rec: 0.4051 acc_train: 0.7979 loss_val: 0.4217 acc_val: 0.7883 time: 0.0020s\n",
      "Test set results: loss= 0.4419 accuracy= 0.7866\n",
      "Epoch: 02602 loss_train: 0.4051 loss_rec: 0.4051 acc_train: 0.7979 loss_val: 0.4217 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02603 loss_train: 0.4050 loss_rec: 0.4050 acc_train: 0.7979 loss_val: 0.4216 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02604 loss_train: 0.4050 loss_rec: 0.4050 acc_train: 0.7979 loss_val: 0.4216 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02605 loss_train: 0.4050 loss_rec: 0.4050 acc_train: 0.7979 loss_val: 0.4216 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02606 loss_train: 0.4050 loss_rec: 0.4050 acc_train: 0.7979 loss_val: 0.4216 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02607 loss_train: 0.4050 loss_rec: 0.4050 acc_train: 0.7979 loss_val: 0.4216 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02608 loss_train: 0.4050 loss_rec: 0.4050 acc_train: 0.7979 loss_val: 0.4216 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02609 loss_train: 0.4050 loss_rec: 0.4050 acc_train: 0.7979 loss_val: 0.4216 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02610 loss_train: 0.4050 loss_rec: 0.4050 acc_train: 0.7979 loss_val: 0.4216 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02611 loss_train: 0.4050 loss_rec: 0.4050 acc_train: 0.7980 loss_val: 0.4216 acc_val: 0.7883 time: 0.0020s\n",
      "Test set results: loss= 0.4419 accuracy= 0.7866\n",
      "Epoch: 02612 loss_train: 0.4050 loss_rec: 0.4050 acc_train: 0.7980 loss_val: 0.4216 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02613 loss_train: 0.4049 loss_rec: 0.4049 acc_train: 0.7980 loss_val: 0.4216 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02614 loss_train: 0.4049 loss_rec: 0.4049 acc_train: 0.7980 loss_val: 0.4216 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02615 loss_train: 0.4049 loss_rec: 0.4049 acc_train: 0.7980 loss_val: 0.4216 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02616 loss_train: 0.4049 loss_rec: 0.4049 acc_train: 0.7980 loss_val: 0.4216 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02617 loss_train: 0.4049 loss_rec: 0.4049 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02618 loss_train: 0.4049 loss_rec: 0.4049 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02619 loss_train: 0.4049 loss_rec: 0.4049 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02620 loss_train: 0.4049 loss_rec: 0.4049 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02621 loss_train: 0.4049 loss_rec: 0.4049 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4419 accuracy= 0.7867\n",
      "Epoch: 02622 loss_train: 0.4049 loss_rec: 0.4049 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02623 loss_train: 0.4049 loss_rec: 0.4049 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02624 loss_train: 0.4048 loss_rec: 0.4048 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02625 loss_train: 0.4048 loss_rec: 0.4048 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02626 loss_train: 0.4048 loss_rec: 0.4048 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02627 loss_train: 0.4048 loss_rec: 0.4048 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02628 loss_train: 0.4048 loss_rec: 0.4048 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02629 loss_train: 0.4048 loss_rec: 0.4048 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02630 loss_train: 0.4048 loss_rec: 0.4048 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02631 loss_train: 0.4048 loss_rec: 0.4048 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4420 accuracy= 0.7867\n",
      "Epoch: 02632 loss_train: 0.4048 loss_rec: 0.4048 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02633 loss_train: 0.4048 loss_rec: 0.4048 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02634 loss_train: 0.4048 loss_rec: 0.4048 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02635 loss_train: 0.4047 loss_rec: 0.4047 acc_train: 0.7980 loss_val: 0.4215 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02636 loss_train: 0.4047 loss_rec: 0.4047 acc_train: 0.7980 loss_val: 0.4214 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02637 loss_train: 0.4047 loss_rec: 0.4047 acc_train: 0.7980 loss_val: 0.4214 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02638 loss_train: 0.4047 loss_rec: 0.4047 acc_train: 0.7980 loss_val: 0.4214 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02639 loss_train: 0.4047 loss_rec: 0.4047 acc_train: 0.7980 loss_val: 0.4214 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02640 loss_train: 0.4047 loss_rec: 0.4047 acc_train: 0.7980 loss_val: 0.4214 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02641 loss_train: 0.4047 loss_rec: 0.4047 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4421 accuracy= 0.7867\n",
      "Epoch: 02642 loss_train: 0.4047 loss_rec: 0.4047 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02643 loss_train: 0.4047 loss_rec: 0.4047 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0025s\n",
      "Epoch: 02644 loss_train: 0.4047 loss_rec: 0.4047 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02645 loss_train: 0.4047 loss_rec: 0.4047 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02646 loss_train: 0.4046 loss_rec: 0.4046 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02647 loss_train: 0.4046 loss_rec: 0.4046 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02648 loss_train: 0.4046 loss_rec: 0.4046 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02649 loss_train: 0.4046 loss_rec: 0.4046 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02650 loss_train: 0.4046 loss_rec: 0.4046 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02651 loss_train: 0.4046 loss_rec: 0.4046 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4422 accuracy= 0.7866\n",
      "Epoch: 02652 loss_train: 0.4046 loss_rec: 0.4046 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02653 loss_train: 0.4046 loss_rec: 0.4046 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02654 loss_train: 0.4046 loss_rec: 0.4046 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02655 loss_train: 0.4046 loss_rec: 0.4046 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02656 loss_train: 0.4046 loss_rec: 0.4046 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02657 loss_train: 0.4046 loss_rec: 0.4046 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02658 loss_train: 0.4046 loss_rec: 0.4046 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02659 loss_train: 0.4045 loss_rec: 0.4045 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02660 loss_train: 0.4045 loss_rec: 0.4045 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02661 loss_train: 0.4045 loss_rec: 0.4045 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4422 accuracy= 0.7866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02662 loss_train: 0.4045 loss_rec: 0.4045 acc_train: 0.7981 loss_val: 0.4213 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02663 loss_train: 0.4045 loss_rec: 0.4045 acc_train: 0.7981 loss_val: 0.4214 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02664 loss_train: 0.4045 loss_rec: 0.4045 acc_train: 0.7980 loss_val: 0.4214 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02665 loss_train: 0.4045 loss_rec: 0.4045 acc_train: 0.7980 loss_val: 0.4214 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02666 loss_train: 0.4045 loss_rec: 0.4045 acc_train: 0.7981 loss_val: 0.4213 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02667 loss_train: 0.4045 loss_rec: 0.4045 acc_train: 0.7981 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02668 loss_train: 0.4045 loss_rec: 0.4045 acc_train: 0.7981 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02669 loss_train: 0.4045 loss_rec: 0.4045 acc_train: 0.7981 loss_val: 0.4213 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02670 loss_train: 0.4045 loss_rec: 0.4045 acc_train: 0.7981 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02671 loss_train: 0.4045 loss_rec: 0.4045 acc_train: 0.7981 loss_val: 0.4213 acc_val: 0.7888 time: 0.0020s\n",
      "Test set results: loss= 0.4424 accuracy= 0.7866\n",
      "Epoch: 02672 loss_train: 0.4044 loss_rec: 0.4044 acc_train: 0.7981 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02673 loss_train: 0.4044 loss_rec: 0.4044 acc_train: 0.7981 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02674 loss_train: 0.4044 loss_rec: 0.4044 acc_train: 0.7981 loss_val: 0.4213 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02675 loss_train: 0.4044 loss_rec: 0.4044 acc_train: 0.7981 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02676 loss_train: 0.4044 loss_rec: 0.4044 acc_train: 0.7981 loss_val: 0.4213 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02677 loss_train: 0.4044 loss_rec: 0.4044 acc_train: 0.7981 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02678 loss_train: 0.4044 loss_rec: 0.4044 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02679 loss_train: 0.4044 loss_rec: 0.4044 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02680 loss_train: 0.4044 loss_rec: 0.4044 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02681 loss_train: 0.4044 loss_rec: 0.4044 acc_train: 0.7981 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4424 accuracy= 0.7864\n",
      "Epoch: 02682 loss_train: 0.4044 loss_rec: 0.4044 acc_train: 0.7981 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02683 loss_train: 0.4044 loss_rec: 0.4044 acc_train: 0.7981 loss_val: 0.4213 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02684 loss_train: 0.4044 loss_rec: 0.4044 acc_train: 0.7980 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02685 loss_train: 0.4043 loss_rec: 0.4043 acc_train: 0.7980 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02686 loss_train: 0.4043 loss_rec: 0.4043 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02687 loss_train: 0.4043 loss_rec: 0.4043 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02688 loss_train: 0.4043 loss_rec: 0.4043 acc_train: 0.7977 loss_val: 0.4213 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02689 loss_train: 0.4043 loss_rec: 0.4043 acc_train: 0.7977 loss_val: 0.4213 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02690 loss_train: 0.4043 loss_rec: 0.4043 acc_train: 0.7977 loss_val: 0.4213 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02691 loss_train: 0.4043 loss_rec: 0.4043 acc_train: 0.7977 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4425 accuracy= 0.7864\n",
      "Epoch: 02692 loss_train: 0.4043 loss_rec: 0.4043 acc_train: 0.7977 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02693 loss_train: 0.4043 loss_rec: 0.4043 acc_train: 0.7977 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02694 loss_train: 0.4043 loss_rec: 0.4043 acc_train: 0.7978 loss_val: 0.4213 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02695 loss_train: 0.4043 loss_rec: 0.4043 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02696 loss_train: 0.4043 loss_rec: 0.4043 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02697 loss_train: 0.4043 loss_rec: 0.4043 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02698 loss_train: 0.4042 loss_rec: 0.4042 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02699 loss_train: 0.4042 loss_rec: 0.4042 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02700 loss_train: 0.4042 loss_rec: 0.4042 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02701 loss_train: 0.4042 loss_rec: 0.4042 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4425 accuracy= 0.7864\n",
      "Epoch: 02702 loss_train: 0.4042 loss_rec: 0.4042 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02703 loss_train: 0.4042 loss_rec: 0.4042 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02704 loss_train: 0.4042 loss_rec: 0.4042 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02705 loss_train: 0.4042 loss_rec: 0.4042 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02706 loss_train: 0.4042 loss_rec: 0.4042 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0025s\n",
      "Epoch: 02707 loss_train: 0.4042 loss_rec: 0.4042 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02708 loss_train: 0.4042 loss_rec: 0.4042 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02709 loss_train: 0.4042 loss_rec: 0.4042 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02710 loss_train: 0.4042 loss_rec: 0.4042 acc_train: 0.7979 loss_val: 0.4213 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02711 loss_train: 0.4042 loss_rec: 0.4042 acc_train: 0.7979 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4426 accuracy= 0.7865\n",
      "Epoch: 02712 loss_train: 0.4041 loss_rec: 0.4041 acc_train: 0.7979 loss_val: 0.4212 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02713 loss_train: 0.4041 loss_rec: 0.4041 acc_train: 0.7979 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02714 loss_train: 0.4041 loss_rec: 0.4041 acc_train: 0.7979 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02715 loss_train: 0.4041 loss_rec: 0.4041 acc_train: 0.7979 loss_val: 0.4212 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02716 loss_train: 0.4041 loss_rec: 0.4041 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02717 loss_train: 0.4041 loss_rec: 0.4041 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02718 loss_train: 0.4041 loss_rec: 0.4041 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02719 loss_train: 0.4041 loss_rec: 0.4041 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02720 loss_train: 0.4041 loss_rec: 0.4041 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02721 loss_train: 0.4041 loss_rec: 0.4041 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4427 accuracy= 0.7862\n",
      "Epoch: 02722 loss_train: 0.4041 loss_rec: 0.4041 acc_train: 0.7979 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02723 loss_train: 0.4041 loss_rec: 0.4041 acc_train: 0.7979 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02724 loss_train: 0.4041 loss_rec: 0.4041 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02725 loss_train: 0.4041 loss_rec: 0.4041 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02726 loss_train: 0.4040 loss_rec: 0.4040 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02727 loss_train: 0.4040 loss_rec: 0.4040 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02728 loss_train: 0.4040 loss_rec: 0.4040 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02729 loss_train: 0.4040 loss_rec: 0.4040 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02730 loss_train: 0.4040 loss_rec: 0.4040 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02731 loss_train: 0.4040 loss_rec: 0.4040 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4428 accuracy= 0.7866\n",
      "Epoch: 02732 loss_train: 0.4040 loss_rec: 0.4040 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02733 loss_train: 0.4040 loss_rec: 0.4040 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02734 loss_train: 0.4040 loss_rec: 0.4040 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02735 loss_train: 0.4040 loss_rec: 0.4040 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02736 loss_train: 0.4040 loss_rec: 0.4040 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02737 loss_train: 0.4040 loss_rec: 0.4040 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02738 loss_train: 0.4040 loss_rec: 0.4040 acc_train: 0.7979 loss_val: 0.4212 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02739 loss_train: 0.4040 loss_rec: 0.4040 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02740 loss_train: 0.4040 loss_rec: 0.4040 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02741 loss_train: 0.4039 loss_rec: 0.4039 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4428 accuracy= 0.7865\n",
      "Epoch: 02742 loss_train: 0.4039 loss_rec: 0.4039 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02743 loss_train: 0.4039 loss_rec: 0.4039 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0025s\n",
      "Epoch: 02744 loss_train: 0.4039 loss_rec: 0.4039 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02745 loss_train: 0.4039 loss_rec: 0.4039 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02746 loss_train: 0.4039 loss_rec: 0.4039 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02747 loss_train: 0.4039 loss_rec: 0.4039 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02748 loss_train: 0.4039 loss_rec: 0.4039 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02749 loss_train: 0.4039 loss_rec: 0.4039 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02750 loss_train: 0.4039 loss_rec: 0.4039 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02751 loss_train: 0.4039 loss_rec: 0.4039 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0020s\n",
      "Test set results: loss= 0.4429 accuracy= 0.7865\n",
      "Epoch: 02752 loss_train: 0.4039 loss_rec: 0.4039 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02753 loss_train: 0.4039 loss_rec: 0.4039 acc_train: 0.7980 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02754 loss_train: 0.4039 loss_rec: 0.4039 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02755 loss_train: 0.4039 loss_rec: 0.4039 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0016s\n",
      "Epoch: 02756 loss_train: 0.4038 loss_rec: 0.4038 acc_train: 0.7980 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02757 loss_train: 0.4038 loss_rec: 0.4038 acc_train: 0.7980 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02758 loss_train: 0.4038 loss_rec: 0.4038 acc_train: 0.7980 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02759 loss_train: 0.4038 loss_rec: 0.4038 acc_train: 0.7981 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02760 loss_train: 0.4038 loss_rec: 0.4038 acc_train: 0.7981 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02761 loss_train: 0.4038 loss_rec: 0.4038 acc_train: 0.7980 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Test set results: loss= 0.4431 accuracy= 0.7865\n",
      "Epoch: 02762 loss_train: 0.4038 loss_rec: 0.4038 acc_train: 0.7979 loss_val: 0.4212 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02763 loss_train: 0.4038 loss_rec: 0.4038 acc_train: 0.7979 loss_val: 0.4212 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02764 loss_train: 0.4038 loss_rec: 0.4038 acc_train: 0.7980 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02765 loss_train: 0.4038 loss_rec: 0.4038 acc_train: 0.7980 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02766 loss_train: 0.4038 loss_rec: 0.4038 acc_train: 0.7980 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02767 loss_train: 0.4038 loss_rec: 0.4038 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02768 loss_train: 0.4038 loss_rec: 0.4038 acc_train: 0.7980 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02769 loss_train: 0.4038 loss_rec: 0.4038 acc_train: 0.7980 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02770 loss_train: 0.4038 loss_rec: 0.4038 acc_train: 0.7980 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02771 loss_train: 0.4037 loss_rec: 0.4037 acc_train: 0.7980 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Test set results: loss= 0.4432 accuracy= 0.7865\n",
      "Epoch: 02772 loss_train: 0.4037 loss_rec: 0.4037 acc_train: 0.7980 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02773 loss_train: 0.4037 loss_rec: 0.4037 acc_train: 0.7980 loss_val: 0.4211 acc_val: 0.7888 time: 0.0025s\n",
      "Epoch: 02774 loss_train: 0.4037 loss_rec: 0.4037 acc_train: 0.7980 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02775 loss_train: 0.4037 loss_rec: 0.4037 acc_train: 0.7980 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02776 loss_train: 0.4037 loss_rec: 0.4037 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02777 loss_train: 0.4037 loss_rec: 0.4037 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02778 loss_train: 0.4037 loss_rec: 0.4037 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0025s\n",
      "Epoch: 02779 loss_train: 0.4037 loss_rec: 0.4037 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02780 loss_train: 0.4037 loss_rec: 0.4037 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02781 loss_train: 0.4037 loss_rec: 0.4037 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Test set results: loss= 0.4431 accuracy= 0.7865\n",
      "Epoch: 02782 loss_train: 0.4037 loss_rec: 0.4037 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02783 loss_train: 0.4037 loss_rec: 0.4037 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02784 loss_train: 0.4036 loss_rec: 0.4036 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02785 loss_train: 0.4036 loss_rec: 0.4036 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02786 loss_train: 0.4036 loss_rec: 0.4036 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02787 loss_train: 0.4036 loss_rec: 0.4036 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02788 loss_train: 0.4036 loss_rec: 0.4036 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02789 loss_train: 0.4036 loss_rec: 0.4036 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02790 loss_train: 0.4036 loss_rec: 0.4036 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02791 loss_train: 0.4036 loss_rec: 0.4036 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Test set results: loss= 0.4432 accuracy= 0.7865\n",
      "Epoch: 02792 loss_train: 0.4036 loss_rec: 0.4036 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02793 loss_train: 0.4036 loss_rec: 0.4036 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02794 loss_train: 0.4036 loss_rec: 0.4036 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02795 loss_train: 0.4036 loss_rec: 0.4036 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02796 loss_train: 0.4036 loss_rec: 0.4036 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02797 loss_train: 0.4036 loss_rec: 0.4036 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02798 loss_train: 0.4036 loss_rec: 0.4036 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02799 loss_train: 0.4035 loss_rec: 0.4035 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02800 loss_train: 0.4035 loss_rec: 0.4035 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02801 loss_train: 0.4035 loss_rec: 0.4035 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Test set results: loss= 0.4433 accuracy= 0.7864\n",
      "Epoch: 02802 loss_train: 0.4035 loss_rec: 0.4035 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02803 loss_train: 0.4035 loss_rec: 0.4035 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02804 loss_train: 0.4035 loss_rec: 0.4035 acc_train: 0.7982 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02805 loss_train: 0.4035 loss_rec: 0.4035 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02806 loss_train: 0.4035 loss_rec: 0.4035 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02807 loss_train: 0.4035 loss_rec: 0.4035 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02808 loss_train: 0.4035 loss_rec: 0.4035 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02809 loss_train: 0.4035 loss_rec: 0.4035 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02810 loss_train: 0.4035 loss_rec: 0.4035 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02811 loss_train: 0.4035 loss_rec: 0.4035 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4434 accuracy= 0.7864\n",
      "Epoch: 02812 loss_train: 0.4034 loss_rec: 0.4034 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0025s\n",
      "Epoch: 02813 loss_train: 0.4034 loss_rec: 0.4034 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02814 loss_train: 0.4034 loss_rec: 0.4034 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02815 loss_train: 0.4034 loss_rec: 0.4034 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02816 loss_train: 0.4034 loss_rec: 0.4034 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02817 loss_train: 0.4034 loss_rec: 0.4034 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02818 loss_train: 0.4034 loss_rec: 0.4034 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02819 loss_train: 0.4034 loss_rec: 0.4034 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02820 loss_train: 0.4034 loss_rec: 0.4034 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02821 loss_train: 0.4034 loss_rec: 0.4034 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4434 accuracy= 0.7864\n",
      "Epoch: 02822 loss_train: 0.4034 loss_rec: 0.4034 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02823 loss_train: 0.4034 loss_rec: 0.4034 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02824 loss_train: 0.4034 loss_rec: 0.4034 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02825 loss_train: 0.4034 loss_rec: 0.4034 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02826 loss_train: 0.4034 loss_rec: 0.4034 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02827 loss_train: 0.4033 loss_rec: 0.4033 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02828 loss_train: 0.4033 loss_rec: 0.4033 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02829 loss_train: 0.4033 loss_rec: 0.4033 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02830 loss_train: 0.4033 loss_rec: 0.4033 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02831 loss_train: 0.4033 loss_rec: 0.4033 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Test set results: loss= 0.4436 accuracy= 0.7866\n",
      "Epoch: 02832 loss_train: 0.4033 loss_rec: 0.4033 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02833 loss_train: 0.4033 loss_rec: 0.4033 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02834 loss_train: 0.4033 loss_rec: 0.4033 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02835 loss_train: 0.4033 loss_rec: 0.4033 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02836 loss_train: 0.4033 loss_rec: 0.4033 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02837 loss_train: 0.4033 loss_rec: 0.4033 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02838 loss_train: 0.4033 loss_rec: 0.4033 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02839 loss_train: 0.4033 loss_rec: 0.4033 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02840 loss_train: 0.4033 loss_rec: 0.4033 acc_train: 0.7983 loss_val: 0.4211 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02841 loss_train: 0.4032 loss_rec: 0.4032 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4436 accuracy= 0.7865\n",
      "Epoch: 02842 loss_train: 0.4032 loss_rec: 0.4032 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02843 loss_train: 0.4032 loss_rec: 0.4032 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02844 loss_train: 0.4032 loss_rec: 0.4032 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02845 loss_train: 0.4032 loss_rec: 0.4032 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02846 loss_train: 0.4032 loss_rec: 0.4032 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02847 loss_train: 0.4032 loss_rec: 0.4032 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02848 loss_train: 0.4032 loss_rec: 0.4032 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02849 loss_train: 0.4032 loss_rec: 0.4032 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02850 loss_train: 0.4032 loss_rec: 0.4032 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02851 loss_train: 0.4032 loss_rec: 0.4032 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Test set results: loss= 0.4437 accuracy= 0.7865\n",
      "Epoch: 02852 loss_train: 0.4032 loss_rec: 0.4032 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02853 loss_train: 0.4032 loss_rec: 0.4032 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02854 loss_train: 0.4032 loss_rec: 0.4032 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02855 loss_train: 0.4032 loss_rec: 0.4032 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02856 loss_train: 0.4031 loss_rec: 0.4031 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02857 loss_train: 0.4031 loss_rec: 0.4031 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02858 loss_train: 0.4031 loss_rec: 0.4031 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02859 loss_train: 0.4031 loss_rec: 0.4031 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02860 loss_train: 0.4031 loss_rec: 0.4031 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02861 loss_train: 0.4031 loss_rec: 0.4031 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Test set results: loss= 0.4438 accuracy= 0.7865\n",
      "Epoch: 02862 loss_train: 0.4031 loss_rec: 0.4031 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02863 loss_train: 0.4031 loss_rec: 0.4031 acc_train: 0.7984 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02864 loss_train: 0.4031 loss_rec: 0.4031 acc_train: 0.7984 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02865 loss_train: 0.4031 loss_rec: 0.4031 acc_train: 0.7984 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02866 loss_train: 0.4031 loss_rec: 0.4031 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02867 loss_train: 0.4031 loss_rec: 0.4031 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02868 loss_train: 0.4031 loss_rec: 0.4031 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02869 loss_train: 0.4031 loss_rec: 0.4031 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02870 loss_train: 0.4031 loss_rec: 0.4031 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02871 loss_train: 0.4030 loss_rec: 0.4030 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4439 accuracy= 0.7865\n",
      "Epoch: 02872 loss_train: 0.4030 loss_rec: 0.4030 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02873 loss_train: 0.4030 loss_rec: 0.4030 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02874 loss_train: 0.4030 loss_rec: 0.4030 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02875 loss_train: 0.4030 loss_rec: 0.4030 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02876 loss_train: 0.4030 loss_rec: 0.4030 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02877 loss_train: 0.4030 loss_rec: 0.4030 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02878 loss_train: 0.4030 loss_rec: 0.4030 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02879 loss_train: 0.4030 loss_rec: 0.4030 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02880 loss_train: 0.4030 loss_rec: 0.4030 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02881 loss_train: 0.4030 loss_rec: 0.4030 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4440 accuracy= 0.7865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02882 loss_train: 0.4030 loss_rec: 0.4030 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02883 loss_train: 0.4030 loss_rec: 0.4030 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02884 loss_train: 0.4030 loss_rec: 0.4030 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02885 loss_train: 0.4030 loss_rec: 0.4030 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0025s\n",
      "Epoch: 02886 loss_train: 0.4030 loss_rec: 0.4030 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02887 loss_train: 0.4029 loss_rec: 0.4029 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02888 loss_train: 0.4029 loss_rec: 0.4029 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02889 loss_train: 0.4029 loss_rec: 0.4029 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02890 loss_train: 0.4029 loss_rec: 0.4029 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02891 loss_train: 0.4029 loss_rec: 0.4029 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4441 accuracy= 0.7865\n",
      "Epoch: 02892 loss_train: 0.4029 loss_rec: 0.4029 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02893 loss_train: 0.4029 loss_rec: 0.4029 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02894 loss_train: 0.4029 loss_rec: 0.4029 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02895 loss_train: 0.4029 loss_rec: 0.4029 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02896 loss_train: 0.4029 loss_rec: 0.4029 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02897 loss_train: 0.4029 loss_rec: 0.4029 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02898 loss_train: 0.4029 loss_rec: 0.4029 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 02899 loss_train: 0.4029 loss_rec: 0.4029 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7892 time: 0.0020s\n",
      "Epoch: 02900 loss_train: 0.4029 loss_rec: 0.4029 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 02901 loss_train: 0.4029 loss_rec: 0.4029 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7892 time: 0.0015s\n",
      "Test set results: loss= 0.4442 accuracy= 0.7865\n",
      "Epoch: 02902 loss_train: 0.4029 loss_rec: 0.4029 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7892 time: 0.0020s\n",
      "Epoch: 02903 loss_train: 0.4028 loss_rec: 0.4028 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7892 time: 0.0020s\n",
      "Epoch: 02904 loss_train: 0.4028 loss_rec: 0.4028 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 02905 loss_train: 0.4028 loss_rec: 0.4028 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 02906 loss_train: 0.4028 loss_rec: 0.4028 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 02907 loss_train: 0.4028 loss_rec: 0.4028 acc_train: 0.7983 loss_val: 0.4210 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 02908 loss_train: 0.4028 loss_rec: 0.4028 acc_train: 0.7984 loss_val: 0.4210 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 02909 loss_train: 0.4028 loss_rec: 0.4028 acc_train: 0.7984 loss_val: 0.4210 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 02910 loss_train: 0.4028 loss_rec: 0.4028 acc_train: 0.7984 loss_val: 0.4210 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 02911 loss_train: 0.4028 loss_rec: 0.4028 acc_train: 0.7984 loss_val: 0.4210 acc_val: 0.7892 time: 0.0015s\n",
      "Test set results: loss= 0.4443 accuracy= 0.7865\n",
      "Epoch: 02912 loss_train: 0.4028 loss_rec: 0.4028 acc_train: 0.7984 loss_val: 0.4210 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 02913 loss_train: 0.4028 loss_rec: 0.4028 acc_train: 0.7984 loss_val: 0.4209 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 02914 loss_train: 0.4028 loss_rec: 0.4028 acc_train: 0.7984 loss_val: 0.4209 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 02915 loss_train: 0.4028 loss_rec: 0.4028 acc_train: 0.7984 loss_val: 0.4209 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 02916 loss_train: 0.4028 loss_rec: 0.4028 acc_train: 0.7984 loss_val: 0.4209 acc_val: 0.7892 time: 0.0015s\n",
      "Epoch: 02917 loss_train: 0.4028 loss_rec: 0.4028 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02918 loss_train: 0.4028 loss_rec: 0.4028 acc_train: 0.7984 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02919 loss_train: 0.4028 loss_rec: 0.4028 acc_train: 0.7984 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02920 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7984 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02921 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7984 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4443 accuracy= 0.7866\n",
      "Epoch: 02922 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02923 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02924 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02925 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02926 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02927 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02928 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02929 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02930 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02931 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4445 accuracy= 0.7866\n",
      "Epoch: 02932 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02933 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02934 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02935 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02936 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02937 loss_train: 0.4027 loss_rec: 0.4027 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02938 loss_train: 0.4026 loss_rec: 0.4026 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02939 loss_train: 0.4026 loss_rec: 0.4026 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02940 loss_train: 0.4026 loss_rec: 0.4026 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02941 loss_train: 0.4026 loss_rec: 0.4026 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Test set results: loss= 0.4445 accuracy= 0.7866\n",
      "Epoch: 02942 loss_train: 0.4026 loss_rec: 0.4026 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02943 loss_train: 0.4026 loss_rec: 0.4026 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02944 loss_train: 0.4026 loss_rec: 0.4026 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0020s\n",
      "Epoch: 02945 loss_train: 0.4026 loss_rec: 0.4026 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7888 time: 0.0015s\n",
      "Epoch: 02946 loss_train: 0.4026 loss_rec: 0.4026 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02947 loss_train: 0.4026 loss_rec: 0.4026 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02948 loss_train: 0.4026 loss_rec: 0.4026 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02949 loss_train: 0.4026 loss_rec: 0.4026 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02950 loss_train: 0.4026 loss_rec: 0.4026 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02951 loss_train: 0.4026 loss_rec: 0.4026 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4446 accuracy= 0.7866\n",
      "Epoch: 02952 loss_train: 0.4026 loss_rec: 0.4026 acc_train: 0.7985 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02953 loss_train: 0.4026 loss_rec: 0.4026 acc_train: 0.7986 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02954 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7986 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02955 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7986 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02956 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7986 loss_val: 0.4209 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02957 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7986 loss_val: 0.4209 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02958 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7986 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02959 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7986 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02960 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7986 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02961 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0020s\n",
      "Test set results: loss= 0.4447 accuracy= 0.7866\n",
      "Epoch: 02962 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02963 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02964 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02965 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02966 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02967 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02968 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02969 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02970 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02971 loss_train: 0.4025 loss_rec: 0.4025 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0020s\n",
      "Test set results: loss= 0.4448 accuracy= 0.7866\n",
      "Epoch: 02972 loss_train: 0.4024 loss_rec: 0.4024 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02973 loss_train: 0.4024 loss_rec: 0.4024 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02974 loss_train: 0.4024 loss_rec: 0.4024 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02975 loss_train: 0.4024 loss_rec: 0.4024 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02976 loss_train: 0.4024 loss_rec: 0.4024 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02977 loss_train: 0.4024 loss_rec: 0.4024 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02978 loss_train: 0.4024 loss_rec: 0.4024 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02979 loss_train: 0.4024 loss_rec: 0.4024 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02980 loss_train: 0.4024 loss_rec: 0.4024 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02981 loss_train: 0.4024 loss_rec: 0.4024 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4449 accuracy= 0.7866\n",
      "Epoch: 02982 loss_train: 0.4024 loss_rec: 0.4024 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02983 loss_train: 0.4024 loss_rec: 0.4024 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02984 loss_train: 0.4024 loss_rec: 0.4024 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02985 loss_train: 0.4024 loss_rec: 0.4024 acc_train: 0.7987 loss_val: 0.4209 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02986 loss_train: 0.4024 loss_rec: 0.4024 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02987 loss_train: 0.4024 loss_rec: 0.4024 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02988 loss_train: 0.4024 loss_rec: 0.4024 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02989 loss_train: 0.4023 loss_rec: 0.4023 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02990 loss_train: 0.4023 loss_rec: 0.4023 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7883 time: 0.0020s\n",
      "Epoch: 02991 loss_train: 0.4023 loss_rec: 0.4023 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7883 time: 0.0015s\n",
      "Test set results: loss= 0.4450 accuracy= 0.7866\n",
      "Epoch: 02992 loss_train: 0.4023 loss_rec: 0.4023 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02993 loss_train: 0.4023 loss_rec: 0.4023 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02994 loss_train: 0.4023 loss_rec: 0.4023 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02995 loss_train: 0.4023 loss_rec: 0.4023 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7883 time: 0.0015s\n",
      "Epoch: 02996 loss_train: 0.4023 loss_rec: 0.4023 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02997 loss_train: 0.4023 loss_rec: 0.4023 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7879 time: 0.0015s\n",
      "Epoch: 02998 loss_train: 0.4023 loss_rec: 0.4023 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7879 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02999 loss_train: 0.4023 loss_rec: 0.4023 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7879 time: 0.0025s\n",
      "Epoch: 03000 loss_train: 0.4023 loss_rec: 0.4023 acc_train: 0.7987 loss_val: 0.4208 acc_val: 0.7879 time: 0.0020s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 5.8805s\n",
      "Test set results: loss= 0.4450 accuracy= 0.7866\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t_total = time.time()\n",
    "for epoch in range(3000):\n",
    "    train(epoch)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        output=test(epoch)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        save_model(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "output=test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [1., 1., 0.,  ..., 1., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[idx_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_val[(labels==1)[idx_val]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(output, 'GraphSage_Upsample_output.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = output.max(1)[1].type_as(labels)\n",
    "cout=preds.cpu().detach().numpy()\n",
    "clabels=labels.cpu().detach().numpy()\n",
    "cidx_test=idx_test.cpu().detach().numpy()\n",
    "cidx_valid=idx_val.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 427.9555555555555, 'Predicted label')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "cnf_matrix_n = metrics.confusion_matrix(clabels[cidx_test], cout[cidx_test])\n",
    "class_names=[\"Not_Related\", \"Related\"]\n",
    "#class_names=[\"Not_Related\", \"1\",\"2\",\"3\"] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "sns.heatmap(cnf_matrix_n, annot=True, cmap=\"YlGnBu\" ,fmt='d', xticklabels=class_names, yticklabels=class_names)\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Graph Sage Upsample')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4202"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix_n[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not_Related       0.78      0.87      0.82      4847\n",
      "     Related       0.79      0.68      0.73      3616\n",
      "\n",
      "    accuracy                           0.79      8463\n",
      "   macro avg       0.79      0.77      0.78      8463\n",
      "weighted avg       0.79      0.79      0.78      8463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(clabels[cidx_test], cout[cidx_test], target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4202,  645],\n",
       "       [1161, 2455]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 427.9555555555555, 'Predicted label')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "cnf_matrix_n_valid = metrics.confusion_matrix(clabels[cidx_valid], cout[cidx_valid])\n",
    "class_names=[\"Not_Related\", \"Related\"]\n",
    "#class_names=[\"Not_Related\", \"1\",\"2\",\"3\"] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "sns.heatmap(cnf_matrix_n, annot=True, cmap=\"YlGnBu\" ,fmt='d', xticklabels=class_names, yticklabels=class_names)\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Graph Sage Upsample')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_test=cnf_matrix_n[0][1]\n",
    "true_positive_test=cnf_matrix_n[1][1]\n",
    "true_negative_test=cnf_matrix_n[0][0]\n",
    "false_negative_test=cnf_matrix_n[1][0]\n",
    "\n",
    "false_positive_valid=cnf_matrix_n_valid[0][1]\n",
    "true_positive_valid=cnf_matrix_n_valid[1][1]\n",
    "true_negative_valid=cnf_matrix_n_valid[0][0]\n",
    "false_negative_valid=cnf_matrix_n_valid[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(true_negative,false_positive):\n",
    "    return str(true_negative/(true_negative+false_positive))\n",
    "def sensitivity(true_positive,false_negative):\n",
    "    return str(true_positive/(true_positive+false_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "Specificity: 0.866927996698989\n",
      "Sensitivity: 0.6789269911504425\n"
     ]
    }
   ],
   "source": [
    "print('test')\n",
    "print(\"Specificity: \"+ specificity(true_negative_test,false_positive_test))\n",
    "print(\"Sensitivity: \"+ sensitivity(true_positive_test,false_negative_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n",
      "Specificity: 0.8816666666666667\n",
      "Sensitivity: 0.6941666666666667\n"
     ]
    }
   ],
   "source": [
    "print('validation')\n",
    "print(\"Specificity: \"+ specificity(true_negative_valid,false_positive_valid))\n",
    "print(\"Sensitivity: \"+sensitivity(true_positive_valid,false_negative_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels=clabels[cidx_test]\n",
    "test_output=cout[cidx_test]\n",
    "valid_labels=clabels[cidx_valid]\n",
    "valid_output=cout[cidx_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_enc=np.eye(4)[test_labels]\n",
    "test_output_enc=np.eye(4)[test_output]\n",
    "valid_labels_enc=np.eye(4)[valid_labels]\n",
    "valid_output_enc=np.eye(4)[valid_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7729274939247158"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(clabels[cidx_test], cout[cidx_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7879166666666668"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(clabels[cidx_valid], cout[cidx_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
