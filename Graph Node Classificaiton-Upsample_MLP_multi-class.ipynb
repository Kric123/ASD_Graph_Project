{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "import ipdb\n",
    "from scipy.io import loadmat\n",
    "import networkx as nx\n",
    "import multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from copy import deepcopy\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_arti(labels, c_train_num):\n",
    "    #labels: n-dim Longtensor, each element in [0,...,m-1].\n",
    "    #cora: m=7\n",
    "    num_classes = len(set(labels.tolist()))\n",
    "    c_idxs = [] # class-wise index\n",
    "    train_idx = []\n",
    "    val_idx = []\n",
    "    test_idx = []\n",
    "    c_num_mat = np.zeros((num_classes,3)).astype(int)\n",
    "    c_num_mat[:,1] = 25\n",
    "    c_num_mat[:,2] = 55\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        c_idx = (labels==i).nonzero()[:,-1].tolist()\n",
    "        print('{:d}-th class sample number: {:d}'.format(i,len(c_idx)))\n",
    "        random.shuffle(c_idx)\n",
    "        c_idxs.append(c_idx)\n",
    "\n",
    "        train_idx = train_idx + c_idx[:c_train_num[i]]\n",
    "        c_num_mat[i,0] = c_train_num[i]\n",
    "\n",
    "        val_idx = val_idx + c_idx[c_train_num[i]:(c_train_num[i]+int(c_train_num[i]*.2))]\n",
    "        test_idx = test_idx + c_idx[int(c_train_num[i]+(c_train_num[i]*.2)):]\n",
    "\n",
    "    random.shuffle(train_idx)\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "\n",
    "    train_idx = torch.LongTensor(train_idx)\n",
    "    val_idx = torch.LongTensor(val_idx)\n",
    "    test_idx = torch.LongTensor(test_idx)\n",
    "    #c_num_mat = torch.LongTensor(c_num_mat)\n",
    "\n",
    "\n",
    "    return train_idx, val_idx, test_idx, c_num_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_upsample(adj,features,labels,idx_train, portion=1.0, im_class_num=3):\n",
    "    c_largest = labels.max().item()\n",
    "    adj_back = adj\n",
    "    chosen = None\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    avg_number = int(idx_train.shape[0]/(c_largest+1))\n",
    "\n",
    "    for i in range(im_class_num):\n",
    "        new_chosen = idx_train[(labels==(c_largest-i))[idx_train]]\n",
    "        if portion == 0:#refers to even distribution\n",
    "            c_portion = int(avg_number/new_chosen.shape[0])\n",
    "\n",
    "            for j in range(c_portion):\n",
    "                if chosen is None:\n",
    "                    chosen = new_chosen\n",
    "                else:\n",
    "                    chosen = torch.cat((chosen, new_chosen), 0)\n",
    "\n",
    "        else:\n",
    "            c_portion = int(portion)\n",
    "            portion_rest = portion-c_portion\n",
    "            for j in range(c_portion):\n",
    "                num = int(new_chosen.shape[0])\n",
    "                new_chosen = new_chosen[:num]\n",
    "\n",
    "                if chosen is None:\n",
    "                    chosen = new_chosen\n",
    "                else:\n",
    "                    chosen = torch.cat((chosen, new_chosen), 0)\n",
    "            \n",
    "            num = int(new_chosen.shape[0]*portion_rest)\n",
    "            new_chosen = new_chosen[:num]\n",
    "\n",
    "            if chosen is None:\n",
    "                chosen = new_chosen\n",
    "            else:\n",
    "                chosen = torch.cat((chosen, new_chosen), 0)\n",
    "            \n",
    "\n",
    "    add_num = chosen.shape[0]\n",
    "    new_adj = adj_back.new(torch.Size((adj_back.shape[0]+add_num, adj_back.shape[0]+add_num)))\n",
    "    new_adj[:adj_back.shape[0], :adj_back.shape[0]] = adj_back[:,:]\n",
    "    new_adj[adj_back.shape[0]:, :adj_back.shape[0]] = adj_back[chosen,:]\n",
    "    new_adj[:adj_back.shape[0], adj_back.shape[0]:] = adj_back[:,chosen]\n",
    "    new_adj[adj_back.shape[0]:, adj_back.shape[0]:] = adj_back[chosen,:][:,chosen]\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    features_append = deepcopy(features[chosen,:])\n",
    "    labels_append = deepcopy(labels[chosen])\n",
    "    idx_new = np.arange(adj_back.shape[0], adj_back.shape[0]+add_num)\n",
    "    idx_train_append = idx_train.new(idx_new)\n",
    "\n",
    "    features = torch.cat((features,features_append), 0)\n",
    "    labels = torch.cat((labels,labels_append), 0)\n",
    "    idx_train = torch.cat((idx_train,idx_train_append), 0)\n",
    "    adj = new_adj\n",
    "\n",
    "    return adj, features, labels, idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_smote(adj,features,labels,idx_train, portion=1.0, im_class_num=3):\n",
    "    c_largest = labels.max().item()\n",
    "    adj_back = adj\n",
    "    chosen = None\n",
    "    new_features = None\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    avg_number = int(idx_train.shape[0]/(c_largest+1))\n",
    "\n",
    "    for i in range(im_class_num):\n",
    "        new_chosen = idx_train[(labels==(c_largest-i))[idx_train]]\n",
    "        if portion == 0:#refers to even distribution\n",
    "            c_portion = int(avg_number/new_chosen.shape[0])\n",
    "\n",
    "            portion_rest = (avg_number/new_chosen.shape[0]) - c_portion\n",
    "\n",
    "        else:\n",
    "            c_portion = int(portion)\n",
    "            portion_rest = portion-c_portion\n",
    "            \n",
    "        for j in range(c_portion):\n",
    "            num = int(new_chosen.shape[0])\n",
    "            new_chosen = new_chosen[:num]\n",
    "\n",
    "            chosen_embed = features[new_chosen,:]\n",
    "            distance = squareform(pdist(chosen_embed.detach()))\n",
    "            np.fill_diagonal(distance,distance.max()+100)\n",
    "\n",
    "            idx_neighbor = distance.argmin(axis=-1)\n",
    "            \n",
    "            interp_place = random.random()\n",
    "            embed = chosen_embed + (chosen_embed[idx_neighbor,:]-chosen_embed)*interp_place\n",
    "\n",
    "            if chosen is None:\n",
    "                chosen = new_chosen\n",
    "                new_features = embed\n",
    "            else:\n",
    "                chosen = torch.cat((chosen, new_chosen), 0)\n",
    "                new_features = torch.cat((new_features, embed),0)\n",
    "            \n",
    "        num = int(new_chosen.shape[0]*portion_rest)\n",
    "        new_chosen = new_chosen[:num]\n",
    "\n",
    "        chosen_embed = features[new_chosen,:]\n",
    "        distance = squareform(pdist(chosen_embed.detach()))\n",
    "        np.fill_diagonal(distance,distance.max()+100)\n",
    "\n",
    "        idx_neighbor = distance.argmin(axis=-1)\n",
    "            \n",
    "        interp_place = random.random()\n",
    "        embed = chosen_embed + (chosen_embed[idx_neighbor,:]-chosen_embed)*interp_place\n",
    "\n",
    "        if chosen is None:\n",
    "            chosen = new_chosen\n",
    "            new_features = embed\n",
    "        else:\n",
    "            chosen = torch.cat((chosen, new_chosen), 0)\n",
    "            new_features = torch.cat((new_features, embed),0)\n",
    "            \n",
    "\n",
    "    add_num = chosen.shape[0]\n",
    "    new_adj = adj_back.new(torch.Size((adj_back.shape[0]+add_num, adj_back.shape[0]+add_num)))\n",
    "    new_adj[:adj_back.shape[0], :adj_back.shape[0]] = adj_back[:,:]\n",
    "    new_adj[adj_back.shape[0]:, :adj_back.shape[0]] = adj_back[chosen,:]\n",
    "    new_adj[:adj_back.shape[0], adj_back.shape[0]:] = adj_back[:,chosen]\n",
    "    new_adj[adj_back.shape[0]:, adj_back.shape[0]:] = adj_back[chosen,:][:,chosen]\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    features_append = deepcopy(new_features)\n",
    "    labels_append = deepcopy(labels[chosen])\n",
    "    idx_new = np.arange(adj_back.shape[0], adj_back.shape[0]+add_num)\n",
    "    idx_train_append = idx_train.new(idx_new)\n",
    "\n",
    "    features = torch.cat((features,features_append), 0)\n",
    "    labels = torch.cat((labels,labels_append), 0)\n",
    "    idx_train = torch.cat((idx_train,idx_train_append), 0)\n",
    "    adj = new_adj\n",
    "\n",
    "    return adj, features, labels, idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ffn(hidden_units, dropout_rate, name=None):\n",
    "    fnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(fnn_layers, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        #for 3_D batch, need a loop!!!\n",
    "\n",
    "\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "#Multihead attention layer\n",
    "class MultiHead(Module):#currently, allowed for only one sample each time. As no padding mask is required.\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        num_heads,\n",
    "        kdim=None,\n",
    "        vdim=None,\n",
    "        embed_dim = 128,#should equal num_heads*head dim\n",
    "        v_embed_dim = None,\n",
    "        dropout=0.1,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super(MultiHead, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.kdim = kdim if kdim is not None else input_dim\n",
    "        self.vdim = vdim if vdim is not None else input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.v_embed_dim = v_embed_dim if v_embed_dim is not None else embed_dim\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.bias = bias\n",
    "        assert (\n",
    "            self.head_dim * num_heads == self.embed_dim\n",
    "        ), \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        assert self.v_embed_dim % num_heads ==0, \"v_embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "\n",
    "\n",
    "        self.q_proj = nn.Linear(self.input_dim, self.embed_dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(self.kdim, self.embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(self.vdim, self.v_embed_dim, bias=bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.v_embed_dim, self.v_embed_dim//self.num_heads, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if True:\n",
    "            # Empirically observed the convergence to be much better with\n",
    "            # the scaled initialization\n",
    "            nn.init.normal_(self.k_proj.weight)\n",
    "            nn.init.normal_(self.v_proj.weight)\n",
    "            nn.init.normal_(self.q_proj.weight)\n",
    "        else:\n",
    "            nn.init.normal_(self.k_proj.weight)\n",
    "            nn.init.normal_(self.v_proj.weight)\n",
    "            nn.init.normal_(self.q_proj.weight)\n",
    "\n",
    "        nn.init.normal_(self.out_proj.weight)\n",
    "\n",
    "        if self.out_proj.bias is not None:\n",
    "            nn.init.constant_(self.out_proj.bias, 0.)\n",
    "\n",
    "        if self.bias:\n",
    "            nn.init.constant_(self.k_proj.bias, 0.)\n",
    "            nn.init.constant_(self.v_proj.bias, 0.)\n",
    "            nn.init.constant_(self.q_proj.bias, 0.)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        need_weights: bool = False,\n",
    "        need_head_weights: bool = False,\n",
    "    ):\n",
    "        \"\"\"Input shape: Time x Batch x Channel\n",
    "        Args:\n",
    "            need_weights (bool, optional): return the attention weights,\n",
    "                averaged over heads (default: False).\n",
    "            need_head_weights (bool, optional): return the attention\n",
    "                weights for each head. Implies *need_weights*. Default:\n",
    "                return the average attention weights over all heads.\n",
    "        \"\"\"\n",
    "        if need_head_weights:\n",
    "            need_weights = True\n",
    "\n",
    "        batch_num, node_num, input_dim = query.size()\n",
    "\n",
    "        assert key is not None and value is not None\n",
    "\n",
    "        #project input\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(key)\n",
    "        v = self.v_proj(value)\n",
    "        q = q * self.scaling\n",
    "\n",
    "        #compute attention\n",
    "        q = q.view(batch_num, node_num, self.num_heads, self.head_dim).transpose(-2,-3).contiguous().view(batch_num*self.num_heads, node_num, self.head_dim)\n",
    "        k = k.view(batch_num, node_num, self.num_heads, self.head_dim).transpose(-2,-3).contiguous().view(batch_num*self.num_heads, node_num, self.head_dim)\n",
    "        v = v.view(batch_num, node_num, self.num_heads, self.vdim).transpose(-2,-3).contiguous().view(batch_num*self.num_heads, node_num, self.vdim)\n",
    "        attn_output_weights = torch.bmm(q, k.transpose(-1,-2))\n",
    "        attn_output_weights = F.softmax(attn_output_weights, dim=-1)\n",
    "\n",
    "        #drop out\n",
    "        attn_output_weights = F.dropout(attn_output_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        #collect output\n",
    "        attn_output = torch.bmm(attn_output_weights, v)\n",
    "        attn_output = attn_output.view(batch_num, self.num_heads, node_num, self.vdim).transpose(-2,-3).contiguous().view(batch_num, node_num, self.v_embed_dim)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "\n",
    "        if need_weights:\n",
    "            attn_output_weights = attn_output_weights #view: (batch_num, num_heads, node_num, node_num)\n",
    "            return attn_output, attn_output_weights.sum(dim=1) / self.num_heads\n",
    "        else:\n",
    "            return attn_output\n",
    "\n",
    "\n",
    "#Graphsage layer\n",
    "class SageConv(Module):\n",
    "    \"\"\"\n",
    "    Simple Graphsage layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super(SageConv, self).__init__()\n",
    "\n",
    "        self.proj = nn.Linear(in_features*2, out_features, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "        #print(\"note: for dense graph in graphsage, require it normalized.\")\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        nn.init.normal_(self.proj.weight)\n",
    "\n",
    "        if self.proj.bias is not None:\n",
    "            nn.init.constant_(self.proj.bias, 0.)\n",
    "\n",
    "    def forward(self, features, adj):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            adj: can be sparse or dense matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        #fuse info from neighbors. to be added:\n",
    "        if not isinstance(adj, torch.sparse.FloatTensor):\n",
    "            if len(adj.shape) == 3:\n",
    "                neigh_feature = torch.bmm(adj, features) / (adj.sum(dim=1).reshape((adj.shape[0], adj.shape[1],-1))+1)\n",
    "            else:\n",
    "                neigh_feature = torch.mm(adj, features) / (adj.sum(dim=1).reshape(adj.shape[0], -1)+1)\n",
    "        else:\n",
    "            #print(\"spmm not implemented for batch training. Note!\")\n",
    "            \n",
    "            neigh_feature = torch.spmm(adj, features) / (adj.to_dense().sum(dim=1).reshape(adj.shape[0], -1)+1)\n",
    "\n",
    "        #perform conv\n",
    "        data = torch.cat([features,neigh_feature], dim=-1)\n",
    "        combined = self.proj(data)\n",
    "\n",
    "        return combined\n",
    "\n",
    "#GraphAT layers\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        if isinstance(adj, torch.sparse.FloatTensor):\n",
    "            adj = adj.to_dense()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        N = h.size()[0]\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b\n",
    "\n",
    "\n",
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)\n",
    "\n",
    "    \n",
    "class SpGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(SpGraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
    "                \n",
    "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm = SpecialSpmm()\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        dv = 'cuda' if input.is_cuda else 'cpu'\n",
    "\n",
    "        N = input.size()[0]\n",
    "        edge = adj.nonzero().t()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        # h: N x out\n",
    "        assert not torch.isnan(h).any()\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
    "        # edge: 2*D x E\n",
    "\n",
    "        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "\n",
    "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n",
    "        # e_rowsum: N x 1\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "        \n",
    "        h_prime = h_prime.div(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------\n",
    "### models ###\n",
    "#--------------\n",
    "\n",
    "#gcn_encode\n",
    "class GCN_En(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(GCN_En, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GCN_En2(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(GCN_En2, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc2(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "class GCN_Classifier(nn.Module):\n",
    "    def __init__(self, nembed, nhid, nclass, dropout):\n",
    "        super(GCN_Classifier, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nembed, nhid)\n",
    "        self.mlp = nn.Linear(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "#sage model\n",
    "\n",
    "class Sage_En(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(Sage_En, self).__init__()\n",
    "\n",
    "        self.sage1 = SageConv(nfeat, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.sage1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "class Sage_En2(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(Sage_En2, self).__init__()\n",
    "\n",
    "        self.sage1 = SageConv(nfeat, nhid)\n",
    "        self.sage2 = SageConv(nhid, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.sage1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.sage2(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Sage_Classifier(nn.Module):\n",
    "    def __init__(self, nembed, nhid, nclass, dropout):\n",
    "        super(Sage_Classifier, self).__init__()\n",
    "\n",
    "        self.sage1 = SageConv(nembed, nhid)\n",
    "        self.mlp = nn.Linear(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.sage1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "#GAT model\n",
    "\n",
    "class GAT_En(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout, alpha=0.2, nheads=8):\n",
    "        super(GAT_En, self).__init__()\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_proj = nn.Linear(nhid * nheads, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.out_proj.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_proj(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class GAT_En2(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout, alpha=0.2, nheads=8):\n",
    "        super(GAT_En2, self).__init__()\n",
    "\n",
    "        \n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_proj = nn.Linear(nhid * nheads, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions_2 = [GraphAttentionLayer(nembed, nembed, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions_2):\n",
    "            self.add_module('attention2_{}'.format(i), attention)\n",
    "\n",
    "        self.out_proj_2 = nn.Linear(nembed * nheads, nembed)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.out_proj.weight,std=0.05)\n",
    "        nn.init.normal_(self.out_proj_2.weight,std=0.05)\n",
    "\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_proj(x))\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions_2], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_proj_2(x))\n",
    "        return x\n",
    "\n",
    "class GAT_Classifier(nn.Module):\n",
    "    def __init__(self, nembed, nhid, nclass, dropout, alpha=0.2, nheads=8):\n",
    "        super(GAT_Classifier, self).__init__()\n",
    "\n",
    "        \n",
    "        self.attentions = [GraphAttentionLayer(nembed, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_proj = nn.Linear(nhid * nheads, nhid)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.mlp = nn.Linear(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp.weight,std=0.05)\n",
    "        nn.init.normal_(self.out_proj.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_proj(x))\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MLP_En(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(MLP_En, self).__init__()\n",
    "\n",
    "        self.mlp1 = nn.Linear(nfeat, nembed)\n",
    "        self.dropout = dropout\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp1.weight,std=0.05)\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.mlp1(x))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, nembed, nhid, nclass, dropout):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.mlp = nn.Linear(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(Module):\n",
    "    \"\"\"\n",
    "    Simple Graphsage layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nembed, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.de_weight = Parameter(torch.FloatTensor(nembed, nembed))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.de_weight.size(1))\n",
    "        self.de_weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "    def forward(self, node_embed):\n",
    "        \n",
    "        combine = F.linear(node_embed, self.de_weight)\n",
    "        adj_out = torch.sigmoid(torch.mm(combine, combine.transpose(-1,-2)))\n",
    "\n",
    "        return adj_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_genuine(labels):\n",
    "    #labels: n-dim Longtensor, each element in [0,...,m-1].\n",
    "    #cora: m=7\n",
    "    num_classes = len(set(labels.tolist()))\n",
    "    c_idxs = [] # class-wise index\n",
    "    train_idx = []\n",
    "    val_idx = []\n",
    "    test_idx = []\n",
    "    c_num_mat = np.zeros((num_classes,3)).astype(int)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        c_idx = (labels==i).nonzero()[:,-1].tolist()\n",
    "        c_num = len(c_idx)\n",
    "        print('{:d}-th class sample number: {:d}'.format(i,len(c_idx)))\n",
    "        random.shuffle(c_idx)\n",
    "        c_idxs.append(c_idx)\n",
    "\n",
    "        if c_num <4:\n",
    "            if c_num < 3:\n",
    "                print(\"too small class type\")\n",
    "                ipdb.set_trace()\n",
    "            c_num_mat[i,0] = 1\n",
    "            c_num_mat[i,1] = 1\n",
    "            c_num_mat[i,2] = 1\n",
    "        else:\n",
    "            c_num_mat[i,0] = int(c_num/4)\n",
    "            c_num_mat[i,1] = int(c_num/4)\n",
    "            c_num_mat[i,2] = int(c_num/2)\n",
    "\n",
    "\n",
    "        train_idx = train_idx + c_idx[:c_num_mat[i,0]]\n",
    "\n",
    "        val_idx = val_idx + c_idx[c_num_mat[i,0]:c_num_mat[i,0]+c_num_mat[i,1]]\n",
    "        test_idx = test_idx + c_idx[c_num_mat[i,0]+c_num_mat[i,1]:c_num_mat[i,0]+c_num_mat[i,1]+c_num_mat[i,2]]\n",
    "\n",
    "    random.shuffle(train_idx)\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "\n",
    "    train_idx = torch.LongTensor(train_idx)\n",
    "    val_idx = torch.LongTensor(val_idx)\n",
    "    test_idx = torch.LongTensor(test_idx)\n",
    "    #c_num_mat = torch.LongTensor(c_num_mat)\n",
    "\n",
    "\n",
    "    return train_idx, val_idx, test_idx, c_num_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kyle\\AppData\\Local\\Temp\\ipykernel_21844\\55265982.py:6: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj=nx.adjacency_matrix(g,weight=None)\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(\"interactions_smote.csv\")\n",
    "data['edge']=data['Gene Symbol']+',' +data['Interactor Symbol']\n",
    "Graphtype = nx.Graph()\n",
    "data['edge']=data['edge'].astype(str)\n",
    "g = nx.parse_edgelist(data['edge'], delimiter=',', create_using=Graphtype,)\n",
    "adj=nx.adjacency_matrix(g,weight=None)\n",
    "adj=adj.toarray()\n",
    "node_features = np.loadtxt('node_features_smote.txt')\n",
    "labels = np.loadtxt('Multi-Labels.txt')\n",
    "#labels = np.loadtxt('Labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=torch.LongTensor(labels)\n",
    "features=torch.LongTensor(node_features)\n",
    "adj=torch.LongTensor(adj)\n",
    "class_sample_num = 4000\n",
    "c_train_num = []\n",
    "for i in range(labels.max().item() + 1):\n",
    "    if i > labels.max().item()-1: #only imbalance the last classes\n",
    "        c_train_num.append(int(class_sample_num))\n",
    "\n",
    "    else:\n",
    "        c_train_num.append(class_sample_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 214\n",
      "2-th class sample number: 530\n",
      "3-th class sample number: 69\n",
      "-----------------------------------\n",
      "1\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 428\n",
      "2-th class sample number: 1060\n",
      "3-th class sample number: 138\n",
      "-----------------------------------\n",
      "2\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 856\n",
      "2-th class sample number: 2120\n",
      "3-th class sample number: 276\n",
      "-----------------------------------\n",
      "3\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 1712\n",
      "2-th class sample number: 4240\n",
      "3-th class sample number: 552\n",
      "-----------------------------------\n",
      "4\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 3424\n",
      "2-th class sample number: 8240\n",
      "3-th class sample number: 1104\n",
      "-----------------------------------\n",
      "5\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 6848\n",
      "2-th class sample number: 12240\n",
      "3-th class sample number: 2208\n",
      "-----------------------------------\n",
      "6\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 10848\n",
      "2-th class sample number: 16240\n",
      "3-th class sample number: 4416\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#for i in range(4):\n",
    "for i in range(7):\n",
    "        print(i)\n",
    "        print(\"-----------------------------------\")\n",
    "        idx_train, idx_val, idx_test, class_num_mat= split_arti(labels, c_train_num)\n",
    "        adj,features,labels,idx_train = src_upsample(adj,features,labels,idx_train,portion=1, im_class_num=3)\n",
    "        #adj,features,labels,idx_train = src_upsample(adj,features,labels,idx_train,portion=1, im_class_num=1)\n",
    "        print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th class sample number: 11403\n",
      "1-th class sample number: 14848\n",
      "2-th class sample number: 20240\n",
      "3-th class sample number: 8416\n"
     ]
    }
   ],
   "source": [
    "idx_train, idx_val, idx_test, class_num_mat= split_arti(labels, c_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = MLP_En(nfeat=features.shape[1],\n",
    "        nhid=1,\n",
    "        nembed=1,\n",
    "        dropout=0)\n",
    "classifier = Classifier(nembed=1, \n",
    "        nhid=1, \n",
    "        nclass=labels.max().item() + 1, \n",
    "        dropout=0)\n",
    "decoder = Decoder(nembed=1,\n",
    "        dropout=0)\n",
    "\n",
    "\n",
    "optimizer_en = optim.AdamW(encoder.parameters(),\n",
    "                       lr=0.001,weight_decay=5e-4,eps=1e-04)\n",
    "optimizer_cls = optim.AdamW(classifier.parameters(),\n",
    "                       lr=0.001,weight_decay=5e-4,eps=1e-04)\n",
    "optimizer_de = optim.AdamW(decoder.parameters(),\n",
    "                       lr=0.001,weight_decay=5e-4,eps=1e-04)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_upsample(embed, labels, idx_train, adj=None, portion=1.0, im_class_num=3):\n",
    "    c_largest = labels.max().item()\n",
    "    avg_number = int(idx_train.shape[0]/(c_largest+1))\n",
    "    #ipdb.set_trace()\n",
    "    adj_new = None\n",
    "\n",
    "    for i in range(im_class_num):\n",
    "        chosen = idx_train[(labels==(c_largest-i))[idx_train]]\n",
    "        num = int(chosen.shape[0]*portion)\n",
    "        if portion == 0:\n",
    "            c_portion = int(avg_number/chosen.shape[0])\n",
    "            num = chosen.shape[0]\n",
    "        else:\n",
    "            c_portion = 1\n",
    "\n",
    "        for j in range(c_portion):\n",
    "            chosen = chosen[:num]\n",
    "\n",
    "            chosen_embed = embed[chosen,:]\n",
    "            distance = squareform(pdist(chosen_embed.detach()))\n",
    "            np.fill_diagonal(distance,distance.max()+100)\n",
    "\n",
    "            idx_neighbor = distance.argmin(axis=-1)\n",
    "            \n",
    "            interp_place = random.random()\n",
    "            new_embed = embed[chosen,:] + (chosen_embed[idx_neighbor[:],:]-embed[chosen,:])*interp_place\n",
    "\n",
    "\n",
    "            new_labels = labels.new(torch.Size((chosen.shape[0],1))).reshape(-1).fill_(c_largest-i)\n",
    "            idx_new = np.arange(embed.shape[0], embed.shape[0]+chosen.shape[0])\n",
    "            idx_train_append = idx_train.new(idx_new)\n",
    "\n",
    "            embed = torch.cat((embed,new_embed), 0)\n",
    "            labels = torch.cat((labels,new_labels), 0)\n",
    "            idx_train = torch.cat((idx_train,idx_train_append), 0)\n",
    "\n",
    "            if adj is not None:\n",
    "                if adj_new is None:\n",
    "                    adj_new = adj.new(torch.clamp(adj[chosen,:] + adj[idx_neighbor,:], min=0.0, max = 1.0))\n",
    "                else:\n",
    "                    temp = adj.new(torch.clamp(adj[chosen,:] + adj[idx_neighbor,:], min=0.0, max = 1.0))\n",
    "                    adj_new = torch.cat((adj_new, temp), 0)\n",
    "\n",
    "    if adj is not None:\n",
    "        add_num = adj_new.shape[0]\n",
    "        new_adj = adj.new(torch.Size((adj.shape[0]+add_num, adj.shape[0]+add_num))).fill_(0.0)\n",
    "        new_adj[:adj.shape[0], :adj.shape[0]] = adj[:,:]\n",
    "        new_adj[adj.shape[0]:, :adj.shape[0]] = adj_new[:,:]\n",
    "        new_adj[:adj.shape[0], adj.shape[0]:] = torch.transpose(adj_new, 0, 1)[:,:]\n",
    "\n",
    "        return embed, labels, idx_train, new_adj.detach()\n",
    "\n",
    "    else:\n",
    "        return embed, labels, idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "    decoder.train()\n",
    "    optimizer_en.zero_grad()\n",
    "    optimizer_cls.zero_grad()\n",
    "    optimizer_de.zero_grad()\n",
    "\n",
    "    embed = encoder(features, adj)\n",
    "\n",
    "    #perform SMOTE in embedding space\n",
    "    labels_new = labels\n",
    "    idx_train_new = idx_train\n",
    "    adj_new = adj\n",
    "\n",
    "   \n",
    "    #ipdb.set_trace()\n",
    "    output = classifier(embed, adj_new)\n",
    "\n",
    "    loss_train = F.cross_entropy(output[idx_train_new], labels_new[idx_train_new])\n",
    "    \n",
    "    acc_train = accuracy(output[idx_train], labels_new[idx_train])\n",
    "    loss = loss_train\n",
    "    loss_rec = loss_train\n",
    "    loss.backward()\n",
    "    optimizer_en.step()\n",
    "\n",
    "    optimizer_cls.step()\n",
    "    loss_val = F.cross_entropy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "\n",
    "    print('Epoch: {:05d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'loss_rec: {:.4f}'.format(loss_rec.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test(epoch = 0):\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    decoder.eval()\n",
    "    embed = encoder(features, adj)\n",
    "    output = classifier(embed, adj)\n",
    "    loss_test = F.cross_entropy(output[idx_test], labels[idx_test])\n",
    "    \n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "  \n",
    "\n",
    "    '''\n",
    "    if epoch==40:\n",
    "        torch\n",
    "    '''\n",
    "    return output\n",
    "\n",
    "\n",
    "def save_model(epoch):\n",
    "    saved_content = {}\n",
    "\n",
    "    saved_content['encoder'] = encoder.state_dict()\n",
    "    saved_content['decoder'] = decoder.state_dict()\n",
    "    saved_content['classifier'] = classifier.state_dict()\n",
    "\n",
    "    torch.save(saved_content, 'model_checkpoint.pth')\n",
    "\n",
    "    return\n",
    "\n",
    "def load_model(filename):\n",
    "    loaded_content = torch.load('checkpoint/{}/{}.pth')\n",
    "\n",
    "    encoder.load_state_dict(loaded_content['encoder'])\n",
    "    decoder.load_state_dict(loaded_content['decoder'])\n",
    "    classifier.load_state_dict(loaded_content['classifier'])\n",
    "\n",
    "    print(\"successfully loaded: \"+ filename)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.cuda()\n",
    "classifier = classifier.cuda()\n",
    "decoder = decoder.cuda()\n",
    "labels = labels.cuda()\n",
    "idx_train = idx_train.cuda()\n",
    "idx_val = idx_val.cuda()\n",
    "idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kyle\\AppData\\Local\\Temp\\ipykernel_21844\\4161052455.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  adj = torch.tensor(adj, dtype=torch.float)\n",
      "C:\\Users\\Kyle\\AppData\\Local\\Temp\\ipykernel_21844\\4161052455.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  features = torch.tensor(features, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "adj = torch.tensor(adj, dtype=torch.float)\n",
    "features = torch.tensor(features, dtype=torch.float)\n",
    "features = features.cuda()\n",
    "adj = adj.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00001 loss_train: 1.4978 loss_rec: 1.4978 acc_train: 0.2500 loss_val: 1.4978 acc_val: 0.2500 time: 2.9540s\n",
      "Test set results: loss= 1.5421 accuracy= 0.1849\n",
      "Epoch: 00002 loss_train: 1.4974 loss_rec: 1.4974 acc_train: 0.2500 loss_val: 1.4974 acc_val: 0.2500 time: 0.0040s\n",
      "Epoch: 00003 loss_train: 1.4971 loss_rec: 1.4971 acc_train: 0.2500 loss_val: 1.4971 acc_val: 0.2500 time: 0.0040s\n",
      "Epoch: 00004 loss_train: 1.4967 loss_rec: 1.4967 acc_train: 0.2500 loss_val: 1.4967 acc_val: 0.2500 time: 0.0060s\n",
      "Epoch: 00005 loss_train: 1.4964 loss_rec: 1.4964 acc_train: 0.2500 loss_val: 1.4963 acc_val: 0.2500 time: 0.0025s\n",
      "Epoch: 00006 loss_train: 1.4960 loss_rec: 1.4960 acc_train: 0.2500 loss_val: 1.4960 acc_val: 0.2500 time: 0.0040s\n",
      "Epoch: 00007 loss_train: 1.4957 loss_rec: 1.4957 acc_train: 0.2500 loss_val: 1.4956 acc_val: 0.2500 time: 0.0030s\n",
      "Epoch: 00008 loss_train: 1.4953 loss_rec: 1.4953 acc_train: 0.2500 loss_val: 1.4953 acc_val: 0.2500 time: 0.0035s\n",
      "Epoch: 00009 loss_train: 1.4949 loss_rec: 1.4949 acc_train: 0.2500 loss_val: 1.4949 acc_val: 0.2500 time: 0.0025s\n",
      "Epoch: 00010 loss_train: 1.4946 loss_rec: 1.4946 acc_train: 0.2500 loss_val: 1.4945 acc_val: 0.2500 time: 0.0020s\n",
      "Epoch: 00011 loss_train: 1.4942 loss_rec: 1.4942 acc_train: 0.2500 loss_val: 1.4941 acc_val: 0.2500 time: 0.0025s\n",
      "Test set results: loss= 1.5376 accuracy= 0.1849\n",
      "Epoch: 00012 loss_train: 1.4938 loss_rec: 1.4938 acc_train: 0.2500 loss_val: 1.4937 acc_val: 0.2500 time: 0.0020s\n",
      "Epoch: 00013 loss_train: 1.4934 loss_rec: 1.4934 acc_train: 0.2500 loss_val: 1.4934 acc_val: 0.2500 time: 0.0020s\n",
      "Epoch: 00014 loss_train: 1.4930 loss_rec: 1.4930 acc_train: 0.2500 loss_val: 1.4930 acc_val: 0.2500 time: 0.0020s\n",
      "Epoch: 00015 loss_train: 1.4926 loss_rec: 1.4926 acc_train: 0.2500 loss_val: 1.4926 acc_val: 0.2500 time: 0.0025s\n",
      "Epoch: 00016 loss_train: 1.4922 loss_rec: 1.4922 acc_train: 0.2500 loss_val: 1.4921 acc_val: 0.2500 time: 0.0030s\n",
      "Epoch: 00017 loss_train: 1.4918 loss_rec: 1.4918 acc_train: 0.2500 loss_val: 1.4917 acc_val: 0.2500 time: 0.0020s\n",
      "Epoch: 00018 loss_train: 1.4914 loss_rec: 1.4914 acc_train: 0.2500 loss_val: 1.4913 acc_val: 0.2500 time: 0.0025s\n",
      "Epoch: 00019 loss_train: 1.4910 loss_rec: 1.4910 acc_train: 0.2500 loss_val: 1.4909 acc_val: 0.2500 time: 0.0020s\n",
      "Epoch: 00020 loss_train: 1.4905 loss_rec: 1.4905 acc_train: 0.2500 loss_val: 1.4904 acc_val: 0.2500 time: 0.0025s\n",
      "Epoch: 00021 loss_train: 1.4901 loss_rec: 1.4901 acc_train: 0.2500 loss_val: 1.4900 acc_val: 0.2500 time: 0.0020s\n",
      "Test set results: loss= 1.5325 accuracy= 0.1849\n",
      "Epoch: 00022 loss_train: 1.4896 loss_rec: 1.4896 acc_train: 0.2500 loss_val: 1.4895 acc_val: 0.2500 time: 0.0025s\n",
      "Epoch: 00023 loss_train: 1.4892 loss_rec: 1.4892 acc_train: 0.2500 loss_val: 1.4891 acc_val: 0.2500 time: 0.0020s\n",
      "Epoch: 00024 loss_train: 1.4887 loss_rec: 1.4887 acc_train: 0.2500 loss_val: 1.4886 acc_val: 0.2500 time: 0.0025s\n",
      "Epoch: 00025 loss_train: 1.4883 loss_rec: 1.4883 acc_train: 0.2500 loss_val: 1.4881 acc_val: 0.2500 time: 0.0025s\n",
      "Epoch: 00026 loss_train: 1.4878 loss_rec: 1.4878 acc_train: 0.2500 loss_val: 1.4876 acc_val: 0.2500 time: 0.0020s\n",
      "Epoch: 00027 loss_train: 1.4873 loss_rec: 1.4873 acc_train: 0.2500 loss_val: 1.4871 acc_val: 0.2500 time: 0.0025s\n",
      "Epoch: 00028 loss_train: 1.4868 loss_rec: 1.4868 acc_train: 0.2500 loss_val: 1.4866 acc_val: 0.2500 time: 0.0030s\n",
      "Epoch: 00029 loss_train: 1.4863 loss_rec: 1.4863 acc_train: 0.2500 loss_val: 1.4861 acc_val: 0.2500 time: 0.0020s\n",
      "Epoch: 00030 loss_train: 1.4857 loss_rec: 1.4857 acc_train: 0.2500 loss_val: 1.4855 acc_val: 0.2500 time: 0.0025s\n",
      "Epoch: 00031 loss_train: 1.4852 loss_rec: 1.4852 acc_train: 0.2500 loss_val: 1.4850 acc_val: 0.2500 time: 0.0020s\n",
      "Test set results: loss= 1.5263 accuracy= 0.1849\n",
      "Epoch: 00032 loss_train: 1.4847 loss_rec: 1.4847 acc_train: 0.2500 loss_val: 1.4844 acc_val: 0.2500 time: 0.0025s\n",
      "Epoch: 00033 loss_train: 1.4841 loss_rec: 1.4841 acc_train: 0.2500 loss_val: 1.4839 acc_val: 0.2500 time: 0.0020s\n",
      "Epoch: 00034 loss_train: 1.4836 loss_rec: 1.4836 acc_train: 0.2500 loss_val: 1.4833 acc_val: 0.2500 time: 0.0025s\n",
      "Epoch: 00035 loss_train: 1.4830 loss_rec: 1.4830 acc_train: 0.2500 loss_val: 1.4827 acc_val: 0.2500 time: 0.0025s\n",
      "Epoch: 00036 loss_train: 1.4824 loss_rec: 1.4824 acc_train: 0.2500 loss_val: 1.4821 acc_val: 0.2500 time: 0.0025s\n",
      "Epoch: 00037 loss_train: 1.4818 loss_rec: 1.4818 acc_train: 0.2500 loss_val: 1.4815 acc_val: 0.2500 time: 0.0020s\n",
      "Epoch: 00038 loss_train: 1.4812 loss_rec: 1.4812 acc_train: 0.2500 loss_val: 1.4809 acc_val: 0.2500 time: 0.0025s\n",
      "Epoch: 00039 loss_train: 1.4806 loss_rec: 1.4806 acc_train: 0.2500 loss_val: 1.4803 acc_val: 0.2500 time: 0.0020s\n",
      "Epoch: 00040 loss_train: 1.4800 loss_rec: 1.4800 acc_train: 0.2500 loss_val: 1.4796 acc_val: 0.2500 time: 0.0020s\n",
      "Epoch: 00041 loss_train: 1.4793 loss_rec: 1.4793 acc_train: 0.2500 loss_val: 1.4790 acc_val: 0.2500 time: 0.0025s\n",
      "Test set results: loss= 1.5190 accuracy= 0.1849\n",
      "Epoch: 00042 loss_train: 1.4787 loss_rec: 1.4787 acc_train: 0.2500 loss_val: 1.4783 acc_val: 0.2500 time: 0.0020s\n",
      "Epoch: 00043 loss_train: 1.4780 loss_rec: 1.4780 acc_train: 0.2500 loss_val: 1.4776 acc_val: 0.2500 time: 0.0025s\n",
      "Epoch: 00044 loss_train: 1.4773 loss_rec: 1.4773 acc_train: 0.2500 loss_val: 1.4769 acc_val: 0.2500 time: 0.0020s\n",
      "Epoch: 00045 loss_train: 1.4767 loss_rec: 1.4767 acc_train: 0.2500 loss_val: 1.4762 acc_val: 0.2500 time: 0.0025s\n",
      "Epoch: 00046 loss_train: 1.4760 loss_rec: 1.4760 acc_train: 0.2500 loss_val: 1.4755 acc_val: 0.2500 time: 0.0020s\n",
      "Epoch: 00047 loss_train: 1.4753 loss_rec: 1.4753 acc_train: 0.2511 loss_val: 1.4748 acc_val: 0.2503 time: 0.0025s\n",
      "Epoch: 00048 loss_train: 1.4746 loss_rec: 1.4746 acc_train: 0.2511 loss_val: 1.4741 acc_val: 0.2503 time: 0.0030s\n",
      "Epoch: 00049 loss_train: 1.4738 loss_rec: 1.4738 acc_train: 0.2511 loss_val: 1.4733 acc_val: 0.2503 time: 0.0020s\n",
      "Epoch: 00050 loss_train: 1.4731 loss_rec: 1.4731 acc_train: 0.2511 loss_val: 1.4726 acc_val: 0.2503 time: 0.0020s\n",
      "Epoch: 00051 loss_train: 1.4724 loss_rec: 1.4724 acc_train: 0.2511 loss_val: 1.4718 acc_val: 0.2503 time: 0.0020s\n",
      "Test set results: loss= 1.5106 accuracy= 0.1862\n",
      "Epoch: 00052 loss_train: 1.4716 loss_rec: 1.4716 acc_train: 0.2511 loss_val: 1.4710 acc_val: 0.2503 time: 0.0020s\n",
      "Epoch: 00053 loss_train: 1.4709 loss_rec: 1.4709 acc_train: 0.2511 loss_val: 1.4702 acc_val: 0.2503 time: 0.0020s\n",
      "Epoch: 00054 loss_train: 1.4701 loss_rec: 1.4701 acc_train: 0.2511 loss_val: 1.4694 acc_val: 0.2503 time: 0.0020s\n",
      "Epoch: 00055 loss_train: 1.4693 loss_rec: 1.4693 acc_train: 0.2511 loss_val: 1.4686 acc_val: 0.2503 time: 0.0030s\n",
      "Epoch: 00056 loss_train: 1.4686 loss_rec: 1.4686 acc_train: 0.2511 loss_val: 1.4678 acc_val: 0.2503 time: 0.0025s\n",
      "Epoch: 00057 loss_train: 1.4678 loss_rec: 1.4678 acc_train: 0.2511 loss_val: 1.4670 acc_val: 0.2503 time: 0.0020s\n",
      "Epoch: 00058 loss_train: 1.4670 loss_rec: 1.4670 acc_train: 0.2511 loss_val: 1.4662 acc_val: 0.2503 time: 0.0020s\n",
      "Epoch: 00059 loss_train: 1.4662 loss_rec: 1.4662 acc_train: 0.2525 loss_val: 1.4653 acc_val: 0.2522 time: 0.0020s\n",
      "Epoch: 00060 loss_train: 1.4654 loss_rec: 1.4654 acc_train: 0.2536 loss_val: 1.4645 acc_val: 0.2541 time: 0.0015s\n",
      "Epoch: 00061 loss_train: 1.4646 loss_rec: 1.4646 acc_train: 0.2536 loss_val: 1.4636 acc_val: 0.2541 time: 0.0020s\n",
      "Test set results: loss= 1.5012 accuracy= 0.1886\n",
      "Epoch: 00062 loss_train: 1.4637 loss_rec: 1.4637 acc_train: 0.2536 loss_val: 1.4628 acc_val: 0.2541 time: 0.0020s\n",
      "Epoch: 00063 loss_train: 1.4629 loss_rec: 1.4629 acc_train: 0.2536 loss_val: 1.4619 acc_val: 0.2541 time: 0.0015s\n",
      "Epoch: 00064 loss_train: 1.4621 loss_rec: 1.4621 acc_train: 0.2536 loss_val: 1.4610 acc_val: 0.2541 time: 0.0015s\n",
      "Epoch: 00065 loss_train: 1.4612 loss_rec: 1.4612 acc_train: 0.2536 loss_val: 1.4601 acc_val: 0.2541 time: 0.0020s\n",
      "Epoch: 00066 loss_train: 1.4604 loss_rec: 1.4604 acc_train: 0.2536 loss_val: 1.4593 acc_val: 0.2541 time: 0.0015s\n",
      "Epoch: 00067 loss_train: 1.4596 loss_rec: 1.4596 acc_train: 0.2536 loss_val: 1.4584 acc_val: 0.2541 time: 0.0020s\n",
      "Epoch: 00068 loss_train: 1.4587 loss_rec: 1.4587 acc_train: 0.2536 loss_val: 1.4575 acc_val: 0.2541 time: 0.0015s\n",
      "Epoch: 00069 loss_train: 1.4579 loss_rec: 1.4579 acc_train: 0.2536 loss_val: 1.4566 acc_val: 0.2541 time: 0.0015s\n",
      "Epoch: 00070 loss_train: 1.4570 loss_rec: 1.4570 acc_train: 0.2536 loss_val: 1.4557 acc_val: 0.2541 time: 0.0015s\n",
      "Epoch: 00071 loss_train: 1.4562 loss_rec: 1.4562 acc_train: 0.2536 loss_val: 1.4548 acc_val: 0.2541 time: 0.0015s\n",
      "Test set results: loss= 1.4912 accuracy= 0.1886\n",
      "Epoch: 00072 loss_train: 1.4553 loss_rec: 1.4553 acc_train: 0.2536 loss_val: 1.4539 acc_val: 0.2541 time: 0.0020s\n",
      "Epoch: 00073 loss_train: 1.4545 loss_rec: 1.4545 acc_train: 0.2536 loss_val: 1.4529 acc_val: 0.2541 time: 0.0015s\n",
      "Epoch: 00074 loss_train: 1.4536 loss_rec: 1.4536 acc_train: 0.2536 loss_val: 1.4520 acc_val: 0.2541 time: 0.0020s\n",
      "Epoch: 00075 loss_train: 1.4528 loss_rec: 1.4528 acc_train: 0.2536 loss_val: 1.4511 acc_val: 0.2541 time: 0.0015s\n",
      "Epoch: 00076 loss_train: 1.4519 loss_rec: 1.4519 acc_train: 0.2536 loss_val: 1.4502 acc_val: 0.2541 time: 0.0015s\n",
      "Epoch: 00077 loss_train: 1.4511 loss_rec: 1.4511 acc_train: 0.2536 loss_val: 1.4493 acc_val: 0.2541 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00078 loss_train: 1.4502 loss_rec: 1.4502 acc_train: 0.2536 loss_val: 1.4484 acc_val: 0.2541 time: 0.0025s\n",
      "Epoch: 00079 loss_train: 1.4494 loss_rec: 1.4494 acc_train: 0.2536 loss_val: 1.4475 acc_val: 0.2541 time: 0.0020s\n",
      "Epoch: 00080 loss_train: 1.4485 loss_rec: 1.4485 acc_train: 0.2536 loss_val: 1.4465 acc_val: 0.2541 time: 0.0025s\n",
      "Epoch: 00081 loss_train: 1.4477 loss_rec: 1.4477 acc_train: 0.2536 loss_val: 1.4456 acc_val: 0.2541 time: 0.0015s\n",
      "Test set results: loss= 1.4809 accuracy= 0.1886\n",
      "Epoch: 00082 loss_train: 1.4468 loss_rec: 1.4468 acc_train: 0.2536 loss_val: 1.4447 acc_val: 0.2541 time: 0.0025s\n",
      "Epoch: 00083 loss_train: 1.4459 loss_rec: 1.4459 acc_train: 0.2536 loss_val: 1.4438 acc_val: 0.2541 time: 0.0015s\n",
      "Epoch: 00084 loss_train: 1.4451 loss_rec: 1.4451 acc_train: 0.2536 loss_val: 1.4429 acc_val: 0.2541 time: 0.0020s\n",
      "Epoch: 00085 loss_train: 1.4442 loss_rec: 1.4442 acc_train: 0.2536 loss_val: 1.4420 acc_val: 0.2541 time: 0.0020s\n",
      "Epoch: 00086 loss_train: 1.4434 loss_rec: 1.4434 acc_train: 0.2536 loss_val: 1.4410 acc_val: 0.2541 time: 0.0020s\n",
      "Epoch: 00087 loss_train: 1.4425 loss_rec: 1.4425 acc_train: 0.2536 loss_val: 1.4401 acc_val: 0.2541 time: 0.0015s\n",
      "Epoch: 00088 loss_train: 1.4416 loss_rec: 1.4416 acc_train: 0.2536 loss_val: 1.4392 acc_val: 0.2541 time: 0.0025s\n",
      "Epoch: 00089 loss_train: 1.4408 loss_rec: 1.4408 acc_train: 0.2546 loss_val: 1.4383 acc_val: 0.2562 time: 0.0015s\n",
      "Epoch: 00090 loss_train: 1.4399 loss_rec: 1.4399 acc_train: 0.2546 loss_val: 1.4374 acc_val: 0.2562 time: 0.0020s\n",
      "Epoch: 00091 loss_train: 1.4391 loss_rec: 1.4391 acc_train: 0.2546 loss_val: 1.4365 acc_val: 0.2562 time: 0.0015s\n",
      "Test set results: loss= 1.4704 accuracy= 0.1899\n",
      "Epoch: 00092 loss_train: 1.4382 loss_rec: 1.4382 acc_train: 0.2546 loss_val: 1.4356 acc_val: 0.2562 time: 0.0020s\n",
      "Epoch: 00093 loss_train: 1.4373 loss_rec: 1.4373 acc_train: 0.2546 loss_val: 1.4347 acc_val: 0.2562 time: 0.0020s\n",
      "Epoch: 00094 loss_train: 1.4365 loss_rec: 1.4365 acc_train: 0.2546 loss_val: 1.4338 acc_val: 0.2562 time: 0.0015s\n",
      "Epoch: 00095 loss_train: 1.4356 loss_rec: 1.4356 acc_train: 0.2546 loss_val: 1.4328 acc_val: 0.2562 time: 0.0015s\n",
      "Epoch: 00096 loss_train: 1.4348 loss_rec: 1.4348 acc_train: 0.2551 loss_val: 1.4319 acc_val: 0.2566 time: 0.0020s\n",
      "Epoch: 00097 loss_train: 1.4339 loss_rec: 1.4339 acc_train: 0.2551 loss_val: 1.4310 acc_val: 0.2566 time: 0.0020s\n",
      "Epoch: 00098 loss_train: 1.4330 loss_rec: 1.4330 acc_train: 0.2546 loss_val: 1.4301 acc_val: 0.2562 time: 0.0015s\n",
      "Epoch: 00099 loss_train: 1.4322 loss_rec: 1.4322 acc_train: 0.2554 loss_val: 1.4292 acc_val: 0.2569 time: 0.0015s\n",
      "Epoch: 00100 loss_train: 1.4313 loss_rec: 1.4313 acc_train: 0.2554 loss_val: 1.4283 acc_val: 0.2569 time: 0.0020s\n",
      "Epoch: 00101 loss_train: 1.4304 loss_rec: 1.4304 acc_train: 0.2554 loss_val: 1.4274 acc_val: 0.2569 time: 0.0015s\n",
      "Test set results: loss= 1.4595 accuracy= 0.1906\n",
      "Epoch: 00102 loss_train: 1.4296 loss_rec: 1.4296 acc_train: 0.2554 loss_val: 1.4265 acc_val: 0.2569 time: 0.0015s\n",
      "Epoch: 00103 loss_train: 1.4287 loss_rec: 1.4287 acc_train: 0.2546 loss_val: 1.4256 acc_val: 0.2562 time: 0.0015s\n",
      "Epoch: 00104 loss_train: 1.4278 loss_rec: 1.4278 acc_train: 0.2546 loss_val: 1.4247 acc_val: 0.2562 time: 0.0015s\n",
      "Epoch: 00105 loss_train: 1.4270 loss_rec: 1.4270 acc_train: 0.2546 loss_val: 1.4238 acc_val: 0.2562 time: 0.0020s\n",
      "Epoch: 00106 loss_train: 1.4261 loss_rec: 1.4261 acc_train: 0.2546 loss_val: 1.4229 acc_val: 0.2562 time: 0.0015s\n",
      "Epoch: 00107 loss_train: 1.4252 loss_rec: 1.4252 acc_train: 0.2546 loss_val: 1.4220 acc_val: 0.2562 time: 0.0015s\n",
      "Epoch: 00108 loss_train: 1.4244 loss_rec: 1.4244 acc_train: 0.2549 loss_val: 1.4211 acc_val: 0.2562 time: 0.0015s\n",
      "Epoch: 00109 loss_train: 1.4235 loss_rec: 1.4235 acc_train: 0.2549 loss_val: 1.4202 acc_val: 0.2562 time: 0.0020s\n",
      "Epoch: 00110 loss_train: 1.4226 loss_rec: 1.4226 acc_train: 0.2559 loss_val: 1.4193 acc_val: 0.2578 time: 0.0020s\n",
      "Epoch: 00111 loss_train: 1.4217 loss_rec: 1.4217 acc_train: 0.2559 loss_val: 1.4184 acc_val: 0.2578 time: 0.0015s\n",
      "Test set results: loss= 1.4484 accuracy= 0.1932\n",
      "Epoch: 00112 loss_train: 1.4209 loss_rec: 1.4209 acc_train: 0.2564 loss_val: 1.4175 acc_val: 0.2578 time: 0.0020s\n",
      "Epoch: 00113 loss_train: 1.4200 loss_rec: 1.4200 acc_train: 0.2564 loss_val: 1.4166 acc_val: 0.2578 time: 0.0015s\n",
      "Epoch: 00114 loss_train: 1.4191 loss_rec: 1.4191 acc_train: 0.2564 loss_val: 1.4157 acc_val: 0.2578 time: 0.0020s\n",
      "Epoch: 00115 loss_train: 1.4182 loss_rec: 1.4182 acc_train: 0.2578 loss_val: 1.4148 acc_val: 0.2600 time: 0.0020s\n",
      "Epoch: 00116 loss_train: 1.4174 loss_rec: 1.4174 acc_train: 0.2578 loss_val: 1.4139 acc_val: 0.2600 time: 0.0015s\n",
      "Epoch: 00117 loss_train: 1.4165 loss_rec: 1.4165 acc_train: 0.2578 loss_val: 1.4130 acc_val: 0.2600 time: 0.0020s\n",
      "Epoch: 00118 loss_train: 1.4156 loss_rec: 1.4156 acc_train: 0.2578 loss_val: 1.4121 acc_val: 0.2600 time: 0.0015s\n",
      "Epoch: 00119 loss_train: 1.4147 loss_rec: 1.4147 acc_train: 0.2585 loss_val: 1.4112 acc_val: 0.2603 time: 0.0015s\n",
      "Epoch: 00120 loss_train: 1.4139 loss_rec: 1.4139 acc_train: 0.2585 loss_val: 1.4103 acc_val: 0.2603 time: 0.0015s\n",
      "Epoch: 00121 loss_train: 1.4130 loss_rec: 1.4130 acc_train: 0.2596 loss_val: 1.4094 acc_val: 0.2612 time: 0.0020s\n",
      "Test set results: loss= 1.4370 accuracy= 0.1995\n",
      "Epoch: 00122 loss_train: 1.4121 loss_rec: 1.4121 acc_train: 0.2600 loss_val: 1.4085 acc_val: 0.2616 time: 0.0015s\n",
      "Epoch: 00123 loss_train: 1.4112 loss_rec: 1.4112 acc_train: 0.2601 loss_val: 1.4076 acc_val: 0.2616 time: 0.0020s\n",
      "Epoch: 00124 loss_train: 1.4103 loss_rec: 1.4103 acc_train: 0.2609 loss_val: 1.4067 acc_val: 0.2631 time: 0.0020s\n",
      "Epoch: 00125 loss_train: 1.4095 loss_rec: 1.4095 acc_train: 0.2629 loss_val: 1.4058 acc_val: 0.2662 time: 0.0015s\n",
      "Epoch: 00126 loss_train: 1.4086 loss_rec: 1.4086 acc_train: 0.2644 loss_val: 1.4049 acc_val: 0.2675 time: 0.0015s\n",
      "Epoch: 00127 loss_train: 1.4077 loss_rec: 1.4077 acc_train: 0.2654 loss_val: 1.4040 acc_val: 0.2681 time: 0.0015s\n",
      "Epoch: 00128 loss_train: 1.4068 loss_rec: 1.4068 acc_train: 0.2658 loss_val: 1.4031 acc_val: 0.2684 time: 0.0015s\n",
      "Epoch: 00129 loss_train: 1.4060 loss_rec: 1.4060 acc_train: 0.2667 loss_val: 1.4022 acc_val: 0.2700 time: 0.0015s\n",
      "Epoch: 00130 loss_train: 1.4051 loss_rec: 1.4051 acc_train: 0.2667 loss_val: 1.4013 acc_val: 0.2706 time: 0.0020s\n",
      "Epoch: 00131 loss_train: 1.4042 loss_rec: 1.4042 acc_train: 0.2667 loss_val: 1.4004 acc_val: 0.2706 time: 0.0020s\n",
      "Test set results: loss= 1.4254 accuracy= 0.2099\n",
      "Epoch: 00132 loss_train: 1.4033 loss_rec: 1.4033 acc_train: 0.2669 loss_val: 1.3996 acc_val: 0.2709 time: 0.0020s\n",
      "Epoch: 00133 loss_train: 1.4025 loss_rec: 1.4025 acc_train: 0.2691 loss_val: 1.3987 acc_val: 0.2741 time: 0.0015s\n",
      "Epoch: 00134 loss_train: 1.4016 loss_rec: 1.4016 acc_train: 0.2701 loss_val: 1.3978 acc_val: 0.2756 time: 0.0015s\n",
      "Epoch: 00135 loss_train: 1.4007 loss_rec: 1.4007 acc_train: 0.2709 loss_val: 1.3969 acc_val: 0.2769 time: 0.0020s\n",
      "Epoch: 00136 loss_train: 1.3998 loss_rec: 1.3998 acc_train: 0.2709 loss_val: 1.3960 acc_val: 0.2769 time: 0.0015s\n",
      "Epoch: 00137 loss_train: 1.3990 loss_rec: 1.3990 acc_train: 0.2712 loss_val: 1.3952 acc_val: 0.2775 time: 0.0015s\n",
      "Epoch: 00138 loss_train: 1.3981 loss_rec: 1.3981 acc_train: 0.2719 loss_val: 1.3943 acc_val: 0.2784 time: 0.0020s\n",
      "Epoch: 00139 loss_train: 1.3972 loss_rec: 1.3972 acc_train: 0.2719 loss_val: 1.3934 acc_val: 0.2784 time: 0.0015s\n",
      "Epoch: 00140 loss_train: 1.3964 loss_rec: 1.3964 acc_train: 0.2731 loss_val: 1.3925 acc_val: 0.2800 time: 0.0015s\n",
      "Epoch: 00141 loss_train: 1.3955 loss_rec: 1.3955 acc_train: 0.2766 loss_val: 1.3917 acc_val: 0.2819 time: 0.0015s\n",
      "Test set results: loss= 1.4138 accuracy= 0.2268\n",
      "Epoch: 00142 loss_train: 1.3946 loss_rec: 1.3946 acc_train: 0.2776 loss_val: 1.3908 acc_val: 0.2831 time: 0.0015s\n",
      "Epoch: 00143 loss_train: 1.3938 loss_rec: 1.3938 acc_train: 0.2794 loss_val: 1.3899 acc_val: 0.2850 time: 0.0015s\n",
      "Epoch: 00144 loss_train: 1.3929 loss_rec: 1.3929 acc_train: 0.2811 loss_val: 1.3891 acc_val: 0.2866 time: 0.0015s\n",
      "Epoch: 00145 loss_train: 1.3921 loss_rec: 1.3921 acc_train: 0.2838 loss_val: 1.3882 acc_val: 0.2881 time: 0.0015s\n",
      "Epoch: 00146 loss_train: 1.3912 loss_rec: 1.3912 acc_train: 0.2873 loss_val: 1.3874 acc_val: 0.2909 time: 0.0020s\n",
      "Epoch: 00147 loss_train: 1.3904 loss_rec: 1.3904 acc_train: 0.2887 loss_val: 1.3865 acc_val: 0.2938 time: 0.0015s\n",
      "Epoch: 00148 loss_train: 1.3895 loss_rec: 1.3895 acc_train: 0.2909 loss_val: 1.3857 acc_val: 0.2972 time: 0.0015s\n",
      "Epoch: 00149 loss_train: 1.3887 loss_rec: 1.3887 acc_train: 0.2927 loss_val: 1.3848 acc_val: 0.2988 time: 0.0020s\n",
      "Epoch: 00150 loss_train: 1.3878 loss_rec: 1.3878 acc_train: 0.2926 loss_val: 1.3840 acc_val: 0.2988 time: 0.0015s\n",
      "Epoch: 00151 loss_train: 1.3870 loss_rec: 1.3870 acc_train: 0.2953 loss_val: 1.3831 acc_val: 0.3025 time: 0.0020s\n",
      "Test set results: loss= 1.4023 accuracy= 0.2533\n",
      "Epoch: 00152 loss_train: 1.3861 loss_rec: 1.3861 acc_train: 0.2961 loss_val: 1.3823 acc_val: 0.3031 time: 0.0015s\n",
      "Epoch: 00153 loss_train: 1.3853 loss_rec: 1.3853 acc_train: 0.2984 loss_val: 1.3815 acc_val: 0.3063 time: 0.0015s\n",
      "Epoch: 00154 loss_train: 1.3845 loss_rec: 1.3845 acc_train: 0.2982 loss_val: 1.3806 acc_val: 0.3056 time: 0.0015s\n",
      "Epoch: 00155 loss_train: 1.3836 loss_rec: 1.3836 acc_train: 0.3008 loss_val: 1.3798 acc_val: 0.3084 time: 0.0020s\n",
      "Epoch: 00156 loss_train: 1.3828 loss_rec: 1.3828 acc_train: 0.3008 loss_val: 1.3790 acc_val: 0.3078 time: 0.0015s\n",
      "Epoch: 00157 loss_train: 1.3820 loss_rec: 1.3820 acc_train: 0.3021 loss_val: 1.3782 acc_val: 0.3081 time: 0.0015s\n",
      "Epoch: 00158 loss_train: 1.3812 loss_rec: 1.3812 acc_train: 0.3023 loss_val: 1.3773 acc_val: 0.3094 time: 0.0020s\n",
      "Epoch: 00159 loss_train: 1.3803 loss_rec: 1.3803 acc_train: 0.3046 loss_val: 1.3765 acc_val: 0.3119 time: 0.0015s\n",
      "Epoch: 00160 loss_train: 1.3795 loss_rec: 1.3795 acc_train: 0.3064 loss_val: 1.3757 acc_val: 0.3144 time: 0.0015s\n",
      "Epoch: 00161 loss_train: 1.3787 loss_rec: 1.3787 acc_train: 0.3084 loss_val: 1.3749 acc_val: 0.3156 time: 0.0020s\n",
      "Test set results: loss= 1.3911 accuracy= 0.2831\n",
      "Epoch: 00162 loss_train: 1.3779 loss_rec: 1.3779 acc_train: 0.3117 loss_val: 1.3741 acc_val: 0.3191 time: 0.0015s\n",
      "Epoch: 00163 loss_train: 1.3771 loss_rec: 1.3771 acc_train: 0.3129 loss_val: 1.3733 acc_val: 0.3209 time: 0.0015s\n",
      "Epoch: 00164 loss_train: 1.3763 loss_rec: 1.3763 acc_train: 0.3144 loss_val: 1.3725 acc_val: 0.3212 time: 0.0015s\n",
      "Epoch: 00165 loss_train: 1.3755 loss_rec: 1.3755 acc_train: 0.3161 loss_val: 1.3718 acc_val: 0.3247 time: 0.0015s\n",
      "Epoch: 00166 loss_train: 1.3747 loss_rec: 1.3747 acc_train: 0.3191 loss_val: 1.3710 acc_val: 0.3275 time: 0.0015s\n",
      "Epoch: 00167 loss_train: 1.3739 loss_rec: 1.3739 acc_train: 0.3202 loss_val: 1.3702 acc_val: 0.3309 time: 0.0020s\n",
      "Epoch: 00168 loss_train: 1.3731 loss_rec: 1.3731 acc_train: 0.3226 loss_val: 1.3694 acc_val: 0.3337 time: 0.0015s\n",
      "Epoch: 00169 loss_train: 1.3723 loss_rec: 1.3723 acc_train: 0.3237 loss_val: 1.3687 acc_val: 0.3341 time: 0.0015s\n",
      "Epoch: 00170 loss_train: 1.3716 loss_rec: 1.3716 acc_train: 0.3233 loss_val: 1.3679 acc_val: 0.3337 time: 0.0020s\n",
      "Epoch: 00171 loss_train: 1.3708 loss_rec: 1.3708 acc_train: 0.3239 loss_val: 1.3671 acc_val: 0.3359 time: 0.0020s\n",
      "Test set results: loss= 1.3802 accuracy= 0.3167\n",
      "Epoch: 00172 loss_train: 1.3700 loss_rec: 1.3700 acc_train: 0.3262 loss_val: 1.3664 acc_val: 0.3362 time: 0.0015s\n",
      "Epoch: 00173 loss_train: 1.3693 loss_rec: 1.3693 acc_train: 0.3294 loss_val: 1.3656 acc_val: 0.3384 time: 0.0020s\n",
      "Epoch: 00174 loss_train: 1.3685 loss_rec: 1.3685 acc_train: 0.3286 loss_val: 1.3649 acc_val: 0.3381 time: 0.0015s\n",
      "Epoch: 00175 loss_train: 1.3678 loss_rec: 1.3678 acc_train: 0.3315 loss_val: 1.3642 acc_val: 0.3400 time: 0.0015s\n",
      "Epoch: 00176 loss_train: 1.3670 loss_rec: 1.3670 acc_train: 0.3326 loss_val: 1.3634 acc_val: 0.3406 time: 0.0015s\n",
      "Epoch: 00177 loss_train: 1.3663 loss_rec: 1.3663 acc_train: 0.3366 loss_val: 1.3627 acc_val: 0.3431 time: 0.0020s\n",
      "Epoch: 00178 loss_train: 1.3655 loss_rec: 1.3655 acc_train: 0.3372 loss_val: 1.3620 acc_val: 0.3444 time: 0.0015s\n",
      "Epoch: 00179 loss_train: 1.3648 loss_rec: 1.3648 acc_train: 0.3387 loss_val: 1.3613 acc_val: 0.3459 time: 0.0020s\n",
      "Epoch: 00180 loss_train: 1.3641 loss_rec: 1.3641 acc_train: 0.3416 loss_val: 1.3606 acc_val: 0.3491 time: 0.0015s\n",
      "Epoch: 00181 loss_train: 1.3633 loss_rec: 1.3633 acc_train: 0.3431 loss_val: 1.3598 acc_val: 0.3497 time: 0.0015s\n",
      "Test set results: loss= 1.3698 accuracy= 0.3592\n",
      "Epoch: 00182 loss_train: 1.3626 loss_rec: 1.3626 acc_train: 0.3441 loss_val: 1.3591 acc_val: 0.3519 time: 0.0015s\n",
      "Epoch: 00183 loss_train: 1.3619 loss_rec: 1.3619 acc_train: 0.3449 loss_val: 1.3584 acc_val: 0.3519 time: 0.0020s\n",
      "Epoch: 00184 loss_train: 1.3612 loss_rec: 1.3612 acc_train: 0.3429 loss_val: 1.3578 acc_val: 0.3503 time: 0.0015s\n",
      "Epoch: 00185 loss_train: 1.3605 loss_rec: 1.3605 acc_train: 0.3434 loss_val: 1.3571 acc_val: 0.3506 time: 0.0015s\n",
      "Epoch: 00186 loss_train: 1.3598 loss_rec: 1.3598 acc_train: 0.3441 loss_val: 1.3564 acc_val: 0.3503 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00187 loss_train: 1.3591 loss_rec: 1.3591 acc_train: 0.3437 loss_val: 1.3557 acc_val: 0.3497 time: 0.0020s\n",
      "Epoch: 00188 loss_train: 1.3584 loss_rec: 1.3584 acc_train: 0.3429 loss_val: 1.3550 acc_val: 0.3491 time: 0.0025s\n",
      "Epoch: 00189 loss_train: 1.3577 loss_rec: 1.3577 acc_train: 0.3432 loss_val: 1.3544 acc_val: 0.3503 time: 0.0015s\n",
      "Epoch: 00190 loss_train: 1.3570 loss_rec: 1.3570 acc_train: 0.3429 loss_val: 1.3537 acc_val: 0.3509 time: 0.0020s\n",
      "Epoch: 00191 loss_train: 1.3563 loss_rec: 1.3563 acc_train: 0.3445 loss_val: 1.3531 acc_val: 0.3528 time: 0.0015s\n",
      "Test set results: loss= 1.3599 accuracy= 0.3741\n",
      "Epoch: 00192 loss_train: 1.3557 loss_rec: 1.3557 acc_train: 0.3459 loss_val: 1.3524 acc_val: 0.3544 time: 0.0020s\n",
      "Epoch: 00193 loss_train: 1.3550 loss_rec: 1.3550 acc_train: 0.3446 loss_val: 1.3518 acc_val: 0.3550 time: 0.0015s\n",
      "Epoch: 00194 loss_train: 1.3543 loss_rec: 1.3543 acc_train: 0.3438 loss_val: 1.3511 acc_val: 0.3553 time: 0.0015s\n",
      "Epoch: 00195 loss_train: 1.3537 loss_rec: 1.3537 acc_train: 0.3446 loss_val: 1.3505 acc_val: 0.3566 time: 0.0020s\n",
      "Epoch: 00196 loss_train: 1.3530 loss_rec: 1.3530 acc_train: 0.3445 loss_val: 1.3499 acc_val: 0.3556 time: 0.0015s\n",
      "Epoch: 00197 loss_train: 1.3524 loss_rec: 1.3524 acc_train: 0.3451 loss_val: 1.3493 acc_val: 0.3541 time: 0.0020s\n",
      "Epoch: 00198 loss_train: 1.3517 loss_rec: 1.3517 acc_train: 0.3414 loss_val: 1.3486 acc_val: 0.3494 time: 0.0020s\n",
      "Epoch: 00199 loss_train: 1.3511 loss_rec: 1.3511 acc_train: 0.3427 loss_val: 1.3480 acc_val: 0.3513 time: 0.0015s\n",
      "Epoch: 00200 loss_train: 1.3505 loss_rec: 1.3505 acc_train: 0.3409 loss_val: 1.3474 acc_val: 0.3484 time: 0.0015s\n",
      "Epoch: 00201 loss_train: 1.3498 loss_rec: 1.3498 acc_train: 0.3424 loss_val: 1.3468 acc_val: 0.3506 time: 0.0015s\n",
      "Test set results: loss= 1.3505 accuracy= 0.3932\n",
      "Epoch: 00202 loss_train: 1.3492 loss_rec: 1.3492 acc_train: 0.3443 loss_val: 1.3462 acc_val: 0.3519 time: 0.0020s\n",
      "Epoch: 00203 loss_train: 1.3486 loss_rec: 1.3486 acc_train: 0.3432 loss_val: 1.3456 acc_val: 0.3491 time: 0.0015s\n",
      "Epoch: 00204 loss_train: 1.3480 loss_rec: 1.3480 acc_train: 0.3409 loss_val: 1.3450 acc_val: 0.3491 time: 0.0015s\n",
      "Epoch: 00205 loss_train: 1.3474 loss_rec: 1.3474 acc_train: 0.3414 loss_val: 1.3445 acc_val: 0.3484 time: 0.0015s\n",
      "Epoch: 00206 loss_train: 1.3468 loss_rec: 1.3468 acc_train: 0.3378 loss_val: 1.3439 acc_val: 0.3475 time: 0.0020s\n",
      "Epoch: 00207 loss_train: 1.3462 loss_rec: 1.3462 acc_train: 0.3400 loss_val: 1.3433 acc_val: 0.3494 time: 0.0015s\n",
      "Epoch: 00208 loss_train: 1.3456 loss_rec: 1.3456 acc_train: 0.3399 loss_val: 1.3427 acc_val: 0.3503 time: 0.0015s\n",
      "Epoch: 00209 loss_train: 1.3450 loss_rec: 1.3450 acc_train: 0.3404 loss_val: 1.3422 acc_val: 0.3516 time: 0.0020s\n",
      "Epoch: 00210 loss_train: 1.3444 loss_rec: 1.3444 acc_train: 0.3383 loss_val: 1.3416 acc_val: 0.3503 time: 0.0015s\n",
      "Epoch: 00211 loss_train: 1.3438 loss_rec: 1.3438 acc_train: 0.3367 loss_val: 1.3411 acc_val: 0.3491 time: 0.0015s\n",
      "Test set results: loss= 1.3417 accuracy= 0.4241\n",
      "Epoch: 00212 loss_train: 1.3433 loss_rec: 1.3433 acc_train: 0.3388 loss_val: 1.3405 acc_val: 0.3491 time: 0.0020s\n",
      "Epoch: 00213 loss_train: 1.3427 loss_rec: 1.3427 acc_train: 0.3362 loss_val: 1.3400 acc_val: 0.3463 time: 0.0015s\n",
      "Epoch: 00214 loss_train: 1.3421 loss_rec: 1.3421 acc_train: 0.3362 loss_val: 1.3395 acc_val: 0.3466 time: 0.0015s\n",
      "Epoch: 00215 loss_train: 1.3416 loss_rec: 1.3416 acc_train: 0.3351 loss_val: 1.3389 acc_val: 0.3459 time: 0.0025s\n",
      "Epoch: 00216 loss_train: 1.3410 loss_rec: 1.3410 acc_train: 0.3336 loss_val: 1.3384 acc_val: 0.3441 time: 0.0020s\n",
      "Epoch: 00217 loss_train: 1.3405 loss_rec: 1.3405 acc_train: 0.3331 loss_val: 1.3379 acc_val: 0.3428 time: 0.0020s\n",
      "Epoch: 00218 loss_train: 1.3399 loss_rec: 1.3399 acc_train: 0.3327 loss_val: 1.3374 acc_val: 0.3431 time: 0.0015s\n",
      "Epoch: 00219 loss_train: 1.3394 loss_rec: 1.3394 acc_train: 0.3326 loss_val: 1.3368 acc_val: 0.3441 time: 0.0025s\n",
      "Epoch: 00220 loss_train: 1.3389 loss_rec: 1.3389 acc_train: 0.3322 loss_val: 1.3363 acc_val: 0.3431 time: 0.0015s\n",
      "Epoch: 00221 loss_train: 1.3383 loss_rec: 1.3383 acc_train: 0.3311 loss_val: 1.3358 acc_val: 0.3459 time: 0.0025s\n",
      "Test set results: loss= 1.3336 accuracy= 0.4329\n",
      "Epoch: 00222 loss_train: 1.3378 loss_rec: 1.3378 acc_train: 0.3350 loss_val: 1.3353 acc_val: 0.3484 time: 0.0025s\n",
      "Epoch: 00223 loss_train: 1.3373 loss_rec: 1.3373 acc_train: 0.3366 loss_val: 1.3348 acc_val: 0.3484 time: 0.0020s\n",
      "Epoch: 00224 loss_train: 1.3367 loss_rec: 1.3367 acc_train: 0.3381 loss_val: 1.3343 acc_val: 0.3481 time: 0.0025s\n",
      "Epoch: 00225 loss_train: 1.3362 loss_rec: 1.3362 acc_train: 0.3399 loss_val: 1.3339 acc_val: 0.3491 time: 0.0015s\n",
      "Epoch: 00226 loss_train: 1.3357 loss_rec: 1.3357 acc_train: 0.3405 loss_val: 1.3334 acc_val: 0.3513 time: 0.0025s\n",
      "Epoch: 00227 loss_train: 1.3352 loss_rec: 1.3352 acc_train: 0.3419 loss_val: 1.3329 acc_val: 0.3522 time: 0.0015s\n",
      "Epoch: 00228 loss_train: 1.3347 loss_rec: 1.3347 acc_train: 0.3413 loss_val: 1.3324 acc_val: 0.3506 time: 0.0025s\n",
      "Epoch: 00229 loss_train: 1.3342 loss_rec: 1.3342 acc_train: 0.3416 loss_val: 1.3319 acc_val: 0.3509 time: 0.0020s\n",
      "Epoch: 00230 loss_train: 1.3337 loss_rec: 1.3337 acc_train: 0.3417 loss_val: 1.3315 acc_val: 0.3506 time: 0.0025s\n",
      "Epoch: 00231 loss_train: 1.3332 loss_rec: 1.3332 acc_train: 0.3408 loss_val: 1.3310 acc_val: 0.3506 time: 0.0015s\n",
      "Test set results: loss= 1.3260 accuracy= 0.4392\n",
      "Epoch: 00232 loss_train: 1.3327 loss_rec: 1.3327 acc_train: 0.3405 loss_val: 1.3306 acc_val: 0.3506 time: 0.0020s\n",
      "Epoch: 00233 loss_train: 1.3322 loss_rec: 1.3322 acc_train: 0.3419 loss_val: 1.3301 acc_val: 0.3506 time: 0.0025s\n",
      "Epoch: 00234 loss_train: 1.3318 loss_rec: 1.3318 acc_train: 0.3414 loss_val: 1.3296 acc_val: 0.3497 time: 0.0020s\n",
      "Epoch: 00235 loss_train: 1.3313 loss_rec: 1.3313 acc_train: 0.3419 loss_val: 1.3292 acc_val: 0.3500 time: 0.0025s\n",
      "Epoch: 00236 loss_train: 1.3308 loss_rec: 1.3308 acc_train: 0.3413 loss_val: 1.3287 acc_val: 0.3481 time: 0.0015s\n",
      "Epoch: 00237 loss_train: 1.3303 loss_rec: 1.3303 acc_train: 0.3406 loss_val: 1.3283 acc_val: 0.3475 time: 0.0025s\n",
      "Epoch: 00238 loss_train: 1.3299 loss_rec: 1.3299 acc_train: 0.3401 loss_val: 1.3279 acc_val: 0.3475 time: 0.0015s\n",
      "Epoch: 00239 loss_train: 1.3294 loss_rec: 1.3294 acc_train: 0.3397 loss_val: 1.3274 acc_val: 0.3481 time: 0.0020s\n",
      "Epoch: 00240 loss_train: 1.3289 loss_rec: 1.3289 acc_train: 0.3394 loss_val: 1.3270 acc_val: 0.3478 time: 0.0015s\n",
      "Epoch: 00241 loss_train: 1.3285 loss_rec: 1.3285 acc_train: 0.3393 loss_val: 1.3266 acc_val: 0.3478 time: 0.0025s\n",
      "Test set results: loss= 1.3192 accuracy= 0.4407\n",
      "Epoch: 00242 loss_train: 1.3280 loss_rec: 1.3280 acc_train: 0.3386 loss_val: 1.3261 acc_val: 0.3469 time: 0.0020s\n",
      "Epoch: 00243 loss_train: 1.3276 loss_rec: 1.3276 acc_train: 0.3379 loss_val: 1.3257 acc_val: 0.3466 time: 0.0020s\n",
      "Epoch: 00244 loss_train: 1.3271 loss_rec: 1.3271 acc_train: 0.3372 loss_val: 1.3253 acc_val: 0.3466 time: 0.0020s\n",
      "Epoch: 00245 loss_train: 1.3267 loss_rec: 1.3267 acc_train: 0.3371 loss_val: 1.3249 acc_val: 0.3466 time: 0.0020s\n",
      "Epoch: 00246 loss_train: 1.3262 loss_rec: 1.3262 acc_train: 0.3329 loss_val: 1.3245 acc_val: 0.3438 time: 0.0020s\n",
      "Epoch: 00247 loss_train: 1.3258 loss_rec: 1.3258 acc_train: 0.3322 loss_val: 1.3241 acc_val: 0.3422 time: 0.0015s\n",
      "Epoch: 00248 loss_train: 1.3254 loss_rec: 1.3254 acc_train: 0.3206 loss_val: 1.3236 acc_val: 0.3253 time: 0.0015s\n",
      "Epoch: 00249 loss_train: 1.3249 loss_rec: 1.3249 acc_train: 0.3200 loss_val: 1.3232 acc_val: 0.3247 time: 0.0025s\n",
      "Epoch: 00250 loss_train: 1.3245 loss_rec: 1.3245 acc_train: 0.3194 loss_val: 1.3228 acc_val: 0.3244 time: 0.0015s\n",
      "Epoch: 00251 loss_train: 1.3241 loss_rec: 1.3241 acc_train: 0.3189 loss_val: 1.3224 acc_val: 0.3256 time: 0.0025s\n",
      "Test set results: loss= 1.3129 accuracy= 0.4480\n",
      "Epoch: 00252 loss_train: 1.3236 loss_rec: 1.3236 acc_train: 0.3198 loss_val: 1.3220 acc_val: 0.3256 time: 0.0030s\n",
      "Epoch: 00253 loss_train: 1.3232 loss_rec: 1.3232 acc_train: 0.3196 loss_val: 1.3216 acc_val: 0.3253 time: 0.0020s\n",
      "Epoch: 00254 loss_train: 1.3228 loss_rec: 1.3228 acc_train: 0.3186 loss_val: 1.3212 acc_val: 0.3231 time: 0.0025s\n",
      "Epoch: 00255 loss_train: 1.3224 loss_rec: 1.3224 acc_train: 0.3193 loss_val: 1.3209 acc_val: 0.3219 time: 0.0015s\n",
      "Epoch: 00256 loss_train: 1.3219 loss_rec: 1.3219 acc_train: 0.3176 loss_val: 1.3205 acc_val: 0.3197 time: 0.0020s\n",
      "Epoch: 00257 loss_train: 1.3215 loss_rec: 1.3215 acc_train: 0.3181 loss_val: 1.3201 acc_val: 0.3216 time: 0.0020s\n",
      "Epoch: 00258 loss_train: 1.3211 loss_rec: 1.3211 acc_train: 0.3181 loss_val: 1.3197 acc_val: 0.3212 time: 0.0020s\n",
      "Epoch: 00259 loss_train: 1.3207 loss_rec: 1.3207 acc_train: 0.3181 loss_val: 1.3193 acc_val: 0.3212 time: 0.0015s\n",
      "Epoch: 00260 loss_train: 1.3203 loss_rec: 1.3203 acc_train: 0.3196 loss_val: 1.3189 acc_val: 0.3225 time: 0.0025s\n",
      "Epoch: 00261 loss_train: 1.3199 loss_rec: 1.3199 acc_train: 0.3182 loss_val: 1.3186 acc_val: 0.3212 time: 0.0020s\n",
      "Test set results: loss= 1.3071 accuracy= 0.4494\n",
      "Epoch: 00262 loss_train: 1.3195 loss_rec: 1.3195 acc_train: 0.3181 loss_val: 1.3182 acc_val: 0.3212 time: 0.0020s\n",
      "Epoch: 00263 loss_train: 1.3191 loss_rec: 1.3191 acc_train: 0.3180 loss_val: 1.3178 acc_val: 0.3209 time: 0.0020s\n",
      "Epoch: 00264 loss_train: 1.3187 loss_rec: 1.3187 acc_train: 0.3182 loss_val: 1.3174 acc_val: 0.3209 time: 0.0020s\n",
      "Epoch: 00265 loss_train: 1.3183 loss_rec: 1.3183 acc_train: 0.3178 loss_val: 1.3171 acc_val: 0.3191 time: 0.0020s\n",
      "Epoch: 00266 loss_train: 1.3179 loss_rec: 1.3179 acc_train: 0.3178 loss_val: 1.3167 acc_val: 0.3191 time: 0.0015s\n",
      "Epoch: 00267 loss_train: 1.3175 loss_rec: 1.3175 acc_train: 0.3178 loss_val: 1.3163 acc_val: 0.3191 time: 0.0015s\n",
      "Epoch: 00268 loss_train: 1.3171 loss_rec: 1.3171 acc_train: 0.3178 loss_val: 1.3160 acc_val: 0.3191 time: 0.0025s\n",
      "Epoch: 00269 loss_train: 1.3167 loss_rec: 1.3167 acc_train: 0.3181 loss_val: 1.3156 acc_val: 0.3188 time: 0.0015s\n",
      "Epoch: 00270 loss_train: 1.3163 loss_rec: 1.3163 acc_train: 0.3179 loss_val: 1.3152 acc_val: 0.3184 time: 0.0025s\n",
      "Epoch: 00271 loss_train: 1.3159 loss_rec: 1.3159 acc_train: 0.3178 loss_val: 1.3149 acc_val: 0.3184 time: 0.0015s\n",
      "Test set results: loss= 1.3018 accuracy= 0.4471\n",
      "Epoch: 00272 loss_train: 1.3156 loss_rec: 1.3156 acc_train: 0.3174 loss_val: 1.3145 acc_val: 0.3184 time: 0.0015s\n",
      "Epoch: 00273 loss_train: 1.3152 loss_rec: 1.3152 acc_train: 0.3167 loss_val: 1.3142 acc_val: 0.3172 time: 0.0030s\n",
      "Epoch: 00274 loss_train: 1.3148 loss_rec: 1.3148 acc_train: 0.3157 loss_val: 1.3138 acc_val: 0.3150 time: 0.0015s\n",
      "Epoch: 00275 loss_train: 1.3144 loss_rec: 1.3144 acc_train: 0.3167 loss_val: 1.3135 acc_val: 0.3166 time: 0.0030s\n",
      "Epoch: 00276 loss_train: 1.3140 loss_rec: 1.3140 acc_train: 0.3164 loss_val: 1.3131 acc_val: 0.3159 time: 0.0025s\n",
      "Epoch: 00277 loss_train: 1.3137 loss_rec: 1.3137 acc_train: 0.3164 loss_val: 1.3128 acc_val: 0.3159 time: 0.0015s\n",
      "Epoch: 00278 loss_train: 1.3133 loss_rec: 1.3133 acc_train: 0.3153 loss_val: 1.3124 acc_val: 0.3150 time: 0.0025s\n",
      "Epoch: 00279 loss_train: 1.3129 loss_rec: 1.3129 acc_train: 0.3176 loss_val: 1.3121 acc_val: 0.3163 time: 0.0020s\n",
      "Epoch: 00280 loss_train: 1.3125 loss_rec: 1.3125 acc_train: 0.3176 loss_val: 1.3117 acc_val: 0.3159 time: 0.0015s\n",
      "Epoch: 00281 loss_train: 1.3122 loss_rec: 1.3122 acc_train: 0.3176 loss_val: 1.3114 acc_val: 0.3159 time: 0.0015s\n",
      "Test set results: loss= 1.2970 accuracy= 0.4426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00282 loss_train: 1.3118 loss_rec: 1.3118 acc_train: 0.3165 loss_val: 1.3110 acc_val: 0.3138 time: 0.0035s\n",
      "Epoch: 00283 loss_train: 1.3114 loss_rec: 1.3114 acc_train: 0.3158 loss_val: 1.3107 acc_val: 0.3134 time: 0.0015s\n",
      "Epoch: 00284 loss_train: 1.3111 loss_rec: 1.3111 acc_train: 0.3159 loss_val: 1.3103 acc_val: 0.3147 time: 0.0030s\n",
      "Epoch: 00285 loss_train: 1.3107 loss_rec: 1.3107 acc_train: 0.3154 loss_val: 1.3100 acc_val: 0.3144 time: 0.0015s\n",
      "Epoch: 00286 loss_train: 1.3103 loss_rec: 1.3103 acc_train: 0.3154 loss_val: 1.3097 acc_val: 0.3144 time: 0.0025s\n",
      "Epoch: 00287 loss_train: 1.3100 loss_rec: 1.3100 acc_train: 0.3159 loss_val: 1.3093 acc_val: 0.3156 time: 0.0015s\n",
      "Epoch: 00288 loss_train: 1.3096 loss_rec: 1.3096 acc_train: 0.3157 loss_val: 1.3090 acc_val: 0.3153 time: 0.0020s\n",
      "Epoch: 00289 loss_train: 1.3092 loss_rec: 1.3092 acc_train: 0.3147 loss_val: 1.3086 acc_val: 0.3147 time: 0.0015s\n",
      "Epoch: 00290 loss_train: 1.3089 loss_rec: 1.3089 acc_train: 0.3147 loss_val: 1.3083 acc_val: 0.3147 time: 0.0030s\n",
      "Epoch: 00291 loss_train: 1.3085 loss_rec: 1.3085 acc_train: 0.3147 loss_val: 1.3080 acc_val: 0.3147 time: 0.0020s\n",
      "Test set results: loss= 1.2925 accuracy= 0.4366\n",
      "Epoch: 00292 loss_train: 1.3082 loss_rec: 1.3082 acc_train: 0.3143 loss_val: 1.3076 acc_val: 0.3144 time: 0.0020s\n",
      "Epoch: 00293 loss_train: 1.3078 loss_rec: 1.3078 acc_train: 0.3143 loss_val: 1.3073 acc_val: 0.3144 time: 0.0025s\n",
      "Epoch: 00294 loss_train: 1.3074 loss_rec: 1.3074 acc_train: 0.3139 loss_val: 1.3070 acc_val: 0.3144 time: 0.0015s\n",
      "Epoch: 00295 loss_train: 1.3071 loss_rec: 1.3071 acc_train: 0.3151 loss_val: 1.3067 acc_val: 0.3153 time: 0.0025s\n",
      "Epoch: 00296 loss_train: 1.3067 loss_rec: 1.3067 acc_train: 0.3152 loss_val: 1.3063 acc_val: 0.3166 time: 0.0020s\n",
      "Epoch: 00297 loss_train: 1.3064 loss_rec: 1.3064 acc_train: 0.3146 loss_val: 1.3060 acc_val: 0.3163 time: 0.0030s\n",
      "Epoch: 00298 loss_train: 1.3060 loss_rec: 1.3060 acc_train: 0.3140 loss_val: 1.3057 acc_val: 0.3159 time: 0.0020s\n",
      "Epoch: 00299 loss_train: 1.3057 loss_rec: 1.3057 acc_train: 0.3136 loss_val: 1.3053 acc_val: 0.3153 time: 0.0015s\n",
      "Epoch: 00300 loss_train: 1.3053 loss_rec: 1.3053 acc_train: 0.3136 loss_val: 1.3050 acc_val: 0.3153 time: 0.0015s\n",
      "Epoch: 00301 loss_train: 1.3050 loss_rec: 1.3050 acc_train: 0.3136 loss_val: 1.3047 acc_val: 0.3153 time: 0.0030s\n",
      "Test set results: loss= 1.2884 accuracy= 0.4367\n",
      "Epoch: 00302 loss_train: 1.3046 loss_rec: 1.3046 acc_train: 0.3142 loss_val: 1.3044 acc_val: 0.3153 time: 0.0015s\n",
      "Epoch: 00303 loss_train: 1.3043 loss_rec: 1.3043 acc_train: 0.3142 loss_val: 1.3040 acc_val: 0.3153 time: 0.0020s\n",
      "Epoch: 00304 loss_train: 1.3039 loss_rec: 1.3039 acc_train: 0.3142 loss_val: 1.3037 acc_val: 0.3153 time: 0.0020s\n",
      "Epoch: 00305 loss_train: 1.3036 loss_rec: 1.3036 acc_train: 0.3156 loss_val: 1.3034 acc_val: 0.3188 time: 0.0025s\n",
      "Epoch: 00306 loss_train: 1.3033 loss_rec: 1.3033 acc_train: 0.3171 loss_val: 1.3031 acc_val: 0.3200 time: 0.0025s\n",
      "Epoch: 00307 loss_train: 1.3029 loss_rec: 1.3029 acc_train: 0.3169 loss_val: 1.3027 acc_val: 0.3194 time: 0.0020s\n",
      "Epoch: 00308 loss_train: 1.3026 loss_rec: 1.3026 acc_train: 0.3180 loss_val: 1.3024 acc_val: 0.3203 time: 0.0015s\n",
      "Epoch: 00309 loss_train: 1.3022 loss_rec: 1.3022 acc_train: 0.3178 loss_val: 1.3021 acc_val: 0.3197 time: 0.0015s\n",
      "Epoch: 00310 loss_train: 1.3019 loss_rec: 1.3019 acc_train: 0.3168 loss_val: 1.3018 acc_val: 0.3188 time: 0.0015s\n",
      "Epoch: 00311 loss_train: 1.3015 loss_rec: 1.3015 acc_train: 0.3168 loss_val: 1.3015 acc_val: 0.3188 time: 0.0015s\n",
      "Test set results: loss= 1.2846 accuracy= 0.4431\n",
      "Epoch: 00312 loss_train: 1.3012 loss_rec: 1.3012 acc_train: 0.3193 loss_val: 1.3011 acc_val: 0.3216 time: 0.0020s\n",
      "Epoch: 00313 loss_train: 1.3009 loss_rec: 1.3009 acc_train: 0.3204 loss_val: 1.3008 acc_val: 0.3225 time: 0.0015s\n",
      "Epoch: 00314 loss_train: 1.3005 loss_rec: 1.3005 acc_train: 0.3194 loss_val: 1.3005 acc_val: 0.3212 time: 0.0015s\n",
      "Epoch: 00315 loss_train: 1.3002 loss_rec: 1.3002 acc_train: 0.3196 loss_val: 1.3002 acc_val: 0.3216 time: 0.0015s\n",
      "Epoch: 00316 loss_train: 1.2998 loss_rec: 1.2998 acc_train: 0.3198 loss_val: 1.2999 acc_val: 0.3216 time: 0.0020s\n",
      "Epoch: 00317 loss_train: 1.2995 loss_rec: 1.2995 acc_train: 0.3199 loss_val: 1.2996 acc_val: 0.3216 time: 0.0015s\n",
      "Epoch: 00318 loss_train: 1.2992 loss_rec: 1.2992 acc_train: 0.3201 loss_val: 1.2992 acc_val: 0.3219 time: 0.0015s\n",
      "Epoch: 00319 loss_train: 1.2988 loss_rec: 1.2988 acc_train: 0.3214 loss_val: 1.2989 acc_val: 0.3225 time: 0.0020s\n",
      "Epoch: 00320 loss_train: 1.2985 loss_rec: 1.2985 acc_train: 0.3215 loss_val: 1.2986 acc_val: 0.3225 time: 0.0015s\n",
      "Epoch: 00321 loss_train: 1.2982 loss_rec: 1.2982 acc_train: 0.3211 loss_val: 1.2983 acc_val: 0.3222 time: 0.0015s\n",
      "Test set results: loss= 1.2810 accuracy= 0.4425\n",
      "Epoch: 00322 loss_train: 1.2978 loss_rec: 1.2978 acc_train: 0.3258 loss_val: 1.2980 acc_val: 0.3259 time: 0.0015s\n",
      "Epoch: 00323 loss_train: 1.2975 loss_rec: 1.2975 acc_train: 0.3247 loss_val: 1.2977 acc_val: 0.3247 time: 0.0015s\n",
      "Epoch: 00324 loss_train: 1.2971 loss_rec: 1.2971 acc_train: 0.3247 loss_val: 1.2974 acc_val: 0.3250 time: 0.0015s\n",
      "Epoch: 00325 loss_train: 1.2968 loss_rec: 1.2968 acc_train: 0.3258 loss_val: 1.2970 acc_val: 0.3250 time: 0.0015s\n",
      "Epoch: 00326 loss_train: 1.2965 loss_rec: 1.2965 acc_train: 0.3257 loss_val: 1.2967 acc_val: 0.3244 time: 0.0015s\n",
      "Epoch: 00327 loss_train: 1.2961 loss_rec: 1.2961 acc_train: 0.3257 loss_val: 1.2964 acc_val: 0.3244 time: 0.0015s\n",
      "Epoch: 00328 loss_train: 1.2958 loss_rec: 1.2958 acc_train: 0.3262 loss_val: 1.2961 acc_val: 0.3253 time: 0.0015s\n",
      "Epoch: 00329 loss_train: 1.2955 loss_rec: 1.2955 acc_train: 0.3264 loss_val: 1.2958 acc_val: 0.3256 time: 0.0015s\n",
      "Epoch: 00330 loss_train: 1.2952 loss_rec: 1.2952 acc_train: 0.3271 loss_val: 1.2955 acc_val: 0.3266 time: 0.0015s\n",
      "Epoch: 00331 loss_train: 1.2948 loss_rec: 1.2948 acc_train: 0.3276 loss_val: 1.2952 acc_val: 0.3266 time: 0.0015s\n",
      "Test set results: loss= 1.2777 accuracy= 0.4402\n",
      "Epoch: 00332 loss_train: 1.2945 loss_rec: 1.2945 acc_train: 0.3276 loss_val: 1.2949 acc_val: 0.3266 time: 0.0020s\n",
      "Epoch: 00333 loss_train: 1.2942 loss_rec: 1.2942 acc_train: 0.3288 loss_val: 1.2946 acc_val: 0.3275 time: 0.0015s\n",
      "Epoch: 00334 loss_train: 1.2938 loss_rec: 1.2938 acc_train: 0.3288 loss_val: 1.2942 acc_val: 0.3275 time: 0.0015s\n",
      "Epoch: 00335 loss_train: 1.2935 loss_rec: 1.2935 acc_train: 0.3296 loss_val: 1.2939 acc_val: 0.3284 time: 0.0015s\n",
      "Epoch: 00336 loss_train: 1.2932 loss_rec: 1.2932 acc_train: 0.3297 loss_val: 1.2936 acc_val: 0.3284 time: 0.0015s\n",
      "Epoch: 00337 loss_train: 1.2929 loss_rec: 1.2929 acc_train: 0.3295 loss_val: 1.2933 acc_val: 0.3284 time: 0.0020s\n",
      "Epoch: 00338 loss_train: 1.2925 loss_rec: 1.2925 acc_train: 0.3298 loss_val: 1.2930 acc_val: 0.3287 time: 0.0015s\n",
      "Epoch: 00339 loss_train: 1.2922 loss_rec: 1.2922 acc_train: 0.3291 loss_val: 1.2927 acc_val: 0.3278 time: 0.0015s\n",
      "Epoch: 00340 loss_train: 1.2919 loss_rec: 1.2919 acc_train: 0.3294 loss_val: 1.2924 acc_val: 0.3278 time: 0.0020s\n",
      "Epoch: 00341 loss_train: 1.2916 loss_rec: 1.2916 acc_train: 0.3294 loss_val: 1.2921 acc_val: 0.3278 time: 0.0020s\n",
      "Test set results: loss= 1.2746 accuracy= 0.4406\n",
      "Epoch: 00342 loss_train: 1.2913 loss_rec: 1.2913 acc_train: 0.3293 loss_val: 1.2918 acc_val: 0.3287 time: 0.0015s\n",
      "Epoch: 00343 loss_train: 1.2909 loss_rec: 1.2909 acc_train: 0.3282 loss_val: 1.2915 acc_val: 0.3278 time: 0.0020s\n",
      "Epoch: 00344 loss_train: 1.2906 loss_rec: 1.2906 acc_train: 0.3284 loss_val: 1.2912 acc_val: 0.3278 time: 0.0015s\n",
      "Epoch: 00345 loss_train: 1.2903 loss_rec: 1.2903 acc_train: 0.3279 loss_val: 1.2909 acc_val: 0.3275 time: 0.0015s\n",
      "Epoch: 00346 loss_train: 1.2900 loss_rec: 1.2900 acc_train: 0.3288 loss_val: 1.2906 acc_val: 0.3281 time: 0.0015s\n",
      "Epoch: 00347 loss_train: 1.2897 loss_rec: 1.2897 acc_train: 0.3303 loss_val: 1.2903 acc_val: 0.3287 time: 0.0015s\n",
      "Epoch: 00348 loss_train: 1.2894 loss_rec: 1.2894 acc_train: 0.3286 loss_val: 1.2900 acc_val: 0.3287 time: 0.0015s\n",
      "Epoch: 00349 loss_train: 1.2891 loss_rec: 1.2891 acc_train: 0.3282 loss_val: 1.2897 acc_val: 0.3287 time: 0.0015s\n",
      "Epoch: 00350 loss_train: 1.2887 loss_rec: 1.2887 acc_train: 0.3282 loss_val: 1.2894 acc_val: 0.3287 time: 0.0020s\n",
      "Epoch: 00351 loss_train: 1.2884 loss_rec: 1.2884 acc_train: 0.3287 loss_val: 1.2891 acc_val: 0.3287 time: 0.0015s\n",
      "Test set results: loss= 1.2716 accuracy= 0.4359\n",
      "Epoch: 00352 loss_train: 1.2881 loss_rec: 1.2881 acc_train: 0.3286 loss_val: 1.2888 acc_val: 0.3287 time: 0.0015s\n",
      "Epoch: 00353 loss_train: 1.2878 loss_rec: 1.2878 acc_train: 0.3297 loss_val: 1.2885 acc_val: 0.3303 time: 0.0015s\n",
      "Epoch: 00354 loss_train: 1.2875 loss_rec: 1.2875 acc_train: 0.3301 loss_val: 1.2882 acc_val: 0.3306 time: 0.0015s\n",
      "Epoch: 00355 loss_train: 1.2872 loss_rec: 1.2872 acc_train: 0.3297 loss_val: 1.2879 acc_val: 0.3294 time: 0.0015s\n",
      "Epoch: 00356 loss_train: 1.2869 loss_rec: 1.2869 acc_train: 0.3294 loss_val: 1.2876 acc_val: 0.3278 time: 0.0020s\n",
      "Epoch: 00357 loss_train: 1.2866 loss_rec: 1.2866 acc_train: 0.3386 loss_val: 1.2874 acc_val: 0.3362 time: 0.0015s\n",
      "Epoch: 00358 loss_train: 1.2862 loss_rec: 1.2862 acc_train: 0.3389 loss_val: 1.2871 acc_val: 0.3362 time: 0.0015s\n",
      "Epoch: 00359 loss_train: 1.2859 loss_rec: 1.2859 acc_train: 0.3389 loss_val: 1.2868 acc_val: 0.3362 time: 0.0015s\n",
      "Epoch: 00360 loss_train: 1.2856 loss_rec: 1.2856 acc_train: 0.3385 loss_val: 1.2865 acc_val: 0.3350 time: 0.0020s\n",
      "Epoch: 00361 loss_train: 1.2853 loss_rec: 1.2853 acc_train: 0.3376 loss_val: 1.2862 acc_val: 0.3337 time: 0.0015s\n",
      "Test set results: loss= 1.2688 accuracy= 0.4362\n",
      "Epoch: 00362 loss_train: 1.2850 loss_rec: 1.2850 acc_train: 0.3377 loss_val: 1.2859 acc_val: 0.3337 time: 0.0015s\n",
      "Epoch: 00363 loss_train: 1.2847 loss_rec: 1.2847 acc_train: 0.3373 loss_val: 1.2856 acc_val: 0.3337 time: 0.0020s\n",
      "Epoch: 00364 loss_train: 1.2844 loss_rec: 1.2844 acc_train: 0.3425 loss_val: 1.2853 acc_val: 0.3369 time: 0.0015s\n",
      "Epoch: 00365 loss_train: 1.2841 loss_rec: 1.2841 acc_train: 0.3414 loss_val: 1.2850 acc_val: 0.3334 time: 0.0015s\n",
      "Epoch: 00366 loss_train: 1.2838 loss_rec: 1.2838 acc_train: 0.3411 loss_val: 1.2847 acc_val: 0.3325 time: 0.0015s\n",
      "Epoch: 00367 loss_train: 1.2835 loss_rec: 1.2835 acc_train: 0.3409 loss_val: 1.2844 acc_val: 0.3325 time: 0.0015s\n",
      "Epoch: 00368 loss_train: 1.2832 loss_rec: 1.2832 acc_train: 0.3398 loss_val: 1.2841 acc_val: 0.3322 time: 0.0020s\n",
      "Epoch: 00369 loss_train: 1.2828 loss_rec: 1.2828 acc_train: 0.3401 loss_val: 1.2838 acc_val: 0.3325 time: 0.0020s\n",
      "Epoch: 00370 loss_train: 1.2825 loss_rec: 1.2825 acc_train: 0.3386 loss_val: 1.2835 acc_val: 0.3319 time: 0.0015s\n",
      "Epoch: 00371 loss_train: 1.2822 loss_rec: 1.2822 acc_train: 0.3414 loss_val: 1.2832 acc_val: 0.3366 time: 0.0020s\n",
      "Test set results: loss= 1.2662 accuracy= 0.4334\n",
      "Epoch: 00372 loss_train: 1.2819 loss_rec: 1.2819 acc_train: 0.3391 loss_val: 1.2829 acc_val: 0.3337 time: 0.0015s\n",
      "Epoch: 00373 loss_train: 1.2816 loss_rec: 1.2816 acc_train: 0.3384 loss_val: 1.2827 acc_val: 0.3328 time: 0.0020s\n",
      "Epoch: 00374 loss_train: 1.2813 loss_rec: 1.2813 acc_train: 0.3383 loss_val: 1.2824 acc_val: 0.3328 time: 0.0020s\n",
      "Epoch: 00375 loss_train: 1.2810 loss_rec: 1.2810 acc_train: 0.3418 loss_val: 1.2821 acc_val: 0.3353 time: 0.0015s\n",
      "Epoch: 00376 loss_train: 1.2807 loss_rec: 1.2807 acc_train: 0.3412 loss_val: 1.2818 acc_val: 0.3337 time: 0.0015s\n",
      "Epoch: 00377 loss_train: 1.2804 loss_rec: 1.2804 acc_train: 0.3441 loss_val: 1.2815 acc_val: 0.3378 time: 0.0020s\n",
      "Epoch: 00378 loss_train: 1.2801 loss_rec: 1.2801 acc_train: 0.3469 loss_val: 1.2812 acc_val: 0.3400 time: 0.0020s\n",
      "Epoch: 00379 loss_train: 1.2798 loss_rec: 1.2798 acc_train: 0.3459 loss_val: 1.2809 acc_val: 0.3387 time: 0.0020s\n",
      "Epoch: 00380 loss_train: 1.2795 loss_rec: 1.2795 acc_train: 0.3473 loss_val: 1.2806 acc_val: 0.3391 time: 0.0015s\n",
      "Epoch: 00381 loss_train: 1.2792 loss_rec: 1.2792 acc_train: 0.3467 loss_val: 1.2803 acc_val: 0.3387 time: 0.0015s\n",
      "Test set results: loss= 1.2638 accuracy= 0.4316\n",
      "Epoch: 00382 loss_train: 1.2789 loss_rec: 1.2789 acc_train: 0.3458 loss_val: 1.2800 acc_val: 0.3372 time: 0.0015s\n",
      "Epoch: 00383 loss_train: 1.2786 loss_rec: 1.2786 acc_train: 0.3455 loss_val: 1.2797 acc_val: 0.3372 time: 0.0015s\n",
      "Epoch: 00384 loss_train: 1.2783 loss_rec: 1.2783 acc_train: 0.3448 loss_val: 1.2795 acc_val: 0.3356 time: 0.0020s\n",
      "Epoch: 00385 loss_train: 1.2780 loss_rec: 1.2780 acc_train: 0.3446 loss_val: 1.2792 acc_val: 0.3356 time: 0.0015s\n",
      "Epoch: 00386 loss_train: 1.2777 loss_rec: 1.2777 acc_train: 0.3455 loss_val: 1.2789 acc_val: 0.3425 time: 0.0015s\n",
      "Epoch: 00387 loss_train: 1.2774 loss_rec: 1.2774 acc_train: 0.3438 loss_val: 1.2786 acc_val: 0.3422 time: 0.0020s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00388 loss_train: 1.2771 loss_rec: 1.2771 acc_train: 0.3451 loss_val: 1.2783 acc_val: 0.3419 time: 0.0025s\n",
      "Epoch: 00389 loss_train: 1.2768 loss_rec: 1.2768 acc_train: 0.3449 loss_val: 1.2780 acc_val: 0.3412 time: 0.0020s\n",
      "Epoch: 00390 loss_train: 1.2765 loss_rec: 1.2765 acc_train: 0.3484 loss_val: 1.2777 acc_val: 0.3441 time: 0.0015s\n",
      "Epoch: 00391 loss_train: 1.2762 loss_rec: 1.2762 acc_train: 0.3515 loss_val: 1.2775 acc_val: 0.3459 time: 0.0015s\n",
      "Test set results: loss= 1.2614 accuracy= 0.4174\n",
      "Epoch: 00392 loss_train: 1.2759 loss_rec: 1.2759 acc_train: 0.3504 loss_val: 1.2772 acc_val: 0.3453 time: 0.0015s\n",
      "Epoch: 00393 loss_train: 1.2756 loss_rec: 1.2756 acc_train: 0.3504 loss_val: 1.2769 acc_val: 0.3453 time: 0.0020s\n",
      "Epoch: 00394 loss_train: 1.2753 loss_rec: 1.2753 acc_train: 0.3501 loss_val: 1.2766 acc_val: 0.3456 time: 0.0015s\n",
      "Epoch: 00395 loss_train: 1.2750 loss_rec: 1.2750 acc_train: 0.3537 loss_val: 1.2763 acc_val: 0.3497 time: 0.0015s\n",
      "Epoch: 00396 loss_train: 1.2747 loss_rec: 1.2747 acc_train: 0.3534 loss_val: 1.2760 acc_val: 0.3488 time: 0.0015s\n",
      "Epoch: 00397 loss_train: 1.2744 loss_rec: 1.2744 acc_train: 0.3526 loss_val: 1.2757 acc_val: 0.3481 time: 0.0020s\n",
      "Epoch: 00398 loss_train: 1.2741 loss_rec: 1.2741 acc_train: 0.3526 loss_val: 1.2755 acc_val: 0.3481 time: 0.0015s\n",
      "Epoch: 00399 loss_train: 1.2738 loss_rec: 1.2738 acc_train: 0.3557 loss_val: 1.2752 acc_val: 0.3494 time: 0.0015s\n",
      "Epoch: 00400 loss_train: 1.2735 loss_rec: 1.2735 acc_train: 0.3567 loss_val: 1.2749 acc_val: 0.3503 time: 0.0015s\n",
      "Epoch: 00401 loss_train: 1.2732 loss_rec: 1.2732 acc_train: 0.3569 loss_val: 1.2746 acc_val: 0.3503 time: 0.0020s\n",
      "Test set results: loss= 1.2592 accuracy= 0.3994\n",
      "Epoch: 00402 loss_train: 1.2730 loss_rec: 1.2730 acc_train: 0.3575 loss_val: 1.2743 acc_val: 0.3516 time: 0.0020s\n",
      "Epoch: 00403 loss_train: 1.2727 loss_rec: 1.2727 acc_train: 0.3578 loss_val: 1.2740 acc_val: 0.3516 time: 0.0015s\n",
      "Epoch: 00404 loss_train: 1.2724 loss_rec: 1.2724 acc_train: 0.3558 loss_val: 1.2738 acc_val: 0.3497 time: 0.0015s\n",
      "Epoch: 00405 loss_train: 1.2721 loss_rec: 1.2721 acc_train: 0.3555 loss_val: 1.2735 acc_val: 0.3494 time: 0.0015s\n",
      "Epoch: 00406 loss_train: 1.2718 loss_rec: 1.2718 acc_train: 0.3592 loss_val: 1.2732 acc_val: 0.3525 time: 0.0015s\n",
      "Epoch: 00407 loss_train: 1.2715 loss_rec: 1.2715 acc_train: 0.3611 loss_val: 1.2729 acc_val: 0.3550 time: 0.0015s\n",
      "Epoch: 00408 loss_train: 1.2712 loss_rec: 1.2712 acc_train: 0.3609 loss_val: 1.2726 acc_val: 0.3541 time: 0.0015s\n",
      "Epoch: 00409 loss_train: 1.2709 loss_rec: 1.2709 acc_train: 0.3609 loss_val: 1.2724 acc_val: 0.3538 time: 0.0020s\n",
      "Epoch: 00410 loss_train: 1.2706 loss_rec: 1.2706 acc_train: 0.3597 loss_val: 1.2721 acc_val: 0.3522 time: 0.0015s\n",
      "Epoch: 00411 loss_train: 1.2703 loss_rec: 1.2703 acc_train: 0.3592 loss_val: 1.2718 acc_val: 0.3506 time: 0.0015s\n",
      "Test set results: loss= 1.2571 accuracy= 0.3961\n",
      "Epoch: 00412 loss_train: 1.2700 loss_rec: 1.2700 acc_train: 0.3593 loss_val: 1.2715 acc_val: 0.3513 time: 0.0015s\n",
      "Epoch: 00413 loss_train: 1.2698 loss_rec: 1.2698 acc_train: 0.3591 loss_val: 1.2712 acc_val: 0.3509 time: 0.0015s\n",
      "Epoch: 00414 loss_train: 1.2695 loss_rec: 1.2695 acc_train: 0.3587 loss_val: 1.2709 acc_val: 0.3494 time: 0.0020s\n",
      "Epoch: 00415 loss_train: 1.2692 loss_rec: 1.2692 acc_train: 0.3596 loss_val: 1.2707 acc_val: 0.3513 time: 0.0020s\n",
      "Epoch: 00416 loss_train: 1.2689 loss_rec: 1.2689 acc_train: 0.3630 loss_val: 1.2704 acc_val: 0.3559 time: 0.0015s\n",
      "Epoch: 00417 loss_train: 1.2686 loss_rec: 1.2686 acc_train: 0.3626 loss_val: 1.2701 acc_val: 0.3559 time: 0.0015s\n",
      "Epoch: 00418 loss_train: 1.2683 loss_rec: 1.2683 acc_train: 0.3618 loss_val: 1.2698 acc_val: 0.3553 time: 0.0020s\n",
      "Epoch: 00419 loss_train: 1.2680 loss_rec: 1.2680 acc_train: 0.3603 loss_val: 1.2695 acc_val: 0.3550 time: 0.0015s\n",
      "Epoch: 00420 loss_train: 1.2677 loss_rec: 1.2677 acc_train: 0.3595 loss_val: 1.2693 acc_val: 0.3553 time: 0.0015s\n",
      "Epoch: 00421 loss_train: 1.2675 loss_rec: 1.2675 acc_train: 0.3593 loss_val: 1.2690 acc_val: 0.3553 time: 0.0020s\n",
      "Test set results: loss= 1.2551 accuracy= 0.3900\n",
      "Epoch: 00422 loss_train: 1.2672 loss_rec: 1.2672 acc_train: 0.3587 loss_val: 1.2687 acc_val: 0.3550 time: 0.0020s\n",
      "Epoch: 00423 loss_train: 1.2669 loss_rec: 1.2669 acc_train: 0.3575 loss_val: 1.2684 acc_val: 0.3538 time: 0.0020s\n",
      "Epoch: 00424 loss_train: 1.2666 loss_rec: 1.2666 acc_train: 0.3577 loss_val: 1.2682 acc_val: 0.3538 time: 0.0020s\n",
      "Epoch: 00425 loss_train: 1.2663 loss_rec: 1.2663 acc_train: 0.3573 loss_val: 1.2679 acc_val: 0.3538 time: 0.0015s\n",
      "Epoch: 00426 loss_train: 1.2660 loss_rec: 1.2660 acc_train: 0.3584 loss_val: 1.2676 acc_val: 0.3563 time: 0.0015s\n",
      "Epoch: 00427 loss_train: 1.2657 loss_rec: 1.2657 acc_train: 0.3576 loss_val: 1.2673 acc_val: 0.3556 time: 0.0015s\n",
      "Epoch: 00428 loss_train: 1.2655 loss_rec: 1.2655 acc_train: 0.3576 loss_val: 1.2670 acc_val: 0.3556 time: 0.0015s\n",
      "Epoch: 00429 loss_train: 1.2652 loss_rec: 1.2652 acc_train: 0.3582 loss_val: 1.2668 acc_val: 0.3559 time: 0.0015s\n",
      "Epoch: 00430 loss_train: 1.2649 loss_rec: 1.2649 acc_train: 0.3581 loss_val: 1.2665 acc_val: 0.3559 time: 0.0015s\n",
      "Epoch: 00431 loss_train: 1.2646 loss_rec: 1.2646 acc_train: 0.3624 loss_val: 1.2662 acc_val: 0.3594 time: 0.0015s\n",
      "Test set results: loss= 1.2532 accuracy= 0.3869\n",
      "Epoch: 00432 loss_train: 1.2643 loss_rec: 1.2643 acc_train: 0.3605 loss_val: 1.2659 acc_val: 0.3591 time: 0.0020s\n",
      "Epoch: 00433 loss_train: 1.2640 loss_rec: 1.2640 acc_train: 0.3631 loss_val: 1.2657 acc_val: 0.3628 time: 0.0015s\n",
      "Epoch: 00434 loss_train: 1.2637 loss_rec: 1.2637 acc_train: 0.3666 loss_val: 1.2654 acc_val: 0.3650 time: 0.0015s\n",
      "Epoch: 00435 loss_train: 1.2635 loss_rec: 1.2635 acc_train: 0.3665 loss_val: 1.2651 acc_val: 0.3650 time: 0.0020s\n",
      "Epoch: 00436 loss_train: 1.2632 loss_rec: 1.2632 acc_train: 0.3659 loss_val: 1.2649 acc_val: 0.3641 time: 0.0015s\n",
      "Epoch: 00437 loss_train: 1.2629 loss_rec: 1.2629 acc_train: 0.3664 loss_val: 1.2646 acc_val: 0.3641 time: 0.0015s\n",
      "Epoch: 00438 loss_train: 1.2626 loss_rec: 1.2626 acc_train: 0.3656 loss_val: 1.2643 acc_val: 0.3631 time: 0.0015s\n",
      "Epoch: 00439 loss_train: 1.2623 loss_rec: 1.2623 acc_train: 0.3639 loss_val: 1.2640 acc_val: 0.3625 time: 0.0015s\n",
      "Epoch: 00440 loss_train: 1.2621 loss_rec: 1.2621 acc_train: 0.3637 loss_val: 1.2638 acc_val: 0.3619 time: 0.0015s\n",
      "Epoch: 00441 loss_train: 1.2618 loss_rec: 1.2618 acc_train: 0.3638 loss_val: 1.2635 acc_val: 0.3600 time: 0.0015s\n",
      "Test set results: loss= 1.2514 accuracy= 0.3844\n",
      "Epoch: 00442 loss_train: 1.2615 loss_rec: 1.2615 acc_train: 0.3634 loss_val: 1.2632 acc_val: 0.3597 time: 0.0015s\n",
      "Epoch: 00443 loss_train: 1.2612 loss_rec: 1.2612 acc_train: 0.3634 loss_val: 1.2629 acc_val: 0.3597 time: 0.0015s\n",
      "Epoch: 00444 loss_train: 1.2610 loss_rec: 1.2610 acc_train: 0.3643 loss_val: 1.2627 acc_val: 0.3603 time: 0.0015s\n",
      "Epoch: 00445 loss_train: 1.2607 loss_rec: 1.2607 acc_train: 0.3654 loss_val: 1.2624 acc_val: 0.3625 time: 0.0015s\n",
      "Epoch: 00446 loss_train: 1.2604 loss_rec: 1.2604 acc_train: 0.3653 loss_val: 1.2621 acc_val: 0.3625 time: 0.0015s\n",
      "Epoch: 00447 loss_train: 1.2601 loss_rec: 1.2601 acc_train: 0.3657 loss_val: 1.2619 acc_val: 0.3622 time: 0.0020s\n",
      "Epoch: 00448 loss_train: 1.2598 loss_rec: 1.2598 acc_train: 0.3658 loss_val: 1.2616 acc_val: 0.3622 time: 0.0015s\n",
      "Epoch: 00449 loss_train: 1.2596 loss_rec: 1.2596 acc_train: 0.3657 loss_val: 1.2613 acc_val: 0.3622 time: 0.0015s\n",
      "Epoch: 00450 loss_train: 1.2593 loss_rec: 1.2593 acc_train: 0.3650 loss_val: 1.2611 acc_val: 0.3622 time: 0.0015s\n",
      "Epoch: 00451 loss_train: 1.2590 loss_rec: 1.2590 acc_train: 0.3648 loss_val: 1.2608 acc_val: 0.3622 time: 0.0015s\n",
      "Test set results: loss= 1.2497 accuracy= 0.3821\n",
      "Epoch: 00452 loss_train: 1.2588 loss_rec: 1.2588 acc_train: 0.3631 loss_val: 1.2605 acc_val: 0.3606 time: 0.0020s\n",
      "Epoch: 00453 loss_train: 1.2585 loss_rec: 1.2585 acc_train: 0.3630 loss_val: 1.2603 acc_val: 0.3606 time: 0.0015s\n",
      "Epoch: 00454 loss_train: 1.2582 loss_rec: 1.2582 acc_train: 0.3626 loss_val: 1.2600 acc_val: 0.3606 time: 0.0015s\n",
      "Epoch: 00455 loss_train: 1.2579 loss_rec: 1.2579 acc_train: 0.3619 loss_val: 1.2597 acc_val: 0.3622 time: 0.0015s\n",
      "Epoch: 00456 loss_train: 1.2577 loss_rec: 1.2577 acc_train: 0.3616 loss_val: 1.2595 acc_val: 0.3616 time: 0.0015s\n",
      "Epoch: 00457 loss_train: 1.2574 loss_rec: 1.2574 acc_train: 0.3616 loss_val: 1.2592 acc_val: 0.3616 time: 0.0020s\n",
      "Epoch: 00458 loss_train: 1.2571 loss_rec: 1.2571 acc_train: 0.3613 loss_val: 1.2589 acc_val: 0.3613 time: 0.0015s\n",
      "Epoch: 00459 loss_train: 1.2569 loss_rec: 1.2569 acc_train: 0.3613 loss_val: 1.2587 acc_val: 0.3613 time: 0.0015s\n",
      "Epoch: 00460 loss_train: 1.2566 loss_rec: 1.2566 acc_train: 0.3607 loss_val: 1.2584 acc_val: 0.3606 time: 0.0015s\n",
      "Epoch: 00461 loss_train: 1.2563 loss_rec: 1.2563 acc_train: 0.3641 loss_val: 1.2582 acc_val: 0.3644 time: 0.0020s\n",
      "Test set results: loss= 1.2480 accuracy= 0.3810\n",
      "Epoch: 00462 loss_train: 1.2561 loss_rec: 1.2561 acc_train: 0.3640 loss_val: 1.2579 acc_val: 0.3641 time: 0.0020s\n",
      "Epoch: 00463 loss_train: 1.2558 loss_rec: 1.2558 acc_train: 0.3642 loss_val: 1.2577 acc_val: 0.3641 time: 0.0015s\n",
      "Epoch: 00464 loss_train: 1.2556 loss_rec: 1.2556 acc_train: 0.3646 loss_val: 1.2574 acc_val: 0.3644 time: 0.0015s\n",
      "Epoch: 00465 loss_train: 1.2553 loss_rec: 1.2553 acc_train: 0.3647 loss_val: 1.2571 acc_val: 0.3641 time: 0.0015s\n",
      "Epoch: 00466 loss_train: 1.2550 loss_rec: 1.2550 acc_train: 0.3641 loss_val: 1.2569 acc_val: 0.3641 time: 0.0015s\n",
      "Epoch: 00467 loss_train: 1.2548 loss_rec: 1.2548 acc_train: 0.3646 loss_val: 1.2566 acc_val: 0.3641 time: 0.0015s\n",
      "Epoch: 00468 loss_train: 1.2545 loss_rec: 1.2545 acc_train: 0.3638 loss_val: 1.2564 acc_val: 0.3625 time: 0.0015s\n",
      "Epoch: 00469 loss_train: 1.2543 loss_rec: 1.2543 acc_train: 0.3920 loss_val: 1.2561 acc_val: 0.3934 time: 0.0015s\n",
      "Epoch: 00470 loss_train: 1.2540 loss_rec: 1.2540 acc_train: 0.3919 loss_val: 1.2559 acc_val: 0.3934 time: 0.0015s\n",
      "Epoch: 00471 loss_train: 1.2537 loss_rec: 1.2537 acc_train: 0.3933 loss_val: 1.2556 acc_val: 0.3962 time: 0.0015s\n",
      "Test set results: loss= 1.2462 accuracy= 0.3854\n",
      "Epoch: 00472 loss_train: 1.2535 loss_rec: 1.2535 acc_train: 0.3933 loss_val: 1.2554 acc_val: 0.3962 time: 0.0015s\n",
      "Epoch: 00473 loss_train: 1.2532 loss_rec: 1.2532 acc_train: 0.3927 loss_val: 1.2551 acc_val: 0.3950 time: 0.0020s\n",
      "Epoch: 00474 loss_train: 1.2530 loss_rec: 1.2530 acc_train: 0.3933 loss_val: 1.2549 acc_val: 0.3947 time: 0.0015s\n",
      "Epoch: 00475 loss_train: 1.2527 loss_rec: 1.2527 acc_train: 0.3925 loss_val: 1.2546 acc_val: 0.3937 time: 0.0015s\n",
      "Epoch: 00476 loss_train: 1.2525 loss_rec: 1.2525 acc_train: 0.3926 loss_val: 1.2544 acc_val: 0.3941 time: 0.0015s\n",
      "Epoch: 00477 loss_train: 1.2522 loss_rec: 1.2522 acc_train: 0.3917 loss_val: 1.2541 acc_val: 0.3934 time: 0.0015s\n",
      "Epoch: 00478 loss_train: 1.2519 loss_rec: 1.2519 acc_train: 0.3916 loss_val: 1.2539 acc_val: 0.3931 time: 0.0015s\n",
      "Epoch: 00479 loss_train: 1.2517 loss_rec: 1.2517 acc_train: 0.3955 loss_val: 1.2537 acc_val: 0.3966 time: 0.0015s\n",
      "Epoch: 00480 loss_train: 1.2514 loss_rec: 1.2514 acc_train: 0.3954 loss_val: 1.2534 acc_val: 0.3966 time: 0.0025s\n",
      "Epoch: 00481 loss_train: 1.2512 loss_rec: 1.2512 acc_train: 0.3951 loss_val: 1.2532 acc_val: 0.3959 time: 0.0015s\n",
      "Test set results: loss= 1.2443 accuracy= 0.3822\n",
      "Epoch: 00482 loss_train: 1.2509 loss_rec: 1.2509 acc_train: 0.3944 loss_val: 1.2529 acc_val: 0.3956 time: 0.0015s\n",
      "Epoch: 00483 loss_train: 1.2507 loss_rec: 1.2507 acc_train: 0.3940 loss_val: 1.2527 acc_val: 0.3947 time: 0.0015s\n",
      "Epoch: 00484 loss_train: 1.2504 loss_rec: 1.2504 acc_train: 0.3945 loss_val: 1.2524 acc_val: 0.3947 time: 0.0020s\n",
      "Epoch: 00485 loss_train: 1.2502 loss_rec: 1.2502 acc_train: 0.3952 loss_val: 1.2522 acc_val: 0.3947 time: 0.0015s\n",
      "Epoch: 00486 loss_train: 1.2499 loss_rec: 1.2499 acc_train: 0.3944 loss_val: 1.2519 acc_val: 0.3941 time: 0.0020s\n",
      "Epoch: 00487 loss_train: 1.2497 loss_rec: 1.2497 acc_train: 0.3943 loss_val: 1.2517 acc_val: 0.3941 time: 0.0015s\n",
      "Epoch: 00488 loss_train: 1.2494 loss_rec: 1.2494 acc_train: 0.3939 loss_val: 1.2515 acc_val: 0.3941 time: 0.0020s\n",
      "Epoch: 00489 loss_train: 1.2492 loss_rec: 1.2492 acc_train: 0.3938 loss_val: 1.2512 acc_val: 0.3937 time: 0.0020s\n",
      "Epoch: 00490 loss_train: 1.2489 loss_rec: 1.2489 acc_train: 0.3938 loss_val: 1.2510 acc_val: 0.3937 time: 0.0015s\n",
      "Epoch: 00491 loss_train: 1.2487 loss_rec: 1.2487 acc_train: 0.3939 loss_val: 1.2507 acc_val: 0.3937 time: 0.0015s\n",
      "Test set results: loss= 1.2426 accuracy= 0.3820\n",
      "Epoch: 00492 loss_train: 1.2484 loss_rec: 1.2484 acc_train: 0.3948 loss_val: 1.2505 acc_val: 0.3953 time: 0.0020s\n",
      "Epoch: 00493 loss_train: 1.2482 loss_rec: 1.2482 acc_train: 0.3947 loss_val: 1.2502 acc_val: 0.3950 time: 0.0015s\n",
      "Epoch: 00494 loss_train: 1.2479 loss_rec: 1.2479 acc_train: 0.3939 loss_val: 1.2500 acc_val: 0.3947 time: 0.0020s\n",
      "Epoch: 00495 loss_train: 1.2477 loss_rec: 1.2477 acc_train: 0.3944 loss_val: 1.2498 acc_val: 0.3956 time: 0.0015s\n",
      "Epoch: 00496 loss_train: 1.2474 loss_rec: 1.2474 acc_train: 0.3942 loss_val: 1.2495 acc_val: 0.3956 time: 0.0015s\n",
      "Epoch: 00497 loss_train: 1.2472 loss_rec: 1.2472 acc_train: 0.3934 loss_val: 1.2493 acc_val: 0.3944 time: 0.0020s\n",
      "Epoch: 00498 loss_train: 1.2469 loss_rec: 1.2469 acc_train: 0.3935 loss_val: 1.2490 acc_val: 0.3944 time: 0.0020s\n",
      "Epoch: 00499 loss_train: 1.2467 loss_rec: 1.2467 acc_train: 0.3936 loss_val: 1.2488 acc_val: 0.3944 time: 0.0020s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00500 loss_train: 1.2465 loss_rec: 1.2465 acc_train: 0.3936 loss_val: 1.2486 acc_val: 0.3944 time: 0.0020s\n",
      "Epoch: 00501 loss_train: 1.2462 loss_rec: 1.2462 acc_train: 0.3929 loss_val: 1.2483 acc_val: 0.3928 time: 0.0020s\n",
      "Test set results: loss= 1.2410 accuracy= 0.3792\n",
      "Epoch: 00502 loss_train: 1.2460 loss_rec: 1.2460 acc_train: 0.3928 loss_val: 1.2481 acc_val: 0.3928 time: 0.0020s\n",
      "Epoch: 00503 loss_train: 1.2457 loss_rec: 1.2457 acc_train: 0.3927 loss_val: 1.2479 acc_val: 0.3928 time: 0.0015s\n",
      "Epoch: 00504 loss_train: 1.2455 loss_rec: 1.2455 acc_train: 0.3929 loss_val: 1.2476 acc_val: 0.3928 time: 0.0020s\n",
      "Epoch: 00505 loss_train: 1.2452 loss_rec: 1.2452 acc_train: 0.3921 loss_val: 1.2474 acc_val: 0.3922 time: 0.0015s\n",
      "Epoch: 00506 loss_train: 1.2450 loss_rec: 1.2450 acc_train: 0.3921 loss_val: 1.2472 acc_val: 0.3909 time: 0.0020s\n",
      "Epoch: 00507 loss_train: 1.2448 loss_rec: 1.2448 acc_train: 0.3950 loss_val: 1.2469 acc_val: 0.3922 time: 0.0015s\n",
      "Epoch: 00508 loss_train: 1.2445 loss_rec: 1.2445 acc_train: 0.3984 loss_val: 1.2467 acc_val: 0.3966 time: 0.0015s\n",
      "Epoch: 00509 loss_train: 1.2443 loss_rec: 1.2443 acc_train: 0.3991 loss_val: 1.2465 acc_val: 0.3987 time: 0.0015s\n",
      "Epoch: 00510 loss_train: 1.2441 loss_rec: 1.2441 acc_train: 0.3987 loss_val: 1.2462 acc_val: 0.3978 time: 0.0020s\n",
      "Epoch: 00511 loss_train: 1.2438 loss_rec: 1.2438 acc_train: 0.3987 loss_val: 1.2460 acc_val: 0.3978 time: 0.0015s\n",
      "Test set results: loss= 1.2395 accuracy= 0.3801\n",
      "Epoch: 00512 loss_train: 1.2436 loss_rec: 1.2436 acc_train: 0.3987 loss_val: 1.2458 acc_val: 0.3978 time: 0.0020s\n",
      "Epoch: 00513 loss_train: 1.2434 loss_rec: 1.2434 acc_train: 0.3987 loss_val: 1.2456 acc_val: 0.3978 time: 0.0015s\n",
      "Epoch: 00514 loss_train: 1.2431 loss_rec: 1.2431 acc_train: 0.3986 loss_val: 1.2453 acc_val: 0.3978 time: 0.0015s\n",
      "Epoch: 00515 loss_train: 1.2429 loss_rec: 1.2429 acc_train: 0.3991 loss_val: 1.2451 acc_val: 0.3975 time: 0.0025s\n",
      "Epoch: 00516 loss_train: 1.2427 loss_rec: 1.2427 acc_train: 0.4004 loss_val: 1.2449 acc_val: 0.3981 time: 0.0015s\n",
      "Epoch: 00517 loss_train: 1.2424 loss_rec: 1.2424 acc_train: 0.4006 loss_val: 1.2447 acc_val: 0.3984 time: 0.0015s\n",
      "Epoch: 00518 loss_train: 1.2422 loss_rec: 1.2422 acc_train: 0.4006 loss_val: 1.2444 acc_val: 0.3984 time: 0.0020s\n",
      "Epoch: 00519 loss_train: 1.2420 loss_rec: 1.2420 acc_train: 0.4006 loss_val: 1.2442 acc_val: 0.3984 time: 0.0015s\n",
      "Epoch: 00520 loss_train: 1.2417 loss_rec: 1.2417 acc_train: 0.3997 loss_val: 1.2440 acc_val: 0.3975 time: 0.0020s\n",
      "Epoch: 00521 loss_train: 1.2415 loss_rec: 1.2415 acc_train: 0.3995 loss_val: 1.2438 acc_val: 0.3975 time: 0.0015s\n",
      "Test set results: loss= 1.2379 accuracy= 0.3821\n",
      "Epoch: 00522 loss_train: 1.2413 loss_rec: 1.2413 acc_train: 0.3995 loss_val: 1.2435 acc_val: 0.3975 time: 0.0015s\n",
      "Epoch: 00523 loss_train: 1.2410 loss_rec: 1.2410 acc_train: 0.3994 loss_val: 1.2433 acc_val: 0.3975 time: 0.0015s\n",
      "Epoch: 00524 loss_train: 1.2408 loss_rec: 1.2408 acc_train: 0.3995 loss_val: 1.2431 acc_val: 0.3975 time: 0.0020s\n",
      "Epoch: 00525 loss_train: 1.2406 loss_rec: 1.2406 acc_train: 0.3996 loss_val: 1.2429 acc_val: 0.3975 time: 0.0015s\n",
      "Epoch: 00526 loss_train: 1.2404 loss_rec: 1.2404 acc_train: 0.4104 loss_val: 1.2427 acc_val: 0.4128 time: 0.0025s\n",
      "Epoch: 00527 loss_train: 1.2402 loss_rec: 1.2402 acc_train: 0.4104 loss_val: 1.2425 acc_val: 0.4131 time: 0.0015s\n",
      "Epoch: 00528 loss_train: 1.2399 loss_rec: 1.2399 acc_train: 0.4103 loss_val: 1.2422 acc_val: 0.4131 time: 0.0015s\n",
      "Epoch: 00529 loss_train: 1.2397 loss_rec: 1.2397 acc_train: 0.4102 loss_val: 1.2420 acc_val: 0.4131 time: 0.0015s\n",
      "Epoch: 00530 loss_train: 1.2395 loss_rec: 1.2395 acc_train: 0.4102 loss_val: 1.2418 acc_val: 0.4131 time: 0.0020s\n",
      "Epoch: 00531 loss_train: 1.2393 loss_rec: 1.2393 acc_train: 0.4102 loss_val: 1.2416 acc_val: 0.4131 time: 0.0015s\n",
      "Test set results: loss= 1.2364 accuracy= 0.3717\n",
      "Epoch: 00532 loss_train: 1.2391 loss_rec: 1.2391 acc_train: 0.4102 loss_val: 1.2414 acc_val: 0.4131 time: 0.0015s\n",
      "Epoch: 00533 loss_train: 1.2388 loss_rec: 1.2388 acc_train: 0.4098 loss_val: 1.2412 acc_val: 0.4128 time: 0.0020s\n",
      "Epoch: 00534 loss_train: 1.2386 loss_rec: 1.2386 acc_train: 0.4098 loss_val: 1.2410 acc_val: 0.4128 time: 0.0030s\n",
      "Epoch: 00535 loss_train: 1.2384 loss_rec: 1.2384 acc_train: 0.4094 loss_val: 1.2408 acc_val: 0.4122 time: 0.0020s\n",
      "Epoch: 00536 loss_train: 1.2382 loss_rec: 1.2382 acc_train: 0.4096 loss_val: 1.2405 acc_val: 0.4125 time: 0.0030s\n",
      "Epoch: 00537 loss_train: 1.2380 loss_rec: 1.2380 acc_train: 0.4097 loss_val: 1.2403 acc_val: 0.4125 time: 0.0020s\n",
      "Epoch: 00538 loss_train: 1.2378 loss_rec: 1.2378 acc_train: 0.4102 loss_val: 1.2401 acc_val: 0.4125 time: 0.0020s\n",
      "Epoch: 00539 loss_train: 1.2375 loss_rec: 1.2375 acc_train: 0.4102 loss_val: 1.2399 acc_val: 0.4125 time: 0.0015s\n",
      "Epoch: 00540 loss_train: 1.2373 loss_rec: 1.2373 acc_train: 0.4103 loss_val: 1.2397 acc_val: 0.4125 time: 0.0025s\n",
      "Epoch: 00541 loss_train: 1.2371 loss_rec: 1.2371 acc_train: 0.4103 loss_val: 1.2395 acc_val: 0.4125 time: 0.0015s\n",
      "Test set results: loss= 1.2349 accuracy= 0.3707\n",
      "Epoch: 00542 loss_train: 1.2369 loss_rec: 1.2369 acc_train: 0.4102 loss_val: 1.2393 acc_val: 0.4125 time: 0.0020s\n",
      "Epoch: 00543 loss_train: 1.2367 loss_rec: 1.2367 acc_train: 0.4102 loss_val: 1.2391 acc_val: 0.4125 time: 0.0025s\n",
      "Epoch: 00544 loss_train: 1.2365 loss_rec: 1.2365 acc_train: 0.4143 loss_val: 1.2389 acc_val: 0.4159 time: 0.0015s\n",
      "Epoch: 00545 loss_train: 1.2363 loss_rec: 1.2363 acc_train: 0.4143 loss_val: 1.2387 acc_val: 0.4163 time: 0.0025s\n",
      "Epoch: 00546 loss_train: 1.2361 loss_rec: 1.2361 acc_train: 0.4141 loss_val: 1.2385 acc_val: 0.4163 time: 0.0020s\n",
      "Epoch: 00547 loss_train: 1.2358 loss_rec: 1.2358 acc_train: 0.4136 loss_val: 1.2383 acc_val: 0.4159 time: 0.0030s\n",
      "Epoch: 00548 loss_train: 1.2356 loss_rec: 1.2356 acc_train: 0.4136 loss_val: 1.2381 acc_val: 0.4159 time: 0.0020s\n",
      "Epoch: 00549 loss_train: 1.2354 loss_rec: 1.2354 acc_train: 0.4135 loss_val: 1.2379 acc_val: 0.4163 time: 0.0020s\n",
      "Epoch: 00550 loss_train: 1.2352 loss_rec: 1.2352 acc_train: 0.4123 loss_val: 1.2377 acc_val: 0.4159 time: 0.0015s\n",
      "Epoch: 00551 loss_train: 1.2350 loss_rec: 1.2350 acc_train: 0.4116 loss_val: 1.2375 acc_val: 0.4150 time: 0.0025s\n",
      "Test set results: loss= 1.2335 accuracy= 0.3702\n",
      "Epoch: 00552 loss_train: 1.2348 loss_rec: 1.2348 acc_train: 0.4134 loss_val: 1.2373 acc_val: 0.4169 time: 0.0025s\n",
      "Epoch: 00553 loss_train: 1.2346 loss_rec: 1.2346 acc_train: 0.4131 loss_val: 1.2371 acc_val: 0.4172 time: 0.0015s\n",
      "Epoch: 00554 loss_train: 1.2344 loss_rec: 1.2344 acc_train: 0.4132 loss_val: 1.2369 acc_val: 0.4172 time: 0.0030s\n",
      "Epoch: 00555 loss_train: 1.2342 loss_rec: 1.2342 acc_train: 0.4134 loss_val: 1.2367 acc_val: 0.4172 time: 0.0015s\n",
      "Epoch: 00556 loss_train: 1.2340 loss_rec: 1.2340 acc_train: 0.4134 loss_val: 1.2364 acc_val: 0.4172 time: 0.0025s\n",
      "Epoch: 00557 loss_train: 1.2337 loss_rec: 1.2337 acc_train: 0.4134 loss_val: 1.2362 acc_val: 0.4169 time: 0.0020s\n",
      "Epoch: 00558 loss_train: 1.2335 loss_rec: 1.2335 acc_train: 0.4134 loss_val: 1.2360 acc_val: 0.4169 time: 0.0020s\n",
      "Epoch: 00559 loss_train: 1.2333 loss_rec: 1.2333 acc_train: 0.4134 loss_val: 1.2358 acc_val: 0.4169 time: 0.0015s\n",
      "Epoch: 00560 loss_train: 1.2331 loss_rec: 1.2331 acc_train: 0.4128 loss_val: 1.2356 acc_val: 0.4172 time: 0.0025s\n",
      "Epoch: 00561 loss_train: 1.2329 loss_rec: 1.2329 acc_train: 0.4128 loss_val: 1.2354 acc_val: 0.4172 time: 0.0015s\n",
      "Test set results: loss= 1.2321 accuracy= 0.3699\n",
      "Epoch: 00562 loss_train: 1.2327 loss_rec: 1.2327 acc_train: 0.4130 loss_val: 1.2353 acc_val: 0.4175 time: 0.0020s\n",
      "Epoch: 00563 loss_train: 1.2325 loss_rec: 1.2325 acc_train: 0.4128 loss_val: 1.2351 acc_val: 0.4169 time: 0.0025s\n",
      "Epoch: 00564 loss_train: 1.2323 loss_rec: 1.2323 acc_train: 0.4129 loss_val: 1.2349 acc_val: 0.4169 time: 0.0020s\n",
      "Epoch: 00565 loss_train: 1.2321 loss_rec: 1.2321 acc_train: 0.4132 loss_val: 1.2347 acc_val: 0.4169 time: 0.0020s\n",
      "Epoch: 00566 loss_train: 1.2319 loss_rec: 1.2319 acc_train: 0.4131 loss_val: 1.2345 acc_val: 0.4169 time: 0.0020s\n",
      "Epoch: 00567 loss_train: 1.2317 loss_rec: 1.2317 acc_train: 0.4124 loss_val: 1.2343 acc_val: 0.4169 time: 0.0025s\n",
      "Epoch: 00568 loss_train: 1.2315 loss_rec: 1.2315 acc_train: 0.4119 loss_val: 1.2341 acc_val: 0.4159 time: 0.0015s\n",
      "Epoch: 00569 loss_train: 1.2313 loss_rec: 1.2313 acc_train: 0.4114 loss_val: 1.2339 acc_val: 0.4156 time: 0.0025s\n",
      "Epoch: 00570 loss_train: 1.2311 loss_rec: 1.2311 acc_train: 0.4114 loss_val: 1.2337 acc_val: 0.4156 time: 0.0020s\n",
      "Epoch: 00571 loss_train: 1.2310 loss_rec: 1.2310 acc_train: 0.4107 loss_val: 1.2335 acc_val: 0.4147 time: 0.0030s\n",
      "Test set results: loss= 1.2308 accuracy= 0.3663\n",
      "Epoch: 00572 loss_train: 1.2308 loss_rec: 1.2308 acc_train: 0.4104 loss_val: 1.2333 acc_val: 0.4147 time: 0.0025s\n",
      "Epoch: 00573 loss_train: 1.2306 loss_rec: 1.2306 acc_train: 0.4104 loss_val: 1.2331 acc_val: 0.4147 time: 0.0020s\n",
      "Epoch: 00574 loss_train: 1.2304 loss_rec: 1.2304 acc_train: 0.4098 loss_val: 1.2330 acc_val: 0.4147 time: 0.0030s\n",
      "Epoch: 00575 loss_train: 1.2302 loss_rec: 1.2302 acc_train: 0.4095 loss_val: 1.2328 acc_val: 0.4144 time: 0.0020s\n",
      "Epoch: 00576 loss_train: 1.2300 loss_rec: 1.2300 acc_train: 0.4096 loss_val: 1.2326 acc_val: 0.4144 time: 0.0030s\n",
      "Epoch: 00577 loss_train: 1.2298 loss_rec: 1.2298 acc_train: 0.4093 loss_val: 1.2324 acc_val: 0.4144 time: 0.0015s\n",
      "Epoch: 00578 loss_train: 1.2296 loss_rec: 1.2296 acc_train: 0.4093 loss_val: 1.2322 acc_val: 0.4144 time: 0.0020s\n",
      "Epoch: 00579 loss_train: 1.2294 loss_rec: 1.2294 acc_train: 0.4092 loss_val: 1.2320 acc_val: 0.4144 time: 0.0020s\n",
      "Epoch: 00580 loss_train: 1.2292 loss_rec: 1.2292 acc_train: 0.4092 loss_val: 1.2318 acc_val: 0.4144 time: 0.0015s\n",
      "Epoch: 00581 loss_train: 1.2290 loss_rec: 1.2290 acc_train: 0.4088 loss_val: 1.2316 acc_val: 0.4144 time: 0.0020s\n",
      "Test set results: loss= 1.2294 accuracy= 0.3618\n",
      "Epoch: 00582 loss_train: 1.2288 loss_rec: 1.2288 acc_train: 0.4079 loss_val: 1.2314 acc_val: 0.4122 time: 0.0015s\n",
      "Epoch: 00583 loss_train: 1.2286 loss_rec: 1.2286 acc_train: 0.4079 loss_val: 1.2313 acc_val: 0.4122 time: 0.0020s\n",
      "Epoch: 00584 loss_train: 1.2284 loss_rec: 1.2284 acc_train: 0.4091 loss_val: 1.2311 acc_val: 0.4128 time: 0.0025s\n",
      "Epoch: 00585 loss_train: 1.2282 loss_rec: 1.2282 acc_train: 0.4091 loss_val: 1.2309 acc_val: 0.4125 time: 0.0025s\n",
      "Epoch: 00586 loss_train: 1.2281 loss_rec: 1.2281 acc_train: 0.4094 loss_val: 1.2307 acc_val: 0.4125 time: 0.0015s\n",
      "Epoch: 00587 loss_train: 1.2279 loss_rec: 1.2279 acc_train: 0.4094 loss_val: 1.2305 acc_val: 0.4125 time: 0.0025s\n",
      "Epoch: 00588 loss_train: 1.2277 loss_rec: 1.2277 acc_train: 0.4094 loss_val: 1.2303 acc_val: 0.4125 time: 0.0020s\n",
      "Epoch: 00589 loss_train: 1.2275 loss_rec: 1.2275 acc_train: 0.4088 loss_val: 1.2301 acc_val: 0.4122 time: 0.0025s\n",
      "Epoch: 00590 loss_train: 1.2273 loss_rec: 1.2273 acc_train: 0.4106 loss_val: 1.2300 acc_val: 0.4128 time: 0.0020s\n",
      "Epoch: 00591 loss_train: 1.2271 loss_rec: 1.2271 acc_train: 0.4102 loss_val: 1.2298 acc_val: 0.4131 time: 0.0030s\n",
      "Test set results: loss= 1.2281 accuracy= 0.3637\n",
      "Epoch: 00592 loss_train: 1.2269 loss_rec: 1.2269 acc_train: 0.4142 loss_val: 1.2296 acc_val: 0.4178 time: 0.0030s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00593 loss_train: 1.2267 loss_rec: 1.2267 acc_train: 0.4144 loss_val: 1.2294 acc_val: 0.4178 time: 0.0025s\n",
      "Epoch: 00594 loss_train: 1.2266 loss_rec: 1.2266 acc_train: 0.4144 loss_val: 1.2292 acc_val: 0.4178 time: 0.0020s\n",
      "Epoch: 00595 loss_train: 1.2264 loss_rec: 1.2264 acc_train: 0.4156 loss_val: 1.2291 acc_val: 0.4184 time: 0.0015s\n",
      "Epoch: 00596 loss_train: 1.2262 loss_rec: 1.2262 acc_train: 0.4156 loss_val: 1.2289 acc_val: 0.4184 time: 0.0015s\n",
      "Epoch: 00597 loss_train: 1.2260 loss_rec: 1.2260 acc_train: 0.4156 loss_val: 1.2287 acc_val: 0.4184 time: 0.0030s\n",
      "Epoch: 00598 loss_train: 1.2258 loss_rec: 1.2258 acc_train: 0.4156 loss_val: 1.2285 acc_val: 0.4184 time: 0.0020s\n",
      "Epoch: 00599 loss_train: 1.2256 loss_rec: 1.2256 acc_train: 0.4157 loss_val: 1.2283 acc_val: 0.4184 time: 0.0025s\n",
      "Epoch: 00600 loss_train: 1.2254 loss_rec: 1.2254 acc_train: 0.4157 loss_val: 1.2281 acc_val: 0.4184 time: 0.0020s\n",
      "Epoch: 00601 loss_train: 1.2253 loss_rec: 1.2253 acc_train: 0.4158 loss_val: 1.2280 acc_val: 0.4184 time: 0.0020s\n",
      "Test set results: loss= 1.2270 accuracy= 0.3641\n",
      "Epoch: 00602 loss_train: 1.2251 loss_rec: 1.2251 acc_train: 0.4168 loss_val: 1.2278 acc_val: 0.4178 time: 0.0025s\n",
      "Epoch: 00603 loss_train: 1.2249 loss_rec: 1.2249 acc_train: 0.4172 loss_val: 1.2276 acc_val: 0.4178 time: 0.0025s\n",
      "Epoch: 00604 loss_train: 1.2247 loss_rec: 1.2247 acc_train: 0.4170 loss_val: 1.2274 acc_val: 0.4175 time: 0.0020s\n",
      "Epoch: 00605 loss_train: 1.2245 loss_rec: 1.2245 acc_train: 0.4170 loss_val: 1.2272 acc_val: 0.4172 time: 0.0025s\n",
      "Epoch: 00606 loss_train: 1.2244 loss_rec: 1.2244 acc_train: 0.4171 loss_val: 1.2271 acc_val: 0.4172 time: 0.0015s\n",
      "Epoch: 00607 loss_train: 1.2242 loss_rec: 1.2242 acc_train: 0.4171 loss_val: 1.2269 acc_val: 0.4172 time: 0.0020s\n",
      "Epoch: 00608 loss_train: 1.2240 loss_rec: 1.2240 acc_train: 0.4171 loss_val: 1.2267 acc_val: 0.4175 time: 0.0020s\n",
      "Epoch: 00609 loss_train: 1.2238 loss_rec: 1.2238 acc_train: 0.4173 loss_val: 1.2265 acc_val: 0.4181 time: 0.0025s\n",
      "Epoch: 00610 loss_train: 1.2236 loss_rec: 1.2236 acc_train: 0.4172 loss_val: 1.2263 acc_val: 0.4181 time: 0.0020s\n",
      "Epoch: 00611 loss_train: 1.2234 loss_rec: 1.2234 acc_train: 0.4173 loss_val: 1.2262 acc_val: 0.4178 time: 0.0030s\n",
      "Test set results: loss= 1.2259 accuracy= 0.3655\n",
      "Epoch: 00612 loss_train: 1.2233 loss_rec: 1.2233 acc_train: 0.4173 loss_val: 1.2260 acc_val: 0.4175 time: 0.0025s\n",
      "Epoch: 00613 loss_train: 1.2231 loss_rec: 1.2231 acc_train: 0.4174 loss_val: 1.2258 acc_val: 0.4175 time: 0.0020s\n",
      "Epoch: 00614 loss_train: 1.2229 loss_rec: 1.2229 acc_train: 0.4174 loss_val: 1.2256 acc_val: 0.4175 time: 0.0020s\n",
      "Epoch: 00615 loss_train: 1.2227 loss_rec: 1.2227 acc_train: 0.4174 loss_val: 1.2255 acc_val: 0.4175 time: 0.0015s\n",
      "Epoch: 00616 loss_train: 1.2225 loss_rec: 1.2225 acc_train: 0.4176 loss_val: 1.2253 acc_val: 0.4175 time: 0.0015s\n",
      "Epoch: 00617 loss_train: 1.2224 loss_rec: 1.2224 acc_train: 0.4169 loss_val: 1.2251 acc_val: 0.4172 time: 0.0015s\n",
      "Epoch: 00618 loss_train: 1.2222 loss_rec: 1.2222 acc_train: 0.4169 loss_val: 1.2249 acc_val: 0.4175 time: 0.0015s\n",
      "Epoch: 00619 loss_train: 1.2220 loss_rec: 1.2220 acc_train: 0.4166 loss_val: 1.2248 acc_val: 0.4175 time: 0.0020s\n",
      "Epoch: 00620 loss_train: 1.2218 loss_rec: 1.2218 acc_train: 0.4166 loss_val: 1.2246 acc_val: 0.4175 time: 0.0015s\n",
      "Epoch: 00621 loss_train: 1.2217 loss_rec: 1.2217 acc_train: 0.4166 loss_val: 1.2244 acc_val: 0.4175 time: 0.0020s\n",
      "Test set results: loss= 1.2250 accuracy= 0.3647\n",
      "Epoch: 00622 loss_train: 1.2215 loss_rec: 1.2215 acc_train: 0.4171 loss_val: 1.2242 acc_val: 0.4175 time: 0.0015s\n",
      "Epoch: 00623 loss_train: 1.2213 loss_rec: 1.2213 acc_train: 0.4173 loss_val: 1.2241 acc_val: 0.4175 time: 0.0015s\n",
      "Epoch: 00624 loss_train: 1.2211 loss_rec: 1.2211 acc_train: 0.4174 loss_val: 1.2239 acc_val: 0.4178 time: 0.0015s\n",
      "Epoch: 00625 loss_train: 1.2210 loss_rec: 1.2210 acc_train: 0.4169 loss_val: 1.2237 acc_val: 0.4178 time: 0.0020s\n",
      "Epoch: 00626 loss_train: 1.2208 loss_rec: 1.2208 acc_train: 0.4167 loss_val: 1.2235 acc_val: 0.4178 time: 0.0020s\n",
      "Epoch: 00627 loss_train: 1.2206 loss_rec: 1.2206 acc_train: 0.4167 loss_val: 1.2234 acc_val: 0.4178 time: 0.0020s\n",
      "Epoch: 00628 loss_train: 1.2204 loss_rec: 1.2204 acc_train: 0.4167 loss_val: 1.2232 acc_val: 0.4178 time: 0.0015s\n",
      "Epoch: 00629 loss_train: 1.2203 loss_rec: 1.2203 acc_train: 0.4167 loss_val: 1.2230 acc_val: 0.4178 time: 0.0020s\n",
      "Epoch: 00630 loss_train: 1.2201 loss_rec: 1.2201 acc_train: 0.4098 loss_val: 1.2228 acc_val: 0.4116 time: 0.0015s\n",
      "Epoch: 00631 loss_train: 1.2199 loss_rec: 1.2199 acc_train: 0.4098 loss_val: 1.2227 acc_val: 0.4116 time: 0.0015s\n",
      "Test set results: loss= 1.2240 accuracy= 0.3582\n",
      "Epoch: 00632 loss_train: 1.2197 loss_rec: 1.2197 acc_train: 0.4100 loss_val: 1.2225 acc_val: 0.4116 time: 0.0020s\n",
      "Epoch: 00633 loss_train: 1.2196 loss_rec: 1.2196 acc_train: 0.4101 loss_val: 1.2223 acc_val: 0.4116 time: 0.0015s\n",
      "Epoch: 00634 loss_train: 1.2194 loss_rec: 1.2194 acc_train: 0.4104 loss_val: 1.2222 acc_val: 0.4116 time: 0.0015s\n",
      "Epoch: 00635 loss_train: 1.2192 loss_rec: 1.2192 acc_train: 0.4103 loss_val: 1.2220 acc_val: 0.4116 time: 0.0015s\n",
      "Epoch: 00636 loss_train: 1.2191 loss_rec: 1.2191 acc_train: 0.4102 loss_val: 1.2218 acc_val: 0.4116 time: 0.0015s\n",
      "Epoch: 00637 loss_train: 1.2189 loss_rec: 1.2189 acc_train: 0.4099 loss_val: 1.2217 acc_val: 0.4113 time: 0.0020s\n",
      "Epoch: 00638 loss_train: 1.2187 loss_rec: 1.2187 acc_train: 0.4099 loss_val: 1.2215 acc_val: 0.4113 time: 0.0015s\n",
      "Epoch: 00639 loss_train: 1.2186 loss_rec: 1.2186 acc_train: 0.4099 loss_val: 1.2213 acc_val: 0.4113 time: 0.0015s\n",
      "Epoch: 00640 loss_train: 1.2184 loss_rec: 1.2184 acc_train: 0.4099 loss_val: 1.2212 acc_val: 0.4113 time: 0.0020s\n",
      "Epoch: 00641 loss_train: 1.2182 loss_rec: 1.2182 acc_train: 0.4099 loss_val: 1.2210 acc_val: 0.4113 time: 0.0015s\n",
      "Test set results: loss= 1.2230 accuracy= 0.3561\n",
      "Epoch: 00642 loss_train: 1.2181 loss_rec: 1.2181 acc_train: 0.4091 loss_val: 1.2209 acc_val: 0.4097 time: 0.0015s\n",
      "Epoch: 00643 loss_train: 1.2179 loss_rec: 1.2179 acc_train: 0.4091 loss_val: 1.2207 acc_val: 0.4097 time: 0.0015s\n",
      "Epoch: 00644 loss_train: 1.2177 loss_rec: 1.2177 acc_train: 0.4091 loss_val: 1.2205 acc_val: 0.4097 time: 0.0020s\n",
      "Epoch: 00645 loss_train: 1.2176 loss_rec: 1.2176 acc_train: 0.4091 loss_val: 1.2204 acc_val: 0.4097 time: 0.0020s\n",
      "Epoch: 00646 loss_train: 1.2174 loss_rec: 1.2174 acc_train: 0.4091 loss_val: 1.2202 acc_val: 0.4097 time: 0.0020s\n",
      "Epoch: 00647 loss_train: 1.2172 loss_rec: 1.2172 acc_train: 0.4091 loss_val: 1.2200 acc_val: 0.4097 time: 0.0015s\n",
      "Epoch: 00648 loss_train: 1.2171 loss_rec: 1.2171 acc_train: 0.4091 loss_val: 1.2199 acc_val: 0.4097 time: 0.0020s\n",
      "Epoch: 00649 loss_train: 1.2169 loss_rec: 1.2169 acc_train: 0.4091 loss_val: 1.2197 acc_val: 0.4097 time: 0.0015s\n",
      "Epoch: 00650 loss_train: 1.2167 loss_rec: 1.2167 acc_train: 0.4087 loss_val: 1.2195 acc_val: 0.4094 time: 0.0020s\n",
      "Epoch: 00651 loss_train: 1.2166 loss_rec: 1.2166 acc_train: 0.4087 loss_val: 1.2194 acc_val: 0.4094 time: 0.0015s\n",
      "Test set results: loss= 1.2219 accuracy= 0.3537\n",
      "Epoch: 00652 loss_train: 1.2164 loss_rec: 1.2164 acc_train: 0.4080 loss_val: 1.2192 acc_val: 0.4078 time: 0.0015s\n",
      "Epoch: 00653 loss_train: 1.2162 loss_rec: 1.2162 acc_train: 0.4076 loss_val: 1.2191 acc_val: 0.4075 time: 0.0020s\n",
      "Epoch: 00654 loss_train: 1.2161 loss_rec: 1.2161 acc_train: 0.4076 loss_val: 1.2189 acc_val: 0.4075 time: 0.0015s\n",
      "Epoch: 00655 loss_train: 1.2159 loss_rec: 1.2159 acc_train: 0.4078 loss_val: 1.2188 acc_val: 0.4084 time: 0.0020s\n",
      "Epoch: 00656 loss_train: 1.2158 loss_rec: 1.2158 acc_train: 0.4078 loss_val: 1.2186 acc_val: 0.4084 time: 0.0015s\n",
      "Epoch: 00657 loss_train: 1.2156 loss_rec: 1.2156 acc_train: 0.4081 loss_val: 1.2184 acc_val: 0.4088 time: 0.0015s\n",
      "Epoch: 00658 loss_train: 1.2154 loss_rec: 1.2154 acc_train: 0.4082 loss_val: 1.2183 acc_val: 0.4088 time: 0.0015s\n",
      "Epoch: 00659 loss_train: 1.2153 loss_rec: 1.2153 acc_train: 0.4082 loss_val: 1.2181 acc_val: 0.4088 time: 0.0015s\n",
      "Epoch: 00660 loss_train: 1.2151 loss_rec: 1.2151 acc_train: 0.4082 loss_val: 1.2180 acc_val: 0.4088 time: 0.0020s\n",
      "Epoch: 00661 loss_train: 1.2150 loss_rec: 1.2150 acc_train: 0.4081 loss_val: 1.2178 acc_val: 0.4088 time: 0.0015s\n",
      "Test set results: loss= 1.2209 accuracy= 0.3535\n",
      "Epoch: 00662 loss_train: 1.2148 loss_rec: 1.2148 acc_train: 0.4083 loss_val: 1.2177 acc_val: 0.4091 time: 0.0015s\n",
      "Epoch: 00663 loss_train: 1.2147 loss_rec: 1.2147 acc_train: 0.4083 loss_val: 1.2175 acc_val: 0.4091 time: 0.0015s\n",
      "Epoch: 00664 loss_train: 1.2145 loss_rec: 1.2145 acc_train: 0.4079 loss_val: 1.2173 acc_val: 0.4084 time: 0.0020s\n",
      "Epoch: 00665 loss_train: 1.2143 loss_rec: 1.2143 acc_train: 0.4079 loss_val: 1.2172 acc_val: 0.4084 time: 0.0015s\n",
      "Epoch: 00666 loss_train: 1.2142 loss_rec: 1.2142 acc_train: 0.4085 loss_val: 1.2170 acc_val: 0.4100 time: 0.0015s\n",
      "Epoch: 00667 loss_train: 1.2140 loss_rec: 1.2140 acc_train: 0.4097 loss_val: 1.2169 acc_val: 0.4109 time: 0.0020s\n",
      "Epoch: 00668 loss_train: 1.2139 loss_rec: 1.2139 acc_train: 0.4096 loss_val: 1.2167 acc_val: 0.4109 time: 0.0015s\n",
      "Epoch: 00669 loss_train: 1.2137 loss_rec: 1.2137 acc_train: 0.4096 loss_val: 1.2166 acc_val: 0.4109 time: 0.0015s\n",
      "Epoch: 00670 loss_train: 1.2136 loss_rec: 1.2136 acc_train: 0.4096 loss_val: 1.2164 acc_val: 0.4109 time: 0.0020s\n",
      "Epoch: 00671 loss_train: 1.2134 loss_rec: 1.2134 acc_train: 0.4096 loss_val: 1.2163 acc_val: 0.4109 time: 0.0015s\n",
      "Test set results: loss= 1.2197 accuracy= 0.3541\n",
      "Epoch: 00672 loss_train: 1.2133 loss_rec: 1.2133 acc_train: 0.4096 loss_val: 1.2161 acc_val: 0.4109 time: 0.0015s\n",
      "Epoch: 00673 loss_train: 1.2131 loss_rec: 1.2131 acc_train: 0.4094 loss_val: 1.2160 acc_val: 0.4109 time: 0.0015s\n",
      "Epoch: 00674 loss_train: 1.2130 loss_rec: 1.2130 acc_train: 0.4093 loss_val: 1.2158 acc_val: 0.4109 time: 0.0015s\n",
      "Epoch: 00675 loss_train: 1.2128 loss_rec: 1.2128 acc_train: 0.4093 loss_val: 1.2157 acc_val: 0.4109 time: 0.0020s\n",
      "Epoch: 00676 loss_train: 1.2126 loss_rec: 1.2126 acc_train: 0.4093 loss_val: 1.2155 acc_val: 0.4109 time: 0.0015s\n",
      "Epoch: 00677 loss_train: 1.2125 loss_rec: 1.2125 acc_train: 0.4089 loss_val: 1.2154 acc_val: 0.4109 time: 0.0015s\n",
      "Epoch: 00678 loss_train: 1.2123 loss_rec: 1.2123 acc_train: 0.4089 loss_val: 1.2152 acc_val: 0.4109 time: 0.0020s\n",
      "Epoch: 00679 loss_train: 1.2122 loss_rec: 1.2122 acc_train: 0.4084 loss_val: 1.2151 acc_val: 0.4106 time: 0.0015s\n",
      "Epoch: 00680 loss_train: 1.2120 loss_rec: 1.2120 acc_train: 0.4084 loss_val: 1.2149 acc_val: 0.4106 time: 0.0015s\n",
      "Epoch: 00681 loss_train: 1.2119 loss_rec: 1.2119 acc_train: 0.4116 loss_val: 1.2148 acc_val: 0.4138 time: 0.0020s\n",
      "Test set results: loss= 1.2187 accuracy= 0.3538\n",
      "Epoch: 00682 loss_train: 1.2117 loss_rec: 1.2117 acc_train: 0.4115 loss_val: 1.2146 acc_val: 0.4141 time: 0.0015s\n",
      "Epoch: 00683 loss_train: 1.2116 loss_rec: 1.2116 acc_train: 0.4114 loss_val: 1.2145 acc_val: 0.4141 time: 0.0020s\n",
      "Epoch: 00684 loss_train: 1.2114 loss_rec: 1.2114 acc_train: 0.4114 loss_val: 1.2143 acc_val: 0.4141 time: 0.0015s\n",
      "Epoch: 00685 loss_train: 1.2113 loss_rec: 1.2113 acc_train: 0.4118 loss_val: 1.2142 acc_val: 0.4144 time: 0.0020s\n",
      "Epoch: 00686 loss_train: 1.2111 loss_rec: 1.2111 acc_train: 0.4129 loss_val: 1.2140 acc_val: 0.4156 time: 0.0015s\n",
      "Epoch: 00687 loss_train: 1.2110 loss_rec: 1.2110 acc_train: 0.4131 loss_val: 1.2139 acc_val: 0.4163 time: 0.0020s\n",
      "Epoch: 00688 loss_train: 1.2108 loss_rec: 1.2108 acc_train: 0.4127 loss_val: 1.2137 acc_val: 0.4156 time: 0.0015s\n",
      "Epoch: 00689 loss_train: 1.2107 loss_rec: 1.2107 acc_train: 0.4119 loss_val: 1.2136 acc_val: 0.4159 time: 0.0015s\n",
      "Epoch: 00690 loss_train: 1.2105 loss_rec: 1.2105 acc_train: 0.4119 loss_val: 1.2134 acc_val: 0.4159 time: 0.0015s\n",
      "Epoch: 00691 loss_train: 1.2104 loss_rec: 1.2104 acc_train: 0.4120 loss_val: 1.2133 acc_val: 0.4159 time: 0.0015s\n",
      "Test set results: loss= 1.2177 accuracy= 0.3538\n",
      "Epoch: 00692 loss_train: 1.2102 loss_rec: 1.2102 acc_train: 0.4120 loss_val: 1.2131 acc_val: 0.4159 time: 0.0020s\n",
      "Epoch: 00693 loss_train: 1.2101 loss_rec: 1.2101 acc_train: 0.4109 loss_val: 1.2130 acc_val: 0.4150 time: 0.0015s\n",
      "Epoch: 00694 loss_train: 1.2099 loss_rec: 1.2099 acc_train: 0.4109 loss_val: 1.2128 acc_val: 0.4150 time: 0.0015s\n",
      "Epoch: 00695 loss_train: 1.2098 loss_rec: 1.2098 acc_train: 0.4110 loss_val: 1.2127 acc_val: 0.4153 time: 0.0015s\n",
      "Epoch: 00696 loss_train: 1.2096 loss_rec: 1.2096 acc_train: 0.4111 loss_val: 1.2125 acc_val: 0.4153 time: 0.0015s\n",
      "Epoch: 00697 loss_train: 1.2095 loss_rec: 1.2095 acc_train: 0.4111 loss_val: 1.2124 acc_val: 0.4153 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00698 loss_train: 1.2093 loss_rec: 1.2093 acc_train: 0.4111 loss_val: 1.2122 acc_val: 0.4150 time: 0.0020s\n",
      "Epoch: 00699 loss_train: 1.2092 loss_rec: 1.2092 acc_train: 0.4111 loss_val: 1.2121 acc_val: 0.4150 time: 0.0015s\n",
      "Epoch: 00700 loss_train: 1.2090 loss_rec: 1.2090 acc_train: 0.4111 loss_val: 1.2120 acc_val: 0.4150 time: 0.0015s\n",
      "Epoch: 00701 loss_train: 1.2089 loss_rec: 1.2089 acc_train: 0.4106 loss_val: 1.2118 acc_val: 0.4150 time: 0.0015s\n",
      "Test set results: loss= 1.2169 accuracy= 0.3513\n",
      "Epoch: 00702 loss_train: 1.2088 loss_rec: 1.2088 acc_train: 0.4106 loss_val: 1.2117 acc_val: 0.4150 time: 0.0015s\n",
      "Epoch: 00703 loss_train: 1.2086 loss_rec: 1.2086 acc_train: 0.4106 loss_val: 1.2115 acc_val: 0.4150 time: 0.0015s\n",
      "Epoch: 00704 loss_train: 1.2085 loss_rec: 1.2085 acc_train: 0.4106 loss_val: 1.2114 acc_val: 0.4150 time: 0.0015s\n",
      "Epoch: 00705 loss_train: 1.2083 loss_rec: 1.2083 acc_train: 0.4108 loss_val: 1.2112 acc_val: 0.4150 time: 0.0015s\n",
      "Epoch: 00706 loss_train: 1.2082 loss_rec: 1.2082 acc_train: 0.4108 loss_val: 1.2111 acc_val: 0.4150 time: 0.0015s\n",
      "Epoch: 00707 loss_train: 1.2080 loss_rec: 1.2080 acc_train: 0.4109 loss_val: 1.2109 acc_val: 0.4150 time: 0.0015s\n",
      "Epoch: 00708 loss_train: 1.2079 loss_rec: 1.2079 acc_train: 0.4109 loss_val: 1.2108 acc_val: 0.4150 time: 0.0015s\n",
      "Epoch: 00709 loss_train: 1.2077 loss_rec: 1.2077 acc_train: 0.4104 loss_val: 1.2107 acc_val: 0.4144 time: 0.0020s\n",
      "Epoch: 00710 loss_train: 1.2076 loss_rec: 1.2076 acc_train: 0.4104 loss_val: 1.2105 acc_val: 0.4144 time: 0.0015s\n",
      "Epoch: 00711 loss_train: 1.2075 loss_rec: 1.2075 acc_train: 0.4104 loss_val: 1.2104 acc_val: 0.4144 time: 0.0015s\n",
      "Test set results: loss= 1.2161 accuracy= 0.3508\n",
      "Epoch: 00712 loss_train: 1.2073 loss_rec: 1.2073 acc_train: 0.4104 loss_val: 1.2102 acc_val: 0.4144 time: 0.0020s\n",
      "Epoch: 00713 loss_train: 1.2072 loss_rec: 1.2072 acc_train: 0.4104 loss_val: 1.2101 acc_val: 0.4147 time: 0.0020s\n",
      "Epoch: 00714 loss_train: 1.2070 loss_rec: 1.2070 acc_train: 0.4105 loss_val: 1.2099 acc_val: 0.4150 time: 0.0015s\n",
      "Epoch: 00715 loss_train: 1.2069 loss_rec: 1.2069 acc_train: 0.4105 loss_val: 1.2098 acc_val: 0.4150 time: 0.0020s\n",
      "Epoch: 00716 loss_train: 1.2067 loss_rec: 1.2067 acc_train: 0.4105 loss_val: 1.2097 acc_val: 0.4153 time: 0.0025s\n",
      "Epoch: 00717 loss_train: 1.2066 loss_rec: 1.2066 acc_train: 0.4140 loss_val: 1.2095 acc_val: 0.4184 time: 0.0015s\n",
      "Epoch: 00718 loss_train: 1.2065 loss_rec: 1.2065 acc_train: 0.4146 loss_val: 1.2094 acc_val: 0.4194 time: 0.0025s\n",
      "Epoch: 00719 loss_train: 1.2063 loss_rec: 1.2063 acc_train: 0.4146 loss_val: 1.2092 acc_val: 0.4194 time: 0.0015s\n",
      "Epoch: 00720 loss_train: 1.2062 loss_rec: 1.2062 acc_train: 0.4146 loss_val: 1.2091 acc_val: 0.4194 time: 0.0020s\n",
      "Epoch: 00721 loss_train: 1.2060 loss_rec: 1.2060 acc_train: 0.4146 loss_val: 1.2090 acc_val: 0.4191 time: 0.0015s\n",
      "Test set results: loss= 1.2153 accuracy= 0.3530\n",
      "Epoch: 00722 loss_train: 1.2059 loss_rec: 1.2059 acc_train: 0.4146 loss_val: 1.2088 acc_val: 0.4191 time: 0.0020s\n",
      "Epoch: 00723 loss_train: 1.2057 loss_rec: 1.2057 acc_train: 0.4142 loss_val: 1.2087 acc_val: 0.4184 time: 0.0015s\n",
      "Epoch: 00724 loss_train: 1.2056 loss_rec: 1.2056 acc_train: 0.4142 loss_val: 1.2085 acc_val: 0.4184 time: 0.0020s\n",
      "Epoch: 00725 loss_train: 1.2055 loss_rec: 1.2055 acc_train: 0.4136 loss_val: 1.2084 acc_val: 0.4181 time: 0.0015s\n",
      "Epoch: 00726 loss_train: 1.2053 loss_rec: 1.2053 acc_train: 0.4136 loss_val: 1.2083 acc_val: 0.4181 time: 0.0020s\n",
      "Epoch: 00727 loss_train: 1.2052 loss_rec: 1.2052 acc_train: 0.4137 loss_val: 1.2081 acc_val: 0.4181 time: 0.0020s\n",
      "Epoch: 00728 loss_train: 1.2050 loss_rec: 1.2050 acc_train: 0.4137 loss_val: 1.2080 acc_val: 0.4181 time: 0.0020s\n",
      "Epoch: 00729 loss_train: 1.2049 loss_rec: 1.2049 acc_train: 0.4142 loss_val: 1.2078 acc_val: 0.4184 time: 0.0015s\n",
      "Epoch: 00730 loss_train: 1.2048 loss_rec: 1.2048 acc_train: 0.4153 loss_val: 1.2077 acc_val: 0.4206 time: 0.0020s\n",
      "Epoch: 00731 loss_train: 1.2046 loss_rec: 1.2046 acc_train: 0.4154 loss_val: 1.2076 acc_val: 0.4206 time: 0.0015s\n",
      "Test set results: loss= 1.2146 accuracy= 0.3537\n",
      "Epoch: 00732 loss_train: 1.2045 loss_rec: 1.2045 acc_train: 0.4154 loss_val: 1.2074 acc_val: 0.4206 time: 0.0020s\n",
      "Epoch: 00733 loss_train: 1.2044 loss_rec: 1.2044 acc_train: 0.4154 loss_val: 1.2073 acc_val: 0.4206 time: 0.0020s\n",
      "Epoch: 00734 loss_train: 1.2042 loss_rec: 1.2042 acc_train: 0.4154 loss_val: 1.2072 acc_val: 0.4206 time: 0.0015s\n",
      "Epoch: 00735 loss_train: 1.2041 loss_rec: 1.2041 acc_train: 0.4156 loss_val: 1.2070 acc_val: 0.4206 time: 0.0020s\n",
      "Epoch: 00736 loss_train: 1.2039 loss_rec: 1.2039 acc_train: 0.4156 loss_val: 1.2069 acc_val: 0.4206 time: 0.0015s\n",
      "Epoch: 00737 loss_train: 1.2038 loss_rec: 1.2038 acc_train: 0.4156 loss_val: 1.2068 acc_val: 0.4206 time: 0.0015s\n",
      "Epoch: 00738 loss_train: 1.2037 loss_rec: 1.2037 acc_train: 0.4158 loss_val: 1.2066 acc_val: 0.4206 time: 0.0020s\n",
      "Epoch: 00739 loss_train: 1.2035 loss_rec: 1.2035 acc_train: 0.4161 loss_val: 1.2065 acc_val: 0.4206 time: 0.0015s\n",
      "Epoch: 00740 loss_train: 1.2034 loss_rec: 1.2034 acc_train: 0.4163 loss_val: 1.2064 acc_val: 0.4209 time: 0.0020s\n",
      "Epoch: 00741 loss_train: 1.2033 loss_rec: 1.2033 acc_train: 0.4161 loss_val: 1.2062 acc_val: 0.4206 time: 0.0015s\n",
      "Test set results: loss= 1.2137 accuracy= 0.3543\n",
      "Epoch: 00742 loss_train: 1.2031 loss_rec: 1.2031 acc_train: 0.4161 loss_val: 1.2061 acc_val: 0.4206 time: 0.0020s\n",
      "Epoch: 00743 loss_train: 1.2030 loss_rec: 1.2030 acc_train: 0.4161 loss_val: 1.2060 acc_val: 0.4206 time: 0.0020s\n",
      "Epoch: 00744 loss_train: 1.2029 loss_rec: 1.2029 acc_train: 0.4174 loss_val: 1.2059 acc_val: 0.4222 time: 0.0025s\n",
      "Epoch: 00745 loss_train: 1.2027 loss_rec: 1.2027 acc_train: 0.4174 loss_val: 1.2057 acc_val: 0.4222 time: 0.0015s\n",
      "Epoch: 00746 loss_train: 1.2026 loss_rec: 1.2026 acc_train: 0.4179 loss_val: 1.2056 acc_val: 0.4228 time: 0.0020s\n",
      "Epoch: 00747 loss_train: 1.2025 loss_rec: 1.2025 acc_train: 0.4181 loss_val: 1.2055 acc_val: 0.4231 time: 0.0020s\n",
      "Epoch: 00748 loss_train: 1.2023 loss_rec: 1.2023 acc_train: 0.4181 loss_val: 1.2053 acc_val: 0.4231 time: 0.0020s\n",
      "Epoch: 00749 loss_train: 1.2022 loss_rec: 1.2022 acc_train: 0.4181 loss_val: 1.2052 acc_val: 0.4231 time: 0.0020s\n",
      "Epoch: 00750 loss_train: 1.2021 loss_rec: 1.2021 acc_train: 0.4181 loss_val: 1.2051 acc_val: 0.4231 time: 0.0015s\n",
      "Epoch: 00751 loss_train: 1.2020 loss_rec: 1.2020 acc_train: 0.4181 loss_val: 1.2050 acc_val: 0.4231 time: 0.0020s\n",
      "Test set results: loss= 1.2128 accuracy= 0.3584\n",
      "Epoch: 00752 loss_train: 1.2018 loss_rec: 1.2018 acc_train: 0.4196 loss_val: 1.2048 acc_val: 0.4250 time: 0.0020s\n",
      "Epoch: 00753 loss_train: 1.2017 loss_rec: 1.2017 acc_train: 0.4196 loss_val: 1.2047 acc_val: 0.4250 time: 0.0020s\n",
      "Epoch: 00754 loss_train: 1.2016 loss_rec: 1.2016 acc_train: 0.4196 loss_val: 1.2046 acc_val: 0.4250 time: 0.0015s\n",
      "Epoch: 00755 loss_train: 1.2014 loss_rec: 1.2014 acc_train: 0.4201 loss_val: 1.2044 acc_val: 0.4253 time: 0.0015s\n",
      "Epoch: 00756 loss_train: 1.2013 loss_rec: 1.2013 acc_train: 0.4202 loss_val: 1.2043 acc_val: 0.4250 time: 0.0015s\n",
      "Epoch: 00757 loss_train: 1.2012 loss_rec: 1.2012 acc_train: 0.4202 loss_val: 1.2042 acc_val: 0.4250 time: 0.0015s\n",
      "Epoch: 00758 loss_train: 1.2010 loss_rec: 1.2010 acc_train: 0.4202 loss_val: 1.2041 acc_val: 0.4253 time: 0.0020s\n",
      "Epoch: 00759 loss_train: 1.2009 loss_rec: 1.2009 acc_train: 0.4205 loss_val: 1.2039 acc_val: 0.4256 time: 0.0015s\n",
      "Epoch: 00760 loss_train: 1.2008 loss_rec: 1.2008 acc_train: 0.4211 loss_val: 1.2038 acc_val: 0.4256 time: 0.0020s\n",
      "Epoch: 00761 loss_train: 1.2007 loss_rec: 1.2007 acc_train: 0.4211 loss_val: 1.2037 acc_val: 0.4256 time: 0.0025s\n",
      "Test set results: loss= 1.2119 accuracy= 0.3613\n",
      "Epoch: 00762 loss_train: 1.2005 loss_rec: 1.2005 acc_train: 0.4215 loss_val: 1.2036 acc_val: 0.4266 time: 0.0020s\n",
      "Epoch: 00763 loss_train: 1.2004 loss_rec: 1.2004 acc_train: 0.4215 loss_val: 1.2034 acc_val: 0.4266 time: 0.0020s\n",
      "Epoch: 00764 loss_train: 1.2003 loss_rec: 1.2003 acc_train: 0.4215 loss_val: 1.2033 acc_val: 0.4266 time: 0.0020s\n",
      "Epoch: 00765 loss_train: 1.2002 loss_rec: 1.2002 acc_train: 0.4215 loss_val: 1.2032 acc_val: 0.4266 time: 0.0015s\n",
      "Epoch: 00766 loss_train: 1.2000 loss_rec: 1.2000 acc_train: 0.4216 loss_val: 1.2031 acc_val: 0.4266 time: 0.0020s\n",
      "Epoch: 00767 loss_train: 1.1999 loss_rec: 1.1999 acc_train: 0.4213 loss_val: 1.2030 acc_val: 0.4263 time: 0.0020s\n",
      "Epoch: 00768 loss_train: 1.1998 loss_rec: 1.1998 acc_train: 0.4213 loss_val: 1.2028 acc_val: 0.4263 time: 0.0020s\n",
      "Epoch: 00769 loss_train: 1.1997 loss_rec: 1.1997 acc_train: 0.4214 loss_val: 1.2027 acc_val: 0.4263 time: 0.0020s\n",
      "Epoch: 00770 loss_train: 1.1995 loss_rec: 1.1995 acc_train: 0.4214 loss_val: 1.2026 acc_val: 0.4263 time: 0.0020s\n",
      "Epoch: 00771 loss_train: 1.1994 loss_rec: 1.1994 acc_train: 0.4214 loss_val: 1.2025 acc_val: 0.4263 time: 0.0020s\n",
      "Test set results: loss= 1.2110 accuracy= 0.3611\n",
      "Epoch: 00772 loss_train: 1.1993 loss_rec: 1.1993 acc_train: 0.4214 loss_val: 1.2023 acc_val: 0.4263 time: 0.0020s\n",
      "Epoch: 00773 loss_train: 1.1992 loss_rec: 1.1992 acc_train: 0.4214 loss_val: 1.2022 acc_val: 0.4263 time: 0.0015s\n",
      "Epoch: 00774 loss_train: 1.1990 loss_rec: 1.1990 acc_train: 0.4216 loss_val: 1.2021 acc_val: 0.4272 time: 0.0015s\n",
      "Epoch: 00775 loss_train: 1.1989 loss_rec: 1.1989 acc_train: 0.4218 loss_val: 1.2020 acc_val: 0.4272 time: 0.0020s\n",
      "Epoch: 00776 loss_train: 1.1988 loss_rec: 1.1988 acc_train: 0.4225 loss_val: 1.2019 acc_val: 0.4278 time: 0.0015s\n",
      "Epoch: 00777 loss_train: 1.1987 loss_rec: 1.1987 acc_train: 0.4225 loss_val: 1.2017 acc_val: 0.4278 time: 0.0020s\n",
      "Epoch: 00778 loss_train: 1.1985 loss_rec: 1.1985 acc_train: 0.4225 loss_val: 1.2016 acc_val: 0.4278 time: 0.0015s\n",
      "Epoch: 00779 loss_train: 1.1984 loss_rec: 1.1984 acc_train: 0.4226 loss_val: 1.2015 acc_val: 0.4278 time: 0.0020s\n",
      "Epoch: 00780 loss_train: 1.1983 loss_rec: 1.1983 acc_train: 0.4226 loss_val: 1.2014 acc_val: 0.4278 time: 0.0015s\n",
      "Epoch: 00781 loss_train: 1.1982 loss_rec: 1.1982 acc_train: 0.4235 loss_val: 1.2013 acc_val: 0.4288 time: 0.0015s\n",
      "Test set results: loss= 1.2102 accuracy= 0.3639\n",
      "Epoch: 00782 loss_train: 1.1980 loss_rec: 1.1980 acc_train: 0.4236 loss_val: 1.2011 acc_val: 0.4288 time: 0.0020s\n",
      "Epoch: 00783 loss_train: 1.1979 loss_rec: 1.1979 acc_train: 0.4236 loss_val: 1.2010 acc_val: 0.4288 time: 0.0015s\n",
      "Epoch: 00784 loss_train: 1.1978 loss_rec: 1.1978 acc_train: 0.4232 loss_val: 1.2009 acc_val: 0.4275 time: 0.0020s\n",
      "Epoch: 00785 loss_train: 1.1977 loss_rec: 1.1977 acc_train: 0.4233 loss_val: 1.2008 acc_val: 0.4275 time: 0.0015s\n",
      "Epoch: 00786 loss_train: 1.1975 loss_rec: 1.1975 acc_train: 0.4232 loss_val: 1.2007 acc_val: 0.4275 time: 0.0020s\n",
      "Epoch: 00787 loss_train: 1.1974 loss_rec: 1.1974 acc_train: 0.4238 loss_val: 1.2005 acc_val: 0.4278 time: 0.0015s\n",
      "Epoch: 00788 loss_train: 1.1973 loss_rec: 1.1973 acc_train: 0.4238 loss_val: 1.2004 acc_val: 0.4278 time: 0.0020s\n",
      "Epoch: 00789 loss_train: 1.1972 loss_rec: 1.1972 acc_train: 0.4239 loss_val: 1.2003 acc_val: 0.4278 time: 0.0015s\n",
      "Epoch: 00790 loss_train: 1.1971 loss_rec: 1.1971 acc_train: 0.4241 loss_val: 1.2002 acc_val: 0.4278 time: 0.0020s\n",
      "Epoch: 00791 loss_train: 1.1969 loss_rec: 1.1969 acc_train: 0.4242 loss_val: 1.2001 acc_val: 0.4278 time: 0.0020s\n",
      "Test set results: loss= 1.2094 accuracy= 0.3643\n",
      "Epoch: 00792 loss_train: 1.1968 loss_rec: 1.1968 acc_train: 0.4242 loss_val: 1.1999 acc_val: 0.4278 time: 0.0015s\n",
      "Epoch: 00793 loss_train: 1.1967 loss_rec: 1.1967 acc_train: 0.4242 loss_val: 1.1998 acc_val: 0.4278 time: 0.0020s\n",
      "Epoch: 00794 loss_train: 1.1966 loss_rec: 1.1966 acc_train: 0.4244 loss_val: 1.1997 acc_val: 0.4281 time: 0.0015s\n",
      "Epoch: 00795 loss_train: 1.1965 loss_rec: 1.1965 acc_train: 0.4244 loss_val: 1.1996 acc_val: 0.4281 time: 0.0020s\n",
      "Epoch: 00796 loss_train: 1.1963 loss_rec: 1.1963 acc_train: 0.4236 loss_val: 1.1995 acc_val: 0.4275 time: 0.0020s\n",
      "Epoch: 00797 loss_train: 1.1962 loss_rec: 1.1962 acc_train: 0.4238 loss_val: 1.1994 acc_val: 0.4278 time: 0.0015s\n",
      "Epoch: 00798 loss_train: 1.1961 loss_rec: 1.1961 acc_train: 0.4243 loss_val: 1.1993 acc_val: 0.4278 time: 0.0015s\n",
      "Epoch: 00799 loss_train: 1.1960 loss_rec: 1.1960 acc_train: 0.4251 loss_val: 1.1991 acc_val: 0.4294 time: 0.0020s\n",
      "Epoch: 00800 loss_train: 1.1959 loss_rec: 1.1959 acc_train: 0.4252 loss_val: 1.1990 acc_val: 0.4294 time: 0.0015s\n",
      "Epoch: 00801 loss_train: 1.1958 loss_rec: 1.1958 acc_train: 0.4273 loss_val: 1.1989 acc_val: 0.4309 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 1.2087 accuracy= 0.3672\n",
      "Epoch: 00802 loss_train: 1.1956 loss_rec: 1.1956 acc_train: 0.4273 loss_val: 1.1988 acc_val: 0.4309 time: 0.0020s\n",
      "Epoch: 00803 loss_train: 1.1955 loss_rec: 1.1955 acc_train: 0.4283 loss_val: 1.1987 acc_val: 0.4322 time: 0.0015s\n",
      "Epoch: 00804 loss_train: 1.1954 loss_rec: 1.1954 acc_train: 0.4283 loss_val: 1.1986 acc_val: 0.4322 time: 0.0015s\n",
      "Epoch: 00805 loss_train: 1.1953 loss_rec: 1.1953 acc_train: 0.4283 loss_val: 1.1985 acc_val: 0.4322 time: 0.0015s\n",
      "Epoch: 00806 loss_train: 1.1952 loss_rec: 1.1952 acc_train: 0.4279 loss_val: 1.1983 acc_val: 0.4313 time: 0.0020s\n",
      "Epoch: 00807 loss_train: 1.1951 loss_rec: 1.1951 acc_train: 0.4278 loss_val: 1.1982 acc_val: 0.4313 time: 0.0015s\n",
      "Epoch: 00808 loss_train: 1.1949 loss_rec: 1.1949 acc_train: 0.4278 loss_val: 1.1981 acc_val: 0.4319 time: 0.0015s\n",
      "Epoch: 00809 loss_train: 1.1948 loss_rec: 1.1948 acc_train: 0.4278 loss_val: 1.1980 acc_val: 0.4319 time: 0.0015s\n",
      "Epoch: 00810 loss_train: 1.1947 loss_rec: 1.1947 acc_train: 0.4278 loss_val: 1.1979 acc_val: 0.4319 time: 0.0020s\n",
      "Epoch: 00811 loss_train: 1.1946 loss_rec: 1.1946 acc_train: 0.4278 loss_val: 1.1978 acc_val: 0.4319 time: 0.0015s\n",
      "Test set results: loss= 1.2080 accuracy= 0.3667\n",
      "Epoch: 00812 loss_train: 1.1945 loss_rec: 1.1945 acc_train: 0.4278 loss_val: 1.1977 acc_val: 0.4319 time: 0.0020s\n",
      "Epoch: 00813 loss_train: 1.1944 loss_rec: 1.1944 acc_train: 0.4278 loss_val: 1.1976 acc_val: 0.4319 time: 0.0015s\n",
      "Epoch: 00814 loss_train: 1.1943 loss_rec: 1.1943 acc_train: 0.4278 loss_val: 1.1975 acc_val: 0.4319 time: 0.0020s\n",
      "Epoch: 00815 loss_train: 1.1941 loss_rec: 1.1941 acc_train: 0.4278 loss_val: 1.1974 acc_val: 0.4319 time: 0.0015s\n",
      "Epoch: 00816 loss_train: 1.1940 loss_rec: 1.1940 acc_train: 0.4278 loss_val: 1.1973 acc_val: 0.4319 time: 0.0020s\n",
      "Epoch: 00817 loss_train: 1.1939 loss_rec: 1.1939 acc_train: 0.4273 loss_val: 1.1972 acc_val: 0.4319 time: 0.0020s\n",
      "Epoch: 00818 loss_train: 1.1938 loss_rec: 1.1938 acc_train: 0.4273 loss_val: 1.1971 acc_val: 0.4322 time: 0.0015s\n",
      "Epoch: 00819 loss_train: 1.1937 loss_rec: 1.1937 acc_train: 0.4272 loss_val: 1.1969 acc_val: 0.4322 time: 0.0020s\n",
      "Epoch: 00820 loss_train: 1.1936 loss_rec: 1.1936 acc_train: 0.4272 loss_val: 1.1968 acc_val: 0.4322 time: 0.0020s\n",
      "Epoch: 00821 loss_train: 1.1935 loss_rec: 1.1935 acc_train: 0.4272 loss_val: 1.1967 acc_val: 0.4322 time: 0.0020s\n",
      "Test set results: loss= 1.2073 accuracy= 0.3661\n",
      "Epoch: 00822 loss_train: 1.1934 loss_rec: 1.1934 acc_train: 0.4272 loss_val: 1.1966 acc_val: 0.4322 time: 0.0015s\n",
      "Epoch: 00823 loss_train: 1.1932 loss_rec: 1.1932 acc_train: 0.4269 loss_val: 1.1965 acc_val: 0.4322 time: 0.0020s\n",
      "Epoch: 00824 loss_train: 1.1931 loss_rec: 1.1931 acc_train: 0.4270 loss_val: 1.1964 acc_val: 0.4322 time: 0.0015s\n",
      "Epoch: 00825 loss_train: 1.1930 loss_rec: 1.1930 acc_train: 0.4270 loss_val: 1.1963 acc_val: 0.4322 time: 0.0015s\n",
      "Epoch: 00826 loss_train: 1.1929 loss_rec: 1.1929 acc_train: 0.4271 loss_val: 1.1962 acc_val: 0.4322 time: 0.0015s\n",
      "Epoch: 00827 loss_train: 1.1928 loss_rec: 1.1928 acc_train: 0.4271 loss_val: 1.1961 acc_val: 0.4322 time: 0.0020s\n",
      "Epoch: 00828 loss_train: 1.1927 loss_rec: 1.1927 acc_train: 0.4271 loss_val: 1.1960 acc_val: 0.4322 time: 0.0015s\n",
      "Epoch: 00829 loss_train: 1.1926 loss_rec: 1.1926 acc_train: 0.4265 loss_val: 1.1959 acc_val: 0.4309 time: 0.0015s\n",
      "Epoch: 00830 loss_train: 1.1925 loss_rec: 1.1925 acc_train: 0.4265 loss_val: 1.1958 acc_val: 0.4309 time: 0.0020s\n",
      "Epoch: 00831 loss_train: 1.1923 loss_rec: 1.1923 acc_train: 0.4265 loss_val: 1.1957 acc_val: 0.4309 time: 0.0015s\n",
      "Test set results: loss= 1.2067 accuracy= 0.3631\n",
      "Epoch: 00832 loss_train: 1.1922 loss_rec: 1.1922 acc_train: 0.4262 loss_val: 1.1956 acc_val: 0.4303 time: 0.0015s\n",
      "Epoch: 00833 loss_train: 1.1921 loss_rec: 1.1921 acc_train: 0.4262 loss_val: 1.1955 acc_val: 0.4303 time: 0.0020s\n",
      "Epoch: 00834 loss_train: 1.1920 loss_rec: 1.1920 acc_train: 0.4263 loss_val: 1.1954 acc_val: 0.4303 time: 0.0015s\n",
      "Epoch: 00835 loss_train: 1.1919 loss_rec: 1.1919 acc_train: 0.4268 loss_val: 1.1953 acc_val: 0.4306 time: 0.0015s\n",
      "Epoch: 00836 loss_train: 1.1918 loss_rec: 1.1918 acc_train: 0.4268 loss_val: 1.1952 acc_val: 0.4306 time: 0.0015s\n",
      "Epoch: 00837 loss_train: 1.1917 loss_rec: 1.1917 acc_train: 0.4268 loss_val: 1.1951 acc_val: 0.4306 time: 0.0015s\n",
      "Epoch: 00838 loss_train: 1.1916 loss_rec: 1.1916 acc_train: 0.4267 loss_val: 1.1950 acc_val: 0.4306 time: 0.0015s\n",
      "Epoch: 00839 loss_train: 1.1915 loss_rec: 1.1915 acc_train: 0.4267 loss_val: 1.1949 acc_val: 0.4306 time: 0.0020s\n",
      "Epoch: 00840 loss_train: 1.1914 loss_rec: 1.1914 acc_train: 0.4267 loss_val: 1.1948 acc_val: 0.4306 time: 0.0015s\n",
      "Epoch: 00841 loss_train: 1.1913 loss_rec: 1.1913 acc_train: 0.4264 loss_val: 1.1946 acc_val: 0.4303 time: 0.0015s\n",
      "Test set results: loss= 1.2060 accuracy= 0.3628\n",
      "Epoch: 00842 loss_train: 1.1911 loss_rec: 1.1911 acc_train: 0.4264 loss_val: 1.1945 acc_val: 0.4303 time: 0.0020s\n",
      "Epoch: 00843 loss_train: 1.1910 loss_rec: 1.1910 acc_train: 0.4264 loss_val: 1.1944 acc_val: 0.4303 time: 0.0015s\n",
      "Epoch: 00844 loss_train: 1.1909 loss_rec: 1.1909 acc_train: 0.4264 loss_val: 1.1943 acc_val: 0.4303 time: 0.0015s\n",
      "Epoch: 00845 loss_train: 1.1908 loss_rec: 1.1908 acc_train: 0.4264 loss_val: 1.1942 acc_val: 0.4306 time: 0.0020s\n",
      "Epoch: 00846 loss_train: 1.1907 loss_rec: 1.1907 acc_train: 0.4264 loss_val: 1.1941 acc_val: 0.4306 time: 0.0015s\n",
      "Epoch: 00847 loss_train: 1.1906 loss_rec: 1.1906 acc_train: 0.4264 loss_val: 1.1940 acc_val: 0.4306 time: 0.0015s\n",
      "Epoch: 00848 loss_train: 1.1905 loss_rec: 1.1905 acc_train: 0.4265 loss_val: 1.1939 acc_val: 0.4306 time: 0.0015s\n",
      "Epoch: 00849 loss_train: 1.1904 loss_rec: 1.1904 acc_train: 0.4266 loss_val: 1.1938 acc_val: 0.4306 time: 0.0020s\n",
      "Epoch: 00850 loss_train: 1.1903 loss_rec: 1.1903 acc_train: 0.4266 loss_val: 1.1937 acc_val: 0.4306 time: 0.0015s\n",
      "Epoch: 00851 loss_train: 1.1902 loss_rec: 1.1902 acc_train: 0.4268 loss_val: 1.1936 acc_val: 0.4309 time: 0.0015s\n",
      "Test set results: loss= 1.2054 accuracy= 0.3642\n",
      "Epoch: 00852 loss_train: 1.1901 loss_rec: 1.1901 acc_train: 0.4273 loss_val: 1.1935 acc_val: 0.4309 time: 0.0020s\n",
      "Epoch: 00853 loss_train: 1.1900 loss_rec: 1.1900 acc_train: 0.4273 loss_val: 1.1934 acc_val: 0.4309 time: 0.0015s\n",
      "Epoch: 00854 loss_train: 1.1899 loss_rec: 1.1899 acc_train: 0.4273 loss_val: 1.1933 acc_val: 0.4309 time: 0.0015s\n",
      "Epoch: 00855 loss_train: 1.1898 loss_rec: 1.1898 acc_train: 0.4273 loss_val: 1.1932 acc_val: 0.4309 time: 0.0020s\n",
      "Epoch: 00856 loss_train: 1.1896 loss_rec: 1.1896 acc_train: 0.4273 loss_val: 1.1931 acc_val: 0.4309 time: 0.0015s\n",
      "Epoch: 00857 loss_train: 1.1895 loss_rec: 1.1895 acc_train: 0.4263 loss_val: 1.1930 acc_val: 0.4300 time: 0.0015s\n",
      "Epoch: 00858 loss_train: 1.1894 loss_rec: 1.1894 acc_train: 0.4263 loss_val: 1.1929 acc_val: 0.4300 time: 0.0020s\n",
      "Epoch: 00859 loss_train: 1.1893 loss_rec: 1.1893 acc_train: 0.4263 loss_val: 1.1928 acc_val: 0.4300 time: 0.0020s\n",
      "Epoch: 00860 loss_train: 1.1892 loss_rec: 1.1892 acc_train: 0.4263 loss_val: 1.1927 acc_val: 0.4300 time: 0.0015s\n",
      "Epoch: 00861 loss_train: 1.1891 loss_rec: 1.1891 acc_train: 0.4263 loss_val: 1.1926 acc_val: 0.4300 time: 0.0015s\n",
      "Test set results: loss= 1.2048 accuracy= 0.3624\n",
      "Epoch: 00862 loss_train: 1.1890 loss_rec: 1.1890 acc_train: 0.4263 loss_val: 1.1925 acc_val: 0.4300 time: 0.0020s\n",
      "Epoch: 00863 loss_train: 1.1889 loss_rec: 1.1889 acc_train: 0.4299 loss_val: 1.1925 acc_val: 0.4322 time: 0.0015s\n",
      "Epoch: 00864 loss_train: 1.1888 loss_rec: 1.1888 acc_train: 0.4303 loss_val: 1.1924 acc_val: 0.4325 time: 0.0015s\n",
      "Epoch: 00865 loss_train: 1.1887 loss_rec: 1.1887 acc_train: 0.4304 loss_val: 1.1923 acc_val: 0.4328 time: 0.0015s\n",
      "Epoch: 00866 loss_train: 1.1886 loss_rec: 1.1886 acc_train: 0.4304 loss_val: 1.1922 acc_val: 0.4328 time: 0.0020s\n",
      "Epoch: 00867 loss_train: 1.1885 loss_rec: 1.1885 acc_train: 0.4339 loss_val: 1.1921 acc_val: 0.4378 time: 0.0020s\n",
      "Epoch: 00868 loss_train: 1.1884 loss_rec: 1.1884 acc_train: 0.4339 loss_val: 1.1920 acc_val: 0.4378 time: 0.0015s\n",
      "Epoch: 00869 loss_train: 1.1883 loss_rec: 1.1883 acc_train: 0.4339 loss_val: 1.1919 acc_val: 0.4378 time: 0.0020s\n",
      "Epoch: 00870 loss_train: 1.1882 loss_rec: 1.1882 acc_train: 0.4339 loss_val: 1.1918 acc_val: 0.4378 time: 0.0015s\n",
      "Epoch: 00871 loss_train: 1.1881 loss_rec: 1.1881 acc_train: 0.4339 loss_val: 1.1917 acc_val: 0.4378 time: 0.0015s\n",
      "Test set results: loss= 1.2042 accuracy= 0.3665\n",
      "Epoch: 00872 loss_train: 1.1880 loss_rec: 1.1880 acc_train: 0.4339 loss_val: 1.1916 acc_val: 0.4378 time: 0.0020s\n",
      "Epoch: 00873 loss_train: 1.1879 loss_rec: 1.1879 acc_train: 0.4339 loss_val: 1.1915 acc_val: 0.4378 time: 0.0015s\n",
      "Epoch: 00874 loss_train: 1.1878 loss_rec: 1.1878 acc_train: 0.4339 loss_val: 1.1914 acc_val: 0.4378 time: 0.0025s\n",
      "Epoch: 00875 loss_train: 1.1877 loss_rec: 1.1877 acc_train: 0.4338 loss_val: 1.1913 acc_val: 0.4378 time: 0.0020s\n",
      "Epoch: 00876 loss_train: 1.1876 loss_rec: 1.1876 acc_train: 0.4337 loss_val: 1.1912 acc_val: 0.4378 time: 0.0030s\n",
      "Epoch: 00877 loss_train: 1.1875 loss_rec: 1.1875 acc_train: 0.4336 loss_val: 1.1911 acc_val: 0.4378 time: 0.0025s\n",
      "Epoch: 00878 loss_train: 1.1874 loss_rec: 1.1874 acc_train: 0.4337 loss_val: 1.1910 acc_val: 0.4378 time: 0.0020s\n",
      "Epoch: 00879 loss_train: 1.1873 loss_rec: 1.1873 acc_train: 0.4338 loss_val: 1.1909 acc_val: 0.4378 time: 0.0015s\n",
      "Epoch: 00880 loss_train: 1.1872 loss_rec: 1.1872 acc_train: 0.4338 loss_val: 1.1908 acc_val: 0.4378 time: 0.0020s\n",
      "Epoch: 00881 loss_train: 1.1871 loss_rec: 1.1871 acc_train: 0.4341 loss_val: 1.1907 acc_val: 0.4378 time: 0.0015s\n",
      "Test set results: loss= 1.2036 accuracy= 0.3675\n",
      "Epoch: 00882 loss_train: 1.1870 loss_rec: 1.1870 acc_train: 0.4344 loss_val: 1.1906 acc_val: 0.4378 time: 0.0020s\n",
      "Epoch: 00883 loss_train: 1.1869 loss_rec: 1.1869 acc_train: 0.4329 loss_val: 1.1905 acc_val: 0.4353 time: 0.0035s\n",
      "Epoch: 00884 loss_train: 1.1868 loss_rec: 1.1868 acc_train: 0.4329 loss_val: 1.1904 acc_val: 0.4353 time: 0.0025s\n",
      "Epoch: 00885 loss_train: 1.1867 loss_rec: 1.1867 acc_train: 0.4331 loss_val: 1.1903 acc_val: 0.4353 time: 0.0020s\n",
      "Epoch: 00886 loss_train: 1.1866 loss_rec: 1.1866 acc_train: 0.4331 loss_val: 1.1902 acc_val: 0.4350 time: 0.0025s\n",
      "Epoch: 00887 loss_train: 1.1865 loss_rec: 1.1865 acc_train: 0.4331 loss_val: 1.1902 acc_val: 0.4350 time: 0.0020s\n",
      "Epoch: 00888 loss_train: 1.1864 loss_rec: 1.1864 acc_train: 0.4331 loss_val: 1.1901 acc_val: 0.4350 time: 0.0025s\n",
      "Epoch: 00889 loss_train: 1.1863 loss_rec: 1.1863 acc_train: 0.4330 loss_val: 1.1900 acc_val: 0.4350 time: 0.0015s\n",
      "Epoch: 00890 loss_train: 1.1862 loss_rec: 1.1862 acc_train: 0.4330 loss_val: 1.1899 acc_val: 0.4350 time: 0.0030s\n",
      "Epoch: 00891 loss_train: 1.1861 loss_rec: 1.1861 acc_train: 0.4330 loss_val: 1.1898 acc_val: 0.4350 time: 0.0020s\n",
      "Test set results: loss= 1.2031 accuracy= 0.3656\n",
      "Epoch: 00892 loss_train: 1.1860 loss_rec: 1.1860 acc_train: 0.4330 loss_val: 1.1897 acc_val: 0.4350 time: 0.0015s\n",
      "Epoch: 00893 loss_train: 1.1859 loss_rec: 1.1859 acc_train: 0.4330 loss_val: 1.1896 acc_val: 0.4350 time: 0.0030s\n",
      "Epoch: 00894 loss_train: 1.1858 loss_rec: 1.1858 acc_train: 0.4331 loss_val: 1.1895 acc_val: 0.4350 time: 0.0015s\n",
      "Epoch: 00895 loss_train: 1.1857 loss_rec: 1.1857 acc_train: 0.4331 loss_val: 1.1894 acc_val: 0.4344 time: 0.0025s\n",
      "Epoch: 00896 loss_train: 1.1856 loss_rec: 1.1856 acc_train: 0.4331 loss_val: 1.1893 acc_val: 0.4344 time: 0.0020s\n",
      "Epoch: 00897 loss_train: 1.1855 loss_rec: 1.1855 acc_train: 0.4331 loss_val: 1.1892 acc_val: 0.4344 time: 0.0025s\n",
      "Epoch: 00898 loss_train: 1.1854 loss_rec: 1.1854 acc_train: 0.4331 loss_val: 1.1891 acc_val: 0.4344 time: 0.0030s\n",
      "Epoch: 00899 loss_train: 1.1853 loss_rec: 1.1853 acc_train: 0.4331 loss_val: 1.1890 acc_val: 0.4344 time: 0.0015s\n",
      "Epoch: 00900 loss_train: 1.1852 loss_rec: 1.1852 acc_train: 0.4322 loss_val: 1.1889 acc_val: 0.4341 time: 0.0025s\n",
      "Epoch: 00901 loss_train: 1.1851 loss_rec: 1.1851 acc_train: 0.4334 loss_val: 1.1889 acc_val: 0.4350 time: 0.0020s\n",
      "Test set results: loss= 1.2026 accuracy= 0.3670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00902 loss_train: 1.1850 loss_rec: 1.1850 acc_train: 0.4347 loss_val: 1.1888 acc_val: 0.4363 time: 0.0025s\n",
      "Epoch: 00903 loss_train: 1.1849 loss_rec: 1.1849 acc_train: 0.4347 loss_val: 1.1887 acc_val: 0.4363 time: 0.0030s\n",
      "Epoch: 00904 loss_train: 1.1848 loss_rec: 1.1848 acc_train: 0.4347 loss_val: 1.1886 acc_val: 0.4363 time: 0.0025s\n",
      "Epoch: 00905 loss_train: 1.1847 loss_rec: 1.1847 acc_train: 0.4348 loss_val: 1.1885 acc_val: 0.4363 time: 0.0020s\n",
      "Epoch: 00906 loss_train: 1.1846 loss_rec: 1.1846 acc_train: 0.4348 loss_val: 1.1884 acc_val: 0.4363 time: 0.0025s\n",
      "Epoch: 00907 loss_train: 1.1845 loss_rec: 1.1845 acc_train: 0.4346 loss_val: 1.1883 acc_val: 0.4363 time: 0.0020s\n",
      "Epoch: 00908 loss_train: 1.1844 loss_rec: 1.1844 acc_train: 0.4346 loss_val: 1.1882 acc_val: 0.4363 time: 0.0025s\n",
      "Epoch: 00909 loss_train: 1.1843 loss_rec: 1.1843 acc_train: 0.4346 loss_val: 1.1881 acc_val: 0.4363 time: 0.0020s\n",
      "Epoch: 00910 loss_train: 1.1842 loss_rec: 1.1842 acc_train: 0.4346 loss_val: 1.1880 acc_val: 0.4363 time: 0.0030s\n",
      "Epoch: 00911 loss_train: 1.1841 loss_rec: 1.1841 acc_train: 0.4346 loss_val: 1.1879 acc_val: 0.4363 time: 0.0030s\n",
      "Test set results: loss= 1.2020 accuracy= 0.3671\n",
      "Epoch: 00912 loss_train: 1.1840 loss_rec: 1.1840 acc_train: 0.4346 loss_val: 1.1879 acc_val: 0.4363 time: 0.0015s\n",
      "Epoch: 00913 loss_train: 1.1839 loss_rec: 1.1839 acc_train: 0.4346 loss_val: 1.1878 acc_val: 0.4363 time: 0.0020s\n",
      "Epoch: 00914 loss_train: 1.1838 loss_rec: 1.1838 acc_train: 0.4346 loss_val: 1.1877 acc_val: 0.4363 time: 0.0015s\n",
      "Epoch: 00915 loss_train: 1.1837 loss_rec: 1.1837 acc_train: 0.4346 loss_val: 1.1876 acc_val: 0.4363 time: 0.0025s\n",
      "Epoch: 00916 loss_train: 1.1836 loss_rec: 1.1836 acc_train: 0.4346 loss_val: 1.1875 acc_val: 0.4363 time: 0.0015s\n",
      "Epoch: 00917 loss_train: 1.1835 loss_rec: 1.1835 acc_train: 0.4347 loss_val: 1.1874 acc_val: 0.4363 time: 0.0025s\n",
      "Epoch: 00918 loss_train: 1.1834 loss_rec: 1.1834 acc_train: 0.4347 loss_val: 1.1873 acc_val: 0.4363 time: 0.0020s\n",
      "Epoch: 00919 loss_train: 1.1833 loss_rec: 1.1833 acc_train: 0.4348 loss_val: 1.1872 acc_val: 0.4366 time: 0.0015s\n",
      "Epoch: 00920 loss_train: 1.1832 loss_rec: 1.1832 acc_train: 0.4348 loss_val: 1.1871 acc_val: 0.4366 time: 0.0020s\n",
      "Epoch: 00921 loss_train: 1.1831 loss_rec: 1.1831 acc_train: 0.4346 loss_val: 1.1871 acc_val: 0.4356 time: 0.0025s\n",
      "Test set results: loss= 1.2015 accuracy= 0.3665\n",
      "Epoch: 00922 loss_train: 1.1830 loss_rec: 1.1830 acc_train: 0.4345 loss_val: 1.1870 acc_val: 0.4356 time: 0.0030s\n",
      "Epoch: 00923 loss_train: 1.1829 loss_rec: 1.1829 acc_train: 0.4354 loss_val: 1.1869 acc_val: 0.4359 time: 0.0015s\n",
      "Epoch: 00924 loss_train: 1.1828 loss_rec: 1.1828 acc_train: 0.4354 loss_val: 1.1868 acc_val: 0.4359 time: 0.0030s\n",
      "Epoch: 00925 loss_train: 1.1827 loss_rec: 1.1827 acc_train: 0.4354 loss_val: 1.1867 acc_val: 0.4359 time: 0.0020s\n",
      "Epoch: 00926 loss_train: 1.1826 loss_rec: 1.1826 acc_train: 0.4354 loss_val: 1.1866 acc_val: 0.4359 time: 0.0015s\n",
      "Epoch: 00927 loss_train: 1.1825 loss_rec: 1.1825 acc_train: 0.4338 loss_val: 1.1865 acc_val: 0.4341 time: 0.0020s\n",
      "Epoch: 00928 loss_train: 1.1824 loss_rec: 1.1824 acc_train: 0.4339 loss_val: 1.1864 acc_val: 0.4341 time: 0.0025s\n",
      "Epoch: 00929 loss_train: 1.1823 loss_rec: 1.1823 acc_train: 0.4339 loss_val: 1.1864 acc_val: 0.4341 time: 0.0015s\n",
      "Epoch: 00930 loss_train: 1.1823 loss_rec: 1.1823 acc_train: 0.4339 loss_val: 1.1863 acc_val: 0.4341 time: 0.0025s\n",
      "Epoch: 00931 loss_train: 1.1822 loss_rec: 1.1822 acc_train: 0.4339 loss_val: 1.1862 acc_val: 0.4341 time: 0.0020s\n",
      "Test set results: loss= 1.2010 accuracy= 0.3663\n",
      "Epoch: 00932 loss_train: 1.1821 loss_rec: 1.1821 acc_train: 0.4339 loss_val: 1.1861 acc_val: 0.4341 time: 0.0015s\n",
      "Epoch: 00933 loss_train: 1.1820 loss_rec: 1.1820 acc_train: 0.4339 loss_val: 1.1860 acc_val: 0.4341 time: 0.0020s\n",
      "Epoch: 00934 loss_train: 1.1819 loss_rec: 1.1819 acc_train: 0.4339 loss_val: 1.1859 acc_val: 0.4341 time: 0.0020s\n",
      "Epoch: 00935 loss_train: 1.1818 loss_rec: 1.1818 acc_train: 0.4339 loss_val: 1.1858 acc_val: 0.4341 time: 0.0025s\n",
      "Epoch: 00936 loss_train: 1.1817 loss_rec: 1.1817 acc_train: 0.4339 loss_val: 1.1858 acc_val: 0.4341 time: 0.0020s\n",
      "Epoch: 00937 loss_train: 1.1816 loss_rec: 1.1816 acc_train: 0.4339 loss_val: 1.1857 acc_val: 0.4338 time: 0.0025s\n",
      "Epoch: 00938 loss_train: 1.1815 loss_rec: 1.1815 acc_train: 0.4339 loss_val: 1.1856 acc_val: 0.4338 time: 0.0025s\n",
      "Epoch: 00939 loss_train: 1.1814 loss_rec: 1.1814 acc_train: 0.4339 loss_val: 1.1855 acc_val: 0.4338 time: 0.0015s\n",
      "Epoch: 00940 loss_train: 1.1813 loss_rec: 1.1813 acc_train: 0.4339 loss_val: 1.1854 acc_val: 0.4338 time: 0.0020s\n",
      "Epoch: 00941 loss_train: 1.1812 loss_rec: 1.1812 acc_train: 0.4339 loss_val: 1.1853 acc_val: 0.4338 time: 0.0020s\n",
      "Test set results: loss= 1.2005 accuracy= 0.3664\n",
      "Epoch: 00942 loss_train: 1.1811 loss_rec: 1.1811 acc_train: 0.4339 loss_val: 1.1852 acc_val: 0.4338 time: 0.0025s\n",
      "Epoch: 00943 loss_train: 1.1810 loss_rec: 1.1810 acc_train: 0.4341 loss_val: 1.1852 acc_val: 0.4341 time: 0.0020s\n",
      "Epoch: 00944 loss_train: 1.1809 loss_rec: 1.1809 acc_train: 0.4341 loss_val: 1.1851 acc_val: 0.4341 time: 0.0030s\n",
      "Epoch: 00945 loss_train: 1.1808 loss_rec: 1.1808 acc_train: 0.4336 loss_val: 1.1850 acc_val: 0.4338 time: 0.0015s\n",
      "Epoch: 00946 loss_train: 1.1808 loss_rec: 1.1808 acc_train: 0.4336 loss_val: 1.1849 acc_val: 0.4341 time: 0.0025s\n",
      "Epoch: 00947 loss_train: 1.1807 loss_rec: 1.1807 acc_train: 0.4341 loss_val: 1.1848 acc_val: 0.4350 time: 0.0020s\n",
      "Epoch: 00948 loss_train: 1.1806 loss_rec: 1.1806 acc_train: 0.4341 loss_val: 1.1847 acc_val: 0.4350 time: 0.0020s\n",
      "Epoch: 00949 loss_train: 1.1805 loss_rec: 1.1805 acc_train: 0.4341 loss_val: 1.1847 acc_val: 0.4350 time: 0.0015s\n",
      "Epoch: 00950 loss_train: 1.1804 loss_rec: 1.1804 acc_train: 0.4342 loss_val: 1.1846 acc_val: 0.4356 time: 0.0025s\n",
      "Epoch: 00951 loss_train: 1.1803 loss_rec: 1.1803 acc_train: 0.4342 loss_val: 1.1845 acc_val: 0.4356 time: 0.0015s\n",
      "Test set results: loss= 1.1999 accuracy= 0.3674\n",
      "Epoch: 00952 loss_train: 1.1802 loss_rec: 1.1802 acc_train: 0.4342 loss_val: 1.1844 acc_val: 0.4356 time: 0.0015s\n",
      "Epoch: 00953 loss_train: 1.1801 loss_rec: 1.1801 acc_train: 0.4342 loss_val: 1.1843 acc_val: 0.4356 time: 0.0025s\n",
      "Epoch: 00954 loss_train: 1.1800 loss_rec: 1.1800 acc_train: 0.4341 loss_val: 1.1842 acc_val: 0.4356 time: 0.0015s\n",
      "Epoch: 00955 loss_train: 1.1799 loss_rec: 1.1799 acc_train: 0.4341 loss_val: 1.1842 acc_val: 0.4356 time: 0.0025s\n",
      "Epoch: 00956 loss_train: 1.1798 loss_rec: 1.1798 acc_train: 0.4341 loss_val: 1.1841 acc_val: 0.4356 time: 0.0015s\n",
      "Epoch: 00957 loss_train: 1.1797 loss_rec: 1.1797 acc_train: 0.4341 loss_val: 1.1840 acc_val: 0.4356 time: 0.0025s\n",
      "Epoch: 00958 loss_train: 1.1797 loss_rec: 1.1797 acc_train: 0.4342 loss_val: 1.1839 acc_val: 0.4356 time: 0.0020s\n",
      "Epoch: 00959 loss_train: 1.1796 loss_rec: 1.1796 acc_train: 0.4341 loss_val: 1.1838 acc_val: 0.4359 time: 0.0030s\n",
      "Epoch: 00960 loss_train: 1.1795 loss_rec: 1.1795 acc_train: 0.4341 loss_val: 1.1837 acc_val: 0.4359 time: 0.0020s\n",
      "Epoch: 00961 loss_train: 1.1794 loss_rec: 1.1794 acc_train: 0.4341 loss_val: 1.1837 acc_val: 0.4359 time: 0.0020s\n",
      "Test set results: loss= 1.1995 accuracy= 0.3673\n",
      "Epoch: 00962 loss_train: 1.1793 loss_rec: 1.1793 acc_train: 0.4341 loss_val: 1.1836 acc_val: 0.4356 time: 0.0025s\n",
      "Epoch: 00963 loss_train: 1.1792 loss_rec: 1.1792 acc_train: 0.4341 loss_val: 1.1835 acc_val: 0.4356 time: 0.0020s\n",
      "Epoch: 00964 loss_train: 1.1791 loss_rec: 1.1791 acc_train: 0.4341 loss_val: 1.1834 acc_val: 0.4356 time: 0.0025s\n",
      "Epoch: 00965 loss_train: 1.1790 loss_rec: 1.1790 acc_train: 0.4341 loss_val: 1.1833 acc_val: 0.4356 time: 0.0020s\n",
      "Epoch: 00966 loss_train: 1.1789 loss_rec: 1.1789 acc_train: 0.4341 loss_val: 1.1832 acc_val: 0.4356 time: 0.0025s\n",
      "Epoch: 00967 loss_train: 1.1788 loss_rec: 1.1788 acc_train: 0.4340 loss_val: 1.1832 acc_val: 0.4356 time: 0.0015s\n",
      "Epoch: 00968 loss_train: 1.1788 loss_rec: 1.1788 acc_train: 0.4340 loss_val: 1.1831 acc_val: 0.4356 time: 0.0025s\n",
      "Epoch: 00969 loss_train: 1.1787 loss_rec: 1.1787 acc_train: 0.4333 loss_val: 1.1830 acc_val: 0.4353 time: 0.0025s\n",
      "Epoch: 00970 loss_train: 1.1786 loss_rec: 1.1786 acc_train: 0.4333 loss_val: 1.1829 acc_val: 0.4353 time: 0.0020s\n",
      "Epoch: 00971 loss_train: 1.1785 loss_rec: 1.1785 acc_train: 0.4333 loss_val: 1.1828 acc_val: 0.4353 time: 0.0020s\n",
      "Test set results: loss= 1.1990 accuracy= 0.3678\n",
      "Epoch: 00972 loss_train: 1.1784 loss_rec: 1.1784 acc_train: 0.4344 loss_val: 1.1828 acc_val: 0.4363 time: 0.0015s\n",
      "Epoch: 00973 loss_train: 1.1783 loss_rec: 1.1783 acc_train: 0.4343 loss_val: 1.1827 acc_val: 0.4363 time: 0.0030s\n",
      "Epoch: 00974 loss_train: 1.1782 loss_rec: 1.1782 acc_train: 0.4343 loss_val: 1.1826 acc_val: 0.4363 time: 0.0025s\n",
      "Epoch: 00975 loss_train: 1.1781 loss_rec: 1.1781 acc_train: 0.4343 loss_val: 1.1825 acc_val: 0.4363 time: 0.0020s\n",
      "Epoch: 00976 loss_train: 1.1780 loss_rec: 1.1780 acc_train: 0.4343 loss_val: 1.1824 acc_val: 0.4363 time: 0.0020s\n",
      "Epoch: 00977 loss_train: 1.1780 loss_rec: 1.1780 acc_train: 0.4346 loss_val: 1.1824 acc_val: 0.4366 time: 0.0015s\n",
      "Epoch: 00978 loss_train: 1.1779 loss_rec: 1.1779 acc_train: 0.4346 loss_val: 1.1823 acc_val: 0.4366 time: 0.0020s\n",
      "Epoch: 00979 loss_train: 1.1778 loss_rec: 1.1778 acc_train: 0.4346 loss_val: 1.1822 acc_val: 0.4363 time: 0.0025s\n",
      "Epoch: 00980 loss_train: 1.1777 loss_rec: 1.1777 acc_train: 0.4346 loss_val: 1.1821 acc_val: 0.4363 time: 0.0015s\n",
      "Epoch: 00981 loss_train: 1.1776 loss_rec: 1.1776 acc_train: 0.4347 loss_val: 1.1820 acc_val: 0.4363 time: 0.0030s\n",
      "Test set results: loss= 1.1984 accuracy= 0.3681\n",
      "Epoch: 00982 loss_train: 1.1775 loss_rec: 1.1775 acc_train: 0.4348 loss_val: 1.1820 acc_val: 0.4366 time: 0.0025s\n",
      "Epoch: 00983 loss_train: 1.1774 loss_rec: 1.1774 acc_train: 0.4348 loss_val: 1.1819 acc_val: 0.4366 time: 0.0020s\n",
      "Epoch: 00984 loss_train: 1.1773 loss_rec: 1.1773 acc_train: 0.4348 loss_val: 1.1818 acc_val: 0.4366 time: 0.0020s\n",
      "Epoch: 00985 loss_train: 1.1773 loss_rec: 1.1773 acc_train: 0.4348 loss_val: 1.1817 acc_val: 0.4366 time: 0.0015s\n",
      "Epoch: 00986 loss_train: 1.1772 loss_rec: 1.1772 acc_train: 0.4344 loss_val: 1.1816 acc_val: 0.4366 time: 0.0025s\n",
      "Epoch: 00987 loss_train: 1.1771 loss_rec: 1.1771 acc_train: 0.4344 loss_val: 1.1816 acc_val: 0.4366 time: 0.0015s\n",
      "Epoch: 00988 loss_train: 1.1770 loss_rec: 1.1770 acc_train: 0.4343 loss_val: 1.1815 acc_val: 0.4366 time: 0.0030s\n",
      "Epoch: 00989 loss_train: 1.1769 loss_rec: 1.1769 acc_train: 0.4343 loss_val: 1.1814 acc_val: 0.4363 time: 0.0020s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00990 loss_train: 1.1768 loss_rec: 1.1768 acc_train: 0.4343 loss_val: 1.1813 acc_val: 0.4363 time: 0.0025s\n",
      "Epoch: 00991 loss_train: 1.1767 loss_rec: 1.1767 acc_train: 0.4343 loss_val: 1.1813 acc_val: 0.4363 time: 0.0025s\n",
      "Test set results: loss= 1.1980 accuracy= 0.3688\n",
      "Epoch: 00992 loss_train: 1.1766 loss_rec: 1.1766 acc_train: 0.4353 loss_val: 1.1812 acc_val: 0.4369 time: 0.0030s\n",
      "Epoch: 00993 loss_train: 1.1766 loss_rec: 1.1766 acc_train: 0.4353 loss_val: 1.1811 acc_val: 0.4369 time: 0.0015s\n",
      "Epoch: 00994 loss_train: 1.1765 loss_rec: 1.1765 acc_train: 0.4353 loss_val: 1.1810 acc_val: 0.4369 time: 0.0030s\n",
      "Epoch: 00995 loss_train: 1.1764 loss_rec: 1.1764 acc_train: 0.4353 loss_val: 1.1809 acc_val: 0.4356 time: 0.0015s\n",
      "Epoch: 00996 loss_train: 1.1763 loss_rec: 1.1763 acc_train: 0.4353 loss_val: 1.1809 acc_val: 0.4356 time: 0.0025s\n",
      "Epoch: 00997 loss_train: 1.1762 loss_rec: 1.1762 acc_train: 0.4353 loss_val: 1.1808 acc_val: 0.4356 time: 0.0020s\n",
      "Epoch: 00998 loss_train: 1.1761 loss_rec: 1.1761 acc_train: 0.4353 loss_val: 1.1807 acc_val: 0.4356 time: 0.0025s\n",
      "Epoch: 00999 loss_train: 1.1760 loss_rec: 1.1760 acc_train: 0.4353 loss_val: 1.1806 acc_val: 0.4356 time: 0.0015s\n",
      "Epoch: 01000 loss_train: 1.1760 loss_rec: 1.1760 acc_train: 0.4352 loss_val: 1.1806 acc_val: 0.4356 time: 0.0025s\n",
      "Epoch: 01001 loss_train: 1.1759 loss_rec: 1.1759 acc_train: 0.4352 loss_val: 1.1805 acc_val: 0.4356 time: 0.0015s\n",
      "Test set results: loss= 1.1975 accuracy= 0.3672\n",
      "Epoch: 01002 loss_train: 1.1758 loss_rec: 1.1758 acc_train: 0.4352 loss_val: 1.1804 acc_val: 0.4356 time: 0.0025s\n",
      "Epoch: 01003 loss_train: 1.1757 loss_rec: 1.1757 acc_train: 0.4353 loss_val: 1.1803 acc_val: 0.4356 time: 0.0020s\n",
      "Epoch: 01004 loss_train: 1.1756 loss_rec: 1.1756 acc_train: 0.4357 loss_val: 1.1803 acc_val: 0.4356 time: 0.0025s\n",
      "Epoch: 01005 loss_train: 1.1755 loss_rec: 1.1755 acc_train: 0.4357 loss_val: 1.1802 acc_val: 0.4356 time: 0.0015s\n",
      "Epoch: 01006 loss_train: 1.1755 loss_rec: 1.1755 acc_train: 0.4358 loss_val: 1.1801 acc_val: 0.4356 time: 0.0020s\n",
      "Epoch: 01007 loss_train: 1.1754 loss_rec: 1.1754 acc_train: 0.4357 loss_val: 1.1800 acc_val: 0.4356 time: 0.0015s\n",
      "Epoch: 01008 loss_train: 1.1753 loss_rec: 1.1753 acc_train: 0.4357 loss_val: 1.1800 acc_val: 0.4356 time: 0.0020s\n",
      "Epoch: 01009 loss_train: 1.1752 loss_rec: 1.1752 acc_train: 0.4357 loss_val: 1.1799 acc_val: 0.4356 time: 0.0025s\n",
      "Epoch: 01010 loss_train: 1.1751 loss_rec: 1.1751 acc_train: 0.4361 loss_val: 1.1798 acc_val: 0.4356 time: 0.0020s\n",
      "Epoch: 01011 loss_train: 1.1750 loss_rec: 1.1750 acc_train: 0.4361 loss_val: 1.1797 acc_val: 0.4356 time: 0.0015s\n",
      "Test set results: loss= 1.1969 accuracy= 0.3688\n",
      "Epoch: 01012 loss_train: 1.1749 loss_rec: 1.1749 acc_train: 0.4362 loss_val: 1.1797 acc_val: 0.4369 time: 0.0025s\n",
      "Epoch: 01013 loss_train: 1.1749 loss_rec: 1.1749 acc_train: 0.4362 loss_val: 1.1796 acc_val: 0.4369 time: 0.0015s\n",
      "Epoch: 01014 loss_train: 1.1748 loss_rec: 1.1748 acc_train: 0.4362 loss_val: 1.1795 acc_val: 0.4369 time: 0.0025s\n",
      "Epoch: 01015 loss_train: 1.1747 loss_rec: 1.1747 acc_train: 0.4372 loss_val: 1.1794 acc_val: 0.4372 time: 0.0020s\n",
      "Epoch: 01016 loss_train: 1.1746 loss_rec: 1.1746 acc_train: 0.4372 loss_val: 1.1794 acc_val: 0.4372 time: 0.0030s\n",
      "Epoch: 01017 loss_train: 1.1745 loss_rec: 1.1745 acc_train: 0.4373 loss_val: 1.1793 acc_val: 0.4372 time: 0.0015s\n",
      "Epoch: 01018 loss_train: 1.1745 loss_rec: 1.1745 acc_train: 0.4373 loss_val: 1.1792 acc_val: 0.4372 time: 0.0025s\n",
      "Epoch: 01019 loss_train: 1.1744 loss_rec: 1.1744 acc_train: 0.4373 loss_val: 1.1791 acc_val: 0.4372 time: 0.0020s\n",
      "Epoch: 01020 loss_train: 1.1743 loss_rec: 1.1743 acc_train: 0.4373 loss_val: 1.1791 acc_val: 0.4372 time: 0.0025s\n",
      "Epoch: 01021 loss_train: 1.1742 loss_rec: 1.1742 acc_train: 0.4371 loss_val: 1.1790 acc_val: 0.4372 time: 0.0015s\n",
      "Test set results: loss= 1.1964 accuracy= 0.3752\n",
      "Epoch: 01022 loss_train: 1.1741 loss_rec: 1.1741 acc_train: 0.4441 loss_val: 1.1789 acc_val: 0.4434 time: 0.0015s\n",
      "Epoch: 01023 loss_train: 1.1740 loss_rec: 1.1740 acc_train: 0.4441 loss_val: 1.1788 acc_val: 0.4434 time: 0.0030s\n",
      "Epoch: 01024 loss_train: 1.1740 loss_rec: 1.1740 acc_train: 0.4441 loss_val: 1.1788 acc_val: 0.4434 time: 0.0020s\n",
      "Epoch: 01025 loss_train: 1.1739 loss_rec: 1.1739 acc_train: 0.4441 loss_val: 1.1787 acc_val: 0.4434 time: 0.0025s\n",
      "Epoch: 01026 loss_train: 1.1738 loss_rec: 1.1738 acc_train: 0.4441 loss_val: 1.1786 acc_val: 0.4438 time: 0.0015s\n",
      "Epoch: 01027 loss_train: 1.1737 loss_rec: 1.1737 acc_train: 0.4441 loss_val: 1.1786 acc_val: 0.4438 time: 0.0025s\n",
      "Epoch: 01028 loss_train: 1.1736 loss_rec: 1.1736 acc_train: 0.4441 loss_val: 1.1785 acc_val: 0.4438 time: 0.0015s\n",
      "Epoch: 01029 loss_train: 1.1735 loss_rec: 1.1735 acc_train: 0.4441 loss_val: 1.1784 acc_val: 0.4438 time: 0.0025s\n",
      "Epoch: 01030 loss_train: 1.1735 loss_rec: 1.1735 acc_train: 0.4441 loss_val: 1.1783 acc_val: 0.4438 time: 0.0015s\n",
      "Epoch: 01031 loss_train: 1.1734 loss_rec: 1.1734 acc_train: 0.4441 loss_val: 1.1783 acc_val: 0.4438 time: 0.0030s\n",
      "Test set results: loss= 1.1960 accuracy= 0.3752\n",
      "Epoch: 01032 loss_train: 1.1733 loss_rec: 1.1733 acc_train: 0.4441 loss_val: 1.1782 acc_val: 0.4438 time: 0.0030s\n",
      "Epoch: 01033 loss_train: 1.1732 loss_rec: 1.1732 acc_train: 0.4441 loss_val: 1.1781 acc_val: 0.4438 time: 0.0015s\n",
      "Epoch: 01034 loss_train: 1.1731 loss_rec: 1.1731 acc_train: 0.4441 loss_val: 1.1780 acc_val: 0.4438 time: 0.0025s\n",
      "Epoch: 01035 loss_train: 1.1731 loss_rec: 1.1731 acc_train: 0.4441 loss_val: 1.1780 acc_val: 0.4438 time: 0.0020s\n",
      "Epoch: 01036 loss_train: 1.1730 loss_rec: 1.1730 acc_train: 0.4440 loss_val: 1.1779 acc_val: 0.4438 time: 0.0020s\n",
      "Epoch: 01037 loss_train: 1.1729 loss_rec: 1.1729 acc_train: 0.4444 loss_val: 1.1778 acc_val: 0.4441 time: 0.0020s\n",
      "Epoch: 01038 loss_train: 1.1728 loss_rec: 1.1728 acc_train: 0.4442 loss_val: 1.1778 acc_val: 0.4441 time: 0.0030s\n",
      "Epoch: 01039 loss_train: 1.1727 loss_rec: 1.1727 acc_train: 0.4442 loss_val: 1.1777 acc_val: 0.4441 time: 0.0025s\n",
      "Epoch: 01040 loss_train: 1.1727 loss_rec: 1.1727 acc_train: 0.4442 loss_val: 1.1776 acc_val: 0.4441 time: 0.0015s\n",
      "Epoch: 01041 loss_train: 1.1726 loss_rec: 1.1726 acc_train: 0.4442 loss_val: 1.1775 acc_val: 0.4441 time: 0.0015s\n",
      "Test set results: loss= 1.1955 accuracy= 0.3758\n",
      "Epoch: 01042 loss_train: 1.1725 loss_rec: 1.1725 acc_train: 0.4442 loss_val: 1.1775 acc_val: 0.4441 time: 0.0015s\n",
      "Epoch: 01043 loss_train: 1.1724 loss_rec: 1.1724 acc_train: 0.4442 loss_val: 1.1774 acc_val: 0.4441 time: 0.0025s\n",
      "Epoch: 01044 loss_train: 1.1723 loss_rec: 1.1723 acc_train: 0.4442 loss_val: 1.1773 acc_val: 0.4441 time: 0.0015s\n",
      "Epoch: 01045 loss_train: 1.1723 loss_rec: 1.1723 acc_train: 0.4456 loss_val: 1.1773 acc_val: 0.4453 time: 0.0030s\n",
      "Epoch: 01046 loss_train: 1.1722 loss_rec: 1.1722 acc_train: 0.4461 loss_val: 1.1772 acc_val: 0.4456 time: 0.0020s\n",
      "Epoch: 01047 loss_train: 1.1721 loss_rec: 1.1721 acc_train: 0.4461 loss_val: 1.1771 acc_val: 0.4456 time: 0.0025s\n",
      "Epoch: 01048 loss_train: 1.1720 loss_rec: 1.1720 acc_train: 0.4468 loss_val: 1.1771 acc_val: 0.4459 time: 0.0015s\n",
      "Epoch: 01049 loss_train: 1.1719 loss_rec: 1.1719 acc_train: 0.4467 loss_val: 1.1770 acc_val: 0.4459 time: 0.0025s\n",
      "Epoch: 01050 loss_train: 1.1719 loss_rec: 1.1719 acc_train: 0.4466 loss_val: 1.1769 acc_val: 0.4459 time: 0.0020s\n",
      "Epoch: 01051 loss_train: 1.1718 loss_rec: 1.1718 acc_train: 0.4466 loss_val: 1.1768 acc_val: 0.4459 time: 0.0020s\n",
      "Test set results: loss= 1.1951 accuracy= 0.3787\n",
      "Epoch: 01052 loss_train: 1.1717 loss_rec: 1.1717 acc_train: 0.4466 loss_val: 1.1768 acc_val: 0.4459 time: 0.0030s\n",
      "Epoch: 01053 loss_train: 1.1716 loss_rec: 1.1716 acc_train: 0.4472 loss_val: 1.1767 acc_val: 0.4463 time: 0.0020s\n",
      "Epoch: 01054 loss_train: 1.1715 loss_rec: 1.1715 acc_train: 0.4472 loss_val: 1.1766 acc_val: 0.4463 time: 0.0025s\n",
      "Epoch: 01055 loss_train: 1.1715 loss_rec: 1.1715 acc_train: 0.4472 loss_val: 1.1766 acc_val: 0.4463 time: 0.0020s\n",
      "Epoch: 01056 loss_train: 1.1714 loss_rec: 1.1714 acc_train: 0.4472 loss_val: 1.1765 acc_val: 0.4463 time: 0.0025s\n",
      "Epoch: 01057 loss_train: 1.1713 loss_rec: 1.1713 acc_train: 0.4474 loss_val: 1.1764 acc_val: 0.4459 time: 0.0015s\n",
      "Epoch: 01058 loss_train: 1.1712 loss_rec: 1.1712 acc_train: 0.4474 loss_val: 1.1764 acc_val: 0.4459 time: 0.0025s\n",
      "Epoch: 01059 loss_train: 1.1712 loss_rec: 1.1712 acc_train: 0.4474 loss_val: 1.1763 acc_val: 0.4459 time: 0.0015s\n",
      "Epoch: 01060 loss_train: 1.1711 loss_rec: 1.1711 acc_train: 0.4483 loss_val: 1.1762 acc_val: 0.4463 time: 0.0030s\n",
      "Epoch: 01061 loss_train: 1.1710 loss_rec: 1.1710 acc_train: 0.4485 loss_val: 1.1761 acc_val: 0.4469 time: 0.0025s\n",
      "Test set results: loss= 1.1947 accuracy= 0.3817\n",
      "Epoch: 01062 loss_train: 1.1709 loss_rec: 1.1709 acc_train: 0.4485 loss_val: 1.1761 acc_val: 0.4469 time: 0.0015s\n",
      "Epoch: 01063 loss_train: 1.1708 loss_rec: 1.1708 acc_train: 0.4485 loss_val: 1.1760 acc_val: 0.4469 time: 0.0025s\n",
      "Epoch: 01064 loss_train: 1.1708 loss_rec: 1.1708 acc_train: 0.4484 loss_val: 1.1759 acc_val: 0.4469 time: 0.0015s\n",
      "Epoch: 01065 loss_train: 1.1707 loss_rec: 1.1707 acc_train: 0.4484 loss_val: 1.1759 acc_val: 0.4469 time: 0.0025s\n",
      "Epoch: 01066 loss_train: 1.1706 loss_rec: 1.1706 acc_train: 0.4484 loss_val: 1.1758 acc_val: 0.4469 time: 0.0020s\n",
      "Epoch: 01067 loss_train: 1.1705 loss_rec: 1.1705 acc_train: 0.4483 loss_val: 1.1757 acc_val: 0.4466 time: 0.0025s\n",
      "Epoch: 01068 loss_train: 1.1705 loss_rec: 1.1705 acc_train: 0.4487 loss_val: 1.1757 acc_val: 0.4475 time: 0.0025s\n",
      "Epoch: 01069 loss_train: 1.1704 loss_rec: 1.1704 acc_train: 0.4487 loss_val: 1.1756 acc_val: 0.4475 time: 0.0020s\n",
      "Epoch: 01070 loss_train: 1.1703 loss_rec: 1.1703 acc_train: 0.4487 loss_val: 1.1755 acc_val: 0.4475 time: 0.0025s\n",
      "Epoch: 01071 loss_train: 1.1702 loss_rec: 1.1702 acc_train: 0.4487 loss_val: 1.1755 acc_val: 0.4475 time: 0.0020s\n",
      "Test set results: loss= 1.1941 accuracy= 0.3826\n",
      "Epoch: 01072 loss_train: 1.1702 loss_rec: 1.1702 acc_train: 0.4487 loss_val: 1.1754 acc_val: 0.4475 time: 0.0025s\n",
      "Epoch: 01073 loss_train: 1.1701 loss_rec: 1.1701 acc_train: 0.4487 loss_val: 1.1753 acc_val: 0.4472 time: 0.0015s\n",
      "Epoch: 01074 loss_train: 1.1700 loss_rec: 1.1700 acc_train: 0.4487 loss_val: 1.1753 acc_val: 0.4472 time: 0.0030s\n",
      "Epoch: 01075 loss_train: 1.1699 loss_rec: 1.1699 acc_train: 0.4487 loss_val: 1.1752 acc_val: 0.4472 time: 0.0020s\n",
      "Epoch: 01076 loss_train: 1.1698 loss_rec: 1.1698 acc_train: 0.4487 loss_val: 1.1751 acc_val: 0.4472 time: 0.0015s\n",
      "Epoch: 01077 loss_train: 1.1698 loss_rec: 1.1698 acc_train: 0.4487 loss_val: 1.1751 acc_val: 0.4472 time: 0.0020s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01078 loss_train: 1.1697 loss_rec: 1.1697 acc_train: 0.4492 loss_val: 1.1750 acc_val: 0.4469 time: 0.0020s\n",
      "Epoch: 01079 loss_train: 1.1696 loss_rec: 1.1696 acc_train: 0.4492 loss_val: 1.1749 acc_val: 0.4469 time: 0.0020s\n",
      "Epoch: 01080 loss_train: 1.1695 loss_rec: 1.1695 acc_train: 0.4493 loss_val: 1.1749 acc_val: 0.4469 time: 0.0025s\n",
      "Epoch: 01081 loss_train: 1.1695 loss_rec: 1.1695 acc_train: 0.4493 loss_val: 1.1748 acc_val: 0.4469 time: 0.0020s\n",
      "Test set results: loss= 1.1937 accuracy= 0.3831\n",
      "Epoch: 01082 loss_train: 1.1694 loss_rec: 1.1694 acc_train: 0.4493 loss_val: 1.1747 acc_val: 0.4469 time: 0.0020s\n",
      "Epoch: 01083 loss_train: 1.1693 loss_rec: 1.1693 acc_train: 0.4492 loss_val: 1.1747 acc_val: 0.4469 time: 0.0020s\n",
      "Epoch: 01084 loss_train: 1.1692 loss_rec: 1.1692 acc_train: 0.4492 loss_val: 1.1746 acc_val: 0.4469 time: 0.0020s\n",
      "Epoch: 01085 loss_train: 1.1692 loss_rec: 1.1692 acc_train: 0.4492 loss_val: 1.1745 acc_val: 0.4469 time: 0.0025s\n",
      "Epoch: 01086 loss_train: 1.1691 loss_rec: 1.1691 acc_train: 0.4492 loss_val: 1.1745 acc_val: 0.4469 time: 0.0015s\n",
      "Epoch: 01087 loss_train: 1.1690 loss_rec: 1.1690 acc_train: 0.4496 loss_val: 1.1744 acc_val: 0.4469 time: 0.0025s\n",
      "Epoch: 01088 loss_train: 1.1689 loss_rec: 1.1689 acc_train: 0.4496 loss_val: 1.1743 acc_val: 0.4469 time: 0.0020s\n",
      "Epoch: 01089 loss_train: 1.1689 loss_rec: 1.1689 acc_train: 0.4496 loss_val: 1.1743 acc_val: 0.4472 time: 0.0025s\n",
      "Epoch: 01090 loss_train: 1.1688 loss_rec: 1.1688 acc_train: 0.4497 loss_val: 1.1742 acc_val: 0.4472 time: 0.0020s\n",
      "Epoch: 01091 loss_train: 1.1687 loss_rec: 1.1687 acc_train: 0.4500 loss_val: 1.1742 acc_val: 0.4472 time: 0.0015s\n",
      "Test set results: loss= 1.1933 accuracy= 0.3852\n",
      "Epoch: 01092 loss_train: 1.1686 loss_rec: 1.1686 acc_train: 0.4500 loss_val: 1.1741 acc_val: 0.4475 time: 0.0020s\n",
      "Epoch: 01093 loss_train: 1.1686 loss_rec: 1.1686 acc_train: 0.4500 loss_val: 1.1740 acc_val: 0.4475 time: 0.0020s\n",
      "Epoch: 01094 loss_train: 1.1685 loss_rec: 1.1685 acc_train: 0.4503 loss_val: 1.1740 acc_val: 0.4478 time: 0.0015s\n",
      "Epoch: 01095 loss_train: 1.1684 loss_rec: 1.1684 acc_train: 0.4503 loss_val: 1.1739 acc_val: 0.4478 time: 0.0020s\n",
      "Epoch: 01096 loss_train: 1.1683 loss_rec: 1.1683 acc_train: 0.4502 loss_val: 1.1738 acc_val: 0.4478 time: 0.0015s\n",
      "Epoch: 01097 loss_train: 1.1683 loss_rec: 1.1683 acc_train: 0.4501 loss_val: 1.1738 acc_val: 0.4478 time: 0.0020s\n",
      "Epoch: 01098 loss_train: 1.1682 loss_rec: 1.1682 acc_train: 0.4506 loss_val: 1.1737 acc_val: 0.4484 time: 0.0020s\n",
      "Epoch: 01099 loss_train: 1.1681 loss_rec: 1.1681 acc_train: 0.4507 loss_val: 1.1736 acc_val: 0.4487 time: 0.0015s\n",
      "Epoch: 01100 loss_train: 1.1681 loss_rec: 1.1681 acc_train: 0.4507 loss_val: 1.1736 acc_val: 0.4487 time: 0.0020s\n",
      "Epoch: 01101 loss_train: 1.1680 loss_rec: 1.1680 acc_train: 0.4507 loss_val: 1.1735 acc_val: 0.4487 time: 0.0015s\n",
      "Test set results: loss= 1.1929 accuracy= 0.3865\n",
      "Epoch: 01102 loss_train: 1.1679 loss_rec: 1.1679 acc_train: 0.4507 loss_val: 1.1734 acc_val: 0.4487 time: 0.0015s\n",
      "Epoch: 01103 loss_train: 1.1678 loss_rec: 1.1678 acc_train: 0.4507 loss_val: 1.1734 acc_val: 0.4491 time: 0.0020s\n",
      "Epoch: 01104 loss_train: 1.1678 loss_rec: 1.1678 acc_train: 0.4510 loss_val: 1.1733 acc_val: 0.4491 time: 0.0015s\n",
      "Epoch: 01105 loss_train: 1.1677 loss_rec: 1.1677 acc_train: 0.4510 loss_val: 1.1732 acc_val: 0.4487 time: 0.0020s\n",
      "Epoch: 01106 loss_train: 1.1676 loss_rec: 1.1676 acc_train: 0.4510 loss_val: 1.1732 acc_val: 0.4487 time: 0.0020s\n",
      "Epoch: 01107 loss_train: 1.1675 loss_rec: 1.1675 acc_train: 0.4510 loss_val: 1.1731 acc_val: 0.4487 time: 0.0025s\n",
      "Epoch: 01108 loss_train: 1.1675 loss_rec: 1.1675 acc_train: 0.4510 loss_val: 1.1730 acc_val: 0.4487 time: 0.0015s\n",
      "Epoch: 01109 loss_train: 1.1674 loss_rec: 1.1674 acc_train: 0.4509 loss_val: 1.1730 acc_val: 0.4491 time: 0.0025s\n",
      "Epoch: 01110 loss_train: 1.1673 loss_rec: 1.1673 acc_train: 0.4509 loss_val: 1.1729 acc_val: 0.4491 time: 0.0020s\n",
      "Epoch: 01111 loss_train: 1.1672 loss_rec: 1.1672 acc_train: 0.4509 loss_val: 1.1729 acc_val: 0.4491 time: 0.0025s\n",
      "Test set results: loss= 1.1925 accuracy= 0.3875\n",
      "Epoch: 01112 loss_train: 1.1672 loss_rec: 1.1672 acc_train: 0.4509 loss_val: 1.1728 acc_val: 0.4491 time: 0.0030s\n",
      "Epoch: 01113 loss_train: 1.1671 loss_rec: 1.1671 acc_train: 0.4509 loss_val: 1.1727 acc_val: 0.4494 time: 0.0020s\n",
      "Epoch: 01114 loss_train: 1.1670 loss_rec: 1.1670 acc_train: 0.4509 loss_val: 1.1727 acc_val: 0.4494 time: 0.0025s\n",
      "Epoch: 01115 loss_train: 1.1669 loss_rec: 1.1669 acc_train: 0.4509 loss_val: 1.1726 acc_val: 0.4494 time: 0.0020s\n",
      "Epoch: 01116 loss_train: 1.1669 loss_rec: 1.1669 acc_train: 0.4509 loss_val: 1.1725 acc_val: 0.4494 time: 0.0020s\n",
      "Epoch: 01117 loss_train: 1.1668 loss_rec: 1.1668 acc_train: 0.4509 loss_val: 1.1725 acc_val: 0.4494 time: 0.0015s\n",
      "Epoch: 01118 loss_train: 1.1667 loss_rec: 1.1667 acc_train: 0.4509 loss_val: 1.1724 acc_val: 0.4494 time: 0.0025s\n",
      "Epoch: 01119 loss_train: 1.1666 loss_rec: 1.1666 acc_train: 0.4508 loss_val: 1.1723 acc_val: 0.4494 time: 0.0015s\n",
      "Epoch: 01120 loss_train: 1.1666 loss_rec: 1.1666 acc_train: 0.4508 loss_val: 1.1723 acc_val: 0.4494 time: 0.0030s\n",
      "Epoch: 01121 loss_train: 1.1665 loss_rec: 1.1665 acc_train: 0.4508 loss_val: 1.1722 acc_val: 0.4494 time: 0.0020s\n",
      "Test set results: loss= 1.1922 accuracy= 0.3877\n",
      "Epoch: 01122 loss_train: 1.1664 loss_rec: 1.1664 acc_train: 0.4507 loss_val: 1.1722 acc_val: 0.4494 time: 0.0015s\n",
      "Epoch: 01123 loss_train: 1.1664 loss_rec: 1.1664 acc_train: 0.4507 loss_val: 1.1721 acc_val: 0.4494 time: 0.0025s\n",
      "Epoch: 01124 loss_train: 1.1663 loss_rec: 1.1663 acc_train: 0.4507 loss_val: 1.1720 acc_val: 0.4494 time: 0.0015s\n",
      "Epoch: 01125 loss_train: 1.1662 loss_rec: 1.1662 acc_train: 0.4507 loss_val: 1.1720 acc_val: 0.4497 time: 0.0025s\n",
      "Epoch: 01126 loss_train: 1.1661 loss_rec: 1.1661 acc_train: 0.4507 loss_val: 1.1719 acc_val: 0.4497 time: 0.0020s\n",
      "Epoch: 01127 loss_train: 1.1661 loss_rec: 1.1661 acc_train: 0.4507 loss_val: 1.1718 acc_val: 0.4497 time: 0.0030s\n",
      "Epoch: 01128 loss_train: 1.1660 loss_rec: 1.1660 acc_train: 0.4506 loss_val: 1.1718 acc_val: 0.4497 time: 0.0020s\n",
      "Epoch: 01129 loss_train: 1.1659 loss_rec: 1.1659 acc_train: 0.4511 loss_val: 1.1717 acc_val: 0.4512 time: 0.0020s\n",
      "Epoch: 01130 loss_train: 1.1659 loss_rec: 1.1659 acc_train: 0.4511 loss_val: 1.1717 acc_val: 0.4512 time: 0.0015s\n",
      "Epoch: 01131 loss_train: 1.1658 loss_rec: 1.1658 acc_train: 0.4511 loss_val: 1.1716 acc_val: 0.4512 time: 0.0025s\n",
      "Test set results: loss= 1.1918 accuracy= 0.3884\n",
      "Epoch: 01132 loss_train: 1.1657 loss_rec: 1.1657 acc_train: 0.4511 loss_val: 1.1715 acc_val: 0.4512 time: 0.0030s\n",
      "Epoch: 01133 loss_train: 1.1656 loss_rec: 1.1656 acc_train: 0.4500 loss_val: 1.1715 acc_val: 0.4506 time: 0.0015s\n",
      "Epoch: 01134 loss_train: 1.1656 loss_rec: 1.1656 acc_train: 0.4500 loss_val: 1.1714 acc_val: 0.4506 time: 0.0030s\n",
      "Epoch: 01135 loss_train: 1.1655 loss_rec: 1.1655 acc_train: 0.4500 loss_val: 1.1713 acc_val: 0.4506 time: 0.0015s\n",
      "Epoch: 01136 loss_train: 1.1654 loss_rec: 1.1654 acc_train: 0.4500 loss_val: 1.1713 acc_val: 0.4506 time: 0.0020s\n",
      "Epoch: 01137 loss_train: 1.1654 loss_rec: 1.1654 acc_train: 0.4500 loss_val: 1.1712 acc_val: 0.4506 time: 0.0020s\n",
      "Epoch: 01138 loss_train: 1.1653 loss_rec: 1.1653 acc_train: 0.4500 loss_val: 1.1712 acc_val: 0.4506 time: 0.0025s\n",
      "Epoch: 01139 loss_train: 1.1652 loss_rec: 1.1652 acc_train: 0.4501 loss_val: 1.1711 acc_val: 0.4506 time: 0.0015s\n",
      "Epoch: 01140 loss_train: 1.1651 loss_rec: 1.1651 acc_train: 0.4501 loss_val: 1.1710 acc_val: 0.4506 time: 0.0025s\n",
      "Epoch: 01141 loss_train: 1.1651 loss_rec: 1.1651 acc_train: 0.4501 loss_val: 1.1710 acc_val: 0.4506 time: 0.0015s\n",
      "Test set results: loss= 1.1915 accuracy= 0.3882\n",
      "Epoch: 01142 loss_train: 1.1650 loss_rec: 1.1650 acc_train: 0.4506 loss_val: 1.1709 acc_val: 0.4512 time: 0.0020s\n",
      "Epoch: 01143 loss_train: 1.1649 loss_rec: 1.1649 acc_train: 0.4506 loss_val: 1.1709 acc_val: 0.4512 time: 0.0025s\n",
      "Epoch: 01144 loss_train: 1.1649 loss_rec: 1.1649 acc_train: 0.4510 loss_val: 1.1708 acc_val: 0.4512 time: 0.0015s\n",
      "Epoch: 01145 loss_train: 1.1648 loss_rec: 1.1648 acc_train: 0.4510 loss_val: 1.1707 acc_val: 0.4512 time: 0.0025s\n",
      "Epoch: 01146 loss_train: 1.1647 loss_rec: 1.1647 acc_train: 0.4510 loss_val: 1.1707 acc_val: 0.4512 time: 0.0020s\n",
      "Epoch: 01147 loss_train: 1.1647 loss_rec: 1.1647 acc_train: 0.4513 loss_val: 1.1706 acc_val: 0.4512 time: 0.0020s\n",
      "Epoch: 01148 loss_train: 1.1646 loss_rec: 1.1646 acc_train: 0.4513 loss_val: 1.1705 acc_val: 0.4512 time: 0.0020s\n",
      "Epoch: 01149 loss_train: 1.1645 loss_rec: 1.1645 acc_train: 0.4514 loss_val: 1.1705 acc_val: 0.4512 time: 0.0020s\n",
      "Epoch: 01150 loss_train: 1.1644 loss_rec: 1.1644 acc_train: 0.4514 loss_val: 1.1704 acc_val: 0.4512 time: 0.0020s\n",
      "Epoch: 01151 loss_train: 1.1644 loss_rec: 1.1644 acc_train: 0.4514 loss_val: 1.1704 acc_val: 0.4512 time: 0.0015s\n",
      "Test set results: loss= 1.1910 accuracy= 0.3895\n",
      "Epoch: 01152 loss_train: 1.1643 loss_rec: 1.1643 acc_train: 0.4514 loss_val: 1.1703 acc_val: 0.4512 time: 0.0020s\n",
      "Epoch: 01153 loss_train: 1.1642 loss_rec: 1.1642 acc_train: 0.4515 loss_val: 1.1702 acc_val: 0.4512 time: 0.0020s\n",
      "Epoch: 01154 loss_train: 1.1642 loss_rec: 1.1642 acc_train: 0.4515 loss_val: 1.1702 acc_val: 0.4516 time: 0.0015s\n",
      "Epoch: 01155 loss_train: 1.1641 loss_rec: 1.1641 acc_train: 0.4511 loss_val: 1.1701 acc_val: 0.4516 time: 0.0020s\n",
      "Epoch: 01156 loss_train: 1.1640 loss_rec: 1.1640 acc_train: 0.4511 loss_val: 1.1701 acc_val: 0.4516 time: 0.0015s\n",
      "Epoch: 01157 loss_train: 1.1640 loss_rec: 1.1640 acc_train: 0.4511 loss_val: 1.1700 acc_val: 0.4519 time: 0.0020s\n",
      "Epoch: 01158 loss_train: 1.1639 loss_rec: 1.1639 acc_train: 0.4511 loss_val: 1.1700 acc_val: 0.4519 time: 0.0020s\n",
      "Epoch: 01159 loss_train: 1.1638 loss_rec: 1.1638 acc_train: 0.4511 loss_val: 1.1699 acc_val: 0.4519 time: 0.0020s\n",
      "Epoch: 01160 loss_train: 1.1638 loss_rec: 1.1638 acc_train: 0.4511 loss_val: 1.1698 acc_val: 0.4519 time: 0.0020s\n",
      "Epoch: 01161 loss_train: 1.1637 loss_rec: 1.1637 acc_train: 0.4512 loss_val: 1.1698 acc_val: 0.4519 time: 0.0020s\n",
      "Test set results: loss= 1.1906 accuracy= 0.3890\n",
      "Epoch: 01162 loss_train: 1.1636 loss_rec: 1.1636 acc_train: 0.4512 loss_val: 1.1697 acc_val: 0.4519 time: 0.0020s\n",
      "Epoch: 01163 loss_train: 1.1636 loss_rec: 1.1636 acc_train: 0.4512 loss_val: 1.1697 acc_val: 0.4519 time: 0.0015s\n",
      "Epoch: 01164 loss_train: 1.1635 loss_rec: 1.1635 acc_train: 0.4512 loss_val: 1.1696 acc_val: 0.4519 time: 0.0020s\n",
      "Epoch: 01165 loss_train: 1.1634 loss_rec: 1.1634 acc_train: 0.4512 loss_val: 1.1695 acc_val: 0.4519 time: 0.0015s\n",
      "Epoch: 01166 loss_train: 1.1634 loss_rec: 1.1634 acc_train: 0.4513 loss_val: 1.1695 acc_val: 0.4519 time: 0.0025s\n",
      "Epoch: 01167 loss_train: 1.1633 loss_rec: 1.1633 acc_train: 0.4513 loss_val: 1.1694 acc_val: 0.4519 time: 0.0020s\n",
      "Epoch: 01168 loss_train: 1.1632 loss_rec: 1.1632 acc_train: 0.4513 loss_val: 1.1694 acc_val: 0.4522 time: 0.0015s\n",
      "Epoch: 01169 loss_train: 1.1632 loss_rec: 1.1632 acc_train: 0.4513 loss_val: 1.1693 acc_val: 0.4522 time: 0.0020s\n",
      "Epoch: 01170 loss_train: 1.1631 loss_rec: 1.1631 acc_train: 0.4509 loss_val: 1.1693 acc_val: 0.4516 time: 0.0020s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01171 loss_train: 1.1630 loss_rec: 1.1630 acc_train: 0.4509 loss_val: 1.1692 acc_val: 0.4516 time: 0.0020s\n",
      "Test set results: loss= 1.1901 accuracy= 0.3881\n",
      "Epoch: 01172 loss_train: 1.1630 loss_rec: 1.1630 acc_train: 0.4509 loss_val: 1.1691 acc_val: 0.4516 time: 0.0015s\n",
      "Epoch: 01173 loss_train: 1.1629 loss_rec: 1.1629 acc_train: 0.4509 loss_val: 1.1691 acc_val: 0.4516 time: 0.0020s\n",
      "Epoch: 01174 loss_train: 1.1628 loss_rec: 1.1628 acc_train: 0.4509 loss_val: 1.1690 acc_val: 0.4516 time: 0.0020s\n",
      "Epoch: 01175 loss_train: 1.1628 loss_rec: 1.1628 acc_train: 0.4509 loss_val: 1.1690 acc_val: 0.4516 time: 0.0020s\n",
      "Epoch: 01176 loss_train: 1.1627 loss_rec: 1.1627 acc_train: 0.4509 loss_val: 1.1689 acc_val: 0.4516 time: 0.0015s\n",
      "Epoch: 01177 loss_train: 1.1626 loss_rec: 1.1626 acc_train: 0.4509 loss_val: 1.1689 acc_val: 0.4516 time: 0.0020s\n",
      "Epoch: 01178 loss_train: 1.1626 loss_rec: 1.1626 acc_train: 0.4509 loss_val: 1.1688 acc_val: 0.4516 time: 0.0015s\n",
      "Epoch: 01179 loss_train: 1.1625 loss_rec: 1.1625 acc_train: 0.4510 loss_val: 1.1687 acc_val: 0.4516 time: 0.0020s\n",
      "Epoch: 01180 loss_train: 1.1624 loss_rec: 1.1624 acc_train: 0.4511 loss_val: 1.1687 acc_val: 0.4516 time: 0.0020s\n",
      "Epoch: 01181 loss_train: 1.1624 loss_rec: 1.1624 acc_train: 0.4525 loss_val: 1.1686 acc_val: 0.4519 time: 0.0020s\n",
      "Test set results: loss= 1.1898 accuracy= 0.3896\n",
      "Epoch: 01182 loss_train: 1.1623 loss_rec: 1.1623 acc_train: 0.4525 loss_val: 1.1686 acc_val: 0.4519 time: 0.0020s\n",
      "Epoch: 01183 loss_train: 1.1622 loss_rec: 1.1622 acc_train: 0.4525 loss_val: 1.1685 acc_val: 0.4519 time: 0.0020s\n",
      "Epoch: 01184 loss_train: 1.1622 loss_rec: 1.1622 acc_train: 0.4525 loss_val: 1.1685 acc_val: 0.4519 time: 0.0020s\n",
      "Epoch: 01185 loss_train: 1.1621 loss_rec: 1.1621 acc_train: 0.4525 loss_val: 1.1684 acc_val: 0.4519 time: 0.0015s\n",
      "Epoch: 01186 loss_train: 1.1620 loss_rec: 1.1620 acc_train: 0.4526 loss_val: 1.1683 acc_val: 0.4516 time: 0.0020s\n",
      "Epoch: 01187 loss_train: 1.1620 loss_rec: 1.1620 acc_train: 0.4526 loss_val: 1.1683 acc_val: 0.4516 time: 0.0015s\n",
      "Epoch: 01188 loss_train: 1.1619 loss_rec: 1.1619 acc_train: 0.4526 loss_val: 1.1682 acc_val: 0.4516 time: 0.0020s\n",
      "Epoch: 01189 loss_train: 1.1618 loss_rec: 1.1618 acc_train: 0.4526 loss_val: 1.1682 acc_val: 0.4516 time: 0.0015s\n",
      "Epoch: 01190 loss_train: 1.1618 loss_rec: 1.1618 acc_train: 0.4526 loss_val: 1.1681 acc_val: 0.4516 time: 0.0015s\n",
      "Epoch: 01191 loss_train: 1.1617 loss_rec: 1.1617 acc_train: 0.4526 loss_val: 1.1681 acc_val: 0.4516 time: 0.0025s\n",
      "Test set results: loss= 1.1894 accuracy= 0.3896\n",
      "Epoch: 01192 loss_train: 1.1616 loss_rec: 1.1616 acc_train: 0.4526 loss_val: 1.1680 acc_val: 0.4519 time: 0.0015s\n",
      "Epoch: 01193 loss_train: 1.1616 loss_rec: 1.1616 acc_train: 0.4526 loss_val: 1.1679 acc_val: 0.4519 time: 0.0020s\n",
      "Epoch: 01194 loss_train: 1.1615 loss_rec: 1.1615 acc_train: 0.4527 loss_val: 1.1679 acc_val: 0.4519 time: 0.0015s\n",
      "Epoch: 01195 loss_train: 1.1614 loss_rec: 1.1614 acc_train: 0.4527 loss_val: 1.1678 acc_val: 0.4519 time: 0.0020s\n",
      "Epoch: 01196 loss_train: 1.1614 loss_rec: 1.1614 acc_train: 0.4527 loss_val: 1.1678 acc_val: 0.4519 time: 0.0015s\n",
      "Epoch: 01197 loss_train: 1.1613 loss_rec: 1.1613 acc_train: 0.4527 loss_val: 1.1677 acc_val: 0.4519 time: 0.0020s\n",
      "Epoch: 01198 loss_train: 1.1612 loss_rec: 1.1612 acc_train: 0.4530 loss_val: 1.1677 acc_val: 0.4522 time: 0.0015s\n",
      "Epoch: 01199 loss_train: 1.1612 loss_rec: 1.1612 acc_train: 0.4529 loss_val: 1.1676 acc_val: 0.4522 time: 0.0015s\n",
      "Epoch: 01200 loss_train: 1.1611 loss_rec: 1.1611 acc_train: 0.4529 loss_val: 1.1676 acc_val: 0.4522 time: 0.0020s\n",
      "Epoch: 01201 loss_train: 1.1611 loss_rec: 1.1611 acc_train: 0.4529 loss_val: 1.1675 acc_val: 0.4522 time: 0.0020s\n",
      "Test set results: loss= 1.1890 accuracy= 0.3899\n",
      "Epoch: 01202 loss_train: 1.1610 loss_rec: 1.1610 acc_train: 0.4529 loss_val: 1.1674 acc_val: 0.4522 time: 0.0020s\n",
      "Epoch: 01203 loss_train: 1.1609 loss_rec: 1.1609 acc_train: 0.4530 loss_val: 1.1674 acc_val: 0.4522 time: 0.0020s\n",
      "Epoch: 01204 loss_train: 1.1609 loss_rec: 1.1609 acc_train: 0.4528 loss_val: 1.1673 acc_val: 0.4522 time: 0.0015s\n",
      "Epoch: 01205 loss_train: 1.1608 loss_rec: 1.1608 acc_train: 0.4527 loss_val: 1.1673 acc_val: 0.4522 time: 0.0020s\n",
      "Epoch: 01206 loss_train: 1.1607 loss_rec: 1.1607 acc_train: 0.4527 loss_val: 1.1672 acc_val: 0.4522 time: 0.0015s\n",
      "Epoch: 01207 loss_train: 1.1607 loss_rec: 1.1607 acc_train: 0.4527 loss_val: 1.1672 acc_val: 0.4522 time: 0.0015s\n",
      "Epoch: 01208 loss_train: 1.1606 loss_rec: 1.1606 acc_train: 0.4527 loss_val: 1.1671 acc_val: 0.4522 time: 0.0020s\n",
      "Epoch: 01209 loss_train: 1.1605 loss_rec: 1.1605 acc_train: 0.4527 loss_val: 1.1671 acc_val: 0.4522 time: 0.0020s\n",
      "Epoch: 01210 loss_train: 1.1605 loss_rec: 1.1605 acc_train: 0.4527 loss_val: 1.1670 acc_val: 0.4522 time: 0.0020s\n",
      "Epoch: 01211 loss_train: 1.1604 loss_rec: 1.1604 acc_train: 0.4527 loss_val: 1.1670 acc_val: 0.4522 time: 0.0015s\n",
      "Test set results: loss= 1.1887 accuracy= 0.3896\n",
      "Epoch: 01212 loss_train: 1.1604 loss_rec: 1.1604 acc_train: 0.4527 loss_val: 1.1669 acc_val: 0.4522 time: 0.0020s\n",
      "Epoch: 01213 loss_train: 1.1603 loss_rec: 1.1603 acc_train: 0.4528 loss_val: 1.1668 acc_val: 0.4522 time: 0.0020s\n",
      "Epoch: 01214 loss_train: 1.1602 loss_rec: 1.1602 acc_train: 0.4528 loss_val: 1.1668 acc_val: 0.4522 time: 0.0020s\n",
      "Epoch: 01215 loss_train: 1.1602 loss_rec: 1.1602 acc_train: 0.4528 loss_val: 1.1667 acc_val: 0.4522 time: 0.0020s\n",
      "Epoch: 01216 loss_train: 1.1601 loss_rec: 1.1601 acc_train: 0.4527 loss_val: 1.1667 acc_val: 0.4522 time: 0.0020s\n",
      "Epoch: 01217 loss_train: 1.1600 loss_rec: 1.1600 acc_train: 0.4527 loss_val: 1.1666 acc_val: 0.4522 time: 0.0025s\n",
      "Epoch: 01218 loss_train: 1.1600 loss_rec: 1.1600 acc_train: 0.4531 loss_val: 1.1666 acc_val: 0.4528 time: 0.0015s\n",
      "Epoch: 01219 loss_train: 1.1599 loss_rec: 1.1599 acc_train: 0.4531 loss_val: 1.1665 acc_val: 0.4528 time: 0.0020s\n",
      "Epoch: 01220 loss_train: 1.1599 loss_rec: 1.1599 acc_train: 0.4531 loss_val: 1.1665 acc_val: 0.4528 time: 0.0020s\n",
      "Epoch: 01221 loss_train: 1.1598 loss_rec: 1.1598 acc_train: 0.4522 loss_val: 1.1664 acc_val: 0.4516 time: 0.0015s\n",
      "Test set results: loss= 1.1883 accuracy= 0.3890\n",
      "Epoch: 01222 loss_train: 1.1597 loss_rec: 1.1597 acc_train: 0.4522 loss_val: 1.1664 acc_val: 0.4516 time: 0.0015s\n",
      "Epoch: 01223 loss_train: 1.1597 loss_rec: 1.1597 acc_train: 0.4522 loss_val: 1.1663 acc_val: 0.4516 time: 0.0030s\n",
      "Epoch: 01224 loss_train: 1.1596 loss_rec: 1.1596 acc_train: 0.4523 loss_val: 1.1663 acc_val: 0.4516 time: 0.0020s\n",
      "Epoch: 01225 loss_train: 1.1595 loss_rec: 1.1595 acc_train: 0.4523 loss_val: 1.1662 acc_val: 0.4516 time: 0.0025s\n",
      "Epoch: 01226 loss_train: 1.1595 loss_rec: 1.1595 acc_train: 0.4523 loss_val: 1.1661 acc_val: 0.4516 time: 0.0020s\n",
      "Epoch: 01227 loss_train: 1.1594 loss_rec: 1.1594 acc_train: 0.4523 loss_val: 1.1661 acc_val: 0.4516 time: 0.0025s\n",
      "Epoch: 01228 loss_train: 1.1594 loss_rec: 1.1594 acc_train: 0.4537 loss_val: 1.1660 acc_val: 0.4528 time: 0.0020s\n",
      "Epoch: 01229 loss_train: 1.1593 loss_rec: 1.1593 acc_train: 0.4537 loss_val: 1.1660 acc_val: 0.4528 time: 0.0020s\n",
      "Epoch: 01230 loss_train: 1.1592 loss_rec: 1.1592 acc_train: 0.4537 loss_val: 1.1659 acc_val: 0.4528 time: 0.0025s\n",
      "Epoch: 01231 loss_train: 1.1592 loss_rec: 1.1592 acc_train: 0.4537 loss_val: 1.1659 acc_val: 0.4528 time: 0.0020s\n",
      "Test set results: loss= 1.1880 accuracy= 0.3906\n",
      "Epoch: 01232 loss_train: 1.1591 loss_rec: 1.1591 acc_train: 0.4537 loss_val: 1.1658 acc_val: 0.4528 time: 0.0025s\n",
      "Epoch: 01233 loss_train: 1.1591 loss_rec: 1.1591 acc_train: 0.4536 loss_val: 1.1658 acc_val: 0.4519 time: 0.0020s\n",
      "Epoch: 01234 loss_train: 1.1590 loss_rec: 1.1590 acc_train: 0.4536 loss_val: 1.1657 acc_val: 0.4519 time: 0.0020s\n",
      "Epoch: 01235 loss_train: 1.1589 loss_rec: 1.1589 acc_train: 0.4536 loss_val: 1.1657 acc_val: 0.4519 time: 0.0020s\n",
      "Epoch: 01236 loss_train: 1.1589 loss_rec: 1.1589 acc_train: 0.4536 loss_val: 1.1656 acc_val: 0.4519 time: 0.0025s\n",
      "Epoch: 01237 loss_train: 1.1588 loss_rec: 1.1588 acc_train: 0.4536 loss_val: 1.1656 acc_val: 0.4519 time: 0.0015s\n",
      "Epoch: 01238 loss_train: 1.1587 loss_rec: 1.1587 acc_train: 0.4539 loss_val: 1.1655 acc_val: 0.4531 time: 0.0030s\n",
      "Epoch: 01239 loss_train: 1.1587 loss_rec: 1.1587 acc_train: 0.4539 loss_val: 1.1655 acc_val: 0.4531 time: 0.0020s\n",
      "Epoch: 01240 loss_train: 1.1586 loss_rec: 1.1586 acc_train: 0.4539 loss_val: 1.1654 acc_val: 0.4531 time: 0.0020s\n",
      "Epoch: 01241 loss_train: 1.1586 loss_rec: 1.1586 acc_train: 0.4534 loss_val: 1.1654 acc_val: 0.4534 time: 0.0015s\n",
      "Test set results: loss= 1.1877 accuracy= 0.3917\n",
      "Epoch: 01242 loss_train: 1.1585 loss_rec: 1.1585 acc_train: 0.4534 loss_val: 1.1653 acc_val: 0.4534 time: 0.0015s\n",
      "Epoch: 01243 loss_train: 1.1584 loss_rec: 1.1584 acc_train: 0.4534 loss_val: 1.1653 acc_val: 0.4534 time: 0.0025s\n",
      "Epoch: 01244 loss_train: 1.1584 loss_rec: 1.1584 acc_train: 0.4534 loss_val: 1.1652 acc_val: 0.4534 time: 0.0015s\n",
      "Epoch: 01245 loss_train: 1.1583 loss_rec: 1.1583 acc_train: 0.4534 loss_val: 1.1651 acc_val: 0.4534 time: 0.0030s\n",
      "Epoch: 01246 loss_train: 1.1583 loss_rec: 1.1583 acc_train: 0.4534 loss_val: 1.1651 acc_val: 0.4534 time: 0.0020s\n",
      "Epoch: 01247 loss_train: 1.1582 loss_rec: 1.1582 acc_train: 0.4534 loss_val: 1.1650 acc_val: 0.4534 time: 0.0015s\n",
      "Epoch: 01248 loss_train: 1.1581 loss_rec: 1.1581 acc_train: 0.4534 loss_val: 1.1650 acc_val: 0.4534 time: 0.0020s\n",
      "Epoch: 01249 loss_train: 1.1581 loss_rec: 1.1581 acc_train: 0.4534 loss_val: 1.1649 acc_val: 0.4534 time: 0.0025s\n",
      "Epoch: 01250 loss_train: 1.1580 loss_rec: 1.1580 acc_train: 0.4534 loss_val: 1.1649 acc_val: 0.4534 time: 0.0015s\n",
      "Epoch: 01251 loss_train: 1.1580 loss_rec: 1.1580 acc_train: 0.4534 loss_val: 1.1648 acc_val: 0.4534 time: 0.0025s\n",
      "Test set results: loss= 1.1874 accuracy= 0.3917\n",
      "Epoch: 01252 loss_train: 1.1579 loss_rec: 1.1579 acc_train: 0.4534 loss_val: 1.1648 acc_val: 0.4534 time: 0.0020s\n",
      "Epoch: 01253 loss_train: 1.1578 loss_rec: 1.1578 acc_train: 0.4529 loss_val: 1.1647 acc_val: 0.4531 time: 0.0025s\n",
      "Epoch: 01254 loss_train: 1.1578 loss_rec: 1.1578 acc_train: 0.4529 loss_val: 1.1647 acc_val: 0.4531 time: 0.0020s\n",
      "Epoch: 01255 loss_train: 1.1577 loss_rec: 1.1577 acc_train: 0.4529 loss_val: 1.1646 acc_val: 0.4531 time: 0.0020s\n",
      "Epoch: 01256 loss_train: 1.1577 loss_rec: 1.1577 acc_train: 0.4531 loss_val: 1.1646 acc_val: 0.4531 time: 0.0015s\n",
      "Epoch: 01257 loss_train: 1.1576 loss_rec: 1.1576 acc_train: 0.4531 loss_val: 1.1645 acc_val: 0.4531 time: 0.0015s\n",
      "Epoch: 01258 loss_train: 1.1575 loss_rec: 1.1575 acc_train: 0.4531 loss_val: 1.1645 acc_val: 0.4531 time: 0.0020s\n",
      "Epoch: 01259 loss_train: 1.1575 loss_rec: 1.1575 acc_train: 0.4531 loss_val: 1.1644 acc_val: 0.4531 time: 0.0015s\n",
      "Epoch: 01260 loss_train: 1.1574 loss_rec: 1.1574 acc_train: 0.4532 loss_val: 1.1644 acc_val: 0.4531 time: 0.0020s\n",
      "Epoch: 01261 loss_train: 1.1573 loss_rec: 1.1573 acc_train: 0.4532 loss_val: 1.1643 acc_val: 0.4531 time: 0.0020s\n",
      "Test set results: loss= 1.1872 accuracy= 0.3915\n",
      "Epoch: 01262 loss_train: 1.1573 loss_rec: 1.1573 acc_train: 0.4532 loss_val: 1.1643 acc_val: 0.4531 time: 0.0015s\n",
      "Epoch: 01263 loss_train: 1.1572 loss_rec: 1.1572 acc_train: 0.4531 loss_val: 1.1642 acc_val: 0.4528 time: 0.0015s\n",
      "Epoch: 01264 loss_train: 1.1572 loss_rec: 1.1572 acc_train: 0.4529 loss_val: 1.1642 acc_val: 0.4525 time: 0.0015s\n",
      "Epoch: 01265 loss_train: 1.1571 loss_rec: 1.1571 acc_train: 0.4529 loss_val: 1.1641 acc_val: 0.4525 time: 0.0015s\n",
      "Epoch: 01266 loss_train: 1.1570 loss_rec: 1.1570 acc_train: 0.4529 loss_val: 1.1641 acc_val: 0.4525 time: 0.0015s\n",
      "Epoch: 01267 loss_train: 1.1570 loss_rec: 1.1570 acc_train: 0.4532 loss_val: 1.1640 acc_val: 0.4528 time: 0.0020s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01268 loss_train: 1.1569 loss_rec: 1.1569 acc_train: 0.4532 loss_val: 1.1639 acc_val: 0.4525 time: 0.0025s\n",
      "Epoch: 01269 loss_train: 1.1569 loss_rec: 1.1569 acc_train: 0.4532 loss_val: 1.1639 acc_val: 0.4528 time: 0.0015s\n",
      "Epoch: 01270 loss_train: 1.1568 loss_rec: 1.1568 acc_train: 0.4532 loss_val: 1.1638 acc_val: 0.4528 time: 0.0020s\n",
      "Epoch: 01271 loss_train: 1.1567 loss_rec: 1.1567 acc_train: 0.4532 loss_val: 1.1638 acc_val: 0.4528 time: 0.0015s\n",
      "Test set results: loss= 1.1870 accuracy= 0.3920\n",
      "Epoch: 01272 loss_train: 1.1567 loss_rec: 1.1567 acc_train: 0.4532 loss_val: 1.1637 acc_val: 0.4528 time: 0.0020s\n",
      "Epoch: 01273 loss_train: 1.1566 loss_rec: 1.1566 acc_train: 0.4532 loss_val: 1.1637 acc_val: 0.4528 time: 0.0015s\n",
      "Epoch: 01274 loss_train: 1.1566 loss_rec: 1.1566 acc_train: 0.4532 loss_val: 1.1636 acc_val: 0.4528 time: 0.0020s\n",
      "Epoch: 01275 loss_train: 1.1565 loss_rec: 1.1565 acc_train: 0.4532 loss_val: 1.1636 acc_val: 0.4528 time: 0.0015s\n",
      "Epoch: 01276 loss_train: 1.1564 loss_rec: 1.1564 acc_train: 0.4532 loss_val: 1.1635 acc_val: 0.4528 time: 0.0015s\n",
      "Epoch: 01277 loss_train: 1.1564 loss_rec: 1.1564 acc_train: 0.4532 loss_val: 1.1635 acc_val: 0.4528 time: 0.0015s\n",
      "Epoch: 01278 loss_train: 1.1563 loss_rec: 1.1563 acc_train: 0.4532 loss_val: 1.1634 acc_val: 0.4528 time: 0.0015s\n",
      "Epoch: 01279 loss_train: 1.1563 loss_rec: 1.1563 acc_train: 0.4537 loss_val: 1.1634 acc_val: 0.4528 time: 0.0020s\n",
      "Epoch: 01280 loss_train: 1.1562 loss_rec: 1.1562 acc_train: 0.4537 loss_val: 1.1633 acc_val: 0.4528 time: 0.0020s\n",
      "Epoch: 01281 loss_train: 1.1561 loss_rec: 1.1561 acc_train: 0.4537 loss_val: 1.1633 acc_val: 0.4528 time: 0.0015s\n",
      "Test set results: loss= 1.1868 accuracy= 0.3929\n",
      "Epoch: 01282 loss_train: 1.1561 loss_rec: 1.1561 acc_train: 0.4537 loss_val: 1.1632 acc_val: 0.4528 time: 0.0020s\n",
      "Epoch: 01283 loss_train: 1.1560 loss_rec: 1.1560 acc_train: 0.4537 loss_val: 1.1632 acc_val: 0.4528 time: 0.0015s\n",
      "Epoch: 01284 loss_train: 1.1560 loss_rec: 1.1560 acc_train: 0.4537 loss_val: 1.1631 acc_val: 0.4528 time: 0.0020s\n",
      "Epoch: 01285 loss_train: 1.1559 loss_rec: 1.1559 acc_train: 0.4537 loss_val: 1.1630 acc_val: 0.4525 time: 0.0020s\n",
      "Epoch: 01286 loss_train: 1.1558 loss_rec: 1.1558 acc_train: 0.4537 loss_val: 1.1630 acc_val: 0.4525 time: 0.0015s\n",
      "Epoch: 01287 loss_train: 1.1558 loss_rec: 1.1558 acc_train: 0.4540 loss_val: 1.1629 acc_val: 0.4531 time: 0.0020s\n",
      "Epoch: 01288 loss_train: 1.1557 loss_rec: 1.1557 acc_train: 0.4539 loss_val: 1.1629 acc_val: 0.4531 time: 0.0020s\n",
      "Epoch: 01289 loss_train: 1.1556 loss_rec: 1.1556 acc_train: 0.4539 loss_val: 1.1628 acc_val: 0.4531 time: 0.0015s\n",
      "Epoch: 01290 loss_train: 1.1556 loss_rec: 1.1556 acc_train: 0.4539 loss_val: 1.1628 acc_val: 0.4531 time: 0.0015s\n",
      "Epoch: 01291 loss_train: 1.1555 loss_rec: 1.1555 acc_train: 0.4539 loss_val: 1.1627 acc_val: 0.4531 time: 0.0020s\n",
      "Test set results: loss= 1.1865 accuracy= 0.3940\n",
      "Epoch: 01292 loss_train: 1.1555 loss_rec: 1.1555 acc_train: 0.4546 loss_val: 1.1627 acc_val: 0.4544 time: 0.0015s\n",
      "Epoch: 01293 loss_train: 1.1554 loss_rec: 1.1554 acc_train: 0.4546 loss_val: 1.1626 acc_val: 0.4544 time: 0.0020s\n",
      "Epoch: 01294 loss_train: 1.1553 loss_rec: 1.1553 acc_train: 0.4546 loss_val: 1.1626 acc_val: 0.4544 time: 0.0015s\n",
      "Epoch: 01295 loss_train: 1.1553 loss_rec: 1.1553 acc_train: 0.4546 loss_val: 1.1625 acc_val: 0.4544 time: 0.0020s\n",
      "Epoch: 01296 loss_train: 1.1552 loss_rec: 1.1552 acc_train: 0.4546 loss_val: 1.1625 acc_val: 0.4544 time: 0.0020s\n",
      "Epoch: 01297 loss_train: 1.1552 loss_rec: 1.1552 acc_train: 0.4543 loss_val: 1.1624 acc_val: 0.4541 time: 0.0020s\n",
      "Epoch: 01298 loss_train: 1.1551 loss_rec: 1.1551 acc_train: 0.4542 loss_val: 1.1624 acc_val: 0.4537 time: 0.0020s\n",
      "Epoch: 01299 loss_train: 1.1550 loss_rec: 1.1550 acc_train: 0.4542 loss_val: 1.1623 acc_val: 0.4537 time: 0.0015s\n",
      "Epoch: 01300 loss_train: 1.1550 loss_rec: 1.1550 acc_train: 0.4542 loss_val: 1.1622 acc_val: 0.4537 time: 0.0020s\n",
      "Epoch: 01301 loss_train: 1.1549 loss_rec: 1.1549 acc_train: 0.4542 loss_val: 1.1622 acc_val: 0.4534 time: 0.0015s\n",
      "Test set results: loss= 1.1863 accuracy= 0.3938\n",
      "Epoch: 01302 loss_train: 1.1549 loss_rec: 1.1549 acc_train: 0.4542 loss_val: 1.1621 acc_val: 0.4534 time: 0.0020s\n",
      "Epoch: 01303 loss_train: 1.1548 loss_rec: 1.1548 acc_train: 0.4542 loss_val: 1.1621 acc_val: 0.4534 time: 0.0015s\n",
      "Epoch: 01304 loss_train: 1.1547 loss_rec: 1.1547 acc_train: 0.4542 loss_val: 1.1620 acc_val: 0.4534 time: 0.0020s\n",
      "Epoch: 01305 loss_train: 1.1547 loss_rec: 1.1547 acc_train: 0.4542 loss_val: 1.1620 acc_val: 0.4534 time: 0.0020s\n",
      "Epoch: 01306 loss_train: 1.1546 loss_rec: 1.1546 acc_train: 0.4542 loss_val: 1.1619 acc_val: 0.4534 time: 0.0015s\n",
      "Epoch: 01307 loss_train: 1.1546 loss_rec: 1.1546 acc_train: 0.4544 loss_val: 1.1619 acc_val: 0.4534 time: 0.0020s\n",
      "Epoch: 01308 loss_train: 1.1545 loss_rec: 1.1545 acc_train: 0.4544 loss_val: 1.1618 acc_val: 0.4534 time: 0.0015s\n",
      "Epoch: 01309 loss_train: 1.1544 loss_rec: 1.1544 acc_train: 0.4554 loss_val: 1.1618 acc_val: 0.4559 time: 0.0015s\n",
      "Epoch: 01310 loss_train: 1.1544 loss_rec: 1.1544 acc_train: 0.4555 loss_val: 1.1617 acc_val: 0.4559 time: 0.0015s\n",
      "Epoch: 01311 loss_train: 1.1543 loss_rec: 1.1543 acc_train: 0.4555 loss_val: 1.1617 acc_val: 0.4559 time: 0.0020s\n",
      "Test set results: loss= 1.1859 accuracy= 0.3953\n",
      "Epoch: 01312 loss_train: 1.1543 loss_rec: 1.1543 acc_train: 0.4558 loss_val: 1.1616 acc_val: 0.4559 time: 0.0020s\n",
      "Epoch: 01313 loss_train: 1.1542 loss_rec: 1.1542 acc_train: 0.4558 loss_val: 1.1616 acc_val: 0.4559 time: 0.0025s\n",
      "Epoch: 01314 loss_train: 1.1541 loss_rec: 1.1541 acc_train: 0.4558 loss_val: 1.1615 acc_val: 0.4559 time: 0.0015s\n",
      "Epoch: 01315 loss_train: 1.1541 loss_rec: 1.1541 acc_train: 0.4558 loss_val: 1.1615 acc_val: 0.4559 time: 0.0020s\n",
      "Epoch: 01316 loss_train: 1.1540 loss_rec: 1.1540 acc_train: 0.4558 loss_val: 1.1614 acc_val: 0.4559 time: 0.0015s\n",
      "Epoch: 01317 loss_train: 1.1540 loss_rec: 1.1540 acc_train: 0.4558 loss_val: 1.1614 acc_val: 0.4559 time: 0.0015s\n",
      "Epoch: 01318 loss_train: 1.1539 loss_rec: 1.1539 acc_train: 0.4559 loss_val: 1.1613 acc_val: 0.4559 time: 0.0020s\n",
      "Epoch: 01319 loss_train: 1.1539 loss_rec: 1.1539 acc_train: 0.4559 loss_val: 1.1613 acc_val: 0.4559 time: 0.0015s\n",
      "Epoch: 01320 loss_train: 1.1538 loss_rec: 1.1538 acc_train: 0.4559 loss_val: 1.1612 acc_val: 0.4559 time: 0.0015s\n",
      "Epoch: 01321 loss_train: 1.1537 loss_rec: 1.1537 acc_train: 0.4561 loss_val: 1.1612 acc_val: 0.4559 time: 0.0020s\n",
      "Test set results: loss= 1.1855 accuracy= 0.3956\n",
      "Epoch: 01322 loss_train: 1.1537 loss_rec: 1.1537 acc_train: 0.4561 loss_val: 1.1612 acc_val: 0.4562 time: 0.0015s\n",
      "Epoch: 01323 loss_train: 1.1536 loss_rec: 1.1536 acc_train: 0.4561 loss_val: 1.1611 acc_val: 0.4562 time: 0.0015s\n",
      "Epoch: 01324 loss_train: 1.1536 loss_rec: 1.1536 acc_train: 0.4561 loss_val: 1.1611 acc_val: 0.4562 time: 0.0015s\n",
      "Epoch: 01325 loss_train: 1.1535 loss_rec: 1.1535 acc_train: 0.4561 loss_val: 1.1610 acc_val: 0.4562 time: 0.0015s\n",
      "Epoch: 01326 loss_train: 1.1534 loss_rec: 1.1534 acc_train: 0.4561 loss_val: 1.1610 acc_val: 0.4562 time: 0.0015s\n",
      "Epoch: 01327 loss_train: 1.1534 loss_rec: 1.1534 acc_train: 0.4559 loss_val: 1.1609 acc_val: 0.4556 time: 0.0015s\n",
      "Epoch: 01328 loss_train: 1.1533 loss_rec: 1.1533 acc_train: 0.4560 loss_val: 1.1609 acc_val: 0.4556 time: 0.0020s\n",
      "Epoch: 01329 loss_train: 1.1533 loss_rec: 1.1533 acc_train: 0.4561 loss_val: 1.1608 acc_val: 0.4556 time: 0.0015s\n",
      "Epoch: 01330 loss_train: 1.1532 loss_rec: 1.1532 acc_train: 0.4560 loss_val: 1.1608 acc_val: 0.4556 time: 0.0015s\n",
      "Epoch: 01331 loss_train: 1.1532 loss_rec: 1.1532 acc_train: 0.4560 loss_val: 1.1607 acc_val: 0.4556 time: 0.0015s\n",
      "Test set results: loss= 1.1852 accuracy= 0.3949\n",
      "Epoch: 01332 loss_train: 1.1531 loss_rec: 1.1531 acc_train: 0.4561 loss_val: 1.1607 acc_val: 0.4556 time: 0.0015s\n",
      "Epoch: 01333 loss_train: 1.1530 loss_rec: 1.1530 acc_train: 0.4560 loss_val: 1.1606 acc_val: 0.4556 time: 0.0020s\n",
      "Epoch: 01334 loss_train: 1.1530 loss_rec: 1.1530 acc_train: 0.4560 loss_val: 1.1606 acc_val: 0.4559 time: 0.0015s\n",
      "Epoch: 01335 loss_train: 1.1529 loss_rec: 1.1529 acc_train: 0.4560 loss_val: 1.1605 acc_val: 0.4559 time: 0.0015s\n",
      "Epoch: 01336 loss_train: 1.1529 loss_rec: 1.1529 acc_train: 0.4559 loss_val: 1.1605 acc_val: 0.4559 time: 0.0015s\n",
      "Epoch: 01337 loss_train: 1.1528 loss_rec: 1.1528 acc_train: 0.4559 loss_val: 1.1604 acc_val: 0.4559 time: 0.0020s\n",
      "Epoch: 01338 loss_train: 1.1528 loss_rec: 1.1528 acc_train: 0.4559 loss_val: 1.1604 acc_val: 0.4559 time: 0.0015s\n",
      "Epoch: 01339 loss_train: 1.1527 loss_rec: 1.1527 acc_train: 0.4558 loss_val: 1.1603 acc_val: 0.4559 time: 0.0020s\n",
      "Epoch: 01340 loss_train: 1.1527 loss_rec: 1.1527 acc_train: 0.4560 loss_val: 1.1603 acc_val: 0.4562 time: 0.0015s\n",
      "Epoch: 01341 loss_train: 1.1526 loss_rec: 1.1526 acc_train: 0.4560 loss_val: 1.1602 acc_val: 0.4562 time: 0.0020s\n",
      "Test set results: loss= 1.1848 accuracy= 0.3943\n",
      "Epoch: 01342 loss_train: 1.1525 loss_rec: 1.1525 acc_train: 0.4560 loss_val: 1.1602 acc_val: 0.4562 time: 0.0020s\n",
      "Epoch: 01343 loss_train: 1.1525 loss_rec: 1.1525 acc_train: 0.4560 loss_val: 1.1602 acc_val: 0.4562 time: 0.0015s\n",
      "Epoch: 01344 loss_train: 1.1524 loss_rec: 1.1524 acc_train: 0.4560 loss_val: 1.1601 acc_val: 0.4562 time: 0.0020s\n",
      "Epoch: 01345 loss_train: 1.1524 loss_rec: 1.1524 acc_train: 0.4560 loss_val: 1.1601 acc_val: 0.4562 time: 0.0020s\n",
      "Epoch: 01346 loss_train: 1.1523 loss_rec: 1.1523 acc_train: 0.4560 loss_val: 1.1600 acc_val: 0.4562 time: 0.0015s\n",
      "Epoch: 01347 loss_train: 1.1523 loss_rec: 1.1523 acc_train: 0.4560 loss_val: 1.1600 acc_val: 0.4562 time: 0.0020s\n",
      "Epoch: 01348 loss_train: 1.1522 loss_rec: 1.1522 acc_train: 0.4560 loss_val: 1.1599 acc_val: 0.4562 time: 0.0015s\n",
      "Epoch: 01349 loss_train: 1.1521 loss_rec: 1.1521 acc_train: 0.4550 loss_val: 1.1599 acc_val: 0.4556 time: 0.0020s\n",
      "Epoch: 01350 loss_train: 1.1521 loss_rec: 1.1521 acc_train: 0.4550 loss_val: 1.1598 acc_val: 0.4559 time: 0.0015s\n",
      "Epoch: 01351 loss_train: 1.1520 loss_rec: 1.1520 acc_train: 0.4550 loss_val: 1.1598 acc_val: 0.4559 time: 0.0015s\n",
      "Test set results: loss= 1.1844 accuracy= 0.3930\n",
      "Epoch: 01352 loss_train: 1.1520 loss_rec: 1.1520 acc_train: 0.4551 loss_val: 1.1597 acc_val: 0.4559 time: 0.0020s\n",
      "Epoch: 01353 loss_train: 1.1519 loss_rec: 1.1519 acc_train: 0.4551 loss_val: 1.1597 acc_val: 0.4559 time: 0.0015s\n",
      "Epoch: 01354 loss_train: 1.1519 loss_rec: 1.1519 acc_train: 0.4552 loss_val: 1.1596 acc_val: 0.4559 time: 0.0015s\n",
      "Epoch: 01355 loss_train: 1.1518 loss_rec: 1.1518 acc_train: 0.4548 loss_val: 1.1596 acc_val: 0.4556 time: 0.0020s\n",
      "Epoch: 01356 loss_train: 1.1518 loss_rec: 1.1518 acc_train: 0.4548 loss_val: 1.1596 acc_val: 0.4556 time: 0.0015s\n",
      "Epoch: 01357 loss_train: 1.1517 loss_rec: 1.1517 acc_train: 0.4547 loss_val: 1.1595 acc_val: 0.4556 time: 0.0015s\n",
      "Epoch: 01358 loss_train: 1.1517 loss_rec: 1.1517 acc_train: 0.4547 loss_val: 1.1595 acc_val: 0.4556 time: 0.0020s\n",
      "Epoch: 01359 loss_train: 1.1516 loss_rec: 1.1516 acc_train: 0.4541 loss_val: 1.1594 acc_val: 0.4544 time: 0.0020s\n",
      "Epoch: 01360 loss_train: 1.1516 loss_rec: 1.1516 acc_train: 0.4529 loss_val: 1.1594 acc_val: 0.4550 time: 0.0020s\n",
      "Epoch: 01361 loss_train: 1.1515 loss_rec: 1.1515 acc_train: 0.4529 loss_val: 1.1593 acc_val: 0.4550 time: 0.0015s\n",
      "Test set results: loss= 1.1842 accuracy= 0.3897\n",
      "Epoch: 01362 loss_train: 1.1514 loss_rec: 1.1514 acc_train: 0.4535 loss_val: 1.1593 acc_val: 0.4556 time: 0.0015s\n",
      "Epoch: 01363 loss_train: 1.1514 loss_rec: 1.1514 acc_train: 0.4535 loss_val: 1.1592 acc_val: 0.4556 time: 0.0015s\n",
      "Epoch: 01364 loss_train: 1.1513 loss_rec: 1.1513 acc_train: 0.4535 loss_val: 1.1592 acc_val: 0.4556 time: 0.0015s\n",
      "Epoch: 01365 loss_train: 1.1513 loss_rec: 1.1513 acc_train: 0.4535 loss_val: 1.1591 acc_val: 0.4556 time: 0.0015s\n",
      "Epoch: 01366 loss_train: 1.1512 loss_rec: 1.1512 acc_train: 0.4535 loss_val: 1.1591 acc_val: 0.4556 time: 0.0015s\n",
      "Epoch: 01367 loss_train: 1.1512 loss_rec: 1.1512 acc_train: 0.4535 loss_val: 1.1590 acc_val: 0.4556 time: 0.0015s\n",
      "Epoch: 01368 loss_train: 1.1511 loss_rec: 1.1511 acc_train: 0.4535 loss_val: 1.1590 acc_val: 0.4556 time: 0.0020s\n",
      "Epoch: 01369 loss_train: 1.1511 loss_rec: 1.1511 acc_train: 0.4535 loss_val: 1.1590 acc_val: 0.4556 time: 0.0015s\n",
      "Epoch: 01370 loss_train: 1.1510 loss_rec: 1.1510 acc_train: 0.4535 loss_val: 1.1589 acc_val: 0.4556 time: 0.0015s\n",
      "Epoch: 01371 loss_train: 1.1510 loss_rec: 1.1510 acc_train: 0.4535 loss_val: 1.1589 acc_val: 0.4556 time: 0.0015s\n",
      "Test set results: loss= 1.1839 accuracy= 0.3897\n",
      "Epoch: 01372 loss_train: 1.1509 loss_rec: 1.1509 acc_train: 0.4534 loss_val: 1.1588 acc_val: 0.4556 time: 0.0015s\n",
      "Epoch: 01373 loss_train: 1.1509 loss_rec: 1.1509 acc_train: 0.4535 loss_val: 1.1588 acc_val: 0.4556 time: 0.0020s\n",
      "Epoch: 01374 loss_train: 1.1508 loss_rec: 1.1508 acc_train: 0.4535 loss_val: 1.1587 acc_val: 0.4556 time: 0.0015s\n",
      "Epoch: 01375 loss_train: 1.1507 loss_rec: 1.1507 acc_train: 0.4535 loss_val: 1.1587 acc_val: 0.4556 time: 0.0020s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01376 loss_train: 1.1507 loss_rec: 1.1507 acc_train: 0.4535 loss_val: 1.1586 acc_val: 0.4556 time: 0.0020s\n",
      "Epoch: 01377 loss_train: 1.1506 loss_rec: 1.1506 acc_train: 0.4534 loss_val: 1.1586 acc_val: 0.4556 time: 0.0020s\n",
      "Epoch: 01378 loss_train: 1.1506 loss_rec: 1.1506 acc_train: 0.4534 loss_val: 1.1586 acc_val: 0.4556 time: 0.0020s\n",
      "Epoch: 01379 loss_train: 1.1505 loss_rec: 1.1505 acc_train: 0.4531 loss_val: 1.1585 acc_val: 0.4550 time: 0.0015s\n",
      "Epoch: 01380 loss_train: 1.1505 loss_rec: 1.1505 acc_train: 0.4531 loss_val: 1.1585 acc_val: 0.4550 time: 0.0015s\n",
      "Epoch: 01381 loss_train: 1.1504 loss_rec: 1.1504 acc_train: 0.4531 loss_val: 1.1584 acc_val: 0.4550 time: 0.0015s\n",
      "Test set results: loss= 1.1836 accuracy= 0.3891\n",
      "Epoch: 01382 loss_train: 1.1504 loss_rec: 1.1504 acc_train: 0.4531 loss_val: 1.1584 acc_val: 0.4550 time: 0.0015s\n",
      "Epoch: 01383 loss_train: 1.1503 loss_rec: 1.1503 acc_train: 0.4531 loss_val: 1.1583 acc_val: 0.4553 time: 0.0020s\n",
      "Epoch: 01384 loss_train: 1.1503 loss_rec: 1.1503 acc_train: 0.4531 loss_val: 1.1583 acc_val: 0.4553 time: 0.0015s\n",
      "Epoch: 01385 loss_train: 1.1502 loss_rec: 1.1502 acc_train: 0.4531 loss_val: 1.1582 acc_val: 0.4553 time: 0.0020s\n",
      "Epoch: 01386 loss_train: 1.1502 loss_rec: 1.1502 acc_train: 0.4524 loss_val: 1.1582 acc_val: 0.4550 time: 0.0020s\n",
      "Epoch: 01387 loss_train: 1.1501 loss_rec: 1.1501 acc_train: 0.4524 loss_val: 1.1582 acc_val: 0.4550 time: 0.0015s\n",
      "Epoch: 01388 loss_train: 1.1501 loss_rec: 1.1501 acc_train: 0.4524 loss_val: 1.1581 acc_val: 0.4550 time: 0.0020s\n",
      "Epoch: 01389 loss_train: 1.1500 loss_rec: 1.1500 acc_train: 0.4524 loss_val: 1.1581 acc_val: 0.4550 time: 0.0015s\n",
      "Epoch: 01390 loss_train: 1.1500 loss_rec: 1.1500 acc_train: 0.4524 loss_val: 1.1580 acc_val: 0.4550 time: 0.0020s\n",
      "Epoch: 01391 loss_train: 1.1499 loss_rec: 1.1499 acc_train: 0.4524 loss_val: 1.1580 acc_val: 0.4550 time: 0.0015s\n",
      "Test set results: loss= 1.1833 accuracy= 0.3886\n",
      "Epoch: 01392 loss_train: 1.1499 loss_rec: 1.1499 acc_train: 0.4524 loss_val: 1.1579 acc_val: 0.4550 time: 0.0020s\n",
      "Epoch: 01393 loss_train: 1.1498 loss_rec: 1.1498 acc_train: 0.4524 loss_val: 1.1579 acc_val: 0.4550 time: 0.0015s\n",
      "Epoch: 01394 loss_train: 1.1498 loss_rec: 1.1498 acc_train: 0.4524 loss_val: 1.1579 acc_val: 0.4547 time: 0.0020s\n",
      "Epoch: 01395 loss_train: 1.1497 loss_rec: 1.1497 acc_train: 0.4524 loss_val: 1.1578 acc_val: 0.4547 time: 0.0015s\n",
      "Epoch: 01396 loss_train: 1.1497 loss_rec: 1.1497 acc_train: 0.4524 loss_val: 1.1578 acc_val: 0.4547 time: 0.0015s\n",
      "Epoch: 01397 loss_train: 1.1496 loss_rec: 1.1496 acc_train: 0.4524 loss_val: 1.1577 acc_val: 0.4547 time: 0.0020s\n",
      "Epoch: 01398 loss_train: 1.1496 loss_rec: 1.1496 acc_train: 0.4524 loss_val: 1.1577 acc_val: 0.4547 time: 0.0015s\n",
      "Epoch: 01399 loss_train: 1.1495 loss_rec: 1.1495 acc_train: 0.4524 loss_val: 1.1576 acc_val: 0.4547 time: 0.0020s\n",
      "Epoch: 01400 loss_train: 1.1494 loss_rec: 1.1494 acc_train: 0.4548 loss_val: 1.1576 acc_val: 0.4594 time: 0.0020s\n",
      "Epoch: 01401 loss_train: 1.1494 loss_rec: 1.1494 acc_train: 0.4548 loss_val: 1.1576 acc_val: 0.4594 time: 0.0015s\n",
      "Test set results: loss= 1.1830 accuracy= 0.3896\n",
      "Epoch: 01402 loss_train: 1.1493 loss_rec: 1.1493 acc_train: 0.4548 loss_val: 1.1575 acc_val: 0.4594 time: 0.0025s\n",
      "Epoch: 01403 loss_train: 1.1493 loss_rec: 1.1493 acc_train: 0.4548 loss_val: 1.1575 acc_val: 0.4594 time: 0.0020s\n",
      "Epoch: 01404 loss_train: 1.1492 loss_rec: 1.1492 acc_train: 0.4548 loss_val: 1.1574 acc_val: 0.4594 time: 0.0020s\n",
      "Epoch: 01405 loss_train: 1.1492 loss_rec: 1.1492 acc_train: 0.4549 loss_val: 1.1574 acc_val: 0.4594 time: 0.0020s\n",
      "Epoch: 01406 loss_train: 1.1491 loss_rec: 1.1491 acc_train: 0.4550 loss_val: 1.1573 acc_val: 0.4594 time: 0.0015s\n",
      "Epoch: 01407 loss_train: 1.1491 loss_rec: 1.1491 acc_train: 0.4550 loss_val: 1.1573 acc_val: 0.4594 time: 0.0020s\n",
      "Epoch: 01408 loss_train: 1.1490 loss_rec: 1.1490 acc_train: 0.4550 loss_val: 1.1573 acc_val: 0.4594 time: 0.0015s\n",
      "Epoch: 01409 loss_train: 1.1490 loss_rec: 1.1490 acc_train: 0.4550 loss_val: 1.1572 acc_val: 0.4594 time: 0.0020s\n",
      "Epoch: 01410 loss_train: 1.1489 loss_rec: 1.1489 acc_train: 0.4550 loss_val: 1.1572 acc_val: 0.4594 time: 0.0015s\n",
      "Epoch: 01411 loss_train: 1.1489 loss_rec: 1.1489 acc_train: 0.4550 loss_val: 1.1571 acc_val: 0.4597 time: 0.0020s\n",
      "Test set results: loss= 1.1827 accuracy= 0.3896\n",
      "Epoch: 01412 loss_train: 1.1488 loss_rec: 1.1488 acc_train: 0.4550 loss_val: 1.1571 acc_val: 0.4597 time: 0.0015s\n",
      "Epoch: 01413 loss_train: 1.1488 loss_rec: 1.1488 acc_train: 0.4550 loss_val: 1.1571 acc_val: 0.4597 time: 0.0020s\n",
      "Epoch: 01414 loss_train: 1.1487 loss_rec: 1.1487 acc_train: 0.4550 loss_val: 1.1570 acc_val: 0.4597 time: 0.0015s\n",
      "Epoch: 01415 loss_train: 1.1487 loss_rec: 1.1487 acc_train: 0.4550 loss_val: 1.1570 acc_val: 0.4597 time: 0.0020s\n",
      "Epoch: 01416 loss_train: 1.1486 loss_rec: 1.1486 acc_train: 0.4550 loss_val: 1.1569 acc_val: 0.4597 time: 0.0015s\n",
      "Epoch: 01417 loss_train: 1.1486 loss_rec: 1.1486 acc_train: 0.4550 loss_val: 1.1569 acc_val: 0.4597 time: 0.0020s\n",
      "Epoch: 01418 loss_train: 1.1485 loss_rec: 1.1485 acc_train: 0.4550 loss_val: 1.1568 acc_val: 0.4597 time: 0.0015s\n",
      "Epoch: 01419 loss_train: 1.1485 loss_rec: 1.1485 acc_train: 0.4550 loss_val: 1.1568 acc_val: 0.4597 time: 0.0015s\n",
      "Epoch: 01420 loss_train: 1.1484 loss_rec: 1.1484 acc_train: 0.4556 loss_val: 1.1568 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01421 loss_train: 1.1484 loss_rec: 1.1484 acc_train: 0.4556 loss_val: 1.1567 acc_val: 0.4603 time: 0.0020s\n",
      "Test set results: loss= 1.1825 accuracy= 0.3898\n",
      "Epoch: 01422 loss_train: 1.1483 loss_rec: 1.1483 acc_train: 0.4556 loss_val: 1.1567 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01423 loss_train: 1.1483 loss_rec: 1.1483 acc_train: 0.4556 loss_val: 1.1566 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01424 loss_train: 1.1482 loss_rec: 1.1482 acc_train: 0.4556 loss_val: 1.1566 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01425 loss_train: 1.1482 loss_rec: 1.1482 acc_train: 0.4556 loss_val: 1.1566 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01426 loss_train: 1.1482 loss_rec: 1.1482 acc_train: 0.4556 loss_val: 1.1565 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01427 loss_train: 1.1481 loss_rec: 1.1481 acc_train: 0.4556 loss_val: 1.1565 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01428 loss_train: 1.1481 loss_rec: 1.1481 acc_train: 0.4556 loss_val: 1.1564 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01429 loss_train: 1.1480 loss_rec: 1.1480 acc_train: 0.4556 loss_val: 1.1564 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01430 loss_train: 1.1480 loss_rec: 1.1480 acc_train: 0.4556 loss_val: 1.1564 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01431 loss_train: 1.1479 loss_rec: 1.1479 acc_train: 0.4556 loss_val: 1.1563 acc_val: 0.4603 time: 0.0020s\n",
      "Test set results: loss= 1.1822 accuracy= 0.3898\n",
      "Epoch: 01432 loss_train: 1.1479 loss_rec: 1.1479 acc_train: 0.4556 loss_val: 1.1563 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01433 loss_train: 1.1478 loss_rec: 1.1478 acc_train: 0.4556 loss_val: 1.1562 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01434 loss_train: 1.1478 loss_rec: 1.1478 acc_train: 0.4556 loss_val: 1.1562 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01435 loss_train: 1.1477 loss_rec: 1.1477 acc_train: 0.4556 loss_val: 1.1562 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01436 loss_train: 1.1477 loss_rec: 1.1477 acc_train: 0.4556 loss_val: 1.1561 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01437 loss_train: 1.1476 loss_rec: 1.1476 acc_train: 0.4556 loss_val: 1.1561 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01438 loss_train: 1.1476 loss_rec: 1.1476 acc_train: 0.4556 loss_val: 1.1560 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01439 loss_train: 1.1475 loss_rec: 1.1475 acc_train: 0.4556 loss_val: 1.1560 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01440 loss_train: 1.1475 loss_rec: 1.1475 acc_train: 0.4556 loss_val: 1.1560 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01441 loss_train: 1.1474 loss_rec: 1.1474 acc_train: 0.4556 loss_val: 1.1559 acc_val: 0.4603 time: 0.0015s\n",
      "Test set results: loss= 1.1820 accuracy= 0.3898\n",
      "Epoch: 01442 loss_train: 1.1474 loss_rec: 1.1474 acc_train: 0.4556 loss_val: 1.1559 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01443 loss_train: 1.1473 loss_rec: 1.1473 acc_train: 0.4556 loss_val: 1.1558 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01444 loss_train: 1.1473 loss_rec: 1.1473 acc_train: 0.4560 loss_val: 1.1558 acc_val: 0.4619 time: 0.0020s\n",
      "Epoch: 01445 loss_train: 1.1472 loss_rec: 1.1472 acc_train: 0.4559 loss_val: 1.1558 acc_val: 0.4619 time: 0.0015s\n",
      "Epoch: 01446 loss_train: 1.1472 loss_rec: 1.1472 acc_train: 0.4559 loss_val: 1.1557 acc_val: 0.4619 time: 0.0020s\n",
      "Epoch: 01447 loss_train: 1.1471 loss_rec: 1.1471 acc_train: 0.4560 loss_val: 1.1557 acc_val: 0.4619 time: 0.0020s\n",
      "Epoch: 01448 loss_train: 1.1471 loss_rec: 1.1471 acc_train: 0.4559 loss_val: 1.1556 acc_val: 0.4619 time: 0.0015s\n",
      "Epoch: 01449 loss_train: 1.1470 loss_rec: 1.1470 acc_train: 0.4559 loss_val: 1.1556 acc_val: 0.4619 time: 0.0020s\n",
      "Epoch: 01450 loss_train: 1.1470 loss_rec: 1.1470 acc_train: 0.4559 loss_val: 1.1556 acc_val: 0.4619 time: 0.0015s\n",
      "Epoch: 01451 loss_train: 1.1469 loss_rec: 1.1469 acc_train: 0.4560 loss_val: 1.1555 acc_val: 0.4619 time: 0.0020s\n",
      "Test set results: loss= 1.1817 accuracy= 0.3905\n",
      "Epoch: 01452 loss_train: 1.1469 loss_rec: 1.1469 acc_train: 0.4560 loss_val: 1.1555 acc_val: 0.4619 time: 0.0015s\n",
      "Epoch: 01453 loss_train: 1.1469 loss_rec: 1.1469 acc_train: 0.4559 loss_val: 1.1554 acc_val: 0.4619 time: 0.0015s\n",
      "Epoch: 01454 loss_train: 1.1468 loss_rec: 1.1468 acc_train: 0.4559 loss_val: 1.1554 acc_val: 0.4619 time: 0.0020s\n",
      "Epoch: 01455 loss_train: 1.1468 loss_rec: 1.1468 acc_train: 0.4559 loss_val: 1.1554 acc_val: 0.4619 time: 0.0020s\n",
      "Epoch: 01456 loss_train: 1.1467 loss_rec: 1.1467 acc_train: 0.4559 loss_val: 1.1553 acc_val: 0.4619 time: 0.0020s\n",
      "Epoch: 01457 loss_train: 1.1467 loss_rec: 1.1467 acc_train: 0.4560 loss_val: 1.1553 acc_val: 0.4622 time: 0.0020s\n",
      "Epoch: 01458 loss_train: 1.1466 loss_rec: 1.1466 acc_train: 0.4560 loss_val: 1.1552 acc_val: 0.4622 time: 0.0020s\n",
      "Epoch: 01459 loss_train: 1.1466 loss_rec: 1.1466 acc_train: 0.4561 loss_val: 1.1552 acc_val: 0.4622 time: 0.0020s\n",
      "Epoch: 01460 loss_train: 1.1465 loss_rec: 1.1465 acc_train: 0.4561 loss_val: 1.1552 acc_val: 0.4622 time: 0.0025s\n",
      "Epoch: 01461 loss_train: 1.1465 loss_rec: 1.1465 acc_train: 0.4561 loss_val: 1.1551 acc_val: 0.4622 time: 0.0020s\n",
      "Test set results: loss= 1.1815 accuracy= 0.3905\n",
      "Epoch: 01462 loss_train: 1.1464 loss_rec: 1.1464 acc_train: 0.4561 loss_val: 1.1551 acc_val: 0.4622 time: 0.0025s\n",
      "Epoch: 01463 loss_train: 1.1464 loss_rec: 1.1464 acc_train: 0.4561 loss_val: 1.1550 acc_val: 0.4622 time: 0.0025s\n",
      "Epoch: 01464 loss_train: 1.1463 loss_rec: 1.1463 acc_train: 0.4561 loss_val: 1.1550 acc_val: 0.4622 time: 0.0025s\n",
      "Epoch: 01465 loss_train: 1.1463 loss_rec: 1.1463 acc_train: 0.4561 loss_val: 1.1550 acc_val: 0.4622 time: 0.0015s\n",
      "Epoch: 01466 loss_train: 1.1462 loss_rec: 1.1462 acc_train: 0.4561 loss_val: 1.1549 acc_val: 0.4622 time: 0.0020s\n",
      "Epoch: 01467 loss_train: 1.1462 loss_rec: 1.1462 acc_train: 0.4564 loss_val: 1.1549 acc_val: 0.4622 time: 0.0020s\n",
      "Epoch: 01468 loss_train: 1.1462 loss_rec: 1.1462 acc_train: 0.4565 loss_val: 1.1549 acc_val: 0.4622 time: 0.0015s\n",
      "Epoch: 01469 loss_train: 1.1461 loss_rec: 1.1461 acc_train: 0.4563 loss_val: 1.1548 acc_val: 0.4622 time: 0.0020s\n",
      "Epoch: 01470 loss_train: 1.1461 loss_rec: 1.1461 acc_train: 0.4563 loss_val: 1.1548 acc_val: 0.4622 time: 0.0020s\n",
      "Epoch: 01471 loss_train: 1.1460 loss_rec: 1.1460 acc_train: 0.4563 loss_val: 1.1547 acc_val: 0.4622 time: 0.0025s\n",
      "Test set results: loss= 1.1812 accuracy= 0.3905\n",
      "Epoch: 01472 loss_train: 1.1460 loss_rec: 1.1460 acc_train: 0.4564 loss_val: 1.1547 acc_val: 0.4622 time: 0.0020s\n",
      "Epoch: 01473 loss_train: 1.1459 loss_rec: 1.1459 acc_train: 0.4564 loss_val: 1.1547 acc_val: 0.4622 time: 0.0020s\n",
      "Epoch: 01474 loss_train: 1.1459 loss_rec: 1.1459 acc_train: 0.4564 loss_val: 1.1546 acc_val: 0.4622 time: 0.0020s\n",
      "Epoch: 01475 loss_train: 1.1458 loss_rec: 1.1458 acc_train: 0.4564 loss_val: 1.1546 acc_val: 0.4622 time: 0.0020s\n",
      "Epoch: 01476 loss_train: 1.1458 loss_rec: 1.1458 acc_train: 0.4564 loss_val: 1.1546 acc_val: 0.4622 time: 0.0020s\n",
      "Epoch: 01477 loss_train: 1.1457 loss_rec: 1.1457 acc_train: 0.4564 loss_val: 1.1545 acc_val: 0.4622 time: 0.0015s\n",
      "Epoch: 01478 loss_train: 1.1457 loss_rec: 1.1457 acc_train: 0.4564 loss_val: 1.1545 acc_val: 0.4622 time: 0.0025s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01479 loss_train: 1.1457 loss_rec: 1.1457 acc_train: 0.4556 loss_val: 1.1544 acc_val: 0.4606 time: 0.0020s\n",
      "Epoch: 01480 loss_train: 1.1456 loss_rec: 1.1456 acc_train: 0.4556 loss_val: 1.1544 acc_val: 0.4606 time: 0.0015s\n",
      "Epoch: 01481 loss_train: 1.1456 loss_rec: 1.1456 acc_train: 0.4551 loss_val: 1.1544 acc_val: 0.4597 time: 0.0020s\n",
      "Test set results: loss= 1.1809 accuracy= 0.3889\n",
      "Epoch: 01482 loss_train: 1.1455 loss_rec: 1.1455 acc_train: 0.4555 loss_val: 1.1543 acc_val: 0.4600 time: 0.0015s\n",
      "Epoch: 01483 loss_train: 1.1455 loss_rec: 1.1455 acc_train: 0.4555 loss_val: 1.1543 acc_val: 0.4600 time: 0.0015s\n",
      "Epoch: 01484 loss_train: 1.1454 loss_rec: 1.1454 acc_train: 0.4551 loss_val: 1.1543 acc_val: 0.4597 time: 0.0020s\n",
      "Epoch: 01485 loss_train: 1.1454 loss_rec: 1.1454 acc_train: 0.4551 loss_val: 1.1542 acc_val: 0.4597 time: 0.0015s\n",
      "Epoch: 01486 loss_train: 1.1453 loss_rec: 1.1453 acc_train: 0.4551 loss_val: 1.1542 acc_val: 0.4597 time: 0.0020s\n",
      "Epoch: 01487 loss_train: 1.1453 loss_rec: 1.1453 acc_train: 0.4552 loss_val: 1.1542 acc_val: 0.4597 time: 0.0015s\n",
      "Epoch: 01488 loss_train: 1.1452 loss_rec: 1.1452 acc_train: 0.4552 loss_val: 1.1541 acc_val: 0.4597 time: 0.0015s\n",
      "Epoch: 01489 loss_train: 1.1452 loss_rec: 1.1452 acc_train: 0.4555 loss_val: 1.1541 acc_val: 0.4597 time: 0.0020s\n",
      "Epoch: 01490 loss_train: 1.1452 loss_rec: 1.1452 acc_train: 0.4556 loss_val: 1.1540 acc_val: 0.4597 time: 0.0015s\n",
      "Epoch: 01491 loss_train: 1.1451 loss_rec: 1.1451 acc_train: 0.4559 loss_val: 1.1540 acc_val: 0.4600 time: 0.0015s\n",
      "Test set results: loss= 1.1807 accuracy= 0.3891\n",
      "Epoch: 01492 loss_train: 1.1451 loss_rec: 1.1451 acc_train: 0.4560 loss_val: 1.1540 acc_val: 0.4600 time: 0.0020s\n",
      "Epoch: 01493 loss_train: 1.1450 loss_rec: 1.1450 acc_train: 0.4560 loss_val: 1.1539 acc_val: 0.4600 time: 0.0015s\n",
      "Epoch: 01494 loss_train: 1.1450 loss_rec: 1.1450 acc_train: 0.4560 loss_val: 1.1539 acc_val: 0.4600 time: 0.0015s\n",
      "Epoch: 01495 loss_train: 1.1449 loss_rec: 1.1449 acc_train: 0.4560 loss_val: 1.1539 acc_val: 0.4600 time: 0.0015s\n",
      "Epoch: 01496 loss_train: 1.1449 loss_rec: 1.1449 acc_train: 0.4559 loss_val: 1.1538 acc_val: 0.4600 time: 0.0015s\n",
      "Epoch: 01497 loss_train: 1.1449 loss_rec: 1.1449 acc_train: 0.4561 loss_val: 1.1538 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01498 loss_train: 1.1448 loss_rec: 1.1448 acc_train: 0.4561 loss_val: 1.1538 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01499 loss_train: 1.1448 loss_rec: 1.1448 acc_train: 0.4562 loss_val: 1.1537 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01500 loss_train: 1.1447 loss_rec: 1.1447 acc_train: 0.4562 loss_val: 1.1537 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01501 loss_train: 1.1447 loss_rec: 1.1447 acc_train: 0.4562 loss_val: 1.1536 acc_val: 0.4603 time: 0.0020s\n",
      "Test set results: loss= 1.1804 accuracy= 0.3890\n",
      "Epoch: 01502 loss_train: 1.1446 loss_rec: 1.1446 acc_train: 0.4562 loss_val: 1.1536 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01503 loss_train: 1.1446 loss_rec: 1.1446 acc_train: 0.4562 loss_val: 1.1536 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01504 loss_train: 1.1445 loss_rec: 1.1445 acc_train: 0.4562 loss_val: 1.1535 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01505 loss_train: 1.1445 loss_rec: 1.1445 acc_train: 0.4562 loss_val: 1.1535 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01506 loss_train: 1.1445 loss_rec: 1.1445 acc_train: 0.4562 loss_val: 1.1535 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01507 loss_train: 1.1444 loss_rec: 1.1444 acc_train: 0.4562 loss_val: 1.1534 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01508 loss_train: 1.1444 loss_rec: 1.1444 acc_train: 0.4562 loss_val: 1.1534 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01509 loss_train: 1.1443 loss_rec: 1.1443 acc_train: 0.4562 loss_val: 1.1534 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01510 loss_train: 1.1443 loss_rec: 1.1443 acc_train: 0.4562 loss_val: 1.1533 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01511 loss_train: 1.1442 loss_rec: 1.1442 acc_train: 0.4562 loss_val: 1.1533 acc_val: 0.4603 time: 0.0015s\n",
      "Test set results: loss= 1.1802 accuracy= 0.3891\n",
      "Epoch: 01512 loss_train: 1.1442 loss_rec: 1.1442 acc_train: 0.4562 loss_val: 1.1532 acc_val: 0.4603 time: 0.0025s\n",
      "Epoch: 01513 loss_train: 1.1442 loss_rec: 1.1442 acc_train: 0.4562 loss_val: 1.1532 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01514 loss_train: 1.1441 loss_rec: 1.1441 acc_train: 0.4562 loss_val: 1.1532 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01515 loss_train: 1.1441 loss_rec: 1.1441 acc_train: 0.4562 loss_val: 1.1531 acc_val: 0.4603 time: 0.0025s\n",
      "Epoch: 01516 loss_train: 1.1440 loss_rec: 1.1440 acc_train: 0.4562 loss_val: 1.1531 acc_val: 0.4603 time: 0.0025s\n",
      "Epoch: 01517 loss_train: 1.1440 loss_rec: 1.1440 acc_train: 0.4562 loss_val: 1.1531 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01518 loss_train: 1.1439 loss_rec: 1.1439 acc_train: 0.4563 loss_val: 1.1530 acc_val: 0.4603 time: 0.0025s\n",
      "Epoch: 01519 loss_train: 1.1439 loss_rec: 1.1439 acc_train: 0.4563 loss_val: 1.1530 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01520 loss_train: 1.1439 loss_rec: 1.1439 acc_train: 0.4563 loss_val: 1.1530 acc_val: 0.4603 time: 0.0025s\n",
      "Epoch: 01521 loss_train: 1.1438 loss_rec: 1.1438 acc_train: 0.4563 loss_val: 1.1529 acc_val: 0.4603 time: 0.0015s\n",
      "Test set results: loss= 1.1800 accuracy= 0.3893\n",
      "Epoch: 01522 loss_train: 1.1438 loss_rec: 1.1438 acc_train: 0.4563 loss_val: 1.1529 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01523 loss_train: 1.1437 loss_rec: 1.1437 acc_train: 0.4563 loss_val: 1.1529 acc_val: 0.4603 time: 0.0030s\n",
      "Epoch: 01524 loss_train: 1.1437 loss_rec: 1.1437 acc_train: 0.4563 loss_val: 1.1528 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01525 loss_train: 1.1436 loss_rec: 1.1436 acc_train: 0.4563 loss_val: 1.1528 acc_val: 0.4603 time: 0.0025s\n",
      "Epoch: 01526 loss_train: 1.1436 loss_rec: 1.1436 acc_train: 0.4564 loss_val: 1.1527 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01527 loss_train: 1.1436 loss_rec: 1.1436 acc_train: 0.4564 loss_val: 1.1527 acc_val: 0.4603 time: 0.0025s\n",
      "Epoch: 01528 loss_train: 1.1435 loss_rec: 1.1435 acc_train: 0.4564 loss_val: 1.1527 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01529 loss_train: 1.1435 loss_rec: 1.1435 acc_train: 0.4569 loss_val: 1.1526 acc_val: 0.4612 time: 0.0020s\n",
      "Epoch: 01530 loss_train: 1.1434 loss_rec: 1.1434 acc_train: 0.4598 loss_val: 1.1526 acc_val: 0.4647 time: 0.0020s\n",
      "Epoch: 01531 loss_train: 1.1434 loss_rec: 1.1434 acc_train: 0.4594 loss_val: 1.1526 acc_val: 0.4637 time: 0.0025s\n",
      "Test set results: loss= 1.1797 accuracy= 0.3932\n",
      "Epoch: 01532 loss_train: 1.1434 loss_rec: 1.1434 acc_train: 0.4594 loss_val: 1.1525 acc_val: 0.4641 time: 0.0025s\n",
      "Epoch: 01533 loss_train: 1.1433 loss_rec: 1.1433 acc_train: 0.4599 loss_val: 1.1525 acc_val: 0.4647 time: 0.0015s\n",
      "Epoch: 01534 loss_train: 1.1433 loss_rec: 1.1433 acc_train: 0.4599 loss_val: 1.1525 acc_val: 0.4647 time: 0.0030s\n",
      "Epoch: 01535 loss_train: 1.1432 loss_rec: 1.1432 acc_train: 0.4596 loss_val: 1.1524 acc_val: 0.4644 time: 0.0020s\n",
      "Epoch: 01536 loss_train: 1.1432 loss_rec: 1.1432 acc_train: 0.4596 loss_val: 1.1524 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01537 loss_train: 1.1431 loss_rec: 1.1431 acc_train: 0.4596 loss_val: 1.1524 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01538 loss_train: 1.1431 loss_rec: 1.1431 acc_train: 0.4596 loss_val: 1.1523 acc_val: 0.4644 time: 0.0030s\n",
      "Epoch: 01539 loss_train: 1.1431 loss_rec: 1.1431 acc_train: 0.4596 loss_val: 1.1523 acc_val: 0.4644 time: 0.0030s\n",
      "Epoch: 01540 loss_train: 1.1430 loss_rec: 1.1430 acc_train: 0.4596 loss_val: 1.1523 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01541 loss_train: 1.1430 loss_rec: 1.1430 acc_train: 0.4596 loss_val: 1.1522 acc_val: 0.4644 time: 0.0025s\n",
      "Test set results: loss= 1.1795 accuracy= 0.3934\n",
      "Epoch: 01542 loss_train: 1.1429 loss_rec: 1.1429 acc_train: 0.4596 loss_val: 1.1522 acc_val: 0.4644 time: 0.0020s\n",
      "Epoch: 01543 loss_train: 1.1429 loss_rec: 1.1429 acc_train: 0.4596 loss_val: 1.1522 acc_val: 0.4644 time: 0.0020s\n",
      "Epoch: 01544 loss_train: 1.1429 loss_rec: 1.1429 acc_train: 0.4597 loss_val: 1.1521 acc_val: 0.4644 time: 0.0020s\n",
      "Epoch: 01545 loss_train: 1.1428 loss_rec: 1.1428 acc_train: 0.4597 loss_val: 1.1521 acc_val: 0.4644 time: 0.0025s\n",
      "Epoch: 01546 loss_train: 1.1428 loss_rec: 1.1428 acc_train: 0.4597 loss_val: 1.1521 acc_val: 0.4644 time: 0.0020s\n",
      "Epoch: 01547 loss_train: 1.1427 loss_rec: 1.1427 acc_train: 0.4596 loss_val: 1.1520 acc_val: 0.4644 time: 0.0025s\n",
      "Epoch: 01548 loss_train: 1.1427 loss_rec: 1.1427 acc_train: 0.4596 loss_val: 1.1520 acc_val: 0.4644 time: 0.0020s\n",
      "Epoch: 01549 loss_train: 1.1426 loss_rec: 1.1426 acc_train: 0.4591 loss_val: 1.1520 acc_val: 0.4644 time: 0.0020s\n",
      "Epoch: 01550 loss_train: 1.1426 loss_rec: 1.1426 acc_train: 0.4591 loss_val: 1.1519 acc_val: 0.4644 time: 0.0020s\n",
      "Epoch: 01551 loss_train: 1.1426 loss_rec: 1.1426 acc_train: 0.4590 loss_val: 1.1519 acc_val: 0.4644 time: 0.0020s\n",
      "Test set results: loss= 1.1793 accuracy= 0.3925\n",
      "Epoch: 01552 loss_train: 1.1425 loss_rec: 1.1425 acc_train: 0.4591 loss_val: 1.1519 acc_val: 0.4644 time: 0.0025s\n",
      "Epoch: 01553 loss_train: 1.1425 loss_rec: 1.1425 acc_train: 0.4591 loss_val: 1.1518 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01554 loss_train: 1.1424 loss_rec: 1.1424 acc_train: 0.4589 loss_val: 1.1518 acc_val: 0.4644 time: 0.0025s\n",
      "Epoch: 01555 loss_train: 1.1424 loss_rec: 1.1424 acc_train: 0.4589 loss_val: 1.1518 acc_val: 0.4644 time: 0.0020s\n",
      "Epoch: 01556 loss_train: 1.1424 loss_rec: 1.1424 acc_train: 0.4589 loss_val: 1.1517 acc_val: 0.4644 time: 0.0030s\n",
      "Epoch: 01557 loss_train: 1.1423 loss_rec: 1.1423 acc_train: 0.4589 loss_val: 1.1517 acc_val: 0.4644 time: 0.0030s\n",
      "Epoch: 01558 loss_train: 1.1423 loss_rec: 1.1423 acc_train: 0.4589 loss_val: 1.1517 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01559 loss_train: 1.1422 loss_rec: 1.1422 acc_train: 0.4589 loss_val: 1.1516 acc_val: 0.4644 time: 0.0020s\n",
      "Epoch: 01560 loss_train: 1.1422 loss_rec: 1.1422 acc_train: 0.4589 loss_val: 1.1516 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01561 loss_train: 1.1422 loss_rec: 1.1422 acc_train: 0.4589 loss_val: 1.1516 acc_val: 0.4644 time: 0.0015s\n",
      "Test set results: loss= 1.1791 accuracy= 0.3924\n",
      "Epoch: 01562 loss_train: 1.1421 loss_rec: 1.1421 acc_train: 0.4589 loss_val: 1.1515 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01563 loss_train: 1.1421 loss_rec: 1.1421 acc_train: 0.4589 loss_val: 1.1515 acc_val: 0.4644 time: 0.0020s\n",
      "Epoch: 01564 loss_train: 1.1420 loss_rec: 1.1420 acc_train: 0.4589 loss_val: 1.1514 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01565 loss_train: 1.1420 loss_rec: 1.1420 acc_train: 0.4589 loss_val: 1.1514 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01566 loss_train: 1.1420 loss_rec: 1.1420 acc_train: 0.4589 loss_val: 1.1514 acc_val: 0.4644 time: 0.0020s\n",
      "Epoch: 01567 loss_train: 1.1419 loss_rec: 1.1419 acc_train: 0.4594 loss_val: 1.1513 acc_val: 0.4653 time: 0.0020s\n",
      "Epoch: 01568 loss_train: 1.1419 loss_rec: 1.1419 acc_train: 0.4593 loss_val: 1.1513 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01569 loss_train: 1.1418 loss_rec: 1.1418 acc_train: 0.4593 loss_val: 1.1513 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01570 loss_train: 1.1418 loss_rec: 1.1418 acc_train: 0.4593 loss_val: 1.1512 acc_val: 0.4653 time: 0.0020s\n",
      "Epoch: 01571 loss_train: 1.1418 loss_rec: 1.1418 acc_train: 0.4594 loss_val: 1.1512 acc_val: 0.4653 time: 0.0015s\n",
      "Test set results: loss= 1.1789 accuracy= 0.3919\n",
      "Epoch: 01572 loss_train: 1.1417 loss_rec: 1.1417 acc_train: 0.4582 loss_val: 1.1512 acc_val: 0.4641 time: 0.0020s\n",
      "Epoch: 01573 loss_train: 1.1417 loss_rec: 1.1417 acc_train: 0.4582 loss_val: 1.1511 acc_val: 0.4641 time: 0.0015s\n",
      "Epoch: 01574 loss_train: 1.1416 loss_rec: 1.1416 acc_train: 0.4582 loss_val: 1.1511 acc_val: 0.4641 time: 0.0020s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01575 loss_train: 1.1416 loss_rec: 1.1416 acc_train: 0.4582 loss_val: 1.1511 acc_val: 0.4641 time: 0.0020s\n",
      "Epoch: 01576 loss_train: 1.1416 loss_rec: 1.1416 acc_train: 0.4599 loss_val: 1.1510 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01577 loss_train: 1.1415 loss_rec: 1.1415 acc_train: 0.4599 loss_val: 1.1510 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01578 loss_train: 1.1415 loss_rec: 1.1415 acc_train: 0.4597 loss_val: 1.1510 acc_val: 0.4656 time: 0.0015s\n",
      "Epoch: 01579 loss_train: 1.1414 loss_rec: 1.1414 acc_train: 0.4586 loss_val: 1.1509 acc_val: 0.4647 time: 0.0020s\n",
      "Epoch: 01580 loss_train: 1.1414 loss_rec: 1.1414 acc_train: 0.4587 loss_val: 1.1509 acc_val: 0.4647 time: 0.0015s\n",
      "Epoch: 01581 loss_train: 1.1414 loss_rec: 1.1414 acc_train: 0.4587 loss_val: 1.1509 acc_val: 0.4647 time: 0.0020s\n",
      "Test set results: loss= 1.1787 accuracy= 0.3935\n",
      "Epoch: 01582 loss_train: 1.1413 loss_rec: 1.1413 acc_train: 0.4589 loss_val: 1.1509 acc_val: 0.4647 time: 0.0020s\n",
      "Epoch: 01583 loss_train: 1.1413 loss_rec: 1.1413 acc_train: 0.4592 loss_val: 1.1508 acc_val: 0.4653 time: 0.0015s\n",
      "Epoch: 01584 loss_train: 1.1412 loss_rec: 1.1412 acc_train: 0.4592 loss_val: 1.1508 acc_val: 0.4653 time: 0.0020s\n",
      "Epoch: 01585 loss_train: 1.1412 loss_rec: 1.1412 acc_train: 0.4592 loss_val: 1.1508 acc_val: 0.4653 time: 0.0015s\n",
      "Epoch: 01586 loss_train: 1.1412 loss_rec: 1.1412 acc_train: 0.4592 loss_val: 1.1507 acc_val: 0.4653 time: 0.0015s\n",
      "Epoch: 01587 loss_train: 1.1411 loss_rec: 1.1411 acc_train: 0.4592 loss_val: 1.1507 acc_val: 0.4653 time: 0.0015s\n",
      "Epoch: 01588 loss_train: 1.1411 loss_rec: 1.1411 acc_train: 0.4597 loss_val: 1.1507 acc_val: 0.4653 time: 0.0015s\n",
      "Epoch: 01589 loss_train: 1.1410 loss_rec: 1.1410 acc_train: 0.4597 loss_val: 1.1506 acc_val: 0.4653 time: 0.0020s\n",
      "Epoch: 01590 loss_train: 1.1410 loss_rec: 1.1410 acc_train: 0.4597 loss_val: 1.1506 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01591 loss_train: 1.1410 loss_rec: 1.1410 acc_train: 0.4597 loss_val: 1.1506 acc_val: 0.4650 time: 0.0015s\n",
      "Test set results: loss= 1.1785 accuracy= 0.3956\n",
      "Epoch: 01592 loss_train: 1.1409 loss_rec: 1.1409 acc_train: 0.4602 loss_val: 1.1505 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01593 loss_train: 1.1409 loss_rec: 1.1409 acc_train: 0.4602 loss_val: 1.1505 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01594 loss_train: 1.1409 loss_rec: 1.1409 acc_train: 0.4602 loss_val: 1.1505 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01595 loss_train: 1.1408 loss_rec: 1.1408 acc_train: 0.4602 loss_val: 1.1504 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01596 loss_train: 1.1408 loss_rec: 1.1408 acc_train: 0.4602 loss_val: 1.1504 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01597 loss_train: 1.1407 loss_rec: 1.1407 acc_train: 0.4602 loss_val: 1.1504 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01598 loss_train: 1.1407 loss_rec: 1.1407 acc_train: 0.4601 loss_val: 1.1503 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01599 loss_train: 1.1407 loss_rec: 1.1407 acc_train: 0.4601 loss_val: 1.1503 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01600 loss_train: 1.1406 loss_rec: 1.1406 acc_train: 0.4601 loss_val: 1.1503 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01601 loss_train: 1.1406 loss_rec: 1.1406 acc_train: 0.4601 loss_val: 1.1502 acc_val: 0.4650 time: 0.0020s\n",
      "Test set results: loss= 1.1783 accuracy= 0.3956\n",
      "Epoch: 01602 loss_train: 1.1406 loss_rec: 1.1406 acc_train: 0.4601 loss_val: 1.1502 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01603 loss_train: 1.1405 loss_rec: 1.1405 acc_train: 0.4601 loss_val: 1.1502 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01604 loss_train: 1.1405 loss_rec: 1.1405 acc_train: 0.4601 loss_val: 1.1501 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01605 loss_train: 1.1404 loss_rec: 1.1404 acc_train: 0.4601 loss_val: 1.1501 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01606 loss_train: 1.1404 loss_rec: 1.1404 acc_train: 0.4602 loss_val: 1.1501 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01607 loss_train: 1.1404 loss_rec: 1.1404 acc_train: 0.4602 loss_val: 1.1500 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01608 loss_train: 1.1403 loss_rec: 1.1403 acc_train: 0.4602 loss_val: 1.1500 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01609 loss_train: 1.1403 loss_rec: 1.1403 acc_train: 0.4602 loss_val: 1.1500 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01610 loss_train: 1.1402 loss_rec: 1.1402 acc_train: 0.4602 loss_val: 1.1499 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01611 loss_train: 1.1402 loss_rec: 1.1402 acc_train: 0.4601 loss_val: 1.1499 acc_val: 0.4650 time: 0.0020s\n",
      "Test set results: loss= 1.1781 accuracy= 0.3956\n",
      "Epoch: 01612 loss_train: 1.1402 loss_rec: 1.1402 acc_train: 0.4601 loss_val: 1.1499 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01613 loss_train: 1.1401 loss_rec: 1.1401 acc_train: 0.4601 loss_val: 1.1499 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01614 loss_train: 1.1401 loss_rec: 1.1401 acc_train: 0.4601 loss_val: 1.1498 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01615 loss_train: 1.1401 loss_rec: 1.1401 acc_train: 0.4601 loss_val: 1.1498 acc_val: 0.4647 time: 0.0020s\n",
      "Epoch: 01616 loss_train: 1.1400 loss_rec: 1.1400 acc_train: 0.4601 loss_val: 1.1498 acc_val: 0.4647 time: 0.0020s\n",
      "Epoch: 01617 loss_train: 1.1400 loss_rec: 1.1400 acc_train: 0.4601 loss_val: 1.1497 acc_val: 0.4647 time: 0.0015s\n",
      "Epoch: 01618 loss_train: 1.1400 loss_rec: 1.1400 acc_train: 0.4601 loss_val: 1.1497 acc_val: 0.4647 time: 0.0020s\n",
      "Epoch: 01619 loss_train: 1.1399 loss_rec: 1.1399 acc_train: 0.4601 loss_val: 1.1497 acc_val: 0.4647 time: 0.0015s\n",
      "Epoch: 01620 loss_train: 1.1399 loss_rec: 1.1399 acc_train: 0.4601 loss_val: 1.1496 acc_val: 0.4647 time: 0.0015s\n",
      "Epoch: 01621 loss_train: 1.1398 loss_rec: 1.1398 acc_train: 0.4601 loss_val: 1.1496 acc_val: 0.4647 time: 0.0020s\n",
      "Test set results: loss= 1.1779 accuracy= 0.3956\n",
      "Epoch: 01622 loss_train: 1.1398 loss_rec: 1.1398 acc_train: 0.4601 loss_val: 1.1496 acc_val: 0.4647 time: 0.0015s\n",
      "Epoch: 01623 loss_train: 1.1398 loss_rec: 1.1398 acc_train: 0.4601 loss_val: 1.1495 acc_val: 0.4647 time: 0.0020s\n",
      "Epoch: 01624 loss_train: 1.1397 loss_rec: 1.1397 acc_train: 0.4601 loss_val: 1.1495 acc_val: 0.4647 time: 0.0015s\n",
      "Epoch: 01625 loss_train: 1.1397 loss_rec: 1.1397 acc_train: 0.4601 loss_val: 1.1495 acc_val: 0.4647 time: 0.0025s\n",
      "Epoch: 01626 loss_train: 1.1397 loss_rec: 1.1397 acc_train: 0.4601 loss_val: 1.1495 acc_val: 0.4647 time: 0.0020s\n",
      "Epoch: 01627 loss_train: 1.1396 loss_rec: 1.1396 acc_train: 0.4601 loss_val: 1.1494 acc_val: 0.4647 time: 0.0020s\n",
      "Epoch: 01628 loss_train: 1.1396 loss_rec: 1.1396 acc_train: 0.4601 loss_val: 1.1494 acc_val: 0.4647 time: 0.0015s\n",
      "Epoch: 01629 loss_train: 1.1395 loss_rec: 1.1395 acc_train: 0.4601 loss_val: 1.1494 acc_val: 0.4647 time: 0.0025s\n",
      "Epoch: 01630 loss_train: 1.1395 loss_rec: 1.1395 acc_train: 0.4602 loss_val: 1.1493 acc_val: 0.4647 time: 0.0020s\n",
      "Epoch: 01631 loss_train: 1.1395 loss_rec: 1.1395 acc_train: 0.4602 loss_val: 1.1493 acc_val: 0.4647 time: 0.0020s\n",
      "Test set results: loss= 1.1777 accuracy= 0.3956\n",
      "Epoch: 01632 loss_train: 1.1394 loss_rec: 1.1394 acc_train: 0.4602 loss_val: 1.1493 acc_val: 0.4647 time: 0.0030s\n",
      "Epoch: 01633 loss_train: 1.1394 loss_rec: 1.1394 acc_train: 0.4602 loss_val: 1.1492 acc_val: 0.4647 time: 0.0015s\n",
      "Epoch: 01634 loss_train: 1.1394 loss_rec: 1.1394 acc_train: 0.4602 loss_val: 1.1492 acc_val: 0.4647 time: 0.0025s\n",
      "Epoch: 01635 loss_train: 1.1393 loss_rec: 1.1393 acc_train: 0.4602 loss_val: 1.1492 acc_val: 0.4647 time: 0.0020s\n",
      "Epoch: 01636 loss_train: 1.1393 loss_rec: 1.1393 acc_train: 0.4602 loss_val: 1.1492 acc_val: 0.4647 time: 0.0025s\n",
      "Epoch: 01637 loss_train: 1.1393 loss_rec: 1.1393 acc_train: 0.4602 loss_val: 1.1491 acc_val: 0.4647 time: 0.0020s\n",
      "Epoch: 01638 loss_train: 1.1392 loss_rec: 1.1392 acc_train: 0.4607 loss_val: 1.1491 acc_val: 0.4653 time: 0.0020s\n",
      "Epoch: 01639 loss_train: 1.1392 loss_rec: 1.1392 acc_train: 0.4607 loss_val: 1.1491 acc_val: 0.4653 time: 0.0015s\n",
      "Epoch: 01640 loss_train: 1.1391 loss_rec: 1.1391 acc_train: 0.4607 loss_val: 1.1490 acc_val: 0.4653 time: 0.0030s\n",
      "Epoch: 01641 loss_train: 1.1391 loss_rec: 1.1391 acc_train: 0.4607 loss_val: 1.1490 acc_val: 0.4653 time: 0.0025s\n",
      "Test set results: loss= 1.1775 accuracy= 0.3963\n",
      "Epoch: 01642 loss_train: 1.1391 loss_rec: 1.1391 acc_train: 0.4607 loss_val: 1.1490 acc_val: 0.4653 time: 0.0015s\n",
      "Epoch: 01643 loss_train: 1.1390 loss_rec: 1.1390 acc_train: 0.4606 loss_val: 1.1489 acc_val: 0.4650 time: 0.0025s\n",
      "Epoch: 01644 loss_train: 1.1390 loss_rec: 1.1390 acc_train: 0.4607 loss_val: 1.1489 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01645 loss_train: 1.1390 loss_rec: 1.1390 acc_train: 0.4607 loss_val: 1.1489 acc_val: 0.4650 time: 0.0025s\n",
      "Epoch: 01646 loss_train: 1.1389 loss_rec: 1.1389 acc_train: 0.4607 loss_val: 1.1488 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01647 loss_train: 1.1389 loss_rec: 1.1389 acc_train: 0.4607 loss_val: 1.1488 acc_val: 0.4650 time: 0.0025s\n",
      "Epoch: 01648 loss_train: 1.1389 loss_rec: 1.1389 acc_train: 0.4606 loss_val: 1.1488 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01649 loss_train: 1.1388 loss_rec: 1.1388 acc_train: 0.4606 loss_val: 1.1488 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01650 loss_train: 1.1388 loss_rec: 1.1388 acc_train: 0.4606 loss_val: 1.1487 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01651 loss_train: 1.1387 loss_rec: 1.1387 acc_train: 0.4606 loss_val: 1.1487 acc_val: 0.4650 time: 0.0015s\n",
      "Test set results: loss= 1.1773 accuracy= 0.3963\n",
      "Epoch: 01652 loss_train: 1.1387 loss_rec: 1.1387 acc_train: 0.4606 loss_val: 1.1487 acc_val: 0.4650 time: 0.0025s\n",
      "Epoch: 01653 loss_train: 1.1387 loss_rec: 1.1387 acc_train: 0.4606 loss_val: 1.1486 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01654 loss_train: 1.1386 loss_rec: 1.1386 acc_train: 0.4606 loss_val: 1.1486 acc_val: 0.4650 time: 0.0030s\n",
      "Epoch: 01655 loss_train: 1.1386 loss_rec: 1.1386 acc_train: 0.4606 loss_val: 1.1486 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01656 loss_train: 1.1386 loss_rec: 1.1386 acc_train: 0.4606 loss_val: 1.1486 acc_val: 0.4650 time: 0.0025s\n",
      "Epoch: 01657 loss_train: 1.1385 loss_rec: 1.1385 acc_train: 0.4607 loss_val: 1.1485 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01658 loss_train: 1.1385 loss_rec: 1.1385 acc_train: 0.4607 loss_val: 1.1485 acc_val: 0.4650 time: 0.0025s\n",
      "Epoch: 01659 loss_train: 1.1385 loss_rec: 1.1385 acc_train: 0.4607 loss_val: 1.1485 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01660 loss_train: 1.1384 loss_rec: 1.1384 acc_train: 0.4607 loss_val: 1.1484 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01661 loss_train: 1.1384 loss_rec: 1.1384 acc_train: 0.4607 loss_val: 1.1484 acc_val: 0.4650 time: 0.0015s\n",
      "Test set results: loss= 1.1772 accuracy= 0.3962\n",
      "Epoch: 01662 loss_train: 1.1384 loss_rec: 1.1384 acc_train: 0.4607 loss_val: 1.1484 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01663 loss_train: 1.1383 loss_rec: 1.1383 acc_train: 0.4607 loss_val: 1.1484 acc_val: 0.4650 time: 0.0025s\n",
      "Epoch: 01664 loss_train: 1.1383 loss_rec: 1.1383 acc_train: 0.4607 loss_val: 1.1483 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01665 loss_train: 1.1383 loss_rec: 1.1383 acc_train: 0.4607 loss_val: 1.1483 acc_val: 0.4650 time: 0.0025s\n",
      "Epoch: 01666 loss_train: 1.1382 loss_rec: 1.1382 acc_train: 0.4607 loss_val: 1.1483 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01667 loss_train: 1.1382 loss_rec: 1.1382 acc_train: 0.4607 loss_val: 1.1482 acc_val: 0.4650 time: 0.0025s\n",
      "Epoch: 01668 loss_train: 1.1382 loss_rec: 1.1382 acc_train: 0.4612 loss_val: 1.1482 acc_val: 0.4653 time: 0.0020s\n",
      "Epoch: 01669 loss_train: 1.1381 loss_rec: 1.1381 acc_train: 0.4612 loss_val: 1.1482 acc_val: 0.4653 time: 0.0025s\n",
      "Epoch: 01670 loss_train: 1.1381 loss_rec: 1.1381 acc_train: 0.4612 loss_val: 1.1482 acc_val: 0.4653 time: 0.0025s\n",
      "Epoch: 01671 loss_train: 1.1380 loss_rec: 1.1380 acc_train: 0.4607 loss_val: 1.1481 acc_val: 0.4650 time: 0.0015s\n",
      "Test set results: loss= 1.1770 accuracy= 0.3964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01672 loss_train: 1.1380 loss_rec: 1.1380 acc_train: 0.4607 loss_val: 1.1481 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01673 loss_train: 1.1380 loss_rec: 1.1380 acc_train: 0.4607 loss_val: 1.1481 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01674 loss_train: 1.1379 loss_rec: 1.1379 acc_train: 0.4607 loss_val: 1.1480 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01675 loss_train: 1.1379 loss_rec: 1.1379 acc_train: 0.4607 loss_val: 1.1480 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01676 loss_train: 1.1379 loss_rec: 1.1379 acc_train: 0.4606 loss_val: 1.1480 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01677 loss_train: 1.1378 loss_rec: 1.1378 acc_train: 0.4606 loss_val: 1.1480 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01678 loss_train: 1.1378 loss_rec: 1.1378 acc_train: 0.4606 loss_val: 1.1479 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01679 loss_train: 1.1378 loss_rec: 1.1378 acc_train: 0.4606 loss_val: 1.1479 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01680 loss_train: 1.1377 loss_rec: 1.1377 acc_train: 0.4606 loss_val: 1.1479 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01681 loss_train: 1.1377 loss_rec: 1.1377 acc_train: 0.4606 loss_val: 1.1478 acc_val: 0.4650 time: 0.0015s\n",
      "Test set results: loss= 1.1768 accuracy= 0.3963\n",
      "Epoch: 01682 loss_train: 1.1377 loss_rec: 1.1377 acc_train: 0.4606 loss_val: 1.1478 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01683 loss_train: 1.1376 loss_rec: 1.1376 acc_train: 0.4606 loss_val: 1.1478 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01684 loss_train: 1.1376 loss_rec: 1.1376 acc_train: 0.4607 loss_val: 1.1478 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01685 loss_train: 1.1376 loss_rec: 1.1376 acc_train: 0.4607 loss_val: 1.1477 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01686 loss_train: 1.1375 loss_rec: 1.1375 acc_train: 0.4607 loss_val: 1.1477 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01687 loss_train: 1.1375 loss_rec: 1.1375 acc_train: 0.4607 loss_val: 1.1477 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01688 loss_train: 1.1375 loss_rec: 1.1375 acc_train: 0.4607 loss_val: 1.1476 acc_val: 0.4653 time: 0.0015s\n",
      "Epoch: 01689 loss_train: 1.1374 loss_rec: 1.1374 acc_train: 0.4607 loss_val: 1.1476 acc_val: 0.4653 time: 0.0015s\n",
      "Epoch: 01690 loss_train: 1.1374 loss_rec: 1.1374 acc_train: 0.4608 loss_val: 1.1476 acc_val: 0.4653 time: 0.0020s\n",
      "Epoch: 01691 loss_train: 1.1374 loss_rec: 1.1374 acc_train: 0.4606 loss_val: 1.1476 acc_val: 0.4647 time: 0.0015s\n",
      "Test set results: loss= 1.1767 accuracy= 0.3953\n",
      "Epoch: 01692 loss_train: 1.1373 loss_rec: 1.1373 acc_train: 0.4607 loss_val: 1.1475 acc_val: 0.4647 time: 0.0020s\n",
      "Epoch: 01693 loss_train: 1.1373 loss_rec: 1.1373 acc_train: 0.4601 loss_val: 1.1475 acc_val: 0.4631 time: 0.0015s\n",
      "Epoch: 01694 loss_train: 1.1373 loss_rec: 1.1373 acc_train: 0.4601 loss_val: 1.1475 acc_val: 0.4631 time: 0.0020s\n",
      "Epoch: 01695 loss_train: 1.1372 loss_rec: 1.1372 acc_train: 0.4601 loss_val: 1.1475 acc_val: 0.4631 time: 0.0020s\n",
      "Epoch: 01696 loss_train: 1.1372 loss_rec: 1.1372 acc_train: 0.4606 loss_val: 1.1474 acc_val: 0.4634 time: 0.0015s\n",
      "Epoch: 01697 loss_train: 1.1372 loss_rec: 1.1372 acc_train: 0.4606 loss_val: 1.1474 acc_val: 0.4634 time: 0.0015s\n",
      "Epoch: 01698 loss_train: 1.1371 loss_rec: 1.1371 acc_train: 0.4606 loss_val: 1.1474 acc_val: 0.4634 time: 0.0020s\n",
      "Epoch: 01699 loss_train: 1.1371 loss_rec: 1.1371 acc_train: 0.4606 loss_val: 1.1473 acc_val: 0.4634 time: 0.0020s\n",
      "Epoch: 01700 loss_train: 1.1371 loss_rec: 1.1371 acc_train: 0.4606 loss_val: 1.1473 acc_val: 0.4634 time: 0.0015s\n",
      "Epoch: 01701 loss_train: 1.1370 loss_rec: 1.1370 acc_train: 0.4606 loss_val: 1.1473 acc_val: 0.4634 time: 0.0020s\n",
      "Test set results: loss= 1.1765 accuracy= 0.3949\n",
      "Epoch: 01702 loss_train: 1.1370 loss_rec: 1.1370 acc_train: 0.4606 loss_val: 1.1473 acc_val: 0.4634 time: 0.0020s\n",
      "Epoch: 01703 loss_train: 1.1370 loss_rec: 1.1370 acc_train: 0.4606 loss_val: 1.1472 acc_val: 0.4634 time: 0.0020s\n",
      "Epoch: 01704 loss_train: 1.1369 loss_rec: 1.1369 acc_train: 0.4606 loss_val: 1.1472 acc_val: 0.4634 time: 0.0015s\n",
      "Epoch: 01705 loss_train: 1.1369 loss_rec: 1.1369 acc_train: 0.4609 loss_val: 1.1472 acc_val: 0.4637 time: 0.0015s\n",
      "Epoch: 01706 loss_train: 1.1369 loss_rec: 1.1369 acc_train: 0.4609 loss_val: 1.1472 acc_val: 0.4637 time: 0.0015s\n",
      "Epoch: 01707 loss_train: 1.1368 loss_rec: 1.1368 acc_train: 0.4609 loss_val: 1.1471 acc_val: 0.4637 time: 0.0015s\n",
      "Epoch: 01708 loss_train: 1.1368 loss_rec: 1.1368 acc_train: 0.4609 loss_val: 1.1471 acc_val: 0.4637 time: 0.0020s\n",
      "Epoch: 01709 loss_train: 1.1368 loss_rec: 1.1368 acc_train: 0.4609 loss_val: 1.1471 acc_val: 0.4637 time: 0.0015s\n",
      "Epoch: 01710 loss_train: 1.1367 loss_rec: 1.1367 acc_train: 0.4609 loss_val: 1.1471 acc_val: 0.4637 time: 0.0020s\n",
      "Epoch: 01711 loss_train: 1.1367 loss_rec: 1.1367 acc_train: 0.4609 loss_val: 1.1470 acc_val: 0.4637 time: 0.0015s\n",
      "Test set results: loss= 1.1763 accuracy= 0.3949\n",
      "Epoch: 01712 loss_train: 1.1367 loss_rec: 1.1367 acc_train: 0.4609 loss_val: 1.1470 acc_val: 0.4637 time: 0.0020s\n",
      "Epoch: 01713 loss_train: 1.1366 loss_rec: 1.1366 acc_train: 0.4609 loss_val: 1.1470 acc_val: 0.4637 time: 0.0015s\n",
      "Epoch: 01714 loss_train: 1.1366 loss_rec: 1.1366 acc_train: 0.4609 loss_val: 1.1469 acc_val: 0.4637 time: 0.0015s\n",
      "Epoch: 01715 loss_train: 1.1366 loss_rec: 1.1366 acc_train: 0.4609 loss_val: 1.1469 acc_val: 0.4637 time: 0.0015s\n",
      "Epoch: 01716 loss_train: 1.1365 loss_rec: 1.1365 acc_train: 0.4610 loss_val: 1.1469 acc_val: 0.4641 time: 0.0020s\n",
      "Epoch: 01717 loss_train: 1.1365 loss_rec: 1.1365 acc_train: 0.4610 loss_val: 1.1469 acc_val: 0.4641 time: 0.0015s\n",
      "Epoch: 01718 loss_train: 1.1365 loss_rec: 1.1365 acc_train: 0.4610 loss_val: 1.1468 acc_val: 0.4641 time: 0.0015s\n",
      "Epoch: 01719 loss_train: 1.1364 loss_rec: 1.1364 acc_train: 0.4610 loss_val: 1.1468 acc_val: 0.4641 time: 0.0020s\n",
      "Epoch: 01720 loss_train: 1.1364 loss_rec: 1.1364 acc_train: 0.4610 loss_val: 1.1468 acc_val: 0.4641 time: 0.0020s\n",
      "Epoch: 01721 loss_train: 1.1364 loss_rec: 1.1364 acc_train: 0.4610 loss_val: 1.1468 acc_val: 0.4641 time: 0.0020s\n",
      "Test set results: loss= 1.1761 accuracy= 0.3951\n",
      "Epoch: 01722 loss_train: 1.1363 loss_rec: 1.1363 acc_train: 0.4611 loss_val: 1.1467 acc_val: 0.4641 time: 0.0015s\n",
      "Epoch: 01723 loss_train: 1.1363 loss_rec: 1.1363 acc_train: 0.4611 loss_val: 1.1467 acc_val: 0.4641 time: 0.0030s\n",
      "Epoch: 01724 loss_train: 1.1363 loss_rec: 1.1363 acc_train: 0.4611 loss_val: 1.1467 acc_val: 0.4641 time: 0.0015s\n",
      "Epoch: 01725 loss_train: 1.1363 loss_rec: 1.1363 acc_train: 0.4611 loss_val: 1.1467 acc_val: 0.4641 time: 0.0030s\n",
      "Epoch: 01726 loss_train: 1.1362 loss_rec: 1.1362 acc_train: 0.4613 loss_val: 1.1466 acc_val: 0.4641 time: 0.0020s\n",
      "Epoch: 01727 loss_train: 1.1362 loss_rec: 1.1362 acc_train: 0.4612 loss_val: 1.1466 acc_val: 0.4641 time: 0.0020s\n",
      "Epoch: 01728 loss_train: 1.1362 loss_rec: 1.1362 acc_train: 0.4612 loss_val: 1.1466 acc_val: 0.4641 time: 0.0015s\n",
      "Epoch: 01729 loss_train: 1.1361 loss_rec: 1.1361 acc_train: 0.4611 loss_val: 1.1466 acc_val: 0.4641 time: 0.0030s\n",
      "Epoch: 01730 loss_train: 1.1361 loss_rec: 1.1361 acc_train: 0.4611 loss_val: 1.1465 acc_val: 0.4641 time: 0.0020s\n",
      "Epoch: 01731 loss_train: 1.1361 loss_rec: 1.1361 acc_train: 0.4611 loss_val: 1.1465 acc_val: 0.4641 time: 0.0020s\n",
      "Test set results: loss= 1.1760 accuracy= 0.3936\n",
      "Epoch: 01732 loss_train: 1.1360 loss_rec: 1.1360 acc_train: 0.4600 loss_val: 1.1465 acc_val: 0.4625 time: 0.0020s\n",
      "Epoch: 01733 loss_train: 1.1360 loss_rec: 1.1360 acc_train: 0.4610 loss_val: 1.1465 acc_val: 0.4597 time: 0.0015s\n",
      "Epoch: 01734 loss_train: 1.1360 loss_rec: 1.1360 acc_train: 0.4613 loss_val: 1.1464 acc_val: 0.4597 time: 0.0025s\n",
      "Epoch: 01735 loss_train: 1.1359 loss_rec: 1.1359 acc_train: 0.4613 loss_val: 1.1464 acc_val: 0.4597 time: 0.0020s\n",
      "Epoch: 01736 loss_train: 1.1359 loss_rec: 1.1359 acc_train: 0.4613 loss_val: 1.1464 acc_val: 0.4597 time: 0.0030s\n",
      "Epoch: 01737 loss_train: 1.1359 loss_rec: 1.1359 acc_train: 0.4613 loss_val: 1.1464 acc_val: 0.4597 time: 0.0025s\n",
      "Epoch: 01738 loss_train: 1.1358 loss_rec: 1.1358 acc_train: 0.4613 loss_val: 1.1463 acc_val: 0.4597 time: 0.0015s\n",
      "Epoch: 01739 loss_train: 1.1358 loss_rec: 1.1358 acc_train: 0.4613 loss_val: 1.1463 acc_val: 0.4597 time: 0.0020s\n",
      "Epoch: 01740 loss_train: 1.1358 loss_rec: 1.1358 acc_train: 0.4613 loss_val: 1.1463 acc_val: 0.4600 time: 0.0020s\n",
      "Epoch: 01741 loss_train: 1.1357 loss_rec: 1.1357 acc_train: 0.4613 loss_val: 1.1462 acc_val: 0.4600 time: 0.0020s\n",
      "Test set results: loss= 1.1759 accuracy= 0.3876\n",
      "Epoch: 01742 loss_train: 1.1357 loss_rec: 1.1357 acc_train: 0.4613 loss_val: 1.1462 acc_val: 0.4600 time: 0.0020s\n",
      "Epoch: 01743 loss_train: 1.1357 loss_rec: 1.1357 acc_train: 0.4613 loss_val: 1.1462 acc_val: 0.4600 time: 0.0030s\n",
      "Epoch: 01744 loss_train: 1.1357 loss_rec: 1.1357 acc_train: 0.4613 loss_val: 1.1462 acc_val: 0.4600 time: 0.0030s\n",
      "Epoch: 01745 loss_train: 1.1356 loss_rec: 1.1356 acc_train: 0.4613 loss_val: 1.1461 acc_val: 0.4600 time: 0.0015s\n",
      "Epoch: 01746 loss_train: 1.1356 loss_rec: 1.1356 acc_train: 0.4613 loss_val: 1.1461 acc_val: 0.4600 time: 0.0025s\n",
      "Epoch: 01747 loss_train: 1.1356 loss_rec: 1.1356 acc_train: 0.4613 loss_val: 1.1461 acc_val: 0.4600 time: 0.0020s\n",
      "Epoch: 01748 loss_train: 1.1355 loss_rec: 1.1355 acc_train: 0.4613 loss_val: 1.1461 acc_val: 0.4600 time: 0.0025s\n",
      "Epoch: 01749 loss_train: 1.1355 loss_rec: 1.1355 acc_train: 0.4613 loss_val: 1.1460 acc_val: 0.4600 time: 0.0015s\n",
      "Epoch: 01750 loss_train: 1.1355 loss_rec: 1.1355 acc_train: 0.4613 loss_val: 1.1460 acc_val: 0.4600 time: 0.0025s\n",
      "Epoch: 01751 loss_train: 1.1354 loss_rec: 1.1354 acc_train: 0.4617 loss_val: 1.1460 acc_val: 0.4603 time: 0.0020s\n",
      "Test set results: loss= 1.1758 accuracy= 0.3884\n",
      "Epoch: 01752 loss_train: 1.1354 loss_rec: 1.1354 acc_train: 0.4617 loss_val: 1.1460 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01753 loss_train: 1.1354 loss_rec: 1.1354 acc_train: 0.4617 loss_val: 1.1459 acc_val: 0.4603 time: 0.0025s\n",
      "Epoch: 01754 loss_train: 1.1353 loss_rec: 1.1353 acc_train: 0.4617 loss_val: 1.1459 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01755 loss_train: 1.1353 loss_rec: 1.1353 acc_train: 0.4617 loss_val: 1.1459 acc_val: 0.4603 time: 0.0025s\n",
      "Epoch: 01756 loss_train: 1.1353 loss_rec: 1.1353 acc_train: 0.4617 loss_val: 1.1459 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01757 loss_train: 1.1353 loss_rec: 1.1353 acc_train: 0.4617 loss_val: 1.1458 acc_val: 0.4603 time: 0.0030s\n",
      "Epoch: 01758 loss_train: 1.1352 loss_rec: 1.1352 acc_train: 0.4617 loss_val: 1.1458 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01759 loss_train: 1.1352 loss_rec: 1.1352 acc_train: 0.4617 loss_val: 1.1458 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01760 loss_train: 1.1352 loss_rec: 1.1352 acc_train: 0.4617 loss_val: 1.1458 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01761 loss_train: 1.1351 loss_rec: 1.1351 acc_train: 0.4614 loss_val: 1.1457 acc_val: 0.4603 time: 0.0020s\n",
      "Test set results: loss= 1.1756 accuracy= 0.3882\n",
      "Epoch: 01762 loss_train: 1.1351 loss_rec: 1.1351 acc_train: 0.4614 loss_val: 1.1457 acc_val: 0.4603 time: 0.0025s\n",
      "Epoch: 01763 loss_train: 1.1351 loss_rec: 1.1351 acc_train: 0.4614 loss_val: 1.1457 acc_val: 0.4603 time: 0.0025s\n",
      "Epoch: 01764 loss_train: 1.1350 loss_rec: 1.1350 acc_train: 0.4614 loss_val: 1.1457 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01765 loss_train: 1.1350 loss_rec: 1.1350 acc_train: 0.4614 loss_val: 1.1456 acc_val: 0.4603 time: 0.0030s\n",
      "Epoch: 01766 loss_train: 1.1350 loss_rec: 1.1350 acc_train: 0.4614 loss_val: 1.1456 acc_val: 0.4603 time: 0.0025s\n",
      "Epoch: 01767 loss_train: 1.1349 loss_rec: 1.1349 acc_train: 0.4614 loss_val: 1.1456 acc_val: 0.4603 time: 0.0020s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01768 loss_train: 1.1349 loss_rec: 1.1349 acc_train: 0.4614 loss_val: 1.1456 acc_val: 0.4603 time: 0.0030s\n",
      "Epoch: 01769 loss_train: 1.1349 loss_rec: 1.1349 acc_train: 0.4614 loss_val: 1.1455 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01770 loss_train: 1.1349 loss_rec: 1.1349 acc_train: 0.4614 loss_val: 1.1455 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01771 loss_train: 1.1348 loss_rec: 1.1348 acc_train: 0.4614 loss_val: 1.1455 acc_val: 0.4603 time: 0.0020s\n",
      "Test set results: loss= 1.1755 accuracy= 0.3882\n",
      "Epoch: 01772 loss_train: 1.1348 loss_rec: 1.1348 acc_train: 0.4614 loss_val: 1.1455 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01773 loss_train: 1.1348 loss_rec: 1.1348 acc_train: 0.4614 loss_val: 1.1455 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01774 loss_train: 1.1347 loss_rec: 1.1347 acc_train: 0.4614 loss_val: 1.1454 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01775 loss_train: 1.1347 loss_rec: 1.1347 acc_train: 0.4614 loss_val: 1.1454 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01776 loss_train: 1.1347 loss_rec: 1.1347 acc_train: 0.4614 loss_val: 1.1454 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01777 loss_train: 1.1346 loss_rec: 1.1346 acc_train: 0.4614 loss_val: 1.1454 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01778 loss_train: 1.1346 loss_rec: 1.1346 acc_train: 0.4614 loss_val: 1.1453 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01779 loss_train: 1.1346 loss_rec: 1.1346 acc_train: 0.4614 loss_val: 1.1453 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01780 loss_train: 1.1346 loss_rec: 1.1346 acc_train: 0.4614 loss_val: 1.1453 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01781 loss_train: 1.1345 loss_rec: 1.1345 acc_train: 0.4615 loss_val: 1.1453 acc_val: 0.4603 time: 0.0015s\n",
      "Test set results: loss= 1.1753 accuracy= 0.3882\n",
      "Epoch: 01782 loss_train: 1.1345 loss_rec: 1.1345 acc_train: 0.4615 loss_val: 1.1452 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01783 loss_train: 1.1345 loss_rec: 1.1345 acc_train: 0.4615 loss_val: 1.1452 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01784 loss_train: 1.1344 loss_rec: 1.1344 acc_train: 0.4615 loss_val: 1.1452 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01785 loss_train: 1.1344 loss_rec: 1.1344 acc_train: 0.4617 loss_val: 1.1452 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01786 loss_train: 1.1344 loss_rec: 1.1344 acc_train: 0.4615 loss_val: 1.1451 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01787 loss_train: 1.1344 loss_rec: 1.1344 acc_train: 0.4615 loss_val: 1.1451 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01788 loss_train: 1.1343 loss_rec: 1.1343 acc_train: 0.4615 loss_val: 1.1451 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01789 loss_train: 1.1343 loss_rec: 1.1343 acc_train: 0.4617 loss_val: 1.1451 acc_val: 0.4603 time: 0.0020s\n",
      "Epoch: 01790 loss_train: 1.1343 loss_rec: 1.1343 acc_train: 0.4616 loss_val: 1.1450 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01791 loss_train: 1.1342 loss_rec: 1.1342 acc_train: 0.4616 loss_val: 1.1450 acc_val: 0.4603 time: 0.0015s\n",
      "Test set results: loss= 1.1752 accuracy= 0.3882\n",
      "Epoch: 01792 loss_train: 1.1342 loss_rec: 1.1342 acc_train: 0.4616 loss_val: 1.1450 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01793 loss_train: 1.1342 loss_rec: 1.1342 acc_train: 0.4616 loss_val: 1.1450 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01794 loss_train: 1.1341 loss_rec: 1.1341 acc_train: 0.4616 loss_val: 1.1450 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01795 loss_train: 1.1341 loss_rec: 1.1341 acc_train: 0.4617 loss_val: 1.1449 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01796 loss_train: 1.1341 loss_rec: 1.1341 acc_train: 0.4617 loss_val: 1.1449 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01797 loss_train: 1.1341 loss_rec: 1.1341 acc_train: 0.4616 loss_val: 1.1449 acc_val: 0.4603 time: 0.0015s\n",
      "Epoch: 01798 loss_train: 1.1340 loss_rec: 1.1340 acc_train: 0.4624 loss_val: 1.1449 acc_val: 0.4616 time: 0.0020s\n",
      "Epoch: 01799 loss_train: 1.1340 loss_rec: 1.1340 acc_train: 0.4624 loss_val: 1.1448 acc_val: 0.4616 time: 0.0015s\n",
      "Epoch: 01800 loss_train: 1.1340 loss_rec: 1.1340 acc_train: 0.4624 loss_val: 1.1448 acc_val: 0.4616 time: 0.0015s\n",
      "Epoch: 01801 loss_train: 1.1339 loss_rec: 1.1339 acc_train: 0.4624 loss_val: 1.1448 acc_val: 0.4616 time: 0.0015s\n",
      "Test set results: loss= 1.1750 accuracy= 0.3878\n",
      "Epoch: 01802 loss_train: 1.1339 loss_rec: 1.1339 acc_train: 0.4622 loss_val: 1.1448 acc_val: 0.4616 time: 0.0020s\n",
      "Epoch: 01803 loss_train: 1.1339 loss_rec: 1.1339 acc_train: 0.4622 loss_val: 1.1447 acc_val: 0.4616 time: 0.0015s\n",
      "Epoch: 01804 loss_train: 1.1339 loss_rec: 1.1339 acc_train: 0.4622 loss_val: 1.1447 acc_val: 0.4616 time: 0.0020s\n",
      "Epoch: 01805 loss_train: 1.1338 loss_rec: 1.1338 acc_train: 0.4622 loss_val: 1.1447 acc_val: 0.4616 time: 0.0015s\n",
      "Epoch: 01806 loss_train: 1.1338 loss_rec: 1.1338 acc_train: 0.4622 loss_val: 1.1447 acc_val: 0.4616 time: 0.0015s\n",
      "Epoch: 01807 loss_train: 1.1338 loss_rec: 1.1338 acc_train: 0.4622 loss_val: 1.1446 acc_val: 0.4619 time: 0.0020s\n",
      "Epoch: 01808 loss_train: 1.1337 loss_rec: 1.1337 acc_train: 0.4622 loss_val: 1.1446 acc_val: 0.4619 time: 0.0015s\n",
      "Epoch: 01809 loss_train: 1.1337 loss_rec: 1.1337 acc_train: 0.4622 loss_val: 1.1446 acc_val: 0.4619 time: 0.0015s\n",
      "Epoch: 01810 loss_train: 1.1337 loss_rec: 1.1337 acc_train: 0.4627 loss_val: 1.1446 acc_val: 0.4625 time: 0.0015s\n",
      "Epoch: 01811 loss_train: 1.1337 loss_rec: 1.1337 acc_train: 0.4627 loss_val: 1.1446 acc_val: 0.4625 time: 0.0015s\n",
      "Test set results: loss= 1.1749 accuracy= 0.3887\n",
      "Epoch: 01812 loss_train: 1.1336 loss_rec: 1.1336 acc_train: 0.4627 loss_val: 1.1445 acc_val: 0.4625 time: 0.0015s\n",
      "Epoch: 01813 loss_train: 1.1336 loss_rec: 1.1336 acc_train: 0.4641 loss_val: 1.1445 acc_val: 0.4641 time: 0.0015s\n",
      "Epoch: 01814 loss_train: 1.1336 loss_rec: 1.1336 acc_train: 0.4641 loss_val: 1.1445 acc_val: 0.4641 time: 0.0015s\n",
      "Epoch: 01815 loss_train: 1.1335 loss_rec: 1.1335 acc_train: 0.4639 loss_val: 1.1445 acc_val: 0.4644 time: 0.0020s\n",
      "Epoch: 01816 loss_train: 1.1335 loss_rec: 1.1335 acc_train: 0.4639 loss_val: 1.1444 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01817 loss_train: 1.1335 loss_rec: 1.1335 acc_train: 0.4639 loss_val: 1.1444 acc_val: 0.4644 time: 0.0020s\n",
      "Epoch: 01818 loss_train: 1.1335 loss_rec: 1.1335 acc_train: 0.4639 loss_val: 1.1444 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01819 loss_train: 1.1334 loss_rec: 1.1334 acc_train: 0.4639 loss_val: 1.1444 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01820 loss_train: 1.1334 loss_rec: 1.1334 acc_train: 0.4639 loss_val: 1.1444 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01821 loss_train: 1.1334 loss_rec: 1.1334 acc_train: 0.4639 loss_val: 1.1443 acc_val: 0.4644 time: 0.0015s\n",
      "Test set results: loss= 1.1747 accuracy= 0.3894\n",
      "Epoch: 01822 loss_train: 1.1333 loss_rec: 1.1333 acc_train: 0.4639 loss_val: 1.1443 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01823 loss_train: 1.1333 loss_rec: 1.1333 acc_train: 0.4639 loss_val: 1.1443 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01824 loss_train: 1.1333 loss_rec: 1.1333 acc_train: 0.4639 loss_val: 1.1443 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01825 loss_train: 1.1333 loss_rec: 1.1333 acc_train: 0.4639 loss_val: 1.1442 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01826 loss_train: 1.1332 loss_rec: 1.1332 acc_train: 0.4639 loss_val: 1.1442 acc_val: 0.4644 time: 0.0020s\n",
      "Epoch: 01827 loss_train: 1.1332 loss_rec: 1.1332 acc_train: 0.4639 loss_val: 1.1442 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01828 loss_train: 1.1332 loss_rec: 1.1332 acc_train: 0.4639 loss_val: 1.1442 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01829 loss_train: 1.1331 loss_rec: 1.1331 acc_train: 0.4639 loss_val: 1.1442 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01830 loss_train: 1.1331 loss_rec: 1.1331 acc_train: 0.4639 loss_val: 1.1441 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01831 loss_train: 1.1331 loss_rec: 1.1331 acc_train: 0.4639 loss_val: 1.1441 acc_val: 0.4644 time: 0.0015s\n",
      "Test set results: loss= 1.1746 accuracy= 0.3894\n",
      "Epoch: 01832 loss_train: 1.1331 loss_rec: 1.1331 acc_train: 0.4639 loss_val: 1.1441 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01833 loss_train: 1.1330 loss_rec: 1.1330 acc_train: 0.4641 loss_val: 1.1441 acc_val: 0.4644 time: 0.0020s\n",
      "Epoch: 01834 loss_train: 1.1330 loss_rec: 1.1330 acc_train: 0.4641 loss_val: 1.1440 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01835 loss_train: 1.1330 loss_rec: 1.1330 acc_train: 0.4641 loss_val: 1.1440 acc_val: 0.4644 time: 0.0015s\n",
      "Epoch: 01836 loss_train: 1.1329 loss_rec: 1.1329 acc_train: 0.4640 loss_val: 1.1440 acc_val: 0.4653 time: 0.0020s\n",
      "Epoch: 01837 loss_train: 1.1329 loss_rec: 1.1329 acc_train: 0.4640 loss_val: 1.1440 acc_val: 0.4653 time: 0.0015s\n",
      "Epoch: 01838 loss_train: 1.1329 loss_rec: 1.1329 acc_train: 0.4640 loss_val: 1.1440 acc_val: 0.4653 time: 0.0015s\n",
      "Epoch: 01839 loss_train: 1.1329 loss_rec: 1.1329 acc_train: 0.4640 loss_val: 1.1439 acc_val: 0.4653 time: 0.0015s\n",
      "Epoch: 01840 loss_train: 1.1328 loss_rec: 1.1328 acc_train: 0.4640 loss_val: 1.1439 acc_val: 0.4653 time: 0.0020s\n",
      "Epoch: 01841 loss_train: 1.1328 loss_rec: 1.1328 acc_train: 0.4640 loss_val: 1.1439 acc_val: 0.4653 time: 0.0015s\n",
      "Test set results: loss= 1.1745 accuracy= 0.3887\n",
      "Epoch: 01842 loss_train: 1.1328 loss_rec: 1.1328 acc_train: 0.4640 loss_val: 1.1439 acc_val: 0.4653 time: 0.0015s\n",
      "Epoch: 01843 loss_train: 1.1328 loss_rec: 1.1328 acc_train: 0.4640 loss_val: 1.1439 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01844 loss_train: 1.1327 loss_rec: 1.1327 acc_train: 0.4641 loss_val: 1.1438 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01845 loss_train: 1.1327 loss_rec: 1.1327 acc_train: 0.4641 loss_val: 1.1438 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01846 loss_train: 1.1327 loss_rec: 1.1327 acc_train: 0.4641 loss_val: 1.1438 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01847 loss_train: 1.1327 loss_rec: 1.1327 acc_train: 0.4641 loss_val: 1.1438 acc_val: 0.4650 time: 0.0015s\n",
      "Epoch: 01848 loss_train: 1.1326 loss_rec: 1.1326 acc_train: 0.4641 loss_val: 1.1438 acc_val: 0.4650 time: 0.0020s\n",
      "Epoch: 01849 loss_train: 1.1326 loss_rec: 1.1326 acc_train: 0.4646 loss_val: 1.1437 acc_val: 0.4656 time: 0.0015s\n",
      "Epoch: 01850 loss_train: 1.1326 loss_rec: 1.1326 acc_train: 0.4646 loss_val: 1.1437 acc_val: 0.4656 time: 0.0015s\n",
      "Epoch: 01851 loss_train: 1.1325 loss_rec: 1.1325 acc_train: 0.4646 loss_val: 1.1437 acc_val: 0.4656 time: 0.0015s\n",
      "Test set results: loss= 1.1744 accuracy= 0.3892\n",
      "Epoch: 01852 loss_train: 1.1325 loss_rec: 1.1325 acc_train: 0.4646 loss_val: 1.1437 acc_val: 0.4656 time: 0.0015s\n",
      "Epoch: 01853 loss_train: 1.1325 loss_rec: 1.1325 acc_train: 0.4646 loss_val: 1.1436 acc_val: 0.4656 time: 0.0020s\n",
      "Epoch: 01854 loss_train: 1.1325 loss_rec: 1.1325 acc_train: 0.4646 loss_val: 1.1436 acc_val: 0.4656 time: 0.0015s\n",
      "Epoch: 01855 loss_train: 1.1324 loss_rec: 1.1324 acc_train: 0.4646 loss_val: 1.1436 acc_val: 0.4656 time: 0.0015s\n",
      "Epoch: 01856 loss_train: 1.1324 loss_rec: 1.1324 acc_train: 0.4646 loss_val: 1.1436 acc_val: 0.4656 time: 0.0015s\n",
      "Epoch: 01857 loss_train: 1.1324 loss_rec: 1.1324 acc_train: 0.4646 loss_val: 1.1435 acc_val: 0.4656 time: 0.0015s\n",
      "Epoch: 01858 loss_train: 1.1323 loss_rec: 1.1323 acc_train: 0.4650 loss_val: 1.1435 acc_val: 0.4659 time: 0.0015s\n",
      "Epoch: 01859 loss_train: 1.1323 loss_rec: 1.1323 acc_train: 0.4650 loss_val: 1.1435 acc_val: 0.4659 time: 0.0015s\n",
      "Epoch: 01860 loss_train: 1.1323 loss_rec: 1.1323 acc_train: 0.4651 loss_val: 1.1435 acc_val: 0.4659 time: 0.0015s\n",
      "Epoch: 01861 loss_train: 1.1323 loss_rec: 1.1323 acc_train: 0.4651 loss_val: 1.1435 acc_val: 0.4659 time: 0.0015s\n",
      "Test set results: loss= 1.1743 accuracy= 0.3901\n",
      "Epoch: 01862 loss_train: 1.1322 loss_rec: 1.1322 acc_train: 0.4651 loss_val: 1.1434 acc_val: 0.4659 time: 0.0015s\n",
      "Epoch: 01863 loss_train: 1.1322 loss_rec: 1.1322 acc_train: 0.4651 loss_val: 1.1434 acc_val: 0.4659 time: 0.0015s\n",
      "Epoch: 01864 loss_train: 1.1322 loss_rec: 1.1322 acc_train: 0.4651 loss_val: 1.1434 acc_val: 0.4659 time: 0.0020s\n",
      "Epoch: 01865 loss_train: 1.1322 loss_rec: 1.1322 acc_train: 0.4651 loss_val: 1.1434 acc_val: 0.4659 time: 0.0015s\n",
      "Epoch: 01866 loss_train: 1.1321 loss_rec: 1.1321 acc_train: 0.4651 loss_val: 1.1434 acc_val: 0.4659 time: 0.0015s\n",
      "Epoch: 01867 loss_train: 1.1321 loss_rec: 1.1321 acc_train: 0.4651 loss_val: 1.1433 acc_val: 0.4659 time: 0.0015s\n",
      "Epoch: 01868 loss_train: 1.1321 loss_rec: 1.1321 acc_train: 0.4651 loss_val: 1.1433 acc_val: 0.4659 time: 0.0015s\n",
      "Epoch: 01869 loss_train: 1.1321 loss_rec: 1.1321 acc_train: 0.4651 loss_val: 1.1433 acc_val: 0.4659 time: 0.0020s\n",
      "Epoch: 01870 loss_train: 1.1320 loss_rec: 1.1320 acc_train: 0.4651 loss_val: 1.1433 acc_val: 0.4659 time: 0.0015s\n",
      "Epoch: 01871 loss_train: 1.1320 loss_rec: 1.1320 acc_train: 0.4651 loss_val: 1.1433 acc_val: 0.4659 time: 0.0015s\n",
      "Test set results: loss= 1.1741 accuracy= 0.3901\n",
      "Epoch: 01872 loss_train: 1.1320 loss_rec: 1.1320 acc_train: 0.4651 loss_val: 1.1432 acc_val: 0.4659 time: 0.0020s\n",
      "Epoch: 01873 loss_train: 1.1320 loss_rec: 1.1320 acc_train: 0.4651 loss_val: 1.1432 acc_val: 0.4659 time: 0.0020s\n",
      "Epoch: 01874 loss_train: 1.1319 loss_rec: 1.1319 acc_train: 0.4651 loss_val: 1.1432 acc_val: 0.4659 time: 0.0015s\n",
      "Epoch: 01875 loss_train: 1.1319 loss_rec: 1.1319 acc_train: 0.4657 loss_val: 1.1432 acc_val: 0.4662 time: 0.0015s\n",
      "Epoch: 01876 loss_train: 1.1319 loss_rec: 1.1319 acc_train: 0.4657 loss_val: 1.1432 acc_val: 0.4662 time: 0.0020s\n",
      "Epoch: 01877 loss_train: 1.1318 loss_rec: 1.1318 acc_train: 0.4657 loss_val: 1.1431 acc_val: 0.4662 time: 0.0015s\n",
      "Epoch: 01878 loss_train: 1.1318 loss_rec: 1.1318 acc_train: 0.4663 loss_val: 1.1431 acc_val: 0.4666 time: 0.0015s\n",
      "Epoch: 01879 loss_train: 1.1318 loss_rec: 1.1318 acc_train: 0.4663 loss_val: 1.1431 acc_val: 0.4666 time: 0.0020s\n",
      "Epoch: 01880 loss_train: 1.1318 loss_rec: 1.1318 acc_train: 0.4663 loss_val: 1.1431 acc_val: 0.4666 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01881 loss_train: 1.1317 loss_rec: 1.1317 acc_train: 0.4663 loss_val: 1.1431 acc_val: 0.4666 time: 0.0020s\n",
      "Test set results: loss= 1.1740 accuracy= 0.3914\n",
      "Epoch: 01882 loss_train: 1.1317 loss_rec: 1.1317 acc_train: 0.4662 loss_val: 1.1430 acc_val: 0.4666 time: 0.0020s\n",
      "Epoch: 01883 loss_train: 1.1317 loss_rec: 1.1317 acc_train: 0.4662 loss_val: 1.1430 acc_val: 0.4666 time: 0.0015s\n",
      "Epoch: 01884 loss_train: 1.1317 loss_rec: 1.1317 acc_train: 0.4662 loss_val: 1.1430 acc_val: 0.4666 time: 0.0015s\n",
      "Epoch: 01885 loss_train: 1.1316 loss_rec: 1.1316 acc_train: 0.4662 loss_val: 1.1430 acc_val: 0.4666 time: 0.0015s\n",
      "Epoch: 01886 loss_train: 1.1316 loss_rec: 1.1316 acc_train: 0.4665 loss_val: 1.1430 acc_val: 0.4675 time: 0.0015s\n",
      "Epoch: 01887 loss_train: 1.1316 loss_rec: 1.1316 acc_train: 0.4665 loss_val: 1.1429 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01888 loss_train: 1.1316 loss_rec: 1.1316 acc_train: 0.4665 loss_val: 1.1429 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01889 loss_train: 1.1315 loss_rec: 1.1315 acc_train: 0.4665 loss_val: 1.1429 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01890 loss_train: 1.1315 loss_rec: 1.1315 acc_train: 0.4665 loss_val: 1.1429 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01891 loss_train: 1.1315 loss_rec: 1.1315 acc_train: 0.4665 loss_val: 1.1429 acc_val: 0.4672 time: 0.0015s\n",
      "Test set results: loss= 1.1739 accuracy= 0.3921\n",
      "Epoch: 01892 loss_train: 1.1315 loss_rec: 1.1315 acc_train: 0.4665 loss_val: 1.1428 acc_val: 0.4672 time: 0.0020s\n",
      "Epoch: 01893 loss_train: 1.1314 loss_rec: 1.1314 acc_train: 0.4665 loss_val: 1.1428 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01894 loss_train: 1.1314 loss_rec: 1.1314 acc_train: 0.4665 loss_val: 1.1428 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01895 loss_train: 1.1314 loss_rec: 1.1314 acc_train: 0.4665 loss_val: 1.1428 acc_val: 0.4672 time: 0.0020s\n",
      "Epoch: 01896 loss_train: 1.1314 loss_rec: 1.1314 acc_train: 0.4665 loss_val: 1.1427 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01897 loss_train: 1.1313 loss_rec: 1.1313 acc_train: 0.4665 loss_val: 1.1427 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01898 loss_train: 1.1313 loss_rec: 1.1313 acc_train: 0.4665 loss_val: 1.1427 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01899 loss_train: 1.1313 loss_rec: 1.1313 acc_train: 0.4665 loss_val: 1.1427 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01900 loss_train: 1.1313 loss_rec: 1.1313 acc_train: 0.4665 loss_val: 1.1427 acc_val: 0.4672 time: 0.0020s\n",
      "Epoch: 01901 loss_train: 1.1312 loss_rec: 1.1312 acc_train: 0.4665 loss_val: 1.1427 acc_val: 0.4672 time: 0.0015s\n",
      "Test set results: loss= 1.1738 accuracy= 0.3921\n",
      "Epoch: 01902 loss_train: 1.1312 loss_rec: 1.1312 acc_train: 0.4665 loss_val: 1.1426 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01903 loss_train: 1.1312 loss_rec: 1.1312 acc_train: 0.4666 loss_val: 1.1426 acc_val: 0.4672 time: 0.0020s\n",
      "Epoch: 01904 loss_train: 1.1311 loss_rec: 1.1311 acc_train: 0.4666 loss_val: 1.1426 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01905 loss_train: 1.1311 loss_rec: 1.1311 acc_train: 0.4666 loss_val: 1.1426 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01906 loss_train: 1.1311 loss_rec: 1.1311 acc_train: 0.4666 loss_val: 1.1426 acc_val: 0.4672 time: 0.0020s\n",
      "Epoch: 01907 loss_train: 1.1311 loss_rec: 1.1311 acc_train: 0.4666 loss_val: 1.1425 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01908 loss_train: 1.1311 loss_rec: 1.1311 acc_train: 0.4667 loss_val: 1.1425 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01909 loss_train: 1.1310 loss_rec: 1.1310 acc_train: 0.4667 loss_val: 1.1425 acc_val: 0.4672 time: 0.0020s\n",
      "Epoch: 01910 loss_train: 1.1310 loss_rec: 1.1310 acc_train: 0.4667 loss_val: 1.1425 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01911 loss_train: 1.1310 loss_rec: 1.1310 acc_train: 0.4667 loss_val: 1.1425 acc_val: 0.4672 time: 0.0020s\n",
      "Test set results: loss= 1.1736 accuracy= 0.3921\n",
      "Epoch: 01912 loss_train: 1.1310 loss_rec: 1.1310 acc_train: 0.4667 loss_val: 1.1424 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01913 loss_train: 1.1309 loss_rec: 1.1309 acc_train: 0.4667 loss_val: 1.1424 acc_val: 0.4672 time: 0.0020s\n",
      "Epoch: 01914 loss_train: 1.1309 loss_rec: 1.1309 acc_train: 0.4667 loss_val: 1.1424 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01915 loss_train: 1.1309 loss_rec: 1.1309 acc_train: 0.4667 loss_val: 1.1424 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01916 loss_train: 1.1308 loss_rec: 1.1308 acc_train: 0.4667 loss_val: 1.1424 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01917 loss_train: 1.1308 loss_rec: 1.1308 acc_train: 0.4667 loss_val: 1.1423 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01918 loss_train: 1.1308 loss_rec: 1.1308 acc_train: 0.4667 loss_val: 1.1423 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01919 loss_train: 1.1308 loss_rec: 1.1308 acc_train: 0.4668 loss_val: 1.1423 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01920 loss_train: 1.1308 loss_rec: 1.1308 acc_train: 0.4669 loss_val: 1.1423 acc_val: 0.4672 time: 0.0020s\n",
      "Epoch: 01921 loss_train: 1.1307 loss_rec: 1.1307 acc_train: 0.4669 loss_val: 1.1423 acc_val: 0.4672 time: 0.0015s\n",
      "Test set results: loss= 1.1736 accuracy= 0.3921\n",
      "Epoch: 01922 loss_train: 1.1307 loss_rec: 1.1307 acc_train: 0.4669 loss_val: 1.1422 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01923 loss_train: 1.1307 loss_rec: 1.1307 acc_train: 0.4676 loss_val: 1.1422 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01924 loss_train: 1.1306 loss_rec: 1.1306 acc_train: 0.4676 loss_val: 1.1422 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01925 loss_train: 1.1306 loss_rec: 1.1306 acc_train: 0.4676 loss_val: 1.1422 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01926 loss_train: 1.1306 loss_rec: 1.1306 acc_train: 0.4676 loss_val: 1.1422 acc_val: 0.4672 time: 0.0020s\n",
      "Epoch: 01927 loss_train: 1.1306 loss_rec: 1.1306 acc_train: 0.4676 loss_val: 1.1422 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01928 loss_train: 1.1306 loss_rec: 1.1306 acc_train: 0.4676 loss_val: 1.1421 acc_val: 0.4672 time: 0.0020s\n",
      "Epoch: 01929 loss_train: 1.1305 loss_rec: 1.1305 acc_train: 0.4676 loss_val: 1.1421 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01930 loss_train: 1.1305 loss_rec: 1.1305 acc_train: 0.4676 loss_val: 1.1421 acc_val: 0.4672 time: 0.0020s\n",
      "Epoch: 01931 loss_train: 1.1305 loss_rec: 1.1305 acc_train: 0.4676 loss_val: 1.1421 acc_val: 0.4672 time: 0.0020s\n",
      "Test set results: loss= 1.1735 accuracy= 0.3937\n",
      "Epoch: 01932 loss_train: 1.1305 loss_rec: 1.1305 acc_train: 0.4675 loss_val: 1.1421 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01933 loss_train: 1.1304 loss_rec: 1.1304 acc_train: 0.4675 loss_val: 1.1420 acc_val: 0.4672 time: 0.0020s\n",
      "Epoch: 01934 loss_train: 1.1304 loss_rec: 1.1304 acc_train: 0.4675 loss_val: 1.1420 acc_val: 0.4672 time: 0.0015s\n",
      "Epoch: 01935 loss_train: 1.1304 loss_rec: 1.1304 acc_train: 0.4681 loss_val: 1.1420 acc_val: 0.4681 time: 0.0020s\n",
      "Epoch: 01936 loss_train: 1.1304 loss_rec: 1.1304 acc_train: 0.4681 loss_val: 1.1420 acc_val: 0.4681 time: 0.0015s\n",
      "Epoch: 01937 loss_train: 1.1303 loss_rec: 1.1303 acc_train: 0.4681 loss_val: 1.1420 acc_val: 0.4681 time: 0.0015s\n",
      "Epoch: 01938 loss_train: 1.1303 loss_rec: 1.1303 acc_train: 0.4681 loss_val: 1.1419 acc_val: 0.4681 time: 0.0020s\n",
      "Epoch: 01939 loss_train: 1.1303 loss_rec: 1.1303 acc_train: 0.4681 loss_val: 1.1419 acc_val: 0.4681 time: 0.0015s\n",
      "Epoch: 01940 loss_train: 1.1303 loss_rec: 1.1303 acc_train: 0.4681 loss_val: 1.1419 acc_val: 0.4681 time: 0.0020s\n",
      "Epoch: 01941 loss_train: 1.1302 loss_rec: 1.1302 acc_train: 0.4681 loss_val: 1.1419 acc_val: 0.4681 time: 0.0015s\n",
      "Test set results: loss= 1.1734 accuracy= 0.3940\n",
      "Epoch: 01942 loss_train: 1.1302 loss_rec: 1.1302 acc_train: 0.4681 loss_val: 1.1419 acc_val: 0.4681 time: 0.0015s\n",
      "Epoch: 01943 loss_train: 1.1302 loss_rec: 1.1302 acc_train: 0.4681 loss_val: 1.1419 acc_val: 0.4681 time: 0.0015s\n",
      "Epoch: 01944 loss_train: 1.1302 loss_rec: 1.1302 acc_train: 0.4681 loss_val: 1.1418 acc_val: 0.4678 time: 0.0015s\n",
      "Epoch: 01945 loss_train: 1.1301 loss_rec: 1.1301 acc_train: 0.4681 loss_val: 1.1418 acc_val: 0.4678 time: 0.0020s\n",
      "Epoch: 01946 loss_train: 1.1301 loss_rec: 1.1301 acc_train: 0.4681 loss_val: 1.1418 acc_val: 0.4678 time: 0.0020s\n",
      "Epoch: 01947 loss_train: 1.1301 loss_rec: 1.1301 acc_train: 0.4682 loss_val: 1.1418 acc_val: 0.4678 time: 0.0015s\n",
      "Epoch: 01948 loss_train: 1.1301 loss_rec: 1.1301 acc_train: 0.4681 loss_val: 1.1418 acc_val: 0.4678 time: 0.0020s\n",
      "Epoch: 01949 loss_train: 1.1300 loss_rec: 1.1300 acc_train: 0.4681 loss_val: 1.1417 acc_val: 0.4678 time: 0.0015s\n",
      "Epoch: 01950 loss_train: 1.1300 loss_rec: 1.1300 acc_train: 0.4681 loss_val: 1.1417 acc_val: 0.4678 time: 0.0015s\n",
      "Epoch: 01951 loss_train: 1.1300 loss_rec: 1.1300 acc_train: 0.4681 loss_val: 1.1417 acc_val: 0.4678 time: 0.0020s\n",
      "Test set results: loss= 1.1733 accuracy= 0.3940\n",
      "Epoch: 01952 loss_train: 1.1300 loss_rec: 1.1300 acc_train: 0.4681 loss_val: 1.1417 acc_val: 0.4678 time: 0.0015s\n",
      "Epoch: 01953 loss_train: 1.1299 loss_rec: 1.1299 acc_train: 0.4681 loss_val: 1.1417 acc_val: 0.4678 time: 0.0015s\n",
      "Epoch: 01954 loss_train: 1.1299 loss_rec: 1.1299 acc_train: 0.4680 loss_val: 1.1417 acc_val: 0.4678 time: 0.0020s\n",
      "Epoch: 01955 loss_train: 1.1299 loss_rec: 1.1299 acc_train: 0.4682 loss_val: 1.1416 acc_val: 0.4681 time: 0.0015s\n",
      "Epoch: 01956 loss_train: 1.1299 loss_rec: 1.1299 acc_train: 0.4682 loss_val: 1.1416 acc_val: 0.4681 time: 0.0015s\n",
      "Epoch: 01957 loss_train: 1.1298 loss_rec: 1.1298 acc_train: 0.4681 loss_val: 1.1416 acc_val: 0.4681 time: 0.0015s\n",
      "Epoch: 01958 loss_train: 1.1298 loss_rec: 1.1298 acc_train: 0.4681 loss_val: 1.1416 acc_val: 0.4681 time: 0.0015s\n",
      "Epoch: 01959 loss_train: 1.1298 loss_rec: 1.1298 acc_train: 0.4681 loss_val: 1.1416 acc_val: 0.4681 time: 0.0015s\n",
      "Epoch: 01960 loss_train: 1.1298 loss_rec: 1.1298 acc_train: 0.4682 loss_val: 1.1415 acc_val: 0.4681 time: 0.0020s\n",
      "Epoch: 01961 loss_train: 1.1297 loss_rec: 1.1297 acc_train: 0.4682 loss_val: 1.1415 acc_val: 0.4681 time: 0.0020s\n",
      "Test set results: loss= 1.1731 accuracy= 0.3954\n",
      "Epoch: 01962 loss_train: 1.1297 loss_rec: 1.1297 acc_train: 0.4689 loss_val: 1.1415 acc_val: 0.4688 time: 0.0015s\n",
      "Epoch: 01963 loss_train: 1.1297 loss_rec: 1.1297 acc_train: 0.4700 loss_val: 1.1415 acc_val: 0.4700 time: 0.0020s\n",
      "Epoch: 01964 loss_train: 1.1297 loss_rec: 1.1297 acc_train: 0.4700 loss_val: 1.1415 acc_val: 0.4700 time: 0.0015s\n",
      "Epoch: 01965 loss_train: 1.1297 loss_rec: 1.1297 acc_train: 0.4701 loss_val: 1.1415 acc_val: 0.4700 time: 0.0020s\n",
      "Epoch: 01966 loss_train: 1.1296 loss_rec: 1.1296 acc_train: 0.4701 loss_val: 1.1414 acc_val: 0.4700 time: 0.0015s\n",
      "Epoch: 01967 loss_train: 1.1296 loss_rec: 1.1296 acc_train: 0.4701 loss_val: 1.1414 acc_val: 0.4700 time: 0.0030s\n",
      "Epoch: 01968 loss_train: 1.1296 loss_rec: 1.1296 acc_train: 0.4701 loss_val: 1.1414 acc_val: 0.4700 time: 0.0020s\n",
      "Epoch: 01969 loss_train: 1.1296 loss_rec: 1.1296 acc_train: 0.4701 loss_val: 1.1414 acc_val: 0.4700 time: 0.0025s\n",
      "Epoch: 01970 loss_train: 1.1295 loss_rec: 1.1295 acc_train: 0.4701 loss_val: 1.1414 acc_val: 0.4700 time: 0.0020s\n",
      "Epoch: 01971 loss_train: 1.1295 loss_rec: 1.1295 acc_train: 0.4701 loss_val: 1.1413 acc_val: 0.4700 time: 0.0020s\n",
      "Test set results: loss= 1.1731 accuracy= 0.3964\n",
      "Epoch: 01972 loss_train: 1.1295 loss_rec: 1.1295 acc_train: 0.4701 loss_val: 1.1413 acc_val: 0.4700 time: 0.0030s\n",
      "Epoch: 01973 loss_train: 1.1295 loss_rec: 1.1295 acc_train: 0.4701 loss_val: 1.1413 acc_val: 0.4700 time: 0.0020s\n",
      "Epoch: 01974 loss_train: 1.1294 loss_rec: 1.1294 acc_train: 0.4701 loss_val: 1.1413 acc_val: 0.4697 time: 0.0025s\n",
      "Epoch: 01975 loss_train: 1.1294 loss_rec: 1.1294 acc_train: 0.4701 loss_val: 1.1413 acc_val: 0.4697 time: 0.0015s\n",
      "Epoch: 01976 loss_train: 1.1294 loss_rec: 1.1294 acc_train: 0.4701 loss_val: 1.1413 acc_val: 0.4697 time: 0.0030s\n",
      "Epoch: 01977 loss_train: 1.1294 loss_rec: 1.1294 acc_train: 0.4707 loss_val: 1.1412 acc_val: 0.4700 time: 0.0020s\n",
      "Epoch: 01978 loss_train: 1.1293 loss_rec: 1.1293 acc_train: 0.4707 loss_val: 1.1412 acc_val: 0.4700 time: 0.0020s\n",
      "Epoch: 01979 loss_train: 1.1293 loss_rec: 1.1293 acc_train: 0.4707 loss_val: 1.1412 acc_val: 0.4700 time: 0.0020s\n",
      "Epoch: 01980 loss_train: 1.1293 loss_rec: 1.1293 acc_train: 0.4707 loss_val: 1.1412 acc_val: 0.4700 time: 0.0025s\n",
      "Epoch: 01981 loss_train: 1.1293 loss_rec: 1.1293 acc_train: 0.4707 loss_val: 1.1412 acc_val: 0.4700 time: 0.0025s\n",
      "Test set results: loss= 1.1729 accuracy= 0.3976\n",
      "Epoch: 01982 loss_train: 1.1292 loss_rec: 1.1292 acc_train: 0.4708 loss_val: 1.1412 acc_val: 0.4700 time: 0.0020s\n",
      "Epoch: 01983 loss_train: 1.1292 loss_rec: 1.1292 acc_train: 0.4713 loss_val: 1.1411 acc_val: 0.4706 time: 0.0025s\n",
      "Epoch: 01984 loss_train: 1.1292 loss_rec: 1.1292 acc_train: 0.4713 loss_val: 1.1411 acc_val: 0.4703 time: 0.0020s\n",
      "Epoch: 01985 loss_train: 1.1292 loss_rec: 1.1292 acc_train: 0.4713 loss_val: 1.1411 acc_val: 0.4703 time: 0.0025s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01986 loss_train: 1.1291 loss_rec: 1.1291 acc_train: 0.4719 loss_val: 1.1411 acc_val: 0.4709 time: 0.0025s\n",
      "Epoch: 01987 loss_train: 1.1291 loss_rec: 1.1291 acc_train: 0.4714 loss_val: 1.1411 acc_val: 0.4703 time: 0.0020s\n",
      "Epoch: 01988 loss_train: 1.1291 loss_rec: 1.1291 acc_train: 0.4719 loss_val: 1.1411 acc_val: 0.4709 time: 0.0025s\n",
      "Epoch: 01989 loss_train: 1.1291 loss_rec: 1.1291 acc_train: 0.4719 loss_val: 1.1410 acc_val: 0.4713 time: 0.0020s\n",
      "Epoch: 01990 loss_train: 1.1291 loss_rec: 1.1291 acc_train: 0.4719 loss_val: 1.1410 acc_val: 0.4709 time: 0.0025s\n",
      "Epoch: 01991 loss_train: 1.1290 loss_rec: 1.1290 acc_train: 0.4719 loss_val: 1.1410 acc_val: 0.4713 time: 0.0020s\n",
      "Test set results: loss= 1.1728 accuracy= 0.3992\n",
      "Epoch: 01992 loss_train: 1.1290 loss_rec: 1.1290 acc_train: 0.4718 loss_val: 1.1410 acc_val: 0.4709 time: 0.0020s\n",
      "Epoch: 01993 loss_train: 1.1290 loss_rec: 1.1290 acc_train: 0.4718 loss_val: 1.1410 acc_val: 0.4709 time: 0.0030s\n",
      "Epoch: 01994 loss_train: 1.1290 loss_rec: 1.1290 acc_train: 0.4718 loss_val: 1.1410 acc_val: 0.4709 time: 0.0020s\n",
      "Epoch: 01995 loss_train: 1.1289 loss_rec: 1.1289 acc_train: 0.4718 loss_val: 1.1409 acc_val: 0.4709 time: 0.0020s\n",
      "Epoch: 01996 loss_train: 1.1289 loss_rec: 1.1289 acc_train: 0.4718 loss_val: 1.1409 acc_val: 0.4709 time: 0.0020s\n",
      "Epoch: 01997 loss_train: 1.1289 loss_rec: 1.1289 acc_train: 0.4718 loss_val: 1.1409 acc_val: 0.4709 time: 0.0020s\n",
      "Epoch: 01998 loss_train: 1.1289 loss_rec: 1.1289 acc_train: 0.4718 loss_val: 1.1409 acc_val: 0.4709 time: 0.0020s\n",
      "Epoch: 01999 loss_train: 1.1288 loss_rec: 1.1288 acc_train: 0.4718 loss_val: 1.1409 acc_val: 0.4709 time: 0.0020s\n",
      "Epoch: 02000 loss_train: 1.1288 loss_rec: 1.1288 acc_train: 0.4718 loss_val: 1.1409 acc_val: 0.4713 time: 0.0020s\n",
      "Epoch: 02001 loss_train: 1.1288 loss_rec: 1.1288 acc_train: 0.4718 loss_val: 1.1409 acc_val: 0.4713 time: 0.0030s\n",
      "Test set results: loss= 1.1726 accuracy= 0.3991\n",
      "Epoch: 02002 loss_train: 1.1288 loss_rec: 1.1288 acc_train: 0.4718 loss_val: 1.1408 acc_val: 0.4713 time: 0.0025s\n",
      "Epoch: 02003 loss_train: 1.1288 loss_rec: 1.1288 acc_train: 0.4718 loss_val: 1.1408 acc_val: 0.4713 time: 0.0025s\n",
      "Epoch: 02004 loss_train: 1.1287 loss_rec: 1.1287 acc_train: 0.4718 loss_val: 1.1408 acc_val: 0.4713 time: 0.0020s\n",
      "Epoch: 02005 loss_train: 1.1287 loss_rec: 1.1287 acc_train: 0.4718 loss_val: 1.1408 acc_val: 0.4713 time: 0.0025s\n",
      "Epoch: 02006 loss_train: 1.1287 loss_rec: 1.1287 acc_train: 0.4718 loss_val: 1.1408 acc_val: 0.4713 time: 0.0015s\n",
      "Epoch: 02007 loss_train: 1.1287 loss_rec: 1.1287 acc_train: 0.4718 loss_val: 1.1408 acc_val: 0.4713 time: 0.0030s\n",
      "Epoch: 02008 loss_train: 1.1286 loss_rec: 1.1286 acc_train: 0.4718 loss_val: 1.1407 acc_val: 0.4713 time: 0.0020s\n",
      "Epoch: 02009 loss_train: 1.1286 loss_rec: 1.1286 acc_train: 0.4718 loss_val: 1.1407 acc_val: 0.4713 time: 0.0025s\n",
      "Epoch: 02010 loss_train: 1.1286 loss_rec: 1.1286 acc_train: 0.4718 loss_val: 1.1407 acc_val: 0.4713 time: 0.0015s\n",
      "Epoch: 02011 loss_train: 1.1286 loss_rec: 1.1286 acc_train: 0.4722 loss_val: 1.1407 acc_val: 0.4722 time: 0.0025s\n",
      "Test set results: loss= 1.1725 accuracy= 0.4000\n",
      "Epoch: 02012 loss_train: 1.1285 loss_rec: 1.1285 acc_train: 0.4722 loss_val: 1.1407 acc_val: 0.4722 time: 0.0020s\n",
      "Epoch: 02013 loss_train: 1.1285 loss_rec: 1.1285 acc_train: 0.4722 loss_val: 1.1407 acc_val: 0.4722 time: 0.0020s\n",
      "Epoch: 02014 loss_train: 1.1285 loss_rec: 1.1285 acc_train: 0.4722 loss_val: 1.1406 acc_val: 0.4722 time: 0.0020s\n",
      "Epoch: 02015 loss_train: 1.1285 loss_rec: 1.1285 acc_train: 0.4722 loss_val: 1.1406 acc_val: 0.4722 time: 0.0020s\n",
      "Epoch: 02016 loss_train: 1.1285 loss_rec: 1.1285 acc_train: 0.4722 loss_val: 1.1406 acc_val: 0.4722 time: 0.0015s\n",
      "Epoch: 02017 loss_train: 1.1284 loss_rec: 1.1284 acc_train: 0.4729 loss_val: 1.1406 acc_val: 0.4734 time: 0.0015s\n",
      "Epoch: 02018 loss_train: 1.1284 loss_rec: 1.1284 acc_train: 0.4729 loss_val: 1.1406 acc_val: 0.4734 time: 0.0015s\n",
      "Epoch: 02019 loss_train: 1.1284 loss_rec: 1.1284 acc_train: 0.4764 loss_val: 1.1406 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02020 loss_train: 1.1284 loss_rec: 1.1284 acc_train: 0.4764 loss_val: 1.1405 acc_val: 0.4759 time: 0.0020s\n",
      "Epoch: 02021 loss_train: 1.1283 loss_rec: 1.1283 acc_train: 0.4764 loss_val: 1.1405 acc_val: 0.4759 time: 0.0015s\n",
      "Test set results: loss= 1.1724 accuracy= 0.4025\n",
      "Epoch: 02022 loss_train: 1.1283 loss_rec: 1.1283 acc_train: 0.4764 loss_val: 1.1405 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02023 loss_train: 1.1283 loss_rec: 1.1283 acc_train: 0.4764 loss_val: 1.1405 acc_val: 0.4759 time: 0.0020s\n",
      "Epoch: 02024 loss_train: 1.1283 loss_rec: 1.1283 acc_train: 0.4764 loss_val: 1.1405 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02025 loss_train: 1.1283 loss_rec: 1.1283 acc_train: 0.4764 loss_val: 1.1405 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02026 loss_train: 1.1282 loss_rec: 1.1282 acc_train: 0.4764 loss_val: 1.1405 acc_val: 0.4759 time: 0.0020s\n",
      "Epoch: 02027 loss_train: 1.1282 loss_rec: 1.1282 acc_train: 0.4765 loss_val: 1.1404 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02028 loss_train: 1.1282 loss_rec: 1.1282 acc_train: 0.4766 loss_val: 1.1404 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02029 loss_train: 1.1282 loss_rec: 1.1282 acc_train: 0.4766 loss_val: 1.1404 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02030 loss_train: 1.1281 loss_rec: 1.1281 acc_train: 0.4766 loss_val: 1.1404 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02031 loss_train: 1.1281 loss_rec: 1.1281 acc_train: 0.4766 loss_val: 1.1404 acc_val: 0.4759 time: 0.0015s\n",
      "Test set results: loss= 1.1723 accuracy= 0.4025\n",
      "Epoch: 02032 loss_train: 1.1281 loss_rec: 1.1281 acc_train: 0.4767 loss_val: 1.1404 acc_val: 0.4759 time: 0.0020s\n",
      "Epoch: 02033 loss_train: 1.1281 loss_rec: 1.1281 acc_train: 0.4767 loss_val: 1.1403 acc_val: 0.4759 time: 0.0020s\n",
      "Epoch: 02034 loss_train: 1.1281 loss_rec: 1.1281 acc_train: 0.4767 loss_val: 1.1403 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02035 loss_train: 1.1280 loss_rec: 1.1280 acc_train: 0.4767 loss_val: 1.1403 acc_val: 0.4759 time: 0.0020s\n",
      "Epoch: 02036 loss_train: 1.1280 loss_rec: 1.1280 acc_train: 0.4768 loss_val: 1.1403 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02037 loss_train: 1.1280 loss_rec: 1.1280 acc_train: 0.4768 loss_val: 1.1403 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02038 loss_train: 1.1280 loss_rec: 1.1280 acc_train: 0.4768 loss_val: 1.1403 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02039 loss_train: 1.1279 loss_rec: 1.1279 acc_train: 0.4768 loss_val: 1.1403 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02040 loss_train: 1.1279 loss_rec: 1.1279 acc_train: 0.4768 loss_val: 1.1402 acc_val: 0.4759 time: 0.0020s\n",
      "Epoch: 02041 loss_train: 1.1279 loss_rec: 1.1279 acc_train: 0.4768 loss_val: 1.1402 acc_val: 0.4759 time: 0.0015s\n",
      "Test set results: loss= 1.1721 accuracy= 0.4025\n",
      "Epoch: 02042 loss_train: 1.1279 loss_rec: 1.1279 acc_train: 0.4768 loss_val: 1.1402 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02043 loss_train: 1.1279 loss_rec: 1.1279 acc_train: 0.4768 loss_val: 1.1402 acc_val: 0.4759 time: 0.0020s\n",
      "Epoch: 02044 loss_train: 1.1278 loss_rec: 1.1278 acc_train: 0.4768 loss_val: 1.1402 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02045 loss_train: 1.1278 loss_rec: 1.1278 acc_train: 0.4768 loss_val: 1.1402 acc_val: 0.4759 time: 0.0020s\n",
      "Epoch: 02046 loss_train: 1.1278 loss_rec: 1.1278 acc_train: 0.4768 loss_val: 1.1402 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02047 loss_train: 1.1278 loss_rec: 1.1278 acc_train: 0.4768 loss_val: 1.1401 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02048 loss_train: 1.1277 loss_rec: 1.1277 acc_train: 0.4768 loss_val: 1.1401 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02049 loss_train: 1.1277 loss_rec: 1.1277 acc_train: 0.4768 loss_val: 1.1401 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02050 loss_train: 1.1277 loss_rec: 1.1277 acc_train: 0.4768 loss_val: 1.1401 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02051 loss_train: 1.1277 loss_rec: 1.1277 acc_train: 0.4768 loss_val: 1.1401 acc_val: 0.4759 time: 0.0015s\n",
      "Test set results: loss= 1.1720 accuracy= 0.4025\n",
      "Epoch: 02052 loss_train: 1.1277 loss_rec: 1.1277 acc_train: 0.4768 loss_val: 1.1401 acc_val: 0.4759 time: 0.0020s\n",
      "Epoch: 02053 loss_train: 1.1276 loss_rec: 1.1276 acc_train: 0.4768 loss_val: 1.1400 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02054 loss_train: 1.1276 loss_rec: 1.1276 acc_train: 0.4768 loss_val: 1.1400 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02055 loss_train: 1.1276 loss_rec: 1.1276 acc_train: 0.4768 loss_val: 1.1400 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02056 loss_train: 1.1276 loss_rec: 1.1276 acc_train: 0.4768 loss_val: 1.1400 acc_val: 0.4759 time: 0.0020s\n",
      "Epoch: 02057 loss_train: 1.1275 loss_rec: 1.1275 acc_train: 0.4766 loss_val: 1.1400 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02058 loss_train: 1.1275 loss_rec: 1.1275 acc_train: 0.4766 loss_val: 1.1400 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02059 loss_train: 1.1275 loss_rec: 1.1275 acc_train: 0.4766 loss_val: 1.1400 acc_val: 0.4759 time: 0.0020s\n",
      "Epoch: 02060 loss_train: 1.1275 loss_rec: 1.1275 acc_train: 0.4766 loss_val: 1.1399 acc_val: 0.4759 time: 0.0020s\n",
      "Epoch: 02061 loss_train: 1.1275 loss_rec: 1.1275 acc_train: 0.4766 loss_val: 1.1399 acc_val: 0.4759 time: 0.0025s\n",
      "Test set results: loss= 1.1720 accuracy= 0.4025\n",
      "Epoch: 02062 loss_train: 1.1274 loss_rec: 1.1274 acc_train: 0.4766 loss_val: 1.1399 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02063 loss_train: 1.1274 loss_rec: 1.1274 acc_train: 0.4766 loss_val: 1.1399 acc_val: 0.4759 time: 0.0020s\n",
      "Epoch: 02064 loss_train: 1.1274 loss_rec: 1.1274 acc_train: 0.4766 loss_val: 1.1399 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02065 loss_train: 1.1274 loss_rec: 1.1274 acc_train: 0.4766 loss_val: 1.1399 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02066 loss_train: 1.1274 loss_rec: 1.1274 acc_train: 0.4766 loss_val: 1.1399 acc_val: 0.4759 time: 0.0015s\n",
      "Epoch: 02067 loss_train: 1.1273 loss_rec: 1.1273 acc_train: 0.4771 loss_val: 1.1398 acc_val: 0.4763 time: 0.0015s\n",
      "Epoch: 02068 loss_train: 1.1273 loss_rec: 1.1273 acc_train: 0.4771 loss_val: 1.1398 acc_val: 0.4763 time: 0.0015s\n",
      "Epoch: 02069 loss_train: 1.1273 loss_rec: 1.1273 acc_train: 0.4771 loss_val: 1.1398 acc_val: 0.4763 time: 0.0015s\n",
      "Epoch: 02070 loss_train: 1.1273 loss_rec: 1.1273 acc_train: 0.4771 loss_val: 1.1398 acc_val: 0.4763 time: 0.0020s\n",
      "Epoch: 02071 loss_train: 1.1273 loss_rec: 1.1273 acc_train: 0.4771 loss_val: 1.1398 acc_val: 0.4763 time: 0.0015s\n",
      "Test set results: loss= 1.1718 accuracy= 0.4034\n",
      "Epoch: 02072 loss_train: 1.1272 loss_rec: 1.1272 acc_train: 0.4771 loss_val: 1.1398 acc_val: 0.4763 time: 0.0015s\n",
      "Epoch: 02073 loss_train: 1.1272 loss_rec: 1.1272 acc_train: 0.4771 loss_val: 1.1398 acc_val: 0.4763 time: 0.0020s\n",
      "Epoch: 02074 loss_train: 1.1272 loss_rec: 1.1272 acc_train: 0.4771 loss_val: 1.1397 acc_val: 0.4763 time: 0.0015s\n",
      "Epoch: 02075 loss_train: 1.1272 loss_rec: 1.1272 acc_train: 0.4771 loss_val: 1.1397 acc_val: 0.4763 time: 0.0020s\n",
      "Epoch: 02076 loss_train: 1.1271 loss_rec: 1.1271 acc_train: 0.4771 loss_val: 1.1397 acc_val: 0.4763 time: 0.0015s\n",
      "Epoch: 02077 loss_train: 1.1271 loss_rec: 1.1271 acc_train: 0.4771 loss_val: 1.1397 acc_val: 0.4763 time: 0.0020s\n",
      "Epoch: 02078 loss_train: 1.1271 loss_rec: 1.1271 acc_train: 0.4771 loss_val: 1.1397 acc_val: 0.4759 time: 0.0020s\n",
      "Epoch: 02079 loss_train: 1.1271 loss_rec: 1.1271 acc_train: 0.4771 loss_val: 1.1397 acc_val: 0.4763 time: 0.0015s\n",
      "Epoch: 02080 loss_train: 1.1271 loss_rec: 1.1271 acc_train: 0.4771 loss_val: 1.1397 acc_val: 0.4763 time: 0.0015s\n",
      "Epoch: 02081 loss_train: 1.1270 loss_rec: 1.1270 acc_train: 0.4771 loss_val: 1.1396 acc_val: 0.4759 time: 0.0015s\n",
      "Test set results: loss= 1.1717 accuracy= 0.4042\n",
      "Epoch: 02082 loss_train: 1.1270 loss_rec: 1.1270 acc_train: 0.4774 loss_val: 1.1396 acc_val: 0.4766 time: 0.0020s\n",
      "Epoch: 02083 loss_train: 1.1270 loss_rec: 1.1270 acc_train: 0.4774 loss_val: 1.1396 acc_val: 0.4766 time: 0.0020s\n",
      "Epoch: 02084 loss_train: 1.1270 loss_rec: 1.1270 acc_train: 0.4774 loss_val: 1.1396 acc_val: 0.4766 time: 0.0025s\n",
      "Epoch: 02085 loss_train: 1.1270 loss_rec: 1.1270 acc_train: 0.4774 loss_val: 1.1396 acc_val: 0.4766 time: 0.0020s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02086 loss_train: 1.1269 loss_rec: 1.1269 acc_train: 0.4774 loss_val: 1.1396 acc_val: 0.4766 time: 0.0030s\n",
      "Epoch: 02087 loss_train: 1.1269 loss_rec: 1.1269 acc_train: 0.4774 loss_val: 1.1396 acc_val: 0.4766 time: 0.0030s\n",
      "Epoch: 02088 loss_train: 1.1269 loss_rec: 1.1269 acc_train: 0.4774 loss_val: 1.1395 acc_val: 0.4766 time: 0.0025s\n",
      "Epoch: 02089 loss_train: 1.1269 loss_rec: 1.1269 acc_train: 0.4774 loss_val: 1.1395 acc_val: 0.4766 time: 0.0020s\n",
      "Epoch: 02090 loss_train: 1.1269 loss_rec: 1.1269 acc_train: 0.4774 loss_val: 1.1395 acc_val: 0.4766 time: 0.0030s\n",
      "Epoch: 02091 loss_train: 1.1268 loss_rec: 1.1268 acc_train: 0.4774 loss_val: 1.1395 acc_val: 0.4766 time: 0.0025s\n",
      "Test set results: loss= 1.1716 accuracy= 0.4042\n",
      "Epoch: 02092 loss_train: 1.1268 loss_rec: 1.1268 acc_train: 0.4774 loss_val: 1.1395 acc_val: 0.4766 time: 0.0030s\n",
      "Epoch: 02093 loss_train: 1.1268 loss_rec: 1.1268 acc_train: 0.4774 loss_val: 1.1395 acc_val: 0.4766 time: 0.0020s\n",
      "Epoch: 02094 loss_train: 1.1268 loss_rec: 1.1268 acc_train: 0.4774 loss_val: 1.1395 acc_val: 0.4766 time: 0.0025s\n",
      "Epoch: 02095 loss_train: 1.1267 loss_rec: 1.1267 acc_train: 0.4774 loss_val: 1.1394 acc_val: 0.4766 time: 0.0020s\n",
      "Epoch: 02096 loss_train: 1.1267 loss_rec: 1.1267 acc_train: 0.4774 loss_val: 1.1394 acc_val: 0.4766 time: 0.0025s\n",
      "Epoch: 02097 loss_train: 1.1267 loss_rec: 1.1267 acc_train: 0.4774 loss_val: 1.1394 acc_val: 0.4766 time: 0.0020s\n",
      "Epoch: 02098 loss_train: 1.1267 loss_rec: 1.1267 acc_train: 0.4774 loss_val: 1.1394 acc_val: 0.4766 time: 0.0035s\n",
      "Epoch: 02099 loss_train: 1.1267 loss_rec: 1.1267 acc_train: 0.4774 loss_val: 1.1394 acc_val: 0.4766 time: 0.0025s\n",
      "Epoch: 02100 loss_train: 1.1266 loss_rec: 1.1266 acc_train: 0.4774 loss_val: 1.1394 acc_val: 0.4766 time: 0.0015s\n",
      "Epoch: 02101 loss_train: 1.1266 loss_rec: 1.1266 acc_train: 0.4774 loss_val: 1.1394 acc_val: 0.4766 time: 0.0025s\n",
      "Test set results: loss= 1.1716 accuracy= 0.4042\n",
      "Epoch: 02102 loss_train: 1.1266 loss_rec: 1.1266 acc_train: 0.4774 loss_val: 1.1393 acc_val: 0.4766 time: 0.0025s\n",
      "Epoch: 02103 loss_train: 1.1266 loss_rec: 1.1266 acc_train: 0.4779 loss_val: 1.1393 acc_val: 0.4775 time: 0.0020s\n",
      "Epoch: 02104 loss_train: 1.1266 loss_rec: 1.1266 acc_train: 0.4780 loss_val: 1.1393 acc_val: 0.4775 time: 0.0030s\n",
      "Epoch: 02105 loss_train: 1.1265 loss_rec: 1.1265 acc_train: 0.4781 loss_val: 1.1393 acc_val: 0.4775 time: 0.0020s\n",
      "Epoch: 02106 loss_train: 1.1265 loss_rec: 1.1265 acc_train: 0.4781 loss_val: 1.1393 acc_val: 0.4775 time: 0.0015s\n",
      "Epoch: 02107 loss_train: 1.1265 loss_rec: 1.1265 acc_train: 0.4780 loss_val: 1.1393 acc_val: 0.4775 time: 0.0020s\n",
      "Epoch: 02108 loss_train: 1.1265 loss_rec: 1.1265 acc_train: 0.4779 loss_val: 1.1393 acc_val: 0.4775 time: 0.0025s\n",
      "Epoch: 02109 loss_train: 1.1265 loss_rec: 1.1265 acc_train: 0.4779 loss_val: 1.1393 acc_val: 0.4775 time: 0.0015s\n",
      "Epoch: 02110 loss_train: 1.1264 loss_rec: 1.1264 acc_train: 0.4780 loss_val: 1.1392 acc_val: 0.4775 time: 0.0025s\n",
      "Epoch: 02111 loss_train: 1.1264 loss_rec: 1.1264 acc_train: 0.4784 loss_val: 1.1392 acc_val: 0.4781 time: 0.0020s\n",
      "Test set results: loss= 1.1713 accuracy= 0.4057\n",
      "Epoch: 02112 loss_train: 1.1264 loss_rec: 1.1264 acc_train: 0.4784 loss_val: 1.1392 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02113 loss_train: 1.1264 loss_rec: 1.1264 acc_train: 0.4783 loss_val: 1.1392 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02114 loss_train: 1.1264 loss_rec: 1.1264 acc_train: 0.4783 loss_val: 1.1392 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02115 loss_train: 1.1263 loss_rec: 1.1263 acc_train: 0.4784 loss_val: 1.1392 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02116 loss_train: 1.1263 loss_rec: 1.1263 acc_train: 0.4784 loss_val: 1.1392 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02117 loss_train: 1.1263 loss_rec: 1.1263 acc_train: 0.4784 loss_val: 1.1391 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02118 loss_train: 1.1263 loss_rec: 1.1263 acc_train: 0.4784 loss_val: 1.1391 acc_val: 0.4781 time: 0.0015s\n",
      "Epoch: 02119 loss_train: 1.1263 loss_rec: 1.1263 acc_train: 0.4784 loss_val: 1.1391 acc_val: 0.4781 time: 0.0030s\n",
      "Epoch: 02120 loss_train: 1.1262 loss_rec: 1.1262 acc_train: 0.4784 loss_val: 1.1391 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02121 loss_train: 1.1262 loss_rec: 1.1262 acc_train: 0.4786 loss_val: 1.1391 acc_val: 0.4781 time: 0.0020s\n",
      "Test set results: loss= 1.1713 accuracy= 0.4057\n",
      "Epoch: 02122 loss_train: 1.1262 loss_rec: 1.1262 acc_train: 0.4786 loss_val: 1.1391 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02123 loss_train: 1.1262 loss_rec: 1.1262 acc_train: 0.4786 loss_val: 1.1391 acc_val: 0.4781 time: 0.0015s\n",
      "Epoch: 02124 loss_train: 1.1262 loss_rec: 1.1262 acc_train: 0.4784 loss_val: 1.1391 acc_val: 0.4781 time: 0.0030s\n",
      "Epoch: 02125 loss_train: 1.1261 loss_rec: 1.1261 acc_train: 0.4786 loss_val: 1.1390 acc_val: 0.4781 time: 0.0015s\n",
      "Epoch: 02126 loss_train: 1.1261 loss_rec: 1.1261 acc_train: 0.4786 loss_val: 1.1390 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02127 loss_train: 1.1261 loss_rec: 1.1261 acc_train: 0.4786 loss_val: 1.1390 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02128 loss_train: 1.1261 loss_rec: 1.1261 acc_train: 0.4784 loss_val: 1.1390 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02129 loss_train: 1.1261 loss_rec: 1.1261 acc_train: 0.4784 loss_val: 1.1390 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02130 loss_train: 1.1260 loss_rec: 1.1260 acc_train: 0.4784 loss_val: 1.1390 acc_val: 0.4781 time: 0.0015s\n",
      "Epoch: 02131 loss_train: 1.1260 loss_rec: 1.1260 acc_train: 0.4784 loss_val: 1.1390 acc_val: 0.4781 time: 0.0025s\n",
      "Test set results: loss= 1.1712 accuracy= 0.4057\n",
      "Epoch: 02132 loss_train: 1.1260 loss_rec: 1.1260 acc_train: 0.4784 loss_val: 1.1390 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02133 loss_train: 1.1260 loss_rec: 1.1260 acc_train: 0.4784 loss_val: 1.1389 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02134 loss_train: 1.1260 loss_rec: 1.1260 acc_train: 0.4784 loss_val: 1.1389 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02135 loss_train: 1.1259 loss_rec: 1.1259 acc_train: 0.4784 loss_val: 1.1389 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02136 loss_train: 1.1259 loss_rec: 1.1259 acc_train: 0.4784 loss_val: 1.1389 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02137 loss_train: 1.1259 loss_rec: 1.1259 acc_train: 0.4784 loss_val: 1.1389 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02138 loss_train: 1.1259 loss_rec: 1.1259 acc_train: 0.4784 loss_val: 1.1389 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02139 loss_train: 1.1259 loss_rec: 1.1259 acc_train: 0.4784 loss_val: 1.1389 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02140 loss_train: 1.1258 loss_rec: 1.1258 acc_train: 0.4784 loss_val: 1.1388 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02141 loss_train: 1.1258 loss_rec: 1.1258 acc_train: 0.4784 loss_val: 1.1388 acc_val: 0.4781 time: 0.0020s\n",
      "Test set results: loss= 1.1711 accuracy= 0.4058\n",
      "Epoch: 02142 loss_train: 1.1258 loss_rec: 1.1258 acc_train: 0.4784 loss_val: 1.1388 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02143 loss_train: 1.1258 loss_rec: 1.1258 acc_train: 0.4785 loss_val: 1.1388 acc_val: 0.4781 time: 0.0015s\n",
      "Epoch: 02144 loss_train: 1.1258 loss_rec: 1.1258 acc_train: 0.4785 loss_val: 1.1388 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02145 loss_train: 1.1257 loss_rec: 1.1257 acc_train: 0.4785 loss_val: 1.1388 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02146 loss_train: 1.1257 loss_rec: 1.1257 acc_train: 0.4785 loss_val: 1.1388 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02147 loss_train: 1.1257 loss_rec: 1.1257 acc_train: 0.4785 loss_val: 1.1388 acc_val: 0.4781 time: 0.0015s\n",
      "Epoch: 02148 loss_train: 1.1257 loss_rec: 1.1257 acc_train: 0.4785 loss_val: 1.1387 acc_val: 0.4781 time: 0.0030s\n",
      "Epoch: 02149 loss_train: 1.1257 loss_rec: 1.1257 acc_train: 0.4789 loss_val: 1.1387 acc_val: 0.4784 time: 0.0030s\n",
      "Epoch: 02150 loss_train: 1.1256 loss_rec: 1.1256 acc_train: 0.4789 loss_val: 1.1387 acc_val: 0.4784 time: 0.0015s\n",
      "Epoch: 02151 loss_train: 1.1256 loss_rec: 1.1256 acc_train: 0.4789 loss_val: 1.1387 acc_val: 0.4784 time: 0.0025s\n",
      "Test set results: loss= 1.1710 accuracy= 0.4064\n",
      "Epoch: 02152 loss_train: 1.1256 loss_rec: 1.1256 acc_train: 0.4784 loss_val: 1.1387 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02153 loss_train: 1.1256 loss_rec: 1.1256 acc_train: 0.4784 loss_val: 1.1387 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02154 loss_train: 1.1256 loss_rec: 1.1256 acc_train: 0.4789 loss_val: 1.1387 acc_val: 0.4784 time: 0.0015s\n",
      "Epoch: 02155 loss_train: 1.1256 loss_rec: 1.1256 acc_train: 0.4793 loss_val: 1.1387 acc_val: 0.4784 time: 0.0030s\n",
      "Epoch: 02156 loss_train: 1.1255 loss_rec: 1.1255 acc_train: 0.4793 loss_val: 1.1386 acc_val: 0.4784 time: 0.0025s\n",
      "Epoch: 02157 loss_train: 1.1255 loss_rec: 1.1255 acc_train: 0.4788 loss_val: 1.1386 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02158 loss_train: 1.1255 loss_rec: 1.1255 acc_train: 0.4788 loss_val: 1.1386 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02159 loss_train: 1.1255 loss_rec: 1.1255 acc_train: 0.4788 loss_val: 1.1386 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02160 loss_train: 1.1255 loss_rec: 1.1255 acc_train: 0.4788 loss_val: 1.1386 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02161 loss_train: 1.1254 loss_rec: 1.1254 acc_train: 0.4789 loss_val: 1.1386 acc_val: 0.4781 time: 0.0020s\n",
      "Test set results: loss= 1.1709 accuracy= 0.4075\n",
      "Epoch: 02162 loss_train: 1.1254 loss_rec: 1.1254 acc_train: 0.4793 loss_val: 1.1386 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02163 loss_train: 1.1254 loss_rec: 1.1254 acc_train: 0.4793 loss_val: 1.1386 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02164 loss_train: 1.1254 loss_rec: 1.1254 acc_train: 0.4789 loss_val: 1.1385 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02165 loss_train: 1.1254 loss_rec: 1.1254 acc_train: 0.4793 loss_val: 1.1385 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02166 loss_train: 1.1253 loss_rec: 1.1253 acc_train: 0.4793 loss_val: 1.1385 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02167 loss_train: 1.1253 loss_rec: 1.1253 acc_train: 0.4793 loss_val: 1.1385 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02168 loss_train: 1.1253 loss_rec: 1.1253 acc_train: 0.4800 loss_val: 1.1385 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02169 loss_train: 1.1253 loss_rec: 1.1253 acc_train: 0.4804 loss_val: 1.1385 acc_val: 0.4784 time: 0.0025s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02170 loss_train: 1.1253 loss_rec: 1.1253 acc_train: 0.4801 loss_val: 1.1385 acc_val: 0.4784 time: 0.0030s\n",
      "Epoch: 02171 loss_train: 1.1253 loss_rec: 1.1253 acc_train: 0.4801 loss_val: 1.1385 acc_val: 0.4784 time: 0.0020s\n",
      "Test set results: loss= 1.1708 accuracy= 0.4074\n",
      "Epoch: 02172 loss_train: 1.1252 loss_rec: 1.1252 acc_train: 0.4797 loss_val: 1.1384 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02173 loss_train: 1.1252 loss_rec: 1.1252 acc_train: 0.4797 loss_val: 1.1384 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02174 loss_train: 1.1252 loss_rec: 1.1252 acc_train: 0.4801 loss_val: 1.1384 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02175 loss_train: 1.1252 loss_rec: 1.1252 acc_train: 0.4801 loss_val: 1.1384 acc_val: 0.4781 time: 0.0030s\n",
      "Epoch: 02176 loss_train: 1.1252 loss_rec: 1.1252 acc_train: 0.4801 loss_val: 1.1384 acc_val: 0.4781 time: 0.0015s\n",
      "Epoch: 02177 loss_train: 1.1251 loss_rec: 1.1251 acc_train: 0.4801 loss_val: 1.1384 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02178 loss_train: 1.1251 loss_rec: 1.1251 acc_train: 0.4802 loss_val: 1.1384 acc_val: 0.4781 time: 0.0020s\n",
      "Epoch: 02179 loss_train: 1.1251 loss_rec: 1.1251 acc_train: 0.4802 loss_val: 1.1384 acc_val: 0.4781 time: 0.0025s\n",
      "Epoch: 02180 loss_train: 1.1251 loss_rec: 1.1251 acc_train: 0.4802 loss_val: 1.1383 acc_val: 0.4781 time: 0.0015s\n",
      "Epoch: 02181 loss_train: 1.1251 loss_rec: 1.1251 acc_train: 0.4802 loss_val: 1.1383 acc_val: 0.4781 time: 0.0025s\n",
      "Test set results: loss= 1.1708 accuracy= 0.4091\n",
      "Epoch: 02182 loss_train: 1.1250 loss_rec: 1.1250 acc_train: 0.4809 loss_val: 1.1383 acc_val: 0.4791 time: 0.0030s\n",
      "Epoch: 02183 loss_train: 1.1250 loss_rec: 1.1250 acc_train: 0.4809 loss_val: 1.1383 acc_val: 0.4791 time: 0.0015s\n",
      "Epoch: 02184 loss_train: 1.1250 loss_rec: 1.1250 acc_train: 0.4809 loss_val: 1.1383 acc_val: 0.4791 time: 0.0030s\n",
      "Epoch: 02185 loss_train: 1.1250 loss_rec: 1.1250 acc_train: 0.4809 loss_val: 1.1383 acc_val: 0.4791 time: 0.0020s\n",
      "Epoch: 02186 loss_train: 1.1250 loss_rec: 1.1250 acc_train: 0.4809 loss_val: 1.1383 acc_val: 0.4791 time: 0.0025s\n",
      "Epoch: 02187 loss_train: 1.1249 loss_rec: 1.1249 acc_train: 0.4809 loss_val: 1.1383 acc_val: 0.4791 time: 0.0020s\n",
      "Epoch: 02188 loss_train: 1.1249 loss_rec: 1.1249 acc_train: 0.4809 loss_val: 1.1382 acc_val: 0.4791 time: 0.0020s\n",
      "Epoch: 02189 loss_train: 1.1249 loss_rec: 1.1249 acc_train: 0.4809 loss_val: 1.1382 acc_val: 0.4791 time: 0.0020s\n",
      "Epoch: 02190 loss_train: 1.1249 loss_rec: 1.1249 acc_train: 0.4809 loss_val: 1.1382 acc_val: 0.4791 time: 0.0025s\n",
      "Epoch: 02191 loss_train: 1.1249 loss_rec: 1.1249 acc_train: 0.4809 loss_val: 1.1382 acc_val: 0.4791 time: 0.0030s\n",
      "Test set results: loss= 1.1706 accuracy= 0.4092\n",
      "Epoch: 02192 loss_train: 1.1249 loss_rec: 1.1249 acc_train: 0.4809 loss_val: 1.1382 acc_val: 0.4791 time: 0.0020s\n",
      "Epoch: 02193 loss_train: 1.1248 loss_rec: 1.1248 acc_train: 0.4813 loss_val: 1.1382 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02194 loss_train: 1.1248 loss_rec: 1.1248 acc_train: 0.4813 loss_val: 1.1382 acc_val: 0.4800 time: 0.0025s\n",
      "Epoch: 02195 loss_train: 1.1248 loss_rec: 1.1248 acc_train: 0.4813 loss_val: 1.1382 acc_val: 0.4800 time: 0.0025s\n",
      "Epoch: 02196 loss_train: 1.1248 loss_rec: 1.1248 acc_train: 0.4813 loss_val: 1.1381 acc_val: 0.4800 time: 0.0030s\n",
      "Epoch: 02197 loss_train: 1.1248 loss_rec: 1.1248 acc_train: 0.4813 loss_val: 1.1381 acc_val: 0.4800 time: 0.0025s\n",
      "Epoch: 02198 loss_train: 1.1247 loss_rec: 1.1247 acc_train: 0.4813 loss_val: 1.1381 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02199 loss_train: 1.1247 loss_rec: 1.1247 acc_train: 0.4813 loss_val: 1.1381 acc_val: 0.4800 time: 0.0025s\n",
      "Epoch: 02200 loss_train: 1.1247 loss_rec: 1.1247 acc_train: 0.4813 loss_val: 1.1381 acc_val: 0.4800 time: 0.0015s\n",
      "Epoch: 02201 loss_train: 1.1247 loss_rec: 1.1247 acc_train: 0.4813 loss_val: 1.1381 acc_val: 0.4800 time: 0.0025s\n",
      "Test set results: loss= 1.1705 accuracy= 0.4094\n",
      "Epoch: 02202 loss_train: 1.1247 loss_rec: 1.1247 acc_train: 0.4813 loss_val: 1.1381 acc_val: 0.4800 time: 0.0025s\n",
      "Epoch: 02203 loss_train: 1.1247 loss_rec: 1.1247 acc_train: 0.4813 loss_val: 1.1381 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02204 loss_train: 1.1246 loss_rec: 1.1246 acc_train: 0.4813 loss_val: 1.1381 acc_val: 0.4800 time: 0.0025s\n",
      "Epoch: 02205 loss_train: 1.1246 loss_rec: 1.1246 acc_train: 0.4813 loss_val: 1.1380 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02206 loss_train: 1.1246 loss_rec: 1.1246 acc_train: 0.4813 loss_val: 1.1380 acc_val: 0.4800 time: 0.0025s\n",
      "Epoch: 02207 loss_train: 1.1246 loss_rec: 1.1246 acc_train: 0.4813 loss_val: 1.1380 acc_val: 0.4800 time: 0.0025s\n",
      "Epoch: 02208 loss_train: 1.1246 loss_rec: 1.1246 acc_train: 0.4813 loss_val: 1.1380 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02209 loss_train: 1.1245 loss_rec: 1.1245 acc_train: 0.4813 loss_val: 1.1380 acc_val: 0.4800 time: 0.0025s\n",
      "Epoch: 02210 loss_train: 1.1245 loss_rec: 1.1245 acc_train: 0.4813 loss_val: 1.1380 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02211 loss_train: 1.1245 loss_rec: 1.1245 acc_train: 0.4813 loss_val: 1.1380 acc_val: 0.4800 time: 0.0025s\n",
      "Test set results: loss= 1.1704 accuracy= 0.4093\n",
      "Epoch: 02212 loss_train: 1.1245 loss_rec: 1.1245 acc_train: 0.4813 loss_val: 1.1380 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02213 loss_train: 1.1245 loss_rec: 1.1245 acc_train: 0.4813 loss_val: 1.1379 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02214 loss_train: 1.1244 loss_rec: 1.1244 acc_train: 0.4813 loss_val: 1.1379 acc_val: 0.4800 time: 0.0025s\n",
      "Epoch: 02215 loss_train: 1.1244 loss_rec: 1.1244 acc_train: 0.4813 loss_val: 1.1379 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02216 loss_train: 1.1244 loss_rec: 1.1244 acc_train: 0.4813 loss_val: 1.1379 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02217 loss_train: 1.1244 loss_rec: 1.1244 acc_train: 0.4813 loss_val: 1.1379 acc_val: 0.4800 time: 0.0025s\n",
      "Epoch: 02218 loss_train: 1.1244 loss_rec: 1.1244 acc_train: 0.4813 loss_val: 1.1379 acc_val: 0.4800 time: 0.0015s\n",
      "Epoch: 02219 loss_train: 1.1244 loss_rec: 1.1244 acc_train: 0.4813 loss_val: 1.1379 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02220 loss_train: 1.1243 loss_rec: 1.1243 acc_train: 0.4813 loss_val: 1.1378 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02221 loss_train: 1.1243 loss_rec: 1.1243 acc_train: 0.4813 loss_val: 1.1378 acc_val: 0.4800 time: 0.0020s\n",
      "Test set results: loss= 1.1703 accuracy= 0.4093\n",
      "Epoch: 02222 loss_train: 1.1243 loss_rec: 1.1243 acc_train: 0.4813 loss_val: 1.1378 acc_val: 0.4800 time: 0.0030s\n",
      "Epoch: 02223 loss_train: 1.1243 loss_rec: 1.1243 acc_train: 0.4814 loss_val: 1.1378 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02224 loss_train: 1.1242 loss_rec: 1.1242 acc_train: 0.4814 loss_val: 1.1378 acc_val: 0.4800 time: 0.0025s\n",
      "Epoch: 02225 loss_train: 1.1242 loss_rec: 1.1242 acc_train: 0.4814 loss_val: 1.1378 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02226 loss_train: 1.1242 loss_rec: 1.1242 acc_train: 0.4814 loss_val: 1.1378 acc_val: 0.4800 time: 0.0015s\n",
      "Epoch: 02227 loss_train: 1.1242 loss_rec: 1.1242 acc_train: 0.4814 loss_val: 1.1377 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02228 loss_train: 1.1242 loss_rec: 1.1242 acc_train: 0.4815 loss_val: 1.1377 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02229 loss_train: 1.1241 loss_rec: 1.1241 acc_train: 0.4814 loss_val: 1.1377 acc_val: 0.4803 time: 0.0015s\n",
      "Epoch: 02230 loss_train: 1.1241 loss_rec: 1.1241 acc_train: 0.4814 loss_val: 1.1377 acc_val: 0.4803 time: 0.0020s\n",
      "Epoch: 02231 loss_train: 1.1241 loss_rec: 1.1241 acc_train: 0.4814 loss_val: 1.1377 acc_val: 0.4803 time: 0.0015s\n",
      "Test set results: loss= 1.1702 accuracy= 0.4094\n",
      "Epoch: 02232 loss_train: 1.1241 loss_rec: 1.1241 acc_train: 0.4814 loss_val: 1.1377 acc_val: 0.4803 time: 0.0020s\n",
      "Epoch: 02233 loss_train: 1.1240 loss_rec: 1.1240 acc_train: 0.4814 loss_val: 1.1377 acc_val: 0.4803 time: 0.0025s\n",
      "Epoch: 02234 loss_train: 1.1240 loss_rec: 1.1240 acc_train: 0.4814 loss_val: 1.1376 acc_val: 0.4803 time: 0.0020s\n",
      "Epoch: 02235 loss_train: 1.1240 loss_rec: 1.1240 acc_train: 0.4814 loss_val: 1.1376 acc_val: 0.4803 time: 0.0025s\n",
      "Epoch: 02236 loss_train: 1.1240 loss_rec: 1.1240 acc_train: 0.4814 loss_val: 1.1376 acc_val: 0.4803 time: 0.0015s\n",
      "Epoch: 02237 loss_train: 1.1240 loss_rec: 1.1240 acc_train: 0.4814 loss_val: 1.1376 acc_val: 0.4803 time: 0.0025s\n",
      "Epoch: 02238 loss_train: 1.1239 loss_rec: 1.1239 acc_train: 0.4815 loss_val: 1.1376 acc_val: 0.4803 time: 0.0015s\n",
      "Epoch: 02239 loss_train: 1.1239 loss_rec: 1.1239 acc_train: 0.4815 loss_val: 1.1376 acc_val: 0.4803 time: 0.0030s\n",
      "Epoch: 02240 loss_train: 1.1239 loss_rec: 1.1239 acc_train: 0.4815 loss_val: 1.1376 acc_val: 0.4803 time: 0.0025s\n",
      "Epoch: 02241 loss_train: 1.1239 loss_rec: 1.1239 acc_train: 0.4819 loss_val: 1.1375 acc_val: 0.4806 time: 0.0020s\n",
      "Test set results: loss= 1.1700 accuracy= 0.4101\n",
      "Epoch: 02242 loss_train: 1.1239 loss_rec: 1.1239 acc_train: 0.4819 loss_val: 1.1375 acc_val: 0.4806 time: 0.0025s\n",
      "Epoch: 02243 loss_train: 1.1238 loss_rec: 1.1238 acc_train: 0.4819 loss_val: 1.1375 acc_val: 0.4806 time: 0.0020s\n",
      "Epoch: 02244 loss_train: 1.1238 loss_rec: 1.1238 acc_train: 0.4820 loss_val: 1.1375 acc_val: 0.4806 time: 0.0020s\n",
      "Epoch: 02245 loss_train: 1.1238 loss_rec: 1.1238 acc_train: 0.4820 loss_val: 1.1375 acc_val: 0.4806 time: 0.0020s\n",
      "Epoch: 02246 loss_train: 1.1238 loss_rec: 1.1238 acc_train: 0.4820 loss_val: 1.1375 acc_val: 0.4806 time: 0.0025s\n",
      "Epoch: 02247 loss_train: 1.1237 loss_rec: 1.1237 acc_train: 0.4820 loss_val: 1.1374 acc_val: 0.4806 time: 0.0025s\n",
      "Epoch: 02248 loss_train: 1.1237 loss_rec: 1.1237 acc_train: 0.4820 loss_val: 1.1374 acc_val: 0.4806 time: 0.0015s\n",
      "Epoch: 02249 loss_train: 1.1237 loss_rec: 1.1237 acc_train: 0.4819 loss_val: 1.1374 acc_val: 0.4806 time: 0.0020s\n",
      "Epoch: 02250 loss_train: 1.1237 loss_rec: 1.1237 acc_train: 0.4819 loss_val: 1.1374 acc_val: 0.4806 time: 0.0020s\n",
      "Epoch: 02251 loss_train: 1.1237 loss_rec: 1.1237 acc_train: 0.4820 loss_val: 1.1374 acc_val: 0.4806 time: 0.0015s\n",
      "Test set results: loss= 1.1698 accuracy= 0.4100\n",
      "Epoch: 02252 loss_train: 1.1236 loss_rec: 1.1236 acc_train: 0.4819 loss_val: 1.1374 acc_val: 0.4806 time: 0.0015s\n",
      "Epoch: 02253 loss_train: 1.1236 loss_rec: 1.1236 acc_train: 0.4819 loss_val: 1.1374 acc_val: 0.4806 time: 0.0030s\n",
      "Epoch: 02254 loss_train: 1.1236 loss_rec: 1.1236 acc_train: 0.4819 loss_val: 1.1373 acc_val: 0.4806 time: 0.0015s\n",
      "Epoch: 02255 loss_train: 1.1236 loss_rec: 1.1236 acc_train: 0.4814 loss_val: 1.1373 acc_val: 0.4800 time: 0.0025s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02256 loss_train: 1.1236 loss_rec: 1.1236 acc_train: 0.4814 loss_val: 1.1373 acc_val: 0.4800 time: 0.0030s\n",
      "Epoch: 02257 loss_train: 1.1235 loss_rec: 1.1235 acc_train: 0.4814 loss_val: 1.1373 acc_val: 0.4800 time: 0.0015s\n",
      "Epoch: 02258 loss_train: 1.1235 loss_rec: 1.1235 acc_train: 0.4814 loss_val: 1.1373 acc_val: 0.4800 time: 0.0025s\n",
      "Epoch: 02259 loss_train: 1.1235 loss_rec: 1.1235 acc_train: 0.4814 loss_val: 1.1373 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02260 loss_train: 1.1235 loss_rec: 1.1235 acc_train: 0.4814 loss_val: 1.1373 acc_val: 0.4800 time: 0.0030s\n",
      "Epoch: 02261 loss_train: 1.1235 loss_rec: 1.1235 acc_train: 0.4814 loss_val: 1.1373 acc_val: 0.4800 time: 0.0015s\n",
      "Test set results: loss= 1.1697 accuracy= 0.4092\n",
      "Epoch: 02262 loss_train: 1.1234 loss_rec: 1.1234 acc_train: 0.4814 loss_val: 1.1372 acc_val: 0.4800 time: 0.0015s\n",
      "Epoch: 02263 loss_train: 1.1234 loss_rec: 1.1234 acc_train: 0.4814 loss_val: 1.1372 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02264 loss_train: 1.1234 loss_rec: 1.1234 acc_train: 0.4814 loss_val: 1.1372 acc_val: 0.4800 time: 0.0020s\n",
      "Epoch: 02265 loss_train: 1.1234 loss_rec: 1.1234 acc_train: 0.4823 loss_val: 1.1372 acc_val: 0.4806 time: 0.0020s\n",
      "Epoch: 02266 loss_train: 1.1233 loss_rec: 1.1233 acc_train: 0.4823 loss_val: 1.1372 acc_val: 0.4806 time: 0.0020s\n",
      "Epoch: 02267 loss_train: 1.1233 loss_rec: 1.1233 acc_train: 0.4823 loss_val: 1.1372 acc_val: 0.4806 time: 0.0015s\n",
      "Epoch: 02268 loss_train: 1.1233 loss_rec: 1.1233 acc_train: 0.4824 loss_val: 1.1371 acc_val: 0.4806 time: 0.0035s\n",
      "Epoch: 02269 loss_train: 1.1233 loss_rec: 1.1233 acc_train: 0.4824 loss_val: 1.1371 acc_val: 0.4806 time: 0.0025s\n",
      "Epoch: 02270 loss_train: 1.1233 loss_rec: 1.1233 acc_train: 0.4824 loss_val: 1.1371 acc_val: 0.4806 time: 0.0020s\n",
      "Epoch: 02271 loss_train: 1.1232 loss_rec: 1.1232 acc_train: 0.4824 loss_val: 1.1371 acc_val: 0.4806 time: 0.0025s\n",
      "Test set results: loss= 1.1696 accuracy= 0.4107\n",
      "Epoch: 02272 loss_train: 1.1232 loss_rec: 1.1232 acc_train: 0.4824 loss_val: 1.1371 acc_val: 0.4806 time: 0.0015s\n",
      "Epoch: 02273 loss_train: 1.1232 loss_rec: 1.1232 acc_train: 0.4824 loss_val: 1.1371 acc_val: 0.4806 time: 0.0020s\n",
      "Epoch: 02274 loss_train: 1.1232 loss_rec: 1.1232 acc_train: 0.4824 loss_val: 1.1371 acc_val: 0.4806 time: 0.0020s\n",
      "Epoch: 02275 loss_train: 1.1232 loss_rec: 1.1232 acc_train: 0.4824 loss_val: 1.1371 acc_val: 0.4806 time: 0.0030s\n",
      "Epoch: 02276 loss_train: 1.1231 loss_rec: 1.1231 acc_train: 0.4824 loss_val: 1.1370 acc_val: 0.4806 time: 0.0025s\n",
      "Epoch: 02277 loss_train: 1.1231 loss_rec: 1.1231 acc_train: 0.4824 loss_val: 1.1370 acc_val: 0.4806 time: 0.0020s\n",
      "Epoch: 02278 loss_train: 1.1231 loss_rec: 1.1231 acc_train: 0.4824 loss_val: 1.1370 acc_val: 0.4806 time: 0.0025s\n",
      "Epoch: 02279 loss_train: 1.1231 loss_rec: 1.1231 acc_train: 0.4824 loss_val: 1.1370 acc_val: 0.4806 time: 0.0020s\n",
      "Epoch: 02280 loss_train: 1.1231 loss_rec: 1.1231 acc_train: 0.4824 loss_val: 1.1370 acc_val: 0.4806 time: 0.0025s\n",
      "Epoch: 02281 loss_train: 1.1230 loss_rec: 1.1230 acc_train: 0.4824 loss_val: 1.1370 acc_val: 0.4806 time: 0.0020s\n",
      "Test set results: loss= 1.1695 accuracy= 0.4107\n",
      "Epoch: 02282 loss_train: 1.1230 loss_rec: 1.1230 acc_train: 0.4824 loss_val: 1.1370 acc_val: 0.4806 time: 0.0020s\n",
      "Epoch: 02283 loss_train: 1.1230 loss_rec: 1.1230 acc_train: 0.4824 loss_val: 1.1369 acc_val: 0.4806 time: 0.0025s\n",
      "Epoch: 02284 loss_train: 1.1230 loss_rec: 1.1230 acc_train: 0.4832 loss_val: 1.1369 acc_val: 0.4816 time: 0.0020s\n",
      "Epoch: 02285 loss_train: 1.1230 loss_rec: 1.1230 acc_train: 0.4831 loss_val: 1.1369 acc_val: 0.4816 time: 0.0025s\n",
      "Epoch: 02286 loss_train: 1.1229 loss_rec: 1.1229 acc_train: 0.4831 loss_val: 1.1369 acc_val: 0.4816 time: 0.0015s\n",
      "Epoch: 02287 loss_train: 1.1229 loss_rec: 1.1229 acc_train: 0.4831 loss_val: 1.1369 acc_val: 0.4816 time: 0.0020s\n",
      "Epoch: 02288 loss_train: 1.1229 loss_rec: 1.1229 acc_train: 0.4824 loss_val: 1.1369 acc_val: 0.4806 time: 0.0015s\n",
      "Epoch: 02289 loss_train: 1.1229 loss_rec: 1.1229 acc_train: 0.4824 loss_val: 1.1369 acc_val: 0.4806 time: 0.0030s\n",
      "Epoch: 02290 loss_train: 1.1229 loss_rec: 1.1229 acc_train: 0.4831 loss_val: 1.1368 acc_val: 0.4816 time: 0.0020s\n",
      "Epoch: 02291 loss_train: 1.1228 loss_rec: 1.1228 acc_train: 0.4831 loss_val: 1.1368 acc_val: 0.4816 time: 0.0020s\n",
      "Test set results: loss= 1.1694 accuracy= 0.4115\n",
      "Epoch: 02292 loss_train: 1.1228 loss_rec: 1.1228 acc_train: 0.4831 loss_val: 1.1368 acc_val: 0.4816 time: 0.0025s\n",
      "Epoch: 02293 loss_train: 1.1228 loss_rec: 1.1228 acc_train: 0.4831 loss_val: 1.1368 acc_val: 0.4816 time: 0.0020s\n",
      "Epoch: 02294 loss_train: 1.1228 loss_rec: 1.1228 acc_train: 0.4831 loss_val: 1.1368 acc_val: 0.4816 time: 0.0020s\n",
      "Epoch: 02295 loss_train: 1.1228 loss_rec: 1.1228 acc_train: 0.4844 loss_val: 1.1368 acc_val: 0.4828 time: 0.0015s\n",
      "Epoch: 02296 loss_train: 1.1227 loss_rec: 1.1227 acc_train: 0.4844 loss_val: 1.1368 acc_val: 0.4828 time: 0.0020s\n",
      "Epoch: 02297 loss_train: 1.1227 loss_rec: 1.1227 acc_train: 0.4848 loss_val: 1.1368 acc_val: 0.4831 time: 0.0020s\n",
      "Epoch: 02298 loss_train: 1.1227 loss_rec: 1.1227 acc_train: 0.4848 loss_val: 1.1367 acc_val: 0.4831 time: 0.0015s\n",
      "Epoch: 02299 loss_train: 1.1227 loss_rec: 1.1227 acc_train: 0.4848 loss_val: 1.1367 acc_val: 0.4831 time: 0.0020s\n",
      "Epoch: 02300 loss_train: 1.1227 loss_rec: 1.1227 acc_train: 0.4848 loss_val: 1.1367 acc_val: 0.4831 time: 0.0015s\n",
      "Epoch: 02301 loss_train: 1.1227 loss_rec: 1.1227 acc_train: 0.4848 loss_val: 1.1367 acc_val: 0.4831 time: 0.0020s\n",
      "Test set results: loss= 1.1694 accuracy= 0.4109\n",
      "Epoch: 02302 loss_train: 1.1226 loss_rec: 1.1226 acc_train: 0.4848 loss_val: 1.1367 acc_val: 0.4831 time: 0.0020s\n",
      "Epoch: 02303 loss_train: 1.1226 loss_rec: 1.1226 acc_train: 0.4848 loss_val: 1.1367 acc_val: 0.4834 time: 0.0015s\n",
      "Epoch: 02304 loss_train: 1.1226 loss_rec: 1.1226 acc_train: 0.4848 loss_val: 1.1367 acc_val: 0.4834 time: 0.0025s\n",
      "Epoch: 02305 loss_train: 1.1226 loss_rec: 1.1226 acc_train: 0.4848 loss_val: 1.1366 acc_val: 0.4834 time: 0.0015s\n",
      "Epoch: 02306 loss_train: 1.1226 loss_rec: 1.1226 acc_train: 0.4848 loss_val: 1.1366 acc_val: 0.4834 time: 0.0020s\n",
      "Epoch: 02307 loss_train: 1.1225 loss_rec: 1.1225 acc_train: 0.4848 loss_val: 1.1366 acc_val: 0.4834 time: 0.0015s\n",
      "Epoch: 02308 loss_train: 1.1225 loss_rec: 1.1225 acc_train: 0.4848 loss_val: 1.1366 acc_val: 0.4834 time: 0.0020s\n",
      "Epoch: 02309 loss_train: 1.1225 loss_rec: 1.1225 acc_train: 0.4848 loss_val: 1.1366 acc_val: 0.4834 time: 0.0020s\n",
      "Epoch: 02310 loss_train: 1.1225 loss_rec: 1.1225 acc_train: 0.4848 loss_val: 1.1366 acc_val: 0.4834 time: 0.0015s\n",
      "Epoch: 02311 loss_train: 1.1225 loss_rec: 1.1225 acc_train: 0.4848 loss_val: 1.1366 acc_val: 0.4834 time: 0.0020s\n",
      "Test set results: loss= 1.1693 accuracy= 0.4109\n",
      "Epoch: 02312 loss_train: 1.1224 loss_rec: 1.1224 acc_train: 0.4848 loss_val: 1.1366 acc_val: 0.4834 time: 0.0015s\n",
      "Epoch: 02313 loss_train: 1.1224 loss_rec: 1.1224 acc_train: 0.4848 loss_val: 1.1365 acc_val: 0.4834 time: 0.0020s\n",
      "Epoch: 02314 loss_train: 1.1224 loss_rec: 1.1224 acc_train: 0.4848 loss_val: 1.1365 acc_val: 0.4834 time: 0.0020s\n",
      "Epoch: 02315 loss_train: 1.1224 loss_rec: 1.1224 acc_train: 0.4848 loss_val: 1.1365 acc_val: 0.4834 time: 0.0015s\n",
      "Epoch: 02316 loss_train: 1.1224 loss_rec: 1.1224 acc_train: 0.4848 loss_val: 1.1365 acc_val: 0.4834 time: 0.0015s\n",
      "Epoch: 02317 loss_train: 1.1224 loss_rec: 1.1224 acc_train: 0.4848 loss_val: 1.1365 acc_val: 0.4834 time: 0.0020s\n",
      "Epoch: 02318 loss_train: 1.1223 loss_rec: 1.1223 acc_train: 0.4848 loss_val: 1.1365 acc_val: 0.4834 time: 0.0015s\n",
      "Epoch: 02319 loss_train: 1.1223 loss_rec: 1.1223 acc_train: 0.4848 loss_val: 1.1365 acc_val: 0.4834 time: 0.0020s\n",
      "Epoch: 02320 loss_train: 1.1223 loss_rec: 1.1223 acc_train: 0.4853 loss_val: 1.1365 acc_val: 0.4838 time: 0.0015s\n",
      "Epoch: 02321 loss_train: 1.1223 loss_rec: 1.1223 acc_train: 0.4853 loss_val: 1.1364 acc_val: 0.4838 time: 0.0020s\n",
      "Test set results: loss= 1.1692 accuracy= 0.4117\n",
      "Epoch: 02322 loss_train: 1.1223 loss_rec: 1.1223 acc_train: 0.4853 loss_val: 1.1364 acc_val: 0.4838 time: 0.0015s\n",
      "Epoch: 02323 loss_train: 1.1222 loss_rec: 1.1222 acc_train: 0.4852 loss_val: 1.1364 acc_val: 0.4838 time: 0.0015s\n",
      "Epoch: 02324 loss_train: 1.1222 loss_rec: 1.1222 acc_train: 0.4852 loss_val: 1.1364 acc_val: 0.4838 time: 0.0020s\n",
      "Epoch: 02325 loss_train: 1.1222 loss_rec: 1.1222 acc_train: 0.4852 loss_val: 1.1364 acc_val: 0.4838 time: 0.0020s\n",
      "Epoch: 02326 loss_train: 1.1222 loss_rec: 1.1222 acc_train: 0.4852 loss_val: 1.1364 acc_val: 0.4838 time: 0.0015s\n",
      "Epoch: 02327 loss_train: 1.1222 loss_rec: 1.1222 acc_train: 0.4852 loss_val: 1.1364 acc_val: 0.4838 time: 0.0020s\n",
      "Epoch: 02328 loss_train: 1.1222 loss_rec: 1.1222 acc_train: 0.4852 loss_val: 1.1364 acc_val: 0.4841 time: 0.0015s\n",
      "Epoch: 02329 loss_train: 1.1221 loss_rec: 1.1221 acc_train: 0.4852 loss_val: 1.1364 acc_val: 0.4841 time: 0.0015s\n",
      "Epoch: 02330 loss_train: 1.1221 loss_rec: 1.1221 acc_train: 0.4852 loss_val: 1.1363 acc_val: 0.4841 time: 0.0020s\n",
      "Epoch: 02331 loss_train: 1.1221 loss_rec: 1.1221 acc_train: 0.4852 loss_val: 1.1363 acc_val: 0.4841 time: 0.0020s\n",
      "Test set results: loss= 1.1693 accuracy= 0.4134\n",
      "Epoch: 02332 loss_train: 1.1221 loss_rec: 1.1221 acc_train: 0.4861 loss_val: 1.1363 acc_val: 0.4847 time: 0.0015s\n",
      "Epoch: 02333 loss_train: 1.1221 loss_rec: 1.1221 acc_train: 0.4861 loss_val: 1.1363 acc_val: 0.4847 time: 0.0020s\n",
      "Epoch: 02334 loss_train: 1.1220 loss_rec: 1.1220 acc_train: 0.4861 loss_val: 1.1363 acc_val: 0.4847 time: 0.0020s\n",
      "Epoch: 02335 loss_train: 1.1220 loss_rec: 1.1220 acc_train: 0.4861 loss_val: 1.1363 acc_val: 0.4847 time: 0.0015s\n",
      "Epoch: 02336 loss_train: 1.1220 loss_rec: 1.1220 acc_train: 0.4861 loss_val: 1.1363 acc_val: 0.4847 time: 0.0020s\n",
      "Epoch: 02337 loss_train: 1.1220 loss_rec: 1.1220 acc_train: 0.4861 loss_val: 1.1362 acc_val: 0.4847 time: 0.0015s\n",
      "Epoch: 02338 loss_train: 1.1220 loss_rec: 1.1220 acc_train: 0.4861 loss_val: 1.1362 acc_val: 0.4847 time: 0.0015s\n",
      "Epoch: 02339 loss_train: 1.1220 loss_rec: 1.1220 acc_train: 0.4861 loss_val: 1.1362 acc_val: 0.4847 time: 0.0025s\n",
      "Epoch: 02340 loss_train: 1.1219 loss_rec: 1.1219 acc_train: 0.4861 loss_val: 1.1362 acc_val: 0.4847 time: 0.0015s\n",
      "Epoch: 02341 loss_train: 1.1219 loss_rec: 1.1219 acc_train: 0.4861 loss_val: 1.1362 acc_val: 0.4847 time: 0.0015s\n",
      "Test set results: loss= 1.1691 accuracy= 0.4134\n",
      "Epoch: 02342 loss_train: 1.1219 loss_rec: 1.1219 acc_train: 0.4861 loss_val: 1.1362 acc_val: 0.4847 time: 0.0020s\n",
      "Epoch: 02343 loss_train: 1.1219 loss_rec: 1.1219 acc_train: 0.4861 loss_val: 1.1362 acc_val: 0.4847 time: 0.0015s\n",
      "Epoch: 02344 loss_train: 1.1219 loss_rec: 1.1219 acc_train: 0.4861 loss_val: 1.1362 acc_val: 0.4847 time: 0.0020s\n",
      "Epoch: 02345 loss_train: 1.1219 loss_rec: 1.1219 acc_train: 0.4861 loss_val: 1.1361 acc_val: 0.4847 time: 0.0015s\n",
      "Epoch: 02346 loss_train: 1.1218 loss_rec: 1.1218 acc_train: 0.4861 loss_val: 1.1361 acc_val: 0.4847 time: 0.0015s\n",
      "Epoch: 02347 loss_train: 1.1218 loss_rec: 1.1218 acc_train: 0.4862 loss_val: 1.1361 acc_val: 0.4847 time: 0.0015s\n",
      "Epoch: 02348 loss_train: 1.1218 loss_rec: 1.1218 acc_train: 0.4862 loss_val: 1.1361 acc_val: 0.4847 time: 0.0025s\n",
      "Epoch: 02349 loss_train: 1.1218 loss_rec: 1.1218 acc_train: 0.4862 loss_val: 1.1361 acc_val: 0.4847 time: 0.0015s\n",
      "Epoch: 02350 loss_train: 1.1218 loss_rec: 1.1218 acc_train: 0.4862 loss_val: 1.1361 acc_val: 0.4847 time: 0.0015s\n",
      "Epoch: 02351 loss_train: 1.1217 loss_rec: 1.1217 acc_train: 0.4862 loss_val: 1.1361 acc_val: 0.4847 time: 0.0015s\n",
      "Test set results: loss= 1.1690 accuracy= 0.4134\n",
      "Epoch: 02352 loss_train: 1.1217 loss_rec: 1.1217 acc_train: 0.4862 loss_val: 1.1361 acc_val: 0.4847 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02353 loss_train: 1.1217 loss_rec: 1.1217 acc_train: 0.4862 loss_val: 1.1361 acc_val: 0.4847 time: 0.0020s\n",
      "Epoch: 02354 loss_train: 1.1217 loss_rec: 1.1217 acc_train: 0.4862 loss_val: 1.1360 acc_val: 0.4847 time: 0.0015s\n",
      "Epoch: 02355 loss_train: 1.1217 loss_rec: 1.1217 acc_train: 0.4862 loss_val: 1.1360 acc_val: 0.4847 time: 0.0020s\n",
      "Epoch: 02356 loss_train: 1.1217 loss_rec: 1.1217 acc_train: 0.4863 loss_val: 1.1360 acc_val: 0.4847 time: 0.0015s\n",
      "Epoch: 02357 loss_train: 1.1216 loss_rec: 1.1216 acc_train: 0.4863 loss_val: 1.1360 acc_val: 0.4847 time: 0.0020s\n",
      "Epoch: 02358 loss_train: 1.1216 loss_rec: 1.1216 acc_train: 0.4863 loss_val: 1.1360 acc_val: 0.4847 time: 0.0020s\n",
      "Epoch: 02359 loss_train: 1.1216 loss_rec: 1.1216 acc_train: 0.4863 loss_val: 1.1360 acc_val: 0.4847 time: 0.0015s\n",
      "Epoch: 02360 loss_train: 1.1216 loss_rec: 1.1216 acc_train: 0.4863 loss_val: 1.1360 acc_val: 0.4847 time: 0.0015s\n",
      "Epoch: 02361 loss_train: 1.1216 loss_rec: 1.1216 acc_train: 0.4863 loss_val: 1.1360 acc_val: 0.4847 time: 0.0015s\n",
      "Test set results: loss= 1.1688 accuracy= 0.4135\n",
      "Epoch: 02362 loss_train: 1.1216 loss_rec: 1.1216 acc_train: 0.4864 loss_val: 1.1359 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02363 loss_train: 1.1215 loss_rec: 1.1215 acc_train: 0.4863 loss_val: 1.1359 acc_val: 0.4847 time: 0.0015s\n",
      "Epoch: 02364 loss_train: 1.1215 loss_rec: 1.1215 acc_train: 0.4863 loss_val: 1.1359 acc_val: 0.4847 time: 0.0020s\n",
      "Epoch: 02365 loss_train: 1.1215 loss_rec: 1.1215 acc_train: 0.4864 loss_val: 1.1359 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02366 loss_train: 1.1215 loss_rec: 1.1215 acc_train: 0.4864 loss_val: 1.1359 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02367 loss_train: 1.1215 loss_rec: 1.1215 acc_train: 0.4864 loss_val: 1.1359 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02368 loss_train: 1.1215 loss_rec: 1.1215 acc_train: 0.4864 loss_val: 1.1359 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02369 loss_train: 1.1214 loss_rec: 1.1214 acc_train: 0.4864 loss_val: 1.1359 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02370 loss_train: 1.1214 loss_rec: 1.1214 acc_train: 0.4864 loss_val: 1.1359 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02371 loss_train: 1.1214 loss_rec: 1.1214 acc_train: 0.4864 loss_val: 1.1358 acc_val: 0.4850 time: 0.0015s\n",
      "Test set results: loss= 1.1689 accuracy= 0.4134\n",
      "Epoch: 02372 loss_train: 1.1214 loss_rec: 1.1214 acc_train: 0.4864 loss_val: 1.1358 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02373 loss_train: 1.1214 loss_rec: 1.1214 acc_train: 0.4864 loss_val: 1.1358 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02374 loss_train: 1.1214 loss_rec: 1.1214 acc_train: 0.4864 loss_val: 1.1358 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02375 loss_train: 1.1213 loss_rec: 1.1213 acc_train: 0.4865 loss_val: 1.1358 acc_val: 0.4850 time: 0.0025s\n",
      "Epoch: 02376 loss_train: 1.1213 loss_rec: 1.1213 acc_train: 0.4864 loss_val: 1.1358 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02377 loss_train: 1.1213 loss_rec: 1.1213 acc_train: 0.4864 loss_val: 1.1358 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02378 loss_train: 1.1213 loss_rec: 1.1213 acc_train: 0.4864 loss_val: 1.1358 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02379 loss_train: 1.1213 loss_rec: 1.1213 acc_train: 0.4864 loss_val: 1.1357 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02380 loss_train: 1.1212 loss_rec: 1.1212 acc_train: 0.4864 loss_val: 1.1357 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02381 loss_train: 1.1212 loss_rec: 1.1212 acc_train: 0.4865 loss_val: 1.1357 acc_val: 0.4850 time: 0.0015s\n",
      "Test set results: loss= 1.1687 accuracy= 0.4134\n",
      "Epoch: 02382 loss_train: 1.1212 loss_rec: 1.1212 acc_train: 0.4866 loss_val: 1.1357 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02383 loss_train: 1.1212 loss_rec: 1.1212 acc_train: 0.4866 loss_val: 1.1357 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02384 loss_train: 1.1212 loss_rec: 1.1212 acc_train: 0.4866 loss_val: 1.1357 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02385 loss_train: 1.1212 loss_rec: 1.1212 acc_train: 0.4866 loss_val: 1.1357 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02386 loss_train: 1.1211 loss_rec: 1.1211 acc_train: 0.4866 loss_val: 1.1357 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02387 loss_train: 1.1211 loss_rec: 1.1211 acc_train: 0.4867 loss_val: 1.1357 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02388 loss_train: 1.1211 loss_rec: 1.1211 acc_train: 0.4867 loss_val: 1.1356 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02389 loss_train: 1.1211 loss_rec: 1.1211 acc_train: 0.4867 loss_val: 1.1356 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02390 loss_train: 1.1211 loss_rec: 1.1211 acc_train: 0.4867 loss_val: 1.1356 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02391 loss_train: 1.1211 loss_rec: 1.1211 acc_train: 0.4866 loss_val: 1.1356 acc_val: 0.4850 time: 0.0015s\n",
      "Test set results: loss= 1.1687 accuracy= 0.4133\n",
      "Epoch: 02392 loss_train: 1.1210 loss_rec: 1.1210 acc_train: 0.4866 loss_val: 1.1356 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02393 loss_train: 1.1210 loss_rec: 1.1210 acc_train: 0.4866 loss_val: 1.1356 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02394 loss_train: 1.1210 loss_rec: 1.1210 acc_train: 0.4866 loss_val: 1.1356 acc_val: 0.4850 time: 0.0025s\n",
      "Epoch: 02395 loss_train: 1.1210 loss_rec: 1.1210 acc_train: 0.4866 loss_val: 1.1356 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02396 loss_train: 1.1210 loss_rec: 1.1210 acc_train: 0.4866 loss_val: 1.1356 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02397 loss_train: 1.1210 loss_rec: 1.1210 acc_train: 0.4866 loss_val: 1.1355 acc_val: 0.4850 time: 0.0025s\n",
      "Epoch: 02398 loss_train: 1.1210 loss_rec: 1.1210 acc_train: 0.4866 loss_val: 1.1355 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02399 loss_train: 1.1209 loss_rec: 1.1209 acc_train: 0.4866 loss_val: 1.1355 acc_val: 0.4850 time: 0.0025s\n",
      "Epoch: 02400 loss_train: 1.1209 loss_rec: 1.1209 acc_train: 0.4878 loss_val: 1.1355 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02401 loss_train: 1.1209 loss_rec: 1.1209 acc_train: 0.4878 loss_val: 1.1355 acc_val: 0.4850 time: 0.0020s\n",
      "Test set results: loss= 1.1687 accuracy= 0.4136\n",
      "Epoch: 02402 loss_train: 1.1209 loss_rec: 1.1209 acc_train: 0.4878 loss_val: 1.1355 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02403 loss_train: 1.1209 loss_rec: 1.1209 acc_train: 0.4878 loss_val: 1.1355 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02404 loss_train: 1.1209 loss_rec: 1.1209 acc_train: 0.4878 loss_val: 1.1355 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02405 loss_train: 1.1208 loss_rec: 1.1208 acc_train: 0.4878 loss_val: 1.1355 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02406 loss_train: 1.1208 loss_rec: 1.1208 acc_train: 0.4878 loss_val: 1.1354 acc_val: 0.4850 time: 0.0025s\n",
      "Epoch: 02407 loss_train: 1.1208 loss_rec: 1.1208 acc_train: 0.4878 loss_val: 1.1354 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02408 loss_train: 1.1208 loss_rec: 1.1208 acc_train: 0.4878 loss_val: 1.1354 acc_val: 0.4850 time: 0.0030s\n",
      "Epoch: 02409 loss_train: 1.1208 loss_rec: 1.1208 acc_train: 0.4878 loss_val: 1.1354 acc_val: 0.4850 time: 0.0025s\n",
      "Epoch: 02410 loss_train: 1.1208 loss_rec: 1.1208 acc_train: 0.4878 loss_val: 1.1354 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02411 loss_train: 1.1207 loss_rec: 1.1207 acc_train: 0.4878 loss_val: 1.1354 acc_val: 0.4850 time: 0.0025s\n",
      "Test set results: loss= 1.1686 accuracy= 0.4136\n",
      "Epoch: 02412 loss_train: 1.1207 loss_rec: 1.1207 acc_train: 0.4878 loss_val: 1.1354 acc_val: 0.4850 time: 0.0025s\n",
      "Epoch: 02413 loss_train: 1.1207 loss_rec: 1.1207 acc_train: 0.4878 loss_val: 1.1354 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02414 loss_train: 1.1207 loss_rec: 1.1207 acc_train: 0.4878 loss_val: 1.1354 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02415 loss_train: 1.1207 loss_rec: 1.1207 acc_train: 0.4878 loss_val: 1.1353 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02416 loss_train: 1.1207 loss_rec: 1.1207 acc_train: 0.4878 loss_val: 1.1353 acc_val: 0.4850 time: 0.0035s\n",
      "Epoch: 02417 loss_train: 1.1206 loss_rec: 1.1206 acc_train: 0.4878 loss_val: 1.1353 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02418 loss_train: 1.1206 loss_rec: 1.1206 acc_train: 0.4878 loss_val: 1.1353 acc_val: 0.4850 time: 0.0025s\n",
      "Epoch: 02419 loss_train: 1.1206 loss_rec: 1.1206 acc_train: 0.4878 loss_val: 1.1353 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02420 loss_train: 1.1206 loss_rec: 1.1206 acc_train: 0.4878 loss_val: 1.1353 acc_val: 0.4850 time: 0.0030s\n",
      "Epoch: 02421 loss_train: 1.1206 loss_rec: 1.1206 acc_train: 0.4878 loss_val: 1.1353 acc_val: 0.4850 time: 0.0015s\n",
      "Test set results: loss= 1.1685 accuracy= 0.4136\n",
      "Epoch: 02422 loss_train: 1.1206 loss_rec: 1.1206 acc_train: 0.4878 loss_val: 1.1353 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02423 loss_train: 1.1206 loss_rec: 1.1206 acc_train: 0.4878 loss_val: 1.1353 acc_val: 0.4850 time: 0.0030s\n",
      "Epoch: 02424 loss_train: 1.1205 loss_rec: 1.1205 acc_train: 0.4878 loss_val: 1.1353 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02425 loss_train: 1.1205 loss_rec: 1.1205 acc_train: 0.4878 loss_val: 1.1352 acc_val: 0.4850 time: 0.0025s\n",
      "Epoch: 02426 loss_train: 1.1205 loss_rec: 1.1205 acc_train: 0.4879 loss_val: 1.1352 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02427 loss_train: 1.1205 loss_rec: 1.1205 acc_train: 0.4878 loss_val: 1.1352 acc_val: 0.4850 time: 0.0025s\n",
      "Epoch: 02428 loss_train: 1.1205 loss_rec: 1.1205 acc_train: 0.4878 loss_val: 1.1352 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02429 loss_train: 1.1205 loss_rec: 1.1205 acc_train: 0.4878 loss_val: 1.1352 acc_val: 0.4850 time: 0.0025s\n",
      "Epoch: 02430 loss_train: 1.1204 loss_rec: 1.1204 acc_train: 0.4878 loss_val: 1.1352 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02431 loss_train: 1.1204 loss_rec: 1.1204 acc_train: 0.4878 loss_val: 1.1352 acc_val: 0.4850 time: 0.0015s\n",
      "Test set results: loss= 1.1685 accuracy= 0.4136\n",
      "Epoch: 02432 loss_train: 1.1204 loss_rec: 1.1204 acc_train: 0.4878 loss_val: 1.1352 acc_val: 0.4850 time: 0.0030s\n",
      "Epoch: 02433 loss_train: 1.1204 loss_rec: 1.1204 acc_train: 0.4878 loss_val: 1.1352 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02434 loss_train: 1.1204 loss_rec: 1.1204 acc_train: 0.4879 loss_val: 1.1352 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02435 loss_train: 1.1204 loss_rec: 1.1204 acc_train: 0.4871 loss_val: 1.1351 acc_val: 0.4828 time: 0.0020s\n",
      "Epoch: 02436 loss_train: 1.1204 loss_rec: 1.1204 acc_train: 0.4871 loss_val: 1.1351 acc_val: 0.4828 time: 0.0030s\n",
      "Epoch: 02437 loss_train: 1.1203 loss_rec: 1.1203 acc_train: 0.4879 loss_val: 1.1351 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02438 loss_train: 1.1203 loss_rec: 1.1203 acc_train: 0.4879 loss_val: 1.1351 acc_val: 0.4850 time: 0.0015s\n",
      "Epoch: 02439 loss_train: 1.1203 loss_rec: 1.1203 acc_train: 0.4879 loss_val: 1.1351 acc_val: 0.4850 time: 0.0020s\n",
      "Epoch: 02440 loss_train: 1.1203 loss_rec: 1.1203 acc_train: 0.4879 loss_val: 1.1351 acc_val: 0.4850 time: 0.0025s\n",
      "Epoch: 02441 loss_train: 1.1203 loss_rec: 1.1203 acc_train: 0.4871 loss_val: 1.1351 acc_val: 0.4841 time: 0.0015s\n",
      "Test set results: loss= 1.1684 accuracy= 0.4148\n",
      "Epoch: 02442 loss_train: 1.1203 loss_rec: 1.1203 acc_train: 0.4871 loss_val: 1.1351 acc_val: 0.4841 time: 0.0015s\n",
      "Epoch: 02443 loss_train: 1.1202 loss_rec: 1.1202 acc_train: 0.4883 loss_val: 1.1351 acc_val: 0.4831 time: 0.0030s\n",
      "Epoch: 02444 loss_train: 1.1202 loss_rec: 1.1202 acc_train: 0.4883 loss_val: 1.1350 acc_val: 0.4831 time: 0.0020s\n",
      "Epoch: 02445 loss_train: 1.1202 loss_rec: 1.1202 acc_train: 0.4883 loss_val: 1.1350 acc_val: 0.4831 time: 0.0025s\n",
      "Epoch: 02446 loss_train: 1.1202 loss_rec: 1.1202 acc_train: 0.4884 loss_val: 1.1350 acc_val: 0.4831 time: 0.0015s\n",
      "Epoch: 02447 loss_train: 1.1202 loss_rec: 1.1202 acc_train: 0.4884 loss_val: 1.1350 acc_val: 0.4831 time: 0.0025s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02448 loss_train: 1.1201 loss_rec: 1.1201 acc_train: 0.4885 loss_val: 1.1350 acc_val: 0.4847 time: 0.0030s\n",
      "Epoch: 02449 loss_train: 1.1201 loss_rec: 1.1201 acc_train: 0.4886 loss_val: 1.1349 acc_val: 0.4847 time: 0.0015s\n",
      "Epoch: 02450 loss_train: 1.1201 loss_rec: 1.1201 acc_train: 0.4893 loss_val: 1.1349 acc_val: 0.4856 time: 0.0030s\n",
      "Epoch: 02451 loss_train: 1.1201 loss_rec: 1.1201 acc_train: 0.4893 loss_val: 1.1349 acc_val: 0.4856 time: 0.0015s\n",
      "Test set results: loss= 1.1683 accuracy= 0.4174\n",
      "Epoch: 02452 loss_train: 1.1201 loss_rec: 1.1201 acc_train: 0.4893 loss_val: 1.1349 acc_val: 0.4856 time: 0.0015s\n",
      "Epoch: 02453 loss_train: 1.1201 loss_rec: 1.1201 acc_train: 0.4892 loss_val: 1.1349 acc_val: 0.4856 time: 0.0025s\n",
      "Epoch: 02454 loss_train: 1.1200 loss_rec: 1.1200 acc_train: 0.4892 loss_val: 1.1349 acc_val: 0.4856 time: 0.0020s\n",
      "Epoch: 02455 loss_train: 1.1200 loss_rec: 1.1200 acc_train: 0.4893 loss_val: 1.1348 acc_val: 0.4856 time: 0.0020s\n",
      "Epoch: 02456 loss_train: 1.1200 loss_rec: 1.1200 acc_train: 0.4893 loss_val: 1.1348 acc_val: 0.4856 time: 0.0020s\n",
      "Epoch: 02457 loss_train: 1.1200 loss_rec: 1.1200 acc_train: 0.4893 loss_val: 1.1348 acc_val: 0.4856 time: 0.0020s\n",
      "Epoch: 02458 loss_train: 1.1200 loss_rec: 1.1200 acc_train: 0.4892 loss_val: 1.1348 acc_val: 0.4856 time: 0.0020s\n",
      "Epoch: 02459 loss_train: 1.1199 loss_rec: 1.1199 acc_train: 0.4893 loss_val: 1.1348 acc_val: 0.4856 time: 0.0015s\n",
      "Epoch: 02460 loss_train: 1.1199 loss_rec: 1.1199 acc_train: 0.4890 loss_val: 1.1348 acc_val: 0.4856 time: 0.0025s\n",
      "Epoch: 02461 loss_train: 1.1199 loss_rec: 1.1199 acc_train: 0.4890 loss_val: 1.1347 acc_val: 0.4856 time: 0.0020s\n",
      "Test set results: loss= 1.1682 accuracy= 0.4173\n",
      "Epoch: 02462 loss_train: 1.1199 loss_rec: 1.1199 acc_train: 0.4890 loss_val: 1.1347 acc_val: 0.4856 time: 0.0015s\n",
      "Epoch: 02463 loss_train: 1.1199 loss_rec: 1.1199 acc_train: 0.4890 loss_val: 1.1347 acc_val: 0.4856 time: 0.0025s\n",
      "Epoch: 02464 loss_train: 1.1199 loss_rec: 1.1199 acc_train: 0.4890 loss_val: 1.1347 acc_val: 0.4856 time: 0.0020s\n",
      "Epoch: 02465 loss_train: 1.1198 loss_rec: 1.1198 acc_train: 0.4890 loss_val: 1.1347 acc_val: 0.4856 time: 0.0030s\n",
      "Epoch: 02466 loss_train: 1.1198 loss_rec: 1.1198 acc_train: 0.4890 loss_val: 1.1346 acc_val: 0.4856 time: 0.0020s\n",
      "Epoch: 02467 loss_train: 1.1198 loss_rec: 1.1198 acc_train: 0.4890 loss_val: 1.1346 acc_val: 0.4856 time: 0.0015s\n",
      "Epoch: 02468 loss_train: 1.1198 loss_rec: 1.1198 acc_train: 0.4891 loss_val: 1.1346 acc_val: 0.4859 time: 0.0020s\n",
      "Epoch: 02469 loss_train: 1.1198 loss_rec: 1.1198 acc_train: 0.4891 loss_val: 1.1346 acc_val: 0.4859 time: 0.0025s\n",
      "Epoch: 02470 loss_train: 1.1198 loss_rec: 1.1198 acc_train: 0.4891 loss_val: 1.1346 acc_val: 0.4859 time: 0.0025s\n",
      "Epoch: 02471 loss_train: 1.1197 loss_rec: 1.1197 acc_train: 0.4891 loss_val: 1.1346 acc_val: 0.4859 time: 0.0015s\n",
      "Test set results: loss= 1.1681 accuracy= 0.4181\n",
      "Epoch: 02472 loss_train: 1.1197 loss_rec: 1.1197 acc_train: 0.4891 loss_val: 1.1345 acc_val: 0.4859 time: 0.0030s\n",
      "Epoch: 02473 loss_train: 1.1197 loss_rec: 1.1197 acc_train: 0.4891 loss_val: 1.1345 acc_val: 0.4859 time: 0.0015s\n",
      "Epoch: 02474 loss_train: 1.1197 loss_rec: 1.1197 acc_train: 0.4891 loss_val: 1.1345 acc_val: 0.4859 time: 0.0025s\n",
      "Epoch: 02475 loss_train: 1.1197 loss_rec: 1.1197 acc_train: 0.4891 loss_val: 1.1345 acc_val: 0.4859 time: 0.0015s\n",
      "Epoch: 02476 loss_train: 1.1196 loss_rec: 1.1196 acc_train: 0.4891 loss_val: 1.1345 acc_val: 0.4859 time: 0.0020s\n",
      "Epoch: 02477 loss_train: 1.1196 loss_rec: 1.1196 acc_train: 0.4891 loss_val: 1.1345 acc_val: 0.4859 time: 0.0015s\n",
      "Epoch: 02478 loss_train: 1.1196 loss_rec: 1.1196 acc_train: 0.4891 loss_val: 1.1344 acc_val: 0.4859 time: 0.0025s\n",
      "Epoch: 02479 loss_train: 1.1196 loss_rec: 1.1196 acc_train: 0.4891 loss_val: 1.1344 acc_val: 0.4859 time: 0.0015s\n",
      "Epoch: 02480 loss_train: 1.1196 loss_rec: 1.1196 acc_train: 0.4891 loss_val: 1.1344 acc_val: 0.4859 time: 0.0030s\n",
      "Epoch: 02481 loss_train: 1.1196 loss_rec: 1.1196 acc_train: 0.4891 loss_val: 1.1344 acc_val: 0.4859 time: 0.0030s\n",
      "Test set results: loss= 1.1679 accuracy= 0.4181\n",
      "Epoch: 02482 loss_train: 1.1195 loss_rec: 1.1195 acc_train: 0.4891 loss_val: 1.1344 acc_val: 0.4859 time: 0.0015s\n",
      "Epoch: 02483 loss_train: 1.1195 loss_rec: 1.1195 acc_train: 0.4891 loss_val: 1.1344 acc_val: 0.4859 time: 0.0025s\n",
      "Epoch: 02484 loss_train: 1.1195 loss_rec: 1.1195 acc_train: 0.4891 loss_val: 1.1343 acc_val: 0.4859 time: 0.0015s\n",
      "Epoch: 02485 loss_train: 1.1195 loss_rec: 1.1195 acc_train: 0.4891 loss_val: 1.1343 acc_val: 0.4859 time: 0.0025s\n",
      "Epoch: 02486 loss_train: 1.1195 loss_rec: 1.1195 acc_train: 0.4894 loss_val: 1.1343 acc_val: 0.4863 time: 0.0015s\n",
      "Epoch: 02487 loss_train: 1.1195 loss_rec: 1.1195 acc_train: 0.4897 loss_val: 1.1343 acc_val: 0.4866 time: 0.0025s\n",
      "Epoch: 02488 loss_train: 1.1194 loss_rec: 1.1194 acc_train: 0.4898 loss_val: 1.1343 acc_val: 0.4866 time: 0.0020s\n",
      "Epoch: 02489 loss_train: 1.1194 loss_rec: 1.1194 acc_train: 0.4898 loss_val: 1.1343 acc_val: 0.4866 time: 0.0020s\n",
      "Epoch: 02490 loss_train: 1.1194 loss_rec: 1.1194 acc_train: 0.4898 loss_val: 1.1342 acc_val: 0.4866 time: 0.0025s\n",
      "Epoch: 02491 loss_train: 1.1194 loss_rec: 1.1194 acc_train: 0.4898 loss_val: 1.1342 acc_val: 0.4866 time: 0.0020s\n",
      "Test set results: loss= 1.1679 accuracy= 0.4192\n",
      "Epoch: 02492 loss_train: 1.1194 loss_rec: 1.1194 acc_train: 0.4898 loss_val: 1.1342 acc_val: 0.4866 time: 0.0020s\n",
      "Epoch: 02493 loss_train: 1.1194 loss_rec: 1.1194 acc_train: 0.4898 loss_val: 1.1342 acc_val: 0.4866 time: 0.0025s\n",
      "Epoch: 02494 loss_train: 1.1193 loss_rec: 1.1193 acc_train: 0.4898 loss_val: 1.1342 acc_val: 0.4866 time: 0.0020s\n",
      "Epoch: 02495 loss_train: 1.1193 loss_rec: 1.1193 acc_train: 0.4909 loss_val: 1.1342 acc_val: 0.4875 time: 0.0030s\n",
      "Epoch: 02496 loss_train: 1.1193 loss_rec: 1.1193 acc_train: 0.4909 loss_val: 1.1342 acc_val: 0.4875 time: 0.0015s\n",
      "Epoch: 02497 loss_train: 1.1193 loss_rec: 1.1193 acc_train: 0.4909 loss_val: 1.1341 acc_val: 0.4875 time: 0.0030s\n",
      "Epoch: 02498 loss_train: 1.1193 loss_rec: 1.1193 acc_train: 0.4909 loss_val: 1.1341 acc_val: 0.4875 time: 0.0020s\n",
      "Epoch: 02499 loss_train: 1.1193 loss_rec: 1.1193 acc_train: 0.4909 loss_val: 1.1341 acc_val: 0.4875 time: 0.0020s\n",
      "Epoch: 02500 loss_train: 1.1192 loss_rec: 1.1192 acc_train: 0.4909 loss_val: 1.1341 acc_val: 0.4875 time: 0.0020s\n",
      "Epoch: 02501 loss_train: 1.1192 loss_rec: 1.1192 acc_train: 0.4898 loss_val: 1.1341 acc_val: 0.4866 time: 0.0030s\n",
      "Test set results: loss= 1.1679 accuracy= 0.4189\n",
      "Epoch: 02502 loss_train: 1.1192 loss_rec: 1.1192 acc_train: 0.4896 loss_val: 1.1341 acc_val: 0.4866 time: 0.0015s\n",
      "Epoch: 02503 loss_train: 1.1192 loss_rec: 1.1192 acc_train: 0.4896 loss_val: 1.1341 acc_val: 0.4866 time: 0.0025s\n",
      "Epoch: 02504 loss_train: 1.1192 loss_rec: 1.1192 acc_train: 0.4896 loss_val: 1.1340 acc_val: 0.4866 time: 0.0015s\n",
      "Epoch: 02505 loss_train: 1.1192 loss_rec: 1.1192 acc_train: 0.4896 loss_val: 1.1340 acc_val: 0.4866 time: 0.0020s\n",
      "Epoch: 02506 loss_train: 1.1191 loss_rec: 1.1191 acc_train: 0.4896 loss_val: 1.1340 acc_val: 0.4866 time: 0.0020s\n",
      "Epoch: 02507 loss_train: 1.1191 loss_rec: 1.1191 acc_train: 0.4895 loss_val: 1.1340 acc_val: 0.4863 time: 0.0025s\n",
      "Epoch: 02508 loss_train: 1.1191 loss_rec: 1.1191 acc_train: 0.4895 loss_val: 1.1340 acc_val: 0.4863 time: 0.0020s\n",
      "Epoch: 02509 loss_train: 1.1191 loss_rec: 1.1191 acc_train: 0.4895 loss_val: 1.1340 acc_val: 0.4863 time: 0.0025s\n",
      "Epoch: 02510 loss_train: 1.1191 loss_rec: 1.1191 acc_train: 0.4906 loss_val: 1.1340 acc_val: 0.4872 time: 0.0015s\n",
      "Epoch: 02511 loss_train: 1.1191 loss_rec: 1.1191 acc_train: 0.4906 loss_val: 1.1339 acc_val: 0.4872 time: 0.0025s\n",
      "Test set results: loss= 1.1678 accuracy= 0.4200\n",
      "Epoch: 02512 loss_train: 1.1191 loss_rec: 1.1191 acc_train: 0.4906 loss_val: 1.1339 acc_val: 0.4872 time: 0.0025s\n",
      "Epoch: 02513 loss_train: 1.1190 loss_rec: 1.1190 acc_train: 0.4942 loss_val: 1.1339 acc_val: 0.4900 time: 0.0020s\n",
      "Epoch: 02514 loss_train: 1.1190 loss_rec: 1.1190 acc_train: 0.4942 loss_val: 1.1339 acc_val: 0.4900 time: 0.0025s\n",
      "Epoch: 02515 loss_train: 1.1190 loss_rec: 1.1190 acc_train: 0.4943 loss_val: 1.1339 acc_val: 0.4900 time: 0.0015s\n",
      "Epoch: 02516 loss_train: 1.1190 loss_rec: 1.1190 acc_train: 0.4943 loss_val: 1.1339 acc_val: 0.4900 time: 0.0025s\n",
      "Epoch: 02517 loss_train: 1.1190 loss_rec: 1.1190 acc_train: 0.4943 loss_val: 1.1339 acc_val: 0.4900 time: 0.0020s\n",
      "Epoch: 02518 loss_train: 1.1190 loss_rec: 1.1190 acc_train: 0.4942 loss_val: 1.1338 acc_val: 0.4900 time: 0.0020s\n",
      "Epoch: 02519 loss_train: 1.1189 loss_rec: 1.1189 acc_train: 0.4942 loss_val: 1.1338 acc_val: 0.4900 time: 0.0015s\n",
      "Epoch: 02520 loss_train: 1.1189 loss_rec: 1.1189 acc_train: 0.4942 loss_val: 1.1338 acc_val: 0.4900 time: 0.0025s\n",
      "Epoch: 02521 loss_train: 1.1189 loss_rec: 1.1189 acc_train: 0.4942 loss_val: 1.1338 acc_val: 0.4897 time: 0.0015s\n",
      "Test set results: loss= 1.1678 accuracy= 0.4214\n",
      "Epoch: 02522 loss_train: 1.1189 loss_rec: 1.1189 acc_train: 0.4943 loss_val: 1.1338 acc_val: 0.4897 time: 0.0030s\n",
      "Epoch: 02523 loss_train: 1.1189 loss_rec: 1.1189 acc_train: 0.4943 loss_val: 1.1338 acc_val: 0.4897 time: 0.0025s\n",
      "Epoch: 02524 loss_train: 1.1189 loss_rec: 1.1189 acc_train: 0.4943 loss_val: 1.1338 acc_val: 0.4897 time: 0.0015s\n",
      "Epoch: 02525 loss_train: 1.1189 loss_rec: 1.1189 acc_train: 0.4949 loss_val: 1.1338 acc_val: 0.4897 time: 0.0015s\n",
      "Epoch: 02526 loss_train: 1.1188 loss_rec: 1.1188 acc_train: 0.4949 loss_val: 1.1337 acc_val: 0.4897 time: 0.0015s\n",
      "Epoch: 02527 loss_train: 1.1188 loss_rec: 1.1188 acc_train: 0.4949 loss_val: 1.1337 acc_val: 0.4897 time: 0.0015s\n",
      "Epoch: 02528 loss_train: 1.1188 loss_rec: 1.1188 acc_train: 0.4949 loss_val: 1.1337 acc_val: 0.4897 time: 0.0020s\n",
      "Epoch: 02529 loss_train: 1.1188 loss_rec: 1.1188 acc_train: 0.4949 loss_val: 1.1337 acc_val: 0.4897 time: 0.0015s\n",
      "Epoch: 02530 loss_train: 1.1188 loss_rec: 1.1188 acc_train: 0.4949 loss_val: 1.1337 acc_val: 0.4897 time: 0.0015s\n",
      "Epoch: 02531 loss_train: 1.1188 loss_rec: 1.1188 acc_train: 0.4949 loss_val: 1.1337 acc_val: 0.4897 time: 0.0025s\n",
      "Test set results: loss= 1.1678 accuracy= 0.4224\n",
      "Epoch: 02532 loss_train: 1.1188 loss_rec: 1.1188 acc_train: 0.4949 loss_val: 1.1337 acc_val: 0.4897 time: 0.0020s\n",
      "Epoch: 02533 loss_train: 1.1187 loss_rec: 1.1187 acc_train: 0.4949 loss_val: 1.1337 acc_val: 0.4897 time: 0.0015s\n",
      "Epoch: 02534 loss_train: 1.1187 loss_rec: 1.1187 acc_train: 0.4949 loss_val: 1.1336 acc_val: 0.4897 time: 0.0020s\n",
      "Epoch: 02535 loss_train: 1.1187 loss_rec: 1.1187 acc_train: 0.4949 loss_val: 1.1336 acc_val: 0.4897 time: 0.0015s\n",
      "Epoch: 02536 loss_train: 1.1187 loss_rec: 1.1187 acc_train: 0.4949 loss_val: 1.1336 acc_val: 0.4897 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02537 loss_train: 1.1187 loss_rec: 1.1187 acc_train: 0.4949 loss_val: 1.1336 acc_val: 0.4897 time: 0.0020s\n",
      "Epoch: 02538 loss_train: 1.1187 loss_rec: 1.1187 acc_train: 0.4949 loss_val: 1.1336 acc_val: 0.4897 time: 0.0020s\n",
      "Epoch: 02539 loss_train: 1.1186 loss_rec: 1.1186 acc_train: 0.4949 loss_val: 1.1336 acc_val: 0.4897 time: 0.0025s\n",
      "Epoch: 02540 loss_train: 1.1186 loss_rec: 1.1186 acc_train: 0.4949 loss_val: 1.1336 acc_val: 0.4897 time: 0.0020s\n",
      "Epoch: 02541 loss_train: 1.1186 loss_rec: 1.1186 acc_train: 0.4949 loss_val: 1.1336 acc_val: 0.4897 time: 0.0015s\n",
      "Test set results: loss= 1.1677 accuracy= 0.4224\n",
      "Epoch: 02542 loss_train: 1.1186 loss_rec: 1.1186 acc_train: 0.4949 loss_val: 1.1335 acc_val: 0.4897 time: 0.0015s\n",
      "Epoch: 02543 loss_train: 1.1186 loss_rec: 1.1186 acc_train: 0.4954 loss_val: 1.1335 acc_val: 0.4897 time: 0.0015s\n",
      "Epoch: 02544 loss_train: 1.1186 loss_rec: 1.1186 acc_train: 0.4954 loss_val: 1.1335 acc_val: 0.4897 time: 0.0015s\n",
      "Epoch: 02545 loss_train: 1.1186 loss_rec: 1.1186 acc_train: 0.4954 loss_val: 1.1335 acc_val: 0.4897 time: 0.0020s\n",
      "Epoch: 02546 loss_train: 1.1185 loss_rec: 1.1185 acc_train: 0.4960 loss_val: 1.1335 acc_val: 0.4903 time: 0.0015s\n",
      "Epoch: 02547 loss_train: 1.1185 loss_rec: 1.1185 acc_train: 0.4961 loss_val: 1.1335 acc_val: 0.4903 time: 0.0035s\n",
      "Epoch: 02548 loss_train: 1.1185 loss_rec: 1.1185 acc_train: 0.4960 loss_val: 1.1335 acc_val: 0.4903 time: 0.0020s\n",
      "Epoch: 02549 loss_train: 1.1185 loss_rec: 1.1185 acc_train: 0.4961 loss_val: 1.1335 acc_val: 0.4903 time: 0.0020s\n",
      "Epoch: 02550 loss_train: 1.1185 loss_rec: 1.1185 acc_train: 0.4961 loss_val: 1.1334 acc_val: 0.4903 time: 0.0015s\n",
      "Epoch: 02551 loss_train: 1.1185 loss_rec: 1.1185 acc_train: 0.4960 loss_val: 1.1334 acc_val: 0.4903 time: 0.0015s\n",
      "Test set results: loss= 1.1677 accuracy= 0.4233\n",
      "Epoch: 02552 loss_train: 1.1185 loss_rec: 1.1185 acc_train: 0.4960 loss_val: 1.1334 acc_val: 0.4903 time: 0.0015s\n",
      "Epoch: 02553 loss_train: 1.1184 loss_rec: 1.1184 acc_train: 0.4960 loss_val: 1.1334 acc_val: 0.4903 time: 0.0020s\n",
      "Epoch: 02554 loss_train: 1.1184 loss_rec: 1.1184 acc_train: 0.4960 loss_val: 1.1334 acc_val: 0.4903 time: 0.0015s\n",
      "Epoch: 02555 loss_train: 1.1184 loss_rec: 1.1184 acc_train: 0.4960 loss_val: 1.1334 acc_val: 0.4903 time: 0.0030s\n",
      "Epoch: 02556 loss_train: 1.1184 loss_rec: 1.1184 acc_train: 0.4960 loss_val: 1.1334 acc_val: 0.4903 time: 0.0020s\n",
      "Epoch: 02557 loss_train: 1.1184 loss_rec: 1.1184 acc_train: 0.4960 loss_val: 1.1334 acc_val: 0.4903 time: 0.0015s\n",
      "Epoch: 02558 loss_train: 1.1184 loss_rec: 1.1184 acc_train: 0.4960 loss_val: 1.1334 acc_val: 0.4903 time: 0.0020s\n",
      "Epoch: 02559 loss_train: 1.1184 loss_rec: 1.1184 acc_train: 0.4960 loss_val: 1.1333 acc_val: 0.4903 time: 0.0015s\n",
      "Epoch: 02560 loss_train: 1.1183 loss_rec: 1.1183 acc_train: 0.4960 loss_val: 1.1333 acc_val: 0.4903 time: 0.0015s\n",
      "Epoch: 02561 loss_train: 1.1183 loss_rec: 1.1183 acc_train: 0.4960 loss_val: 1.1333 acc_val: 0.4903 time: 0.0020s\n",
      "Test set results: loss= 1.1676 accuracy= 0.4233\n",
      "Epoch: 02562 loss_train: 1.1183 loss_rec: 1.1183 acc_train: 0.4960 loss_val: 1.1333 acc_val: 0.4903 time: 0.0015s\n",
      "Epoch: 02563 loss_train: 1.1183 loss_rec: 1.1183 acc_train: 0.4959 loss_val: 1.1333 acc_val: 0.4903 time: 0.0030s\n",
      "Epoch: 02564 loss_train: 1.1183 loss_rec: 1.1183 acc_train: 0.4959 loss_val: 1.1333 acc_val: 0.4903 time: 0.0020s\n",
      "Epoch: 02565 loss_train: 1.1183 loss_rec: 1.1183 acc_train: 0.4959 loss_val: 1.1333 acc_val: 0.4903 time: 0.0020s\n",
      "Epoch: 02566 loss_train: 1.1183 loss_rec: 1.1183 acc_train: 0.4959 loss_val: 1.1333 acc_val: 0.4903 time: 0.0015s\n",
      "Epoch: 02567 loss_train: 1.1182 loss_rec: 1.1182 acc_train: 0.4960 loss_val: 1.1333 acc_val: 0.4903 time: 0.0020s\n",
      "Epoch: 02568 loss_train: 1.1182 loss_rec: 1.1182 acc_train: 0.4959 loss_val: 1.1332 acc_val: 0.4903 time: 0.0015s\n",
      "Epoch: 02569 loss_train: 1.1182 loss_rec: 1.1182 acc_train: 0.4959 loss_val: 1.1332 acc_val: 0.4903 time: 0.0030s\n",
      "Epoch: 02570 loss_train: 1.1182 loss_rec: 1.1182 acc_train: 0.4959 loss_val: 1.1332 acc_val: 0.4903 time: 0.0015s\n",
      "Epoch: 02571 loss_train: 1.1182 loss_rec: 1.1182 acc_train: 0.4959 loss_val: 1.1332 acc_val: 0.4903 time: 0.0030s\n",
      "Test set results: loss= 1.1676 accuracy= 0.4233\n",
      "Epoch: 02572 loss_train: 1.1182 loss_rec: 1.1182 acc_train: 0.4960 loss_val: 1.1332 acc_val: 0.4903 time: 0.0025s\n",
      "Epoch: 02573 loss_train: 1.1182 loss_rec: 1.1182 acc_train: 0.4960 loss_val: 1.1332 acc_val: 0.4903 time: 0.0020s\n",
      "Epoch: 02574 loss_train: 1.1181 loss_rec: 1.1181 acc_train: 0.4960 loss_val: 1.1332 acc_val: 0.4903 time: 0.0020s\n",
      "Epoch: 02575 loss_train: 1.1181 loss_rec: 1.1181 acc_train: 0.4960 loss_val: 1.1332 acc_val: 0.4903 time: 0.0015s\n",
      "Epoch: 02576 loss_train: 1.1181 loss_rec: 1.1181 acc_train: 0.4964 loss_val: 1.1332 acc_val: 0.4909 time: 0.0015s\n",
      "Epoch: 02577 loss_train: 1.1181 loss_rec: 1.1181 acc_train: 0.4964 loss_val: 1.1331 acc_val: 0.4909 time: 0.0020s\n",
      "Epoch: 02578 loss_train: 1.1181 loss_rec: 1.1181 acc_train: 0.4964 loss_val: 1.1331 acc_val: 0.4909 time: 0.0020s\n",
      "Epoch: 02579 loss_train: 1.1181 loss_rec: 1.1181 acc_train: 0.4964 loss_val: 1.1331 acc_val: 0.4909 time: 0.0025s\n",
      "Epoch: 02580 loss_train: 1.1181 loss_rec: 1.1181 acc_train: 0.4964 loss_val: 1.1331 acc_val: 0.4909 time: 0.0020s\n",
      "Epoch: 02581 loss_train: 1.1180 loss_rec: 1.1180 acc_train: 0.4964 loss_val: 1.1331 acc_val: 0.4909 time: 0.0015s\n",
      "Test set results: loss= 1.1674 accuracy= 0.4241\n",
      "Epoch: 02582 loss_train: 1.1180 loss_rec: 1.1180 acc_train: 0.4964 loss_val: 1.1331 acc_val: 0.4909 time: 0.0020s\n",
      "Epoch: 02583 loss_train: 1.1180 loss_rec: 1.1180 acc_train: 0.4964 loss_val: 1.1331 acc_val: 0.4909 time: 0.0015s\n",
      "Epoch: 02584 loss_train: 1.1180 loss_rec: 1.1180 acc_train: 0.4964 loss_val: 1.1331 acc_val: 0.4909 time: 0.0015s\n",
      "Epoch: 02585 loss_train: 1.1180 loss_rec: 1.1180 acc_train: 0.4964 loss_val: 1.1331 acc_val: 0.4909 time: 0.0025s\n",
      "Epoch: 02586 loss_train: 1.1180 loss_rec: 1.1180 acc_train: 0.4964 loss_val: 1.1331 acc_val: 0.4909 time: 0.0015s\n",
      "Epoch: 02587 loss_train: 1.1180 loss_rec: 1.1180 acc_train: 0.4965 loss_val: 1.1330 acc_val: 0.4909 time: 0.0025s\n",
      "Epoch: 02588 loss_train: 1.1179 loss_rec: 1.1179 acc_train: 0.4965 loss_val: 1.1330 acc_val: 0.4909 time: 0.0015s\n",
      "Epoch: 02589 loss_train: 1.1179 loss_rec: 1.1179 acc_train: 0.4965 loss_val: 1.1330 acc_val: 0.4909 time: 0.0015s\n",
      "Epoch: 02590 loss_train: 1.1179 loss_rec: 1.1179 acc_train: 0.4965 loss_val: 1.1330 acc_val: 0.4909 time: 0.0015s\n",
      "Epoch: 02591 loss_train: 1.1179 loss_rec: 1.1179 acc_train: 0.4965 loss_val: 1.1330 acc_val: 0.4909 time: 0.0020s\n",
      "Test set results: loss= 1.1674 accuracy= 0.4241\n",
      "Epoch: 02592 loss_train: 1.1179 loss_rec: 1.1179 acc_train: 0.4965 loss_val: 1.1330 acc_val: 0.4909 time: 0.0015s\n",
      "Epoch: 02593 loss_train: 1.1179 loss_rec: 1.1179 acc_train: 0.4965 loss_val: 1.1330 acc_val: 0.4909 time: 0.0015s\n",
      "Epoch: 02594 loss_train: 1.1179 loss_rec: 1.1179 acc_train: 0.4964 loss_val: 1.1330 acc_val: 0.4909 time: 0.0020s\n",
      "Epoch: 02595 loss_train: 1.1178 loss_rec: 1.1178 acc_train: 0.4964 loss_val: 1.1330 acc_val: 0.4909 time: 0.0020s\n",
      "Epoch: 02596 loss_train: 1.1178 loss_rec: 1.1178 acc_train: 0.4964 loss_val: 1.1329 acc_val: 0.4909 time: 0.0015s\n",
      "Epoch: 02597 loss_train: 1.1178 loss_rec: 1.1178 acc_train: 0.4964 loss_val: 1.1329 acc_val: 0.4909 time: 0.0015s\n",
      "Epoch: 02598 loss_train: 1.1178 loss_rec: 1.1178 acc_train: 0.4964 loss_val: 1.1329 acc_val: 0.4909 time: 0.0015s\n",
      "Epoch: 02599 loss_train: 1.1178 loss_rec: 1.1178 acc_train: 0.4969 loss_val: 1.1329 acc_val: 0.4916 time: 0.0020s\n",
      "Epoch: 02600 loss_train: 1.1178 loss_rec: 1.1178 acc_train: 0.4969 loss_val: 1.1329 acc_val: 0.4916 time: 0.0025s\n",
      "Epoch: 02601 loss_train: 1.1178 loss_rec: 1.1178 acc_train: 0.4968 loss_val: 1.1329 acc_val: 0.4916 time: 0.0015s\n",
      "Test set results: loss= 1.1672 accuracy= 0.4240\n",
      "Epoch: 02602 loss_train: 1.1178 loss_rec: 1.1178 acc_train: 0.4964 loss_val: 1.1329 acc_val: 0.4909 time: 0.0015s\n",
      "Epoch: 02603 loss_train: 1.1177 loss_rec: 1.1177 acc_train: 0.4968 loss_val: 1.1329 acc_val: 0.4916 time: 0.0030s\n",
      "Epoch: 02604 loss_train: 1.1177 loss_rec: 1.1177 acc_train: 0.4968 loss_val: 1.1329 acc_val: 0.4916 time: 0.0020s\n",
      "Epoch: 02605 loss_train: 1.1177 loss_rec: 1.1177 acc_train: 0.4968 loss_val: 1.1329 acc_val: 0.4916 time: 0.0020s\n",
      "Epoch: 02606 loss_train: 1.1177 loss_rec: 1.1177 acc_train: 0.4968 loss_val: 1.1328 acc_val: 0.4916 time: 0.0015s\n",
      "Epoch: 02607 loss_train: 1.1177 loss_rec: 1.1177 acc_train: 0.4968 loss_val: 1.1328 acc_val: 0.4916 time: 0.0020s\n",
      "Epoch: 02608 loss_train: 1.1177 loss_rec: 1.1177 acc_train: 0.4968 loss_val: 1.1328 acc_val: 0.4916 time: 0.0015s\n",
      "Epoch: 02609 loss_train: 1.1177 loss_rec: 1.1177 acc_train: 0.4968 loss_val: 1.1328 acc_val: 0.4916 time: 0.0035s\n",
      "Epoch: 02610 loss_train: 1.1177 loss_rec: 1.1177 acc_train: 0.4968 loss_val: 1.1328 acc_val: 0.4916 time: 0.0020s\n",
      "Epoch: 02611 loss_train: 1.1176 loss_rec: 1.1176 acc_train: 0.4968 loss_val: 1.1328 acc_val: 0.4916 time: 0.0025s\n",
      "Test set results: loss= 1.1673 accuracy= 0.4246\n",
      "Epoch: 02612 loss_train: 1.1176 loss_rec: 1.1176 acc_train: 0.4968 loss_val: 1.1328 acc_val: 0.4916 time: 0.0025s\n",
      "Epoch: 02613 loss_train: 1.1176 loss_rec: 1.1176 acc_train: 0.4969 loss_val: 1.1328 acc_val: 0.4916 time: 0.0020s\n",
      "Epoch: 02614 loss_train: 1.1176 loss_rec: 1.1176 acc_train: 0.4969 loss_val: 1.1328 acc_val: 0.4919 time: 0.0035s\n",
      "Epoch: 02615 loss_train: 1.1176 loss_rec: 1.1176 acc_train: 0.4968 loss_val: 1.1328 acc_val: 0.4919 time: 0.0025s\n",
      "Epoch: 02616 loss_train: 1.1176 loss_rec: 1.1176 acc_train: 0.4968 loss_val: 1.1328 acc_val: 0.4919 time: 0.0015s\n",
      "Epoch: 02617 loss_train: 1.1176 loss_rec: 1.1176 acc_train: 0.4968 loss_val: 1.1327 acc_val: 0.4919 time: 0.0015s\n",
      "Epoch: 02618 loss_train: 1.1175 loss_rec: 1.1175 acc_train: 0.4968 loss_val: 1.1327 acc_val: 0.4919 time: 0.0030s\n",
      "Epoch: 02619 loss_train: 1.1175 loss_rec: 1.1175 acc_train: 0.4969 loss_val: 1.1327 acc_val: 0.4919 time: 0.0025s\n",
      "Epoch: 02620 loss_train: 1.1175 loss_rec: 1.1175 acc_train: 0.4968 loss_val: 1.1327 acc_val: 0.4919 time: 0.0015s\n",
      "Epoch: 02621 loss_train: 1.1175 loss_rec: 1.1175 acc_train: 0.4968 loss_val: 1.1327 acc_val: 0.4919 time: 0.0030s\n",
      "Test set results: loss= 1.1672 accuracy= 0.4246\n",
      "Epoch: 02622 loss_train: 1.1175 loss_rec: 1.1175 acc_train: 0.4968 loss_val: 1.1327 acc_val: 0.4919 time: 0.0025s\n",
      "Epoch: 02623 loss_train: 1.1175 loss_rec: 1.1175 acc_train: 0.4971 loss_val: 1.1327 acc_val: 0.4922 time: 0.0015s\n",
      "Epoch: 02624 loss_train: 1.1175 loss_rec: 1.1175 acc_train: 0.4971 loss_val: 1.1327 acc_val: 0.4922 time: 0.0020s\n",
      "Epoch: 02625 loss_train: 1.1174 loss_rec: 1.1174 acc_train: 0.4971 loss_val: 1.1327 acc_val: 0.4922 time: 0.0020s\n",
      "Epoch: 02626 loss_train: 1.1174 loss_rec: 1.1174 acc_train: 0.4971 loss_val: 1.1326 acc_val: 0.4922 time: 0.0030s\n",
      "Epoch: 02627 loss_train: 1.1174 loss_rec: 1.1174 acc_train: 0.4971 loss_val: 1.1326 acc_val: 0.4922 time: 0.0015s\n",
      "Epoch: 02628 loss_train: 1.1174 loss_rec: 1.1174 acc_train: 0.4971 loss_val: 1.1326 acc_val: 0.4922 time: 0.0025s\n",
      "Epoch: 02629 loss_train: 1.1174 loss_rec: 1.1174 acc_train: 0.4971 loss_val: 1.1326 acc_val: 0.4922 time: 0.0020s\n",
      "Epoch: 02630 loss_train: 1.1174 loss_rec: 1.1174 acc_train: 0.4971 loss_val: 1.1326 acc_val: 0.4922 time: 0.0035s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02631 loss_train: 1.1174 loss_rec: 1.1174 acc_train: 0.4971 loss_val: 1.1326 acc_val: 0.4922 time: 0.0025s\n",
      "Test set results: loss= 1.1672 accuracy= 0.4254\n",
      "Epoch: 02632 loss_train: 1.1174 loss_rec: 1.1174 acc_train: 0.4971 loss_val: 1.1326 acc_val: 0.4922 time: 0.0020s\n",
      "Epoch: 02633 loss_train: 1.1173 loss_rec: 1.1173 acc_train: 0.4971 loss_val: 1.1326 acc_val: 0.4922 time: 0.0020s\n",
      "Epoch: 02634 loss_train: 1.1173 loss_rec: 1.1173 acc_train: 0.4977 loss_val: 1.1326 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02635 loss_train: 1.1173 loss_rec: 1.1173 acc_train: 0.4977 loss_val: 1.1326 acc_val: 0.4925 time: 0.0025s\n",
      "Epoch: 02636 loss_train: 1.1173 loss_rec: 1.1173 acc_train: 0.4977 loss_val: 1.1325 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02637 loss_train: 1.1173 loss_rec: 1.1173 acc_train: 0.4977 loss_val: 1.1325 acc_val: 0.4928 time: 0.0025s\n",
      "Epoch: 02638 loss_train: 1.1173 loss_rec: 1.1173 acc_train: 0.4977 loss_val: 1.1325 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02639 loss_train: 1.1173 loss_rec: 1.1173 acc_train: 0.4977 loss_val: 1.1325 acc_val: 0.4928 time: 0.0030s\n",
      "Epoch: 02640 loss_train: 1.1172 loss_rec: 1.1172 acc_train: 0.4977 loss_val: 1.1325 acc_val: 0.4928 time: 0.0030s\n",
      "Epoch: 02641 loss_train: 1.1172 loss_rec: 1.1172 acc_train: 0.4977 loss_val: 1.1325 acc_val: 0.4928 time: 0.0015s\n",
      "Test set results: loss= 1.1671 accuracy= 0.4255\n",
      "Epoch: 02642 loss_train: 1.1172 loss_rec: 1.1172 acc_train: 0.4977 loss_val: 1.1325 acc_val: 0.4928 time: 0.0025s\n",
      "Epoch: 02643 loss_train: 1.1172 loss_rec: 1.1172 acc_train: 0.4977 loss_val: 1.1325 acc_val: 0.4928 time: 0.0020s\n",
      "Epoch: 02644 loss_train: 1.1172 loss_rec: 1.1172 acc_train: 0.4977 loss_val: 1.1325 acc_val: 0.4928 time: 0.0020s\n",
      "Epoch: 02645 loss_train: 1.1172 loss_rec: 1.1172 acc_train: 0.4977 loss_val: 1.1325 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02646 loss_train: 1.1172 loss_rec: 1.1172 acc_train: 0.4977 loss_val: 1.1325 acc_val: 0.4928 time: 0.0030s\n",
      "Epoch: 02647 loss_train: 1.1172 loss_rec: 1.1172 acc_train: 0.4977 loss_val: 1.1324 acc_val: 0.4928 time: 0.0025s\n",
      "Epoch: 02648 loss_train: 1.1171 loss_rec: 1.1171 acc_train: 0.4977 loss_val: 1.1324 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02649 loss_train: 1.1171 loss_rec: 1.1171 acc_train: 0.4977 loss_val: 1.1324 acc_val: 0.4928 time: 0.0025s\n",
      "Epoch: 02650 loss_train: 1.1171 loss_rec: 1.1171 acc_train: 0.4977 loss_val: 1.1324 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02651 loss_train: 1.1171 loss_rec: 1.1171 acc_train: 0.4977 loss_val: 1.1324 acc_val: 0.4928 time: 0.0025s\n",
      "Test set results: loss= 1.1670 accuracy= 0.4257\n",
      "Epoch: 02652 loss_train: 1.1171 loss_rec: 1.1171 acc_train: 0.4977 loss_val: 1.1324 acc_val: 0.4928 time: 0.0020s\n",
      "Epoch: 02653 loss_train: 1.1171 loss_rec: 1.1171 acc_train: 0.4977 loss_val: 1.1324 acc_val: 0.4928 time: 0.0030s\n",
      "Epoch: 02654 loss_train: 1.1171 loss_rec: 1.1171 acc_train: 0.4976 loss_val: 1.1324 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02655 loss_train: 1.1171 loss_rec: 1.1171 acc_train: 0.4976 loss_val: 1.1324 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02656 loss_train: 1.1170 loss_rec: 1.1170 acc_train: 0.4977 loss_val: 1.1324 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02657 loss_train: 1.1170 loss_rec: 1.1170 acc_train: 0.4976 loss_val: 1.1324 acc_val: 0.4925 time: 0.0030s\n",
      "Epoch: 02658 loss_train: 1.1170 loss_rec: 1.1170 acc_train: 0.4976 loss_val: 1.1323 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02659 loss_train: 1.1170 loss_rec: 1.1170 acc_train: 0.4976 loss_val: 1.1323 acc_val: 0.4925 time: 0.0025s\n",
      "Epoch: 02660 loss_train: 1.1170 loss_rec: 1.1170 acc_train: 0.4976 loss_val: 1.1323 acc_val: 0.4925 time: 0.0020s\n",
      "Epoch: 02661 loss_train: 1.1170 loss_rec: 1.1170 acc_train: 0.4976 loss_val: 1.1323 acc_val: 0.4925 time: 0.0030s\n",
      "Test set results: loss= 1.1670 accuracy= 0.4256\n",
      "Epoch: 02662 loss_train: 1.1170 loss_rec: 1.1170 acc_train: 0.4976 loss_val: 1.1323 acc_val: 0.4925 time: 0.0025s\n",
      "Epoch: 02663 loss_train: 1.1169 loss_rec: 1.1169 acc_train: 0.4976 loss_val: 1.1323 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02664 loss_train: 1.1169 loss_rec: 1.1169 acc_train: 0.4976 loss_val: 1.1323 acc_val: 0.4925 time: 0.0030s\n",
      "Epoch: 02665 loss_train: 1.1169 loss_rec: 1.1169 acc_train: 0.4976 loss_val: 1.1323 acc_val: 0.4925 time: 0.0025s\n",
      "Epoch: 02666 loss_train: 1.1169 loss_rec: 1.1169 acc_train: 0.4976 loss_val: 1.1323 acc_val: 0.4925 time: 0.0020s\n",
      "Epoch: 02667 loss_train: 1.1169 loss_rec: 1.1169 acc_train: 0.4976 loss_val: 1.1323 acc_val: 0.4925 time: 0.0025s\n",
      "Epoch: 02668 loss_train: 1.1169 loss_rec: 1.1169 acc_train: 0.4976 loss_val: 1.1323 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02669 loss_train: 1.1169 loss_rec: 1.1169 acc_train: 0.4976 loss_val: 1.1322 acc_val: 0.4925 time: 0.0020s\n",
      "Epoch: 02670 loss_train: 1.1169 loss_rec: 1.1169 acc_train: 0.4976 loss_val: 1.1322 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02671 loss_train: 1.1169 loss_rec: 1.1169 acc_train: 0.4976 loss_val: 1.1322 acc_val: 0.4925 time: 0.0015s\n",
      "Test set results: loss= 1.1670 accuracy= 0.4256\n",
      "Epoch: 02672 loss_train: 1.1168 loss_rec: 1.1168 acc_train: 0.4976 loss_val: 1.1322 acc_val: 0.4925 time: 0.0020s\n",
      "Epoch: 02673 loss_train: 1.1168 loss_rec: 1.1168 acc_train: 0.4976 loss_val: 1.1322 acc_val: 0.4925 time: 0.0025s\n",
      "Epoch: 02674 loss_train: 1.1168 loss_rec: 1.1168 acc_train: 0.4976 loss_val: 1.1322 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02675 loss_train: 1.1168 loss_rec: 1.1168 acc_train: 0.4976 loss_val: 1.1322 acc_val: 0.4925 time: 0.0030s\n",
      "Epoch: 02676 loss_train: 1.1168 loss_rec: 1.1168 acc_train: 0.4976 loss_val: 1.1322 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02677 loss_train: 1.1168 loss_rec: 1.1168 acc_train: 0.4976 loss_val: 1.1322 acc_val: 0.4925 time: 0.0025s\n",
      "Epoch: 02678 loss_train: 1.1168 loss_rec: 1.1168 acc_train: 0.4984 loss_val: 1.1322 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02679 loss_train: 1.1167 loss_rec: 1.1167 acc_train: 0.4984 loss_val: 1.1322 acc_val: 0.4928 time: 0.0030s\n",
      "Epoch: 02680 loss_train: 1.1167 loss_rec: 1.1167 acc_train: 0.4984 loss_val: 1.1321 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02681 loss_train: 1.1167 loss_rec: 1.1167 acc_train: 0.4984 loss_val: 1.1321 acc_val: 0.4928 time: 0.0020s\n",
      "Test set results: loss= 1.1669 accuracy= 0.4264\n",
      "Epoch: 02682 loss_train: 1.1167 loss_rec: 1.1167 acc_train: 0.4985 loss_val: 1.1321 acc_val: 0.4928 time: 0.0025s\n",
      "Epoch: 02683 loss_train: 1.1167 loss_rec: 1.1167 acc_train: 0.4985 loss_val: 1.1321 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02684 loss_train: 1.1167 loss_rec: 1.1167 acc_train: 0.4985 loss_val: 1.1321 acc_val: 0.4928 time: 0.0025s\n",
      "Epoch: 02685 loss_train: 1.1167 loss_rec: 1.1167 acc_train: 0.4985 loss_val: 1.1321 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02686 loss_train: 1.1167 loss_rec: 1.1167 acc_train: 0.4988 loss_val: 1.1321 acc_val: 0.4928 time: 0.0025s\n",
      "Epoch: 02687 loss_train: 1.1166 loss_rec: 1.1166 acc_train: 0.4988 loss_val: 1.1321 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02688 loss_train: 1.1166 loss_rec: 1.1166 acc_train: 0.4988 loss_val: 1.1321 acc_val: 0.4928 time: 0.0025s\n",
      "Epoch: 02689 loss_train: 1.1166 loss_rec: 1.1166 acc_train: 0.4988 loss_val: 1.1321 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02690 loss_train: 1.1166 loss_rec: 1.1166 acc_train: 0.4988 loss_val: 1.1321 acc_val: 0.4928 time: 0.0025s\n",
      "Epoch: 02691 loss_train: 1.1166 loss_rec: 1.1166 acc_train: 0.4988 loss_val: 1.1321 acc_val: 0.4928 time: 0.0015s\n",
      "Test set results: loss= 1.1668 accuracy= 0.4266\n",
      "Epoch: 02692 loss_train: 1.1166 loss_rec: 1.1166 acc_train: 0.4988 loss_val: 1.1320 acc_val: 0.4928 time: 0.0025s\n",
      "Epoch: 02693 loss_train: 1.1166 loss_rec: 1.1166 acc_train: 0.4988 loss_val: 1.1320 acc_val: 0.4928 time: 0.0020s\n",
      "Epoch: 02694 loss_train: 1.1166 loss_rec: 1.1166 acc_train: 0.4988 loss_val: 1.1320 acc_val: 0.4928 time: 0.0025s\n",
      "Epoch: 02695 loss_train: 1.1165 loss_rec: 1.1165 acc_train: 0.4988 loss_val: 1.1320 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02696 loss_train: 1.1165 loss_rec: 1.1165 acc_train: 0.4988 loss_val: 1.1320 acc_val: 0.4928 time: 0.0020s\n",
      "Epoch: 02697 loss_train: 1.1165 loss_rec: 1.1165 acc_train: 0.4988 loss_val: 1.1320 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02698 loss_train: 1.1165 loss_rec: 1.1165 acc_train: 0.4988 loss_val: 1.1320 acc_val: 0.4928 time: 0.0020s\n",
      "Epoch: 02699 loss_train: 1.1165 loss_rec: 1.1165 acc_train: 0.4988 loss_val: 1.1320 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02700 loss_train: 1.1165 loss_rec: 1.1165 acc_train: 0.4988 loss_val: 1.1320 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02701 loss_train: 1.1165 loss_rec: 1.1165 acc_train: 0.4988 loss_val: 1.1320 acc_val: 0.4928 time: 0.0015s\n",
      "Test set results: loss= 1.1667 accuracy= 0.4265\n",
      "Epoch: 02702 loss_train: 1.1165 loss_rec: 1.1165 acc_train: 0.4988 loss_val: 1.1320 acc_val: 0.4925 time: 0.0020s\n",
      "Epoch: 02703 loss_train: 1.1164 loss_rec: 1.1164 acc_train: 0.4988 loss_val: 1.1319 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02704 loss_train: 1.1164 loss_rec: 1.1164 acc_train: 0.4987 loss_val: 1.1319 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02705 loss_train: 1.1164 loss_rec: 1.1164 acc_train: 0.4987 loss_val: 1.1319 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02706 loss_train: 1.1164 loss_rec: 1.1164 acc_train: 0.4987 loss_val: 1.1319 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02707 loss_train: 1.1164 loss_rec: 1.1164 acc_train: 0.4987 loss_val: 1.1319 acc_val: 0.4925 time: 0.0020s\n",
      "Epoch: 02708 loss_train: 1.1164 loss_rec: 1.1164 acc_train: 0.4987 loss_val: 1.1319 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02709 loss_train: 1.1164 loss_rec: 1.1164 acc_train: 0.4987 loss_val: 1.1319 acc_val: 0.4925 time: 0.0020s\n",
      "Epoch: 02710 loss_train: 1.1164 loss_rec: 1.1164 acc_train: 0.4987 loss_val: 1.1319 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02711 loss_train: 1.1163 loss_rec: 1.1163 acc_train: 0.4987 loss_val: 1.1319 acc_val: 0.4925 time: 0.0015s\n",
      "Test set results: loss= 1.1667 accuracy= 0.4265\n",
      "Epoch: 02712 loss_train: 1.1163 loss_rec: 1.1163 acc_train: 0.4987 loss_val: 1.1319 acc_val: 0.4925 time: 0.0020s\n",
      "Epoch: 02713 loss_train: 1.1163 loss_rec: 1.1163 acc_train: 0.4987 loss_val: 1.1319 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02714 loss_train: 1.1163 loss_rec: 1.1163 acc_train: 0.4987 loss_val: 1.1318 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02715 loss_train: 1.1163 loss_rec: 1.1163 acc_train: 0.4987 loss_val: 1.1318 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02716 loss_train: 1.1163 loss_rec: 1.1163 acc_train: 0.4987 loss_val: 1.1318 acc_val: 0.4925 time: 0.0020s\n",
      "Epoch: 02717 loss_train: 1.1163 loss_rec: 1.1163 acc_train: 0.4988 loss_val: 1.1318 acc_val: 0.4925 time: 0.0020s\n",
      "Epoch: 02718 loss_train: 1.1163 loss_rec: 1.1163 acc_train: 0.4987 loss_val: 1.1318 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02719 loss_train: 1.1162 loss_rec: 1.1162 acc_train: 0.4986 loss_val: 1.1318 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02720 loss_train: 1.1162 loss_rec: 1.1162 acc_train: 0.4986 loss_val: 1.1318 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02721 loss_train: 1.1162 loss_rec: 1.1162 acc_train: 0.4986 loss_val: 1.1318 acc_val: 0.4925 time: 0.0015s\n",
      "Test set results: loss= 1.1666 accuracy= 0.4264\n",
      "Epoch: 02722 loss_train: 1.1162 loss_rec: 1.1162 acc_train: 0.4987 loss_val: 1.1318 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02723 loss_train: 1.1162 loss_rec: 1.1162 acc_train: 0.4987 loss_val: 1.1318 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02724 loss_train: 1.1162 loss_rec: 1.1162 acc_train: 0.4987 loss_val: 1.1318 acc_val: 0.4925 time: 0.0015s\n",
      "Epoch: 02725 loss_train: 1.1162 loss_rec: 1.1162 acc_train: 0.4987 loss_val: 1.1318 acc_val: 0.4928 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02726 loss_train: 1.1162 loss_rec: 1.1162 acc_train: 0.4987 loss_val: 1.1317 acc_val: 0.4928 time: 0.0020s\n",
      "Epoch: 02727 loss_train: 1.1162 loss_rec: 1.1162 acc_train: 0.4987 loss_val: 1.1317 acc_val: 0.4928 time: 0.0020s\n",
      "Epoch: 02728 loss_train: 1.1161 loss_rec: 1.1161 acc_train: 0.4987 loss_val: 1.1317 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02729 loss_train: 1.1161 loss_rec: 1.1161 acc_train: 0.4987 loss_val: 1.1317 acc_val: 0.4928 time: 0.0020s\n",
      "Epoch: 02730 loss_train: 1.1161 loss_rec: 1.1161 acc_train: 0.4988 loss_val: 1.1317 acc_val: 0.4928 time: 0.0015s\n",
      "Epoch: 02731 loss_train: 1.1161 loss_rec: 1.1161 acc_train: 0.4988 loss_val: 1.1317 acc_val: 0.4928 time: 0.0015s\n",
      "Test set results: loss= 1.1665 accuracy= 0.4278\n",
      "Epoch: 02732 loss_train: 1.1161 loss_rec: 1.1161 acc_train: 0.5004 loss_val: 1.1317 acc_val: 0.4938 time: 0.0015s\n",
      "Epoch: 02733 loss_train: 1.1161 loss_rec: 1.1161 acc_train: 0.5004 loss_val: 1.1317 acc_val: 0.4938 time: 0.0020s\n",
      "Epoch: 02734 loss_train: 1.1161 loss_rec: 1.1161 acc_train: 0.5004 loss_val: 1.1317 acc_val: 0.4938 time: 0.0015s\n",
      "Epoch: 02735 loss_train: 1.1161 loss_rec: 1.1161 acc_train: 0.5004 loss_val: 1.1317 acc_val: 0.4938 time: 0.0020s\n",
      "Epoch: 02736 loss_train: 1.1160 loss_rec: 1.1160 acc_train: 0.5004 loss_val: 1.1317 acc_val: 0.4938 time: 0.0015s\n",
      "Epoch: 02737 loss_train: 1.1160 loss_rec: 1.1160 acc_train: 0.5003 loss_val: 1.1317 acc_val: 0.4938 time: 0.0015s\n",
      "Epoch: 02738 loss_train: 1.1160 loss_rec: 1.1160 acc_train: 0.5003 loss_val: 1.1317 acc_val: 0.4938 time: 0.0015s\n",
      "Epoch: 02739 loss_train: 1.1160 loss_rec: 1.1160 acc_train: 0.5003 loss_val: 1.1316 acc_val: 0.4938 time: 0.0015s\n",
      "Epoch: 02740 loss_train: 1.1160 loss_rec: 1.1160 acc_train: 0.5003 loss_val: 1.1316 acc_val: 0.4938 time: 0.0015s\n",
      "Epoch: 02741 loss_train: 1.1160 loss_rec: 1.1160 acc_train: 0.5003 loss_val: 1.1316 acc_val: 0.4938 time: 0.0015s\n",
      "Test set results: loss= 1.1665 accuracy= 0.4277\n",
      "Epoch: 02742 loss_train: 1.1160 loss_rec: 1.1160 acc_train: 0.5003 loss_val: 1.1316 acc_val: 0.4938 time: 0.0020s\n",
      "Epoch: 02743 loss_train: 1.1160 loss_rec: 1.1160 acc_train: 0.5002 loss_val: 1.1316 acc_val: 0.4938 time: 0.0015s\n",
      "Epoch: 02744 loss_train: 1.1159 loss_rec: 1.1159 acc_train: 0.5002 loss_val: 1.1316 acc_val: 0.4938 time: 0.0020s\n",
      "Epoch: 02745 loss_train: 1.1159 loss_rec: 1.1159 acc_train: 0.5002 loss_val: 1.1316 acc_val: 0.4938 time: 0.0015s\n",
      "Epoch: 02746 loss_train: 1.1159 loss_rec: 1.1159 acc_train: 0.5002 loss_val: 1.1316 acc_val: 0.4938 time: 0.0015s\n",
      "Epoch: 02747 loss_train: 1.1159 loss_rec: 1.1159 acc_train: 0.5003 loss_val: 1.1316 acc_val: 0.4938 time: 0.0015s\n",
      "Epoch: 02748 loss_train: 1.1159 loss_rec: 1.1159 acc_train: 0.5003 loss_val: 1.1316 acc_val: 0.4938 time: 0.0015s\n",
      "Epoch: 02749 loss_train: 1.1159 loss_rec: 1.1159 acc_train: 0.5003 loss_val: 1.1316 acc_val: 0.4941 time: 0.0020s\n",
      "Epoch: 02750 loss_train: 1.1159 loss_rec: 1.1159 acc_train: 0.5003 loss_val: 1.1316 acc_val: 0.4941 time: 0.0015s\n",
      "Epoch: 02751 loss_train: 1.1159 loss_rec: 1.1159 acc_train: 0.5003 loss_val: 1.1315 acc_val: 0.4941 time: 0.0015s\n",
      "Test set results: loss= 1.1664 accuracy= 0.4277\n",
      "Epoch: 02752 loss_train: 1.1159 loss_rec: 1.1159 acc_train: 0.5003 loss_val: 1.1315 acc_val: 0.4941 time: 0.0015s\n",
      "Epoch: 02753 loss_train: 1.1158 loss_rec: 1.1158 acc_train: 0.5003 loss_val: 1.1315 acc_val: 0.4941 time: 0.0015s\n",
      "Epoch: 02754 loss_train: 1.1158 loss_rec: 1.1158 acc_train: 0.5003 loss_val: 1.1315 acc_val: 0.4944 time: 0.0020s\n",
      "Epoch: 02755 loss_train: 1.1158 loss_rec: 1.1158 acc_train: 0.5004 loss_val: 1.1315 acc_val: 0.4944 time: 0.0015s\n",
      "Epoch: 02756 loss_train: 1.1158 loss_rec: 1.1158 acc_train: 0.5004 loss_val: 1.1315 acc_val: 0.4944 time: 0.0015s\n",
      "Epoch: 02757 loss_train: 1.1158 loss_rec: 1.1158 acc_train: 0.5008 loss_val: 1.1315 acc_val: 0.4953 time: 0.0015s\n",
      "Epoch: 02758 loss_train: 1.1158 loss_rec: 1.1158 acc_train: 0.5008 loss_val: 1.1315 acc_val: 0.4953 time: 0.0015s\n",
      "Epoch: 02759 loss_train: 1.1158 loss_rec: 1.1158 acc_train: 0.5008 loss_val: 1.1315 acc_val: 0.4950 time: 0.0020s\n",
      "Epoch: 02760 loss_train: 1.1158 loss_rec: 1.1158 acc_train: 0.5008 loss_val: 1.1315 acc_val: 0.4950 time: 0.0015s\n",
      "Epoch: 02761 loss_train: 1.1157 loss_rec: 1.1157 acc_train: 0.5008 loss_val: 1.1315 acc_val: 0.4950 time: 0.0015s\n",
      "Test set results: loss= 1.1664 accuracy= 0.4284\n",
      "Epoch: 02762 loss_train: 1.1157 loss_rec: 1.1157 acc_train: 0.5008 loss_val: 1.1315 acc_val: 0.4956 time: 0.0015s\n",
      "Epoch: 02763 loss_train: 1.1157 loss_rec: 1.1157 acc_train: 0.5008 loss_val: 1.1314 acc_val: 0.4956 time: 0.0020s\n",
      "Epoch: 02764 loss_train: 1.1157 loss_rec: 1.1157 acc_train: 0.5008 loss_val: 1.1314 acc_val: 0.4956 time: 0.0015s\n",
      "Epoch: 02765 loss_train: 1.1157 loss_rec: 1.1157 acc_train: 0.5008 loss_val: 1.1314 acc_val: 0.4956 time: 0.0020s\n",
      "Epoch: 02766 loss_train: 1.1157 loss_rec: 1.1157 acc_train: 0.5008 loss_val: 1.1314 acc_val: 0.4956 time: 0.0015s\n",
      "Epoch: 02767 loss_train: 1.1157 loss_rec: 1.1157 acc_train: 0.5008 loss_val: 1.1314 acc_val: 0.4953 time: 0.0015s\n",
      "Epoch: 02768 loss_train: 1.1157 loss_rec: 1.1157 acc_train: 0.5008 loss_val: 1.1314 acc_val: 0.4953 time: 0.0015s\n",
      "Epoch: 02769 loss_train: 1.1157 loss_rec: 1.1157 acc_train: 0.5008 loss_val: 1.1314 acc_val: 0.4953 time: 0.0015s\n",
      "Epoch: 02770 loss_train: 1.1156 loss_rec: 1.1156 acc_train: 0.5008 loss_val: 1.1314 acc_val: 0.4953 time: 0.0020s\n",
      "Epoch: 02771 loss_train: 1.1156 loss_rec: 1.1156 acc_train: 0.5008 loss_val: 1.1314 acc_val: 0.4953 time: 0.0015s\n",
      "Test set results: loss= 1.1663 accuracy= 0.4283\n",
      "Epoch: 02772 loss_train: 1.1156 loss_rec: 1.1156 acc_train: 0.5008 loss_val: 1.1314 acc_val: 0.4953 time: 0.0015s\n",
      "Epoch: 02773 loss_train: 1.1156 loss_rec: 1.1156 acc_train: 0.5008 loss_val: 1.1314 acc_val: 0.4953 time: 0.0020s\n",
      "Epoch: 02774 loss_train: 1.1156 loss_rec: 1.1156 acc_train: 0.5008 loss_val: 1.1314 acc_val: 0.4953 time: 0.0020s\n",
      "Epoch: 02775 loss_train: 1.1156 loss_rec: 1.1156 acc_train: 0.5023 loss_val: 1.1314 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02776 loss_train: 1.1156 loss_rec: 1.1156 acc_train: 0.5023 loss_val: 1.1313 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02777 loss_train: 1.1156 loss_rec: 1.1156 acc_train: 0.5023 loss_val: 1.1313 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02778 loss_train: 1.1155 loss_rec: 1.1155 acc_train: 0.5018 loss_val: 1.1313 acc_val: 0.4991 time: 0.0015s\n",
      "Epoch: 02779 loss_train: 1.1155 loss_rec: 1.1155 acc_train: 0.5018 loss_val: 1.1313 acc_val: 0.4991 time: 0.0015s\n",
      "Epoch: 02780 loss_train: 1.1155 loss_rec: 1.1155 acc_train: 0.5018 loss_val: 1.1313 acc_val: 0.4991 time: 0.0015s\n",
      "Epoch: 02781 loss_train: 1.1155 loss_rec: 1.1155 acc_train: 0.5018 loss_val: 1.1313 acc_val: 0.4991 time: 0.0015s\n",
      "Test set results: loss= 1.1662 accuracy= 0.4290\n",
      "Epoch: 02782 loss_train: 1.1155 loss_rec: 1.1155 acc_train: 0.5018 loss_val: 1.1313 acc_val: 0.4991 time: 0.0020s\n",
      "Epoch: 02783 loss_train: 1.1155 loss_rec: 1.1155 acc_train: 0.5018 loss_val: 1.1313 acc_val: 0.4991 time: 0.0015s\n",
      "Epoch: 02784 loss_train: 1.1155 loss_rec: 1.1155 acc_train: 0.5018 loss_val: 1.1313 acc_val: 0.4991 time: 0.0015s\n",
      "Epoch: 02785 loss_train: 1.1155 loss_rec: 1.1155 acc_train: 0.5018 loss_val: 1.1313 acc_val: 0.4991 time: 0.0015s\n",
      "Epoch: 02786 loss_train: 1.1155 loss_rec: 1.1155 acc_train: 0.5018 loss_val: 1.1313 acc_val: 0.4991 time: 0.0020s\n",
      "Epoch: 02787 loss_train: 1.1154 loss_rec: 1.1154 acc_train: 0.5018 loss_val: 1.1313 acc_val: 0.4991 time: 0.0015s\n",
      "Epoch: 02788 loss_train: 1.1154 loss_rec: 1.1154 acc_train: 0.5018 loss_val: 1.1312 acc_val: 0.4991 time: 0.0015s\n",
      "Epoch: 02789 loss_train: 1.1154 loss_rec: 1.1154 acc_train: 0.5018 loss_val: 1.1312 acc_val: 0.4991 time: 0.0015s\n",
      "Epoch: 02790 loss_train: 1.1154 loss_rec: 1.1154 acc_train: 0.5018 loss_val: 1.1312 acc_val: 0.4991 time: 0.0015s\n",
      "Epoch: 02791 loss_train: 1.1154 loss_rec: 1.1154 acc_train: 0.5018 loss_val: 1.1312 acc_val: 0.4991 time: 0.0015s\n",
      "Test set results: loss= 1.1662 accuracy= 0.4298\n",
      "Epoch: 02792 loss_train: 1.1154 loss_rec: 1.1154 acc_train: 0.5021 loss_val: 1.1312 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02793 loss_train: 1.1154 loss_rec: 1.1154 acc_train: 0.5021 loss_val: 1.1312 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02794 loss_train: 1.1154 loss_rec: 1.1154 acc_train: 0.5021 loss_val: 1.1312 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02795 loss_train: 1.1154 loss_rec: 1.1154 acc_train: 0.5021 loss_val: 1.1312 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02796 loss_train: 1.1153 loss_rec: 1.1153 acc_train: 0.5021 loss_val: 1.1312 acc_val: 0.4994 time: 0.0020s\n",
      "Epoch: 02797 loss_train: 1.1153 loss_rec: 1.1153 acc_train: 0.5021 loss_val: 1.1312 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02798 loss_train: 1.1153 loss_rec: 1.1153 acc_train: 0.5021 loss_val: 1.1312 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02799 loss_train: 1.1153 loss_rec: 1.1153 acc_train: 0.5021 loss_val: 1.1312 acc_val: 0.4994 time: 0.0020s\n",
      "Epoch: 02800 loss_train: 1.1153 loss_rec: 1.1153 acc_train: 0.5022 loss_val: 1.1312 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02801 loss_train: 1.1153 loss_rec: 1.1153 acc_train: 0.5022 loss_val: 1.1311 acc_val: 0.4994 time: 0.0020s\n",
      "Test set results: loss= 1.1661 accuracy= 0.4299\n",
      "Epoch: 02802 loss_train: 1.1153 loss_rec: 1.1153 acc_train: 0.5022 loss_val: 1.1311 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02803 loss_train: 1.1153 loss_rec: 1.1153 acc_train: 0.5022 loss_val: 1.1311 acc_val: 0.4994 time: 0.0020s\n",
      "Epoch: 02804 loss_train: 1.1152 loss_rec: 1.1152 acc_train: 0.5022 loss_val: 1.1311 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02805 loss_train: 1.1152 loss_rec: 1.1152 acc_train: 0.5022 loss_val: 1.1311 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02806 loss_train: 1.1152 loss_rec: 1.1152 acc_train: 0.5008 loss_val: 1.1311 acc_val: 0.4981 time: 0.0015s\n",
      "Epoch: 02807 loss_train: 1.1152 loss_rec: 1.1152 acc_train: 0.5008 loss_val: 1.1311 acc_val: 0.4981 time: 0.0020s\n",
      "Epoch: 02808 loss_train: 1.1152 loss_rec: 1.1152 acc_train: 0.5008 loss_val: 1.1311 acc_val: 0.4981 time: 0.0015s\n",
      "Epoch: 02809 loss_train: 1.1152 loss_rec: 1.1152 acc_train: 0.5008 loss_val: 1.1311 acc_val: 0.4981 time: 0.0015s\n",
      "Epoch: 02810 loss_train: 1.1152 loss_rec: 1.1152 acc_train: 0.5008 loss_val: 1.1311 acc_val: 0.4981 time: 0.0020s\n",
      "Epoch: 02811 loss_train: 1.1152 loss_rec: 1.1152 acc_train: 0.5008 loss_val: 1.1311 acc_val: 0.4981 time: 0.0015s\n",
      "Test set results: loss= 1.1660 accuracy= 0.4286\n",
      "Epoch: 02812 loss_train: 1.1152 loss_rec: 1.1152 acc_train: 0.5008 loss_val: 1.1311 acc_val: 0.4981 time: 0.0015s\n",
      "Epoch: 02813 loss_train: 1.1151 loss_rec: 1.1151 acc_train: 0.5008 loss_val: 1.1311 acc_val: 0.4981 time: 0.0015s\n",
      "Epoch: 02814 loss_train: 1.1151 loss_rec: 1.1151 acc_train: 0.5008 loss_val: 1.1311 acc_val: 0.4981 time: 0.0020s\n",
      "Epoch: 02815 loss_train: 1.1151 loss_rec: 1.1151 acc_train: 0.5008 loss_val: 1.1311 acc_val: 0.4981 time: 0.0015s\n",
      "Epoch: 02816 loss_train: 1.1151 loss_rec: 1.1151 acc_train: 0.5008 loss_val: 1.1310 acc_val: 0.4981 time: 0.0015s\n",
      "Epoch: 02817 loss_train: 1.1151 loss_rec: 1.1151 acc_train: 0.5008 loss_val: 1.1310 acc_val: 0.4981 time: 0.0015s\n",
      "Epoch: 02818 loss_train: 1.1151 loss_rec: 1.1151 acc_train: 0.5008 loss_val: 1.1310 acc_val: 0.4981 time: 0.0015s\n",
      "Epoch: 02819 loss_train: 1.1151 loss_rec: 1.1151 acc_train: 0.5008 loss_val: 1.1310 acc_val: 0.4981 time: 0.0015s\n",
      "Epoch: 02820 loss_train: 1.1151 loss_rec: 1.1151 acc_train: 0.5008 loss_val: 1.1310 acc_val: 0.4981 time: 0.0020s\n",
      "Epoch: 02821 loss_train: 1.1150 loss_rec: 1.1150 acc_train: 0.5008 loss_val: 1.1310 acc_val: 0.4981 time: 0.0015s\n",
      "Test set results: loss= 1.1659 accuracy= 0.4286\n",
      "Epoch: 02822 loss_train: 1.1150 loss_rec: 1.1150 acc_train: 0.5008 loss_val: 1.1310 acc_val: 0.4981 time: 0.0015s\n",
      "Epoch: 02823 loss_train: 1.1150 loss_rec: 1.1150 acc_train: 0.5008 loss_val: 1.1310 acc_val: 0.4981 time: 0.0015s\n",
      "Epoch: 02824 loss_train: 1.1150 loss_rec: 1.1150 acc_train: 0.5008 loss_val: 1.1310 acc_val: 0.4981 time: 0.0020s\n",
      "Epoch: 02825 loss_train: 1.1150 loss_rec: 1.1150 acc_train: 0.5008 loss_val: 1.1310 acc_val: 0.4981 time: 0.0015s\n",
      "Epoch: 02826 loss_train: 1.1150 loss_rec: 1.1150 acc_train: 0.5016 loss_val: 1.1310 acc_val: 0.4988 time: 0.0015s\n",
      "Epoch: 02827 loss_train: 1.1150 loss_rec: 1.1150 acc_train: 0.5016 loss_val: 1.1310 acc_val: 0.4988 time: 0.0015s\n",
      "Epoch: 02828 loss_train: 1.1150 loss_rec: 1.1150 acc_train: 0.5016 loss_val: 1.1310 acc_val: 0.4988 time: 0.0015s\n",
      "Epoch: 02829 loss_train: 1.1150 loss_rec: 1.1150 acc_train: 0.5016 loss_val: 1.1310 acc_val: 0.4988 time: 0.0020s\n",
      "Epoch: 02830 loss_train: 1.1149 loss_rec: 1.1149 acc_train: 0.5016 loss_val: 1.1310 acc_val: 0.4988 time: 0.0015s\n",
      "Epoch: 02831 loss_train: 1.1149 loss_rec: 1.1149 acc_train: 0.5016 loss_val: 1.1310 acc_val: 0.4988 time: 0.0020s\n",
      "Test set results: loss= 1.1658 accuracy= 0.4289\n",
      "Epoch: 02832 loss_train: 1.1149 loss_rec: 1.1149 acc_train: 0.5015 loss_val: 1.1310 acc_val: 0.4988 time: 0.0015s\n",
      "Epoch: 02833 loss_train: 1.1149 loss_rec: 1.1149 acc_train: 0.5015 loss_val: 1.1310 acc_val: 0.4984 time: 0.0015s\n",
      "Epoch: 02834 loss_train: 1.1149 loss_rec: 1.1149 acc_train: 0.5015 loss_val: 1.1310 acc_val: 0.4984 time: 0.0015s\n",
      "Epoch: 02835 loss_train: 1.1149 loss_rec: 1.1149 acc_train: 0.5015 loss_val: 1.1310 acc_val: 0.4984 time: 0.0015s\n",
      "Epoch: 02836 loss_train: 1.1149 loss_rec: 1.1149 acc_train: 0.5015 loss_val: 1.1310 acc_val: 0.4984 time: 0.0015s\n",
      "Epoch: 02837 loss_train: 1.1149 loss_rec: 1.1149 acc_train: 0.5014 loss_val: 1.1310 acc_val: 0.4984 time: 0.0015s\n",
      "Epoch: 02838 loss_train: 1.1149 loss_rec: 1.1149 acc_train: 0.5014 loss_val: 1.1309 acc_val: 0.4984 time: 0.0020s\n",
      "Epoch: 02839 loss_train: 1.1148 loss_rec: 1.1148 acc_train: 0.5014 loss_val: 1.1309 acc_val: 0.4984 time: 0.0015s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02840 loss_train: 1.1148 loss_rec: 1.1148 acc_train: 0.5014 loss_val: 1.1309 acc_val: 0.4984 time: 0.0020s\n",
      "Epoch: 02841 loss_train: 1.1148 loss_rec: 1.1148 acc_train: 0.5014 loss_val: 1.1309 acc_val: 0.4984 time: 0.0015s\n",
      "Test set results: loss= 1.1656 accuracy= 0.4289\n",
      "Epoch: 02842 loss_train: 1.1148 loss_rec: 1.1148 acc_train: 0.5016 loss_val: 1.1309 acc_val: 0.4984 time: 0.0015s\n",
      "Epoch: 02843 loss_train: 1.1148 loss_rec: 1.1148 acc_train: 0.5015 loss_val: 1.1309 acc_val: 0.4984 time: 0.0020s\n",
      "Epoch: 02844 loss_train: 1.1148 loss_rec: 1.1148 acc_train: 0.5016 loss_val: 1.1309 acc_val: 0.4984 time: 0.0015s\n",
      "Epoch: 02845 loss_train: 1.1148 loss_rec: 1.1148 acc_train: 0.5016 loss_val: 1.1309 acc_val: 0.4984 time: 0.0020s\n",
      "Epoch: 02846 loss_train: 1.1148 loss_rec: 1.1148 acc_train: 0.5016 loss_val: 1.1309 acc_val: 0.4984 time: 0.0015s\n",
      "Epoch: 02847 loss_train: 1.1147 loss_rec: 1.1147 acc_train: 0.5016 loss_val: 1.1309 acc_val: 0.4984 time: 0.0020s\n",
      "Epoch: 02848 loss_train: 1.1147 loss_rec: 1.1147 acc_train: 0.5016 loss_val: 1.1309 acc_val: 0.4984 time: 0.0015s\n",
      "Epoch: 02849 loss_train: 1.1147 loss_rec: 1.1147 acc_train: 0.5019 loss_val: 1.1309 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02850 loss_train: 1.1147 loss_rec: 1.1147 acc_train: 0.5019 loss_val: 1.1309 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02851 loss_train: 1.1147 loss_rec: 1.1147 acc_train: 0.5019 loss_val: 1.1309 acc_val: 0.4994 time: 0.0015s\n",
      "Test set results: loss= 1.1656 accuracy= 0.4298\n",
      "Epoch: 02852 loss_train: 1.1147 loss_rec: 1.1147 acc_train: 0.5019 loss_val: 1.1309 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02853 loss_train: 1.1147 loss_rec: 1.1147 acc_train: 0.5019 loss_val: 1.1309 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02854 loss_train: 1.1147 loss_rec: 1.1147 acc_train: 0.5019 loss_val: 1.1309 acc_val: 0.4994 time: 0.0020s\n",
      "Epoch: 02855 loss_train: 1.1147 loss_rec: 1.1147 acc_train: 0.5019 loss_val: 1.1309 acc_val: 0.4994 time: 0.0020s\n",
      "Epoch: 02856 loss_train: 1.1146 loss_rec: 1.1146 acc_train: 0.5019 loss_val: 1.1308 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02857 loss_train: 1.1146 loss_rec: 1.1146 acc_train: 0.5019 loss_val: 1.1308 acc_val: 0.4994 time: 0.0025s\n",
      "Epoch: 02858 loss_train: 1.1146 loss_rec: 1.1146 acc_train: 0.5019 loss_val: 1.1308 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02859 loss_train: 1.1146 loss_rec: 1.1146 acc_train: 0.5019 loss_val: 1.1308 acc_val: 0.4997 time: 0.0020s\n",
      "Epoch: 02860 loss_train: 1.1146 loss_rec: 1.1146 acc_train: 0.5019 loss_val: 1.1308 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02861 loss_train: 1.1146 loss_rec: 1.1146 acc_train: 0.5019 loss_val: 1.1308 acc_val: 0.4997 time: 0.0020s\n",
      "Test set results: loss= 1.1657 accuracy= 0.4298\n",
      "Epoch: 02862 loss_train: 1.1146 loss_rec: 1.1146 acc_train: 0.5019 loss_val: 1.1308 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02863 loss_train: 1.1146 loss_rec: 1.1146 acc_train: 0.5019 loss_val: 1.1308 acc_val: 0.4997 time: 0.0025s\n",
      "Epoch: 02864 loss_train: 1.1146 loss_rec: 1.1146 acc_train: 0.5019 loss_val: 1.1308 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02865 loss_train: 1.1145 loss_rec: 1.1145 acc_train: 0.5019 loss_val: 1.1308 acc_val: 0.4997 time: 0.0025s\n",
      "Epoch: 02866 loss_train: 1.1145 loss_rec: 1.1145 acc_train: 0.5019 loss_val: 1.1308 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02867 loss_train: 1.1145 loss_rec: 1.1145 acc_train: 0.5019 loss_val: 1.1308 acc_val: 0.4997 time: 0.0020s\n",
      "Epoch: 02868 loss_train: 1.1145 loss_rec: 1.1145 acc_train: 0.5019 loss_val: 1.1308 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02869 loss_train: 1.1145 loss_rec: 1.1145 acc_train: 0.5019 loss_val: 1.1308 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02870 loss_train: 1.1145 loss_rec: 1.1145 acc_train: 0.5019 loss_val: 1.1308 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02871 loss_train: 1.1145 loss_rec: 1.1145 acc_train: 0.5019 loss_val: 1.1308 acc_val: 0.4994 time: 0.0020s\n",
      "Test set results: loss= 1.1656 accuracy= 0.4298\n",
      "Epoch: 02872 loss_train: 1.1145 loss_rec: 1.1145 acc_train: 0.5019 loss_val: 1.1307 acc_val: 0.4994 time: 0.0020s\n",
      "Epoch: 02873 loss_train: 1.1145 loss_rec: 1.1145 acc_train: 0.5019 loss_val: 1.1307 acc_val: 0.4994 time: 0.0015s\n",
      "Epoch: 02874 loss_train: 1.1144 loss_rec: 1.1144 acc_train: 0.5019 loss_val: 1.1307 acc_val: 0.4994 time: 0.0025s\n",
      "Epoch: 02875 loss_train: 1.1144 loss_rec: 1.1144 acc_train: 0.5021 loss_val: 1.1307 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02876 loss_train: 1.1144 loss_rec: 1.1144 acc_train: 0.5021 loss_val: 1.1307 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02877 loss_train: 1.1144 loss_rec: 1.1144 acc_train: 0.5021 loss_val: 1.1307 acc_val: 0.4997 time: 0.0020s\n",
      "Epoch: 02878 loss_train: 1.1144 loss_rec: 1.1144 acc_train: 0.5021 loss_val: 1.1307 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02879 loss_train: 1.1144 loss_rec: 1.1144 acc_train: 0.5021 loss_val: 1.1307 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02880 loss_train: 1.1144 loss_rec: 1.1144 acc_train: 0.5022 loss_val: 1.1307 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02881 loss_train: 1.1144 loss_rec: 1.1144 acc_train: 0.5022 loss_val: 1.1307 acc_val: 0.4997 time: 0.0020s\n",
      "Test set results: loss= 1.1656 accuracy= 0.4307\n",
      "Epoch: 02882 loss_train: 1.1144 loss_rec: 1.1144 acc_train: 0.5022 loss_val: 1.1307 acc_val: 0.4997 time: 0.0020s\n",
      "Epoch: 02883 loss_train: 1.1143 loss_rec: 1.1143 acc_train: 0.5022 loss_val: 1.1307 acc_val: 0.4997 time: 0.0020s\n",
      "Epoch: 02884 loss_train: 1.1143 loss_rec: 1.1143 acc_train: 0.5022 loss_val: 1.1307 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02885 loss_train: 1.1143 loss_rec: 1.1143 acc_train: 0.5022 loss_val: 1.1307 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02886 loss_train: 1.1143 loss_rec: 1.1143 acc_train: 0.5022 loss_val: 1.1307 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02887 loss_train: 1.1143 loss_rec: 1.1143 acc_train: 0.5022 loss_val: 1.1307 acc_val: 0.4997 time: 0.0020s\n",
      "Epoch: 02888 loss_train: 1.1143 loss_rec: 1.1143 acc_train: 0.5022 loss_val: 1.1306 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02889 loss_train: 1.1143 loss_rec: 1.1143 acc_train: 0.5022 loss_val: 1.1306 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02890 loss_train: 1.1143 loss_rec: 1.1143 acc_train: 0.5022 loss_val: 1.1306 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02891 loss_train: 1.1143 loss_rec: 1.1143 acc_train: 0.5022 loss_val: 1.1306 acc_val: 0.4997 time: 0.0020s\n",
      "Test set results: loss= 1.1655 accuracy= 0.4307\n",
      "Epoch: 02892 loss_train: 1.1142 loss_rec: 1.1142 acc_train: 0.5022 loss_val: 1.1306 acc_val: 0.4997 time: 0.0025s\n",
      "Epoch: 02893 loss_train: 1.1142 loss_rec: 1.1142 acc_train: 0.5022 loss_val: 1.1306 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02894 loss_train: 1.1142 loss_rec: 1.1142 acc_train: 0.5022 loss_val: 1.1306 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02895 loss_train: 1.1142 loss_rec: 1.1142 acc_train: 0.5022 loss_val: 1.1306 acc_val: 0.4997 time: 0.0015s\n",
      "Epoch: 02896 loss_train: 1.1142 loss_rec: 1.1142 acc_train: 0.5025 loss_val: 1.1306 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02897 loss_train: 1.1142 loss_rec: 1.1142 acc_train: 0.5025 loss_val: 1.1306 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02898 loss_train: 1.1142 loss_rec: 1.1142 acc_train: 0.5026 loss_val: 1.1306 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02899 loss_train: 1.1142 loss_rec: 1.1142 acc_train: 0.5026 loss_val: 1.1306 acc_val: 0.5000 time: 0.0025s\n",
      "Epoch: 02900 loss_train: 1.1142 loss_rec: 1.1142 acc_train: 0.5025 loss_val: 1.1306 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02901 loss_train: 1.1141 loss_rec: 1.1141 acc_train: 0.5025 loss_val: 1.1306 acc_val: 0.5000 time: 0.0025s\n",
      "Test set results: loss= 1.1654 accuracy= 0.4315\n",
      "Epoch: 02902 loss_train: 1.1141 loss_rec: 1.1141 acc_train: 0.5025 loss_val: 1.1306 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02903 loss_train: 1.1141 loss_rec: 1.1141 acc_train: 0.5026 loss_val: 1.1305 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02904 loss_train: 1.1141 loss_rec: 1.1141 acc_train: 0.5033 loss_val: 1.1305 acc_val: 0.5006 time: 0.0015s\n",
      "Epoch: 02905 loss_train: 1.1141 loss_rec: 1.1141 acc_train: 0.5033 loss_val: 1.1305 acc_val: 0.5006 time: 0.0015s\n",
      "Epoch: 02906 loss_train: 1.1141 loss_rec: 1.1141 acc_train: 0.5033 loss_val: 1.1305 acc_val: 0.5006 time: 0.0015s\n",
      "Epoch: 02907 loss_train: 1.1141 loss_rec: 1.1141 acc_train: 0.5033 loss_val: 1.1305 acc_val: 0.5006 time: 0.0025s\n",
      "Epoch: 02908 loss_train: 1.1141 loss_rec: 1.1141 acc_train: 0.5033 loss_val: 1.1305 acc_val: 0.5006 time: 0.0015s\n",
      "Epoch: 02909 loss_train: 1.1141 loss_rec: 1.1141 acc_train: 0.5033 loss_val: 1.1305 acc_val: 0.5006 time: 0.0025s\n",
      "Epoch: 02910 loss_train: 1.1140 loss_rec: 1.1140 acc_train: 0.5033 loss_val: 1.1305 acc_val: 0.5006 time: 0.0020s\n",
      "Epoch: 02911 loss_train: 1.1140 loss_rec: 1.1140 acc_train: 0.5033 loss_val: 1.1305 acc_val: 0.5006 time: 0.0015s\n",
      "Test set results: loss= 1.1653 accuracy= 0.4324\n",
      "Epoch: 02912 loss_train: 1.1140 loss_rec: 1.1140 acc_train: 0.5033 loss_val: 1.1305 acc_val: 0.5006 time: 0.0020s\n",
      "Epoch: 02913 loss_train: 1.1140 loss_rec: 1.1140 acc_train: 0.5033 loss_val: 1.1305 acc_val: 0.5006 time: 0.0015s\n",
      "Epoch: 02914 loss_train: 1.1140 loss_rec: 1.1140 acc_train: 0.5033 loss_val: 1.1305 acc_val: 0.5006 time: 0.0015s\n",
      "Epoch: 02915 loss_train: 1.1140 loss_rec: 1.1140 acc_train: 0.5033 loss_val: 1.1305 acc_val: 0.5006 time: 0.0015s\n",
      "Epoch: 02916 loss_train: 1.1140 loss_rec: 1.1140 acc_train: 0.5033 loss_val: 1.1305 acc_val: 0.5006 time: 0.0020s\n",
      "Epoch: 02917 loss_train: 1.1140 loss_rec: 1.1140 acc_train: 0.5033 loss_val: 1.1304 acc_val: 0.5006 time: 0.0020s\n",
      "Epoch: 02918 loss_train: 1.1140 loss_rec: 1.1140 acc_train: 0.5033 loss_val: 1.1304 acc_val: 0.5006 time: 0.0020s\n",
      "Epoch: 02919 loss_train: 1.1139 loss_rec: 1.1139 acc_train: 0.5033 loss_val: 1.1304 acc_val: 0.5006 time: 0.0015s\n",
      "Epoch: 02920 loss_train: 1.1139 loss_rec: 1.1139 acc_train: 0.5033 loss_val: 1.1304 acc_val: 0.5006 time: 0.0020s\n",
      "Epoch: 02921 loss_train: 1.1139 loss_rec: 1.1139 acc_train: 0.5034 loss_val: 1.1304 acc_val: 0.5006 time: 0.0015s\n",
      "Test set results: loss= 1.1653 accuracy= 0.4323\n",
      "Epoch: 02922 loss_train: 1.1139 loss_rec: 1.1139 acc_train: 0.5034 loss_val: 1.1304 acc_val: 0.5006 time: 0.0015s\n",
      "Epoch: 02923 loss_train: 1.1139 loss_rec: 1.1139 acc_train: 0.5028 loss_val: 1.1304 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02924 loss_train: 1.1139 loss_rec: 1.1139 acc_train: 0.5034 loss_val: 1.1304 acc_val: 0.5006 time: 0.0025s\n",
      "Epoch: 02925 loss_train: 1.1139 loss_rec: 1.1139 acc_train: 0.5034 loss_val: 1.1304 acc_val: 0.5006 time: 0.0015s\n",
      "Epoch: 02926 loss_train: 1.1139 loss_rec: 1.1139 acc_train: 0.5034 loss_val: 1.1304 acc_val: 0.5006 time: 0.0025s\n",
      "Epoch: 02927 loss_train: 1.1139 loss_rec: 1.1139 acc_train: 0.5028 loss_val: 1.1304 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02928 loss_train: 1.1139 loss_rec: 1.1139 acc_train: 0.5028 loss_val: 1.1304 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02929 loss_train: 1.1138 loss_rec: 1.1138 acc_train: 0.5028 loss_val: 1.1304 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02930 loss_train: 1.1138 loss_rec: 1.1138 acc_train: 0.5028 loss_val: 1.1304 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02931 loss_train: 1.1138 loss_rec: 1.1138 acc_train: 0.5028 loss_val: 1.1304 acc_val: 0.5000 time: 0.0015s\n",
      "Test set results: loss= 1.1652 accuracy= 0.4314\n",
      "Epoch: 02932 loss_train: 1.1138 loss_rec: 1.1138 acc_train: 0.5028 loss_val: 1.1303 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02933 loss_train: 1.1138 loss_rec: 1.1138 acc_train: 0.5028 loss_val: 1.1303 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02934 loss_train: 1.1138 loss_rec: 1.1138 acc_train: 0.5028 loss_val: 1.1303 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02935 loss_train: 1.1138 loss_rec: 1.1138 acc_train: 0.5028 loss_val: 1.1303 acc_val: 0.5000 time: 0.0025s\n",
      "Epoch: 02936 loss_train: 1.1138 loss_rec: 1.1138 acc_train: 0.5028 loss_val: 1.1303 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02937 loss_train: 1.1138 loss_rec: 1.1138 acc_train: 0.5028 loss_val: 1.1303 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02938 loss_train: 1.1137 loss_rec: 1.1137 acc_train: 0.5028 loss_val: 1.1303 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02939 loss_train: 1.1137 loss_rec: 1.1137 acc_train: 0.5028 loss_val: 1.1303 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02940 loss_train: 1.1137 loss_rec: 1.1137 acc_train: 0.5028 loss_val: 1.1303 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02941 loss_train: 1.1137 loss_rec: 1.1137 acc_train: 0.5028 loss_val: 1.1303 acc_val: 0.5000 time: 0.0015s\n",
      "Test set results: loss= 1.1652 accuracy= 0.4314\n",
      "Epoch: 02942 loss_train: 1.1137 loss_rec: 1.1137 acc_train: 0.5029 loss_val: 1.1303 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02943 loss_train: 1.1137 loss_rec: 1.1137 acc_train: 0.5029 loss_val: 1.1303 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02944 loss_train: 1.1137 loss_rec: 1.1137 acc_train: 0.5029 loss_val: 1.1303 acc_val: 0.5000 time: 0.0030s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02945 loss_train: 1.1137 loss_rec: 1.1137 acc_train: 0.5029 loss_val: 1.1303 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02946 loss_train: 1.1137 loss_rec: 1.1137 acc_train: 0.5029 loss_val: 1.1302 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02947 loss_train: 1.1136 loss_rec: 1.1136 acc_train: 0.5029 loss_val: 1.1302 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02948 loss_train: 1.1136 loss_rec: 1.1136 acc_train: 0.5030 loss_val: 1.1302 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02949 loss_train: 1.1136 loss_rec: 1.1136 acc_train: 0.5030 loss_val: 1.1302 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02950 loss_train: 1.1136 loss_rec: 1.1136 acc_train: 0.5030 loss_val: 1.1302 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02951 loss_train: 1.1136 loss_rec: 1.1136 acc_train: 0.5030 loss_val: 1.1302 acc_val: 0.5000 time: 0.0015s\n",
      "Test set results: loss= 1.1652 accuracy= 0.4315\n",
      "Epoch: 02952 loss_train: 1.1136 loss_rec: 1.1136 acc_train: 0.5030 loss_val: 1.1302 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02953 loss_train: 1.1136 loss_rec: 1.1136 acc_train: 0.5030 loss_val: 1.1302 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02954 loss_train: 1.1136 loss_rec: 1.1136 acc_train: 0.5030 loss_val: 1.1302 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02955 loss_train: 1.1136 loss_rec: 1.1136 acc_train: 0.5030 loss_val: 1.1302 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02956 loss_train: 1.1136 loss_rec: 1.1136 acc_train: 0.5029 loss_val: 1.1302 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02957 loss_train: 1.1135 loss_rec: 1.1135 acc_train: 0.5029 loss_val: 1.1302 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02958 loss_train: 1.1135 loss_rec: 1.1135 acc_train: 0.5029 loss_val: 1.1302 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02959 loss_train: 1.1135 loss_rec: 1.1135 acc_train: 0.5029 loss_val: 1.1302 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02960 loss_train: 1.1135 loss_rec: 1.1135 acc_train: 0.5029 loss_val: 1.1302 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02961 loss_train: 1.1135 loss_rec: 1.1135 acc_train: 0.5029 loss_val: 1.1302 acc_val: 0.5000 time: 0.0015s\n",
      "Test set results: loss= 1.1650 accuracy= 0.4314\n",
      "Epoch: 02962 loss_train: 1.1135 loss_rec: 1.1135 acc_train: 0.5029 loss_val: 1.1302 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02963 loss_train: 1.1135 loss_rec: 1.1135 acc_train: 0.5028 loss_val: 1.1301 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02964 loss_train: 1.1135 loss_rec: 1.1135 acc_train: 0.5028 loss_val: 1.1301 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02965 loss_train: 1.1135 loss_rec: 1.1135 acc_train: 0.5028 loss_val: 1.1301 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02966 loss_train: 1.1134 loss_rec: 1.1134 acc_train: 0.5028 loss_val: 1.1301 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02967 loss_train: 1.1134 loss_rec: 1.1134 acc_train: 0.5028 loss_val: 1.1301 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02968 loss_train: 1.1134 loss_rec: 1.1134 acc_train: 0.5028 loss_val: 1.1301 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02969 loss_train: 1.1134 loss_rec: 1.1134 acc_train: 0.5028 loss_val: 1.1301 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02970 loss_train: 1.1134 loss_rec: 1.1134 acc_train: 0.5028 loss_val: 1.1301 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02971 loss_train: 1.1134 loss_rec: 1.1134 acc_train: 0.5028 loss_val: 1.1301 acc_val: 0.5000 time: 0.0020s\n",
      "Test set results: loss= 1.1650 accuracy= 0.4315\n",
      "Epoch: 02972 loss_train: 1.1134 loss_rec: 1.1134 acc_train: 0.5029 loss_val: 1.1301 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02973 loss_train: 1.1134 loss_rec: 1.1134 acc_train: 0.5028 loss_val: 1.1301 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02974 loss_train: 1.1134 loss_rec: 1.1134 acc_train: 0.5028 loss_val: 1.1301 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02975 loss_train: 1.1134 loss_rec: 1.1134 acc_train: 0.5028 loss_val: 1.1301 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02976 loss_train: 1.1133 loss_rec: 1.1133 acc_train: 0.5028 loss_val: 1.1301 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02977 loss_train: 1.1133 loss_rec: 1.1133 acc_train: 0.5028 loss_val: 1.1301 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02978 loss_train: 1.1133 loss_rec: 1.1133 acc_train: 0.5028 loss_val: 1.1300 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02979 loss_train: 1.1133 loss_rec: 1.1133 acc_train: 0.5028 loss_val: 1.1300 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02980 loss_train: 1.1133 loss_rec: 1.1133 acc_train: 0.5028 loss_val: 1.1300 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02981 loss_train: 1.1133 loss_rec: 1.1133 acc_train: 0.5028 loss_val: 1.1300 acc_val: 0.5000 time: 0.0015s\n",
      "Test set results: loss= 1.1649 accuracy= 0.4315\n",
      "Epoch: 02982 loss_train: 1.1133 loss_rec: 1.1133 acc_train: 0.5028 loss_val: 1.1300 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02983 loss_train: 1.1133 loss_rec: 1.1133 acc_train: 0.5028 loss_val: 1.1300 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02984 loss_train: 1.1133 loss_rec: 1.1133 acc_train: 0.5028 loss_val: 1.1300 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02985 loss_train: 1.1133 loss_rec: 1.1133 acc_train: 0.5028 loss_val: 1.1300 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02986 loss_train: 1.1132 loss_rec: 1.1132 acc_train: 0.5028 loss_val: 1.1300 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02987 loss_train: 1.1132 loss_rec: 1.1132 acc_train: 0.5028 loss_val: 1.1300 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02988 loss_train: 1.1132 loss_rec: 1.1132 acc_train: 0.5028 loss_val: 1.1300 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02989 loss_train: 1.1132 loss_rec: 1.1132 acc_train: 0.5028 loss_val: 1.1300 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02990 loss_train: 1.1132 loss_rec: 1.1132 acc_train: 0.5028 loss_val: 1.1300 acc_val: 0.5000 time: 0.0025s\n",
      "Epoch: 02991 loss_train: 1.1132 loss_rec: 1.1132 acc_train: 0.5028 loss_val: 1.1300 acc_val: 0.5000 time: 0.0015s\n",
      "Test set results: loss= 1.1650 accuracy= 0.4314\n",
      "Epoch: 02992 loss_train: 1.1132 loss_rec: 1.1132 acc_train: 0.5028 loss_val: 1.1300 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02993 loss_train: 1.1132 loss_rec: 1.1132 acc_train: 0.5028 loss_val: 1.1299 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02994 loss_train: 1.1132 loss_rec: 1.1132 acc_train: 0.5028 loss_val: 1.1299 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02995 loss_train: 1.1131 loss_rec: 1.1131 acc_train: 0.5028 loss_val: 1.1299 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02996 loss_train: 1.1131 loss_rec: 1.1131 acc_train: 0.5028 loss_val: 1.1299 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02997 loss_train: 1.1131 loss_rec: 1.1131 acc_train: 0.5028 loss_val: 1.1299 acc_val: 0.5000 time: 0.0015s\n",
      "Epoch: 02998 loss_train: 1.1131 loss_rec: 1.1131 acc_train: 0.5028 loss_val: 1.1299 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 02999 loss_train: 1.1131 loss_rec: 1.1131 acc_train: 0.5028 loss_val: 1.1299 acc_val: 0.5000 time: 0.0020s\n",
      "Epoch: 03000 loss_train: 1.1131 loss_rec: 1.1131 acc_train: 0.5028 loss_val: 1.1299 acc_val: 0.5000 time: 0.0015s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 9.1355s\n",
      "Test set results: loss= 1.1649 accuracy= 0.4314\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t_total = time.time()\n",
    "for epoch in range(3000):\n",
    "    train(epoch)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        output=test(epoch)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        save_model(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "output=test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[idx_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3200"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_val[(labels==1)[idx_val]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(output, 'GraphSage_Upsample_output.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = output.max(1)[1].type_as(labels)\n",
    "cout=preds.cpu().detach().numpy()\n",
    "clabels=labels.cpu().detach().numpy()\n",
    "cidx_test=idx_test.cpu().detach().numpy()\n",
    "cidx_valid=idx_val.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(true_negative,false_positive):\n",
    "    return str(round(true_negative/(true_negative+false_positive),2))\n",
    "def sensitivity(true_positive,false_negative):\n",
    "    return str(round(true_positive/(true_positive+false_negative),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 427.9555555555555, 'Predicted label')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "cnf_matrix_n = metrics.confusion_matrix(clabels[cidx_test], cout[cidx_test])\n",
    "#class_names=[\"Not_Related\", \"Related\"]\n",
    "class_names=[\"Not_Related\", \"1\",\"2\",\"3\"] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "sns.heatmap(cnf_matrix_n, annot=True, cmap=\"YlGnBu\" ,fmt='d', xticklabels=class_names, yticklabels=class_names)\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Graph Sage Upsample')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2268"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix_n[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not_Related       0.28      0.34      0.31      6603\n",
      "           1       0.54      0.52      0.53     10048\n",
      "           2       0.64      0.33      0.43     15440\n",
      "           3       0.28      0.79      0.41      3616\n",
      "\n",
      "    accuracy                           0.43     35707\n",
      "   macro avg       0.44      0.50      0.42     35707\n",
      "weighted avg       0.51      0.43      0.44     35707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(clabels[cidx_test], cout[cidx_test], target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2268,  430, 1054, 2851],\n",
       "       [2138, 5250, 1614, 1046],\n",
       "       [3002, 3996, 5027, 3415],\n",
       "       [ 630,    0,  126, 2860]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#True Positive\n",
    "True_Positive_0=cnf_matrix_n[0][0]\n",
    "True_Positive_1=cnf_matrix_n[1][1]\n",
    "True_Positive_2=cnf_matrix_n[2][2]\n",
    "True_Positive_3=cnf_matrix_n[3][3]\n",
    "#True Negative\n",
    "True_Negatives_0=cnf_matrix_n[1][1]+cnf_matrix_n[1][2]+cnf_matrix_n[1][3]+cnf_matrix_n[2][1]+cnf_matrix_n[2][2]+cnf_matrix_n[2][3]+cnf_matrix_n[3][1]+cnf_matrix_n[3][2]+cnf_matrix_n[3][3]\n",
    "True_Negatives_1=cnf_matrix_n[0][0]+cnf_matrix_n[0][2]+cnf_matrix_n[0][3]+cnf_matrix_n[2][0]+cnf_matrix_n[2][2]+cnf_matrix_n[2][3]+cnf_matrix_n[3][0]+cnf_matrix_n[3][2]+cnf_matrix_n[3][3]\n",
    "True_Negatives_2=cnf_matrix_n[0][0]+cnf_matrix_n[0][1]+cnf_matrix_n[0][3]+cnf_matrix_n[2][0]+cnf_matrix_n[2][1]+cnf_matrix_n[2][3]+cnf_matrix_n[3][0]+cnf_matrix_n[3][1]+cnf_matrix_n[3][3]\n",
    "True_Negatives_3=cnf_matrix_n[0][0]+cnf_matrix_n[0][1]+cnf_matrix_n[0][2]+cnf_matrix_n[1][0]+cnf_matrix_n[1][1]+cnf_matrix_n[1][2]+cnf_matrix_n[2][0]+cnf_matrix_n[2][1]+cnf_matrix_n[2][2]\n",
    "#False Positive\n",
    "False_Positive_0=cnf_matrix_n[1][0]+cnf_matrix_n[2][0]+cnf_matrix_n[3][0]\n",
    "False_Positive_1=cnf_matrix_n[0][1]+cnf_matrix_n[2][1]+cnf_matrix_n[3][1]\n",
    "False_Positive_2=cnf_matrix_n[0][2]+cnf_matrix_n[1][2]+cnf_matrix_n[3][2]\n",
    "False_Positive_3=cnf_matrix_n[0][3]+cnf_matrix_n[1][3]+cnf_matrix_n[2][3]\n",
    "#False Negative\n",
    "False_Negative_0=cnf_matrix_n[0][1]+cnf_matrix_n[0][2]+cnf_matrix_n[0][3]\n",
    "False_Negative_1=cnf_matrix_n[1][0]+cnf_matrix_n[1][2]+cnf_matrix_n[1][3]\n",
    "False_Negative_2=cnf_matrix_n[2][0]+cnf_matrix_n[2][1]+cnf_matrix_n[2][3]\n",
    "False_Negative_3=cnf_matrix_n[3][0]+cnf_matrix_n[3][1]+cnf_matrix_n[3][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_specificity_0/test_sensitivity_0: 0.8/0.34   test_specificity_1/test_sensitivity_1: 0.83/0.52   test_specificity_2/test_sensitivity_2: 0.87/0.33   test_specificity_3/test_sensitivity_3: 0.77/0.87\n"
     ]
    }
   ],
   "source": [
    "print(\"test_specificity_0/test_sensitivity_0: \" + specificity(True_Negatives_0,False_Positive_0) +\"/\" +sensitivity(True_Positive_0,False_Negative_0)+\n",
    "     \"   test_specificity_1/test_sensitivity_1: \" + specificity(True_Negatives_1,False_Positive_1) +\"/\" +sensitivity(True_Positive_1,False_Negative_1)+\n",
    "    \"   test_specificity_2/test_sensitivity_2: \" + specificity(True_Negatives_2,False_Positive_2) +\"/\" +sensitivity(True_Positive_2,False_Negative_2)+\n",
    "     \"   test_specificity_3/test_sensitivity_3: \" + specificity(True_Negatives_3,False_Positive_3) +\"/\" +sensitivity(True_Positive_1,False_Negative_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not_Related\n",
      "Specificity: 0.8\n",
      "Sensitivity: 0.34\n",
      "1\n",
      "Specificity: 0.83\n",
      "Sensitivity: 0.52\n",
      "2\n",
      "Specificity: 0.87\n",
      "Sensitivity: 0.33\n",
      "3\n",
      "Specificity: 0.77\n",
      "Sensitivity: 0.79\n"
     ]
    }
   ],
   "source": [
    "print(class_names[0])\n",
    "print(\"Specificity: \"+ specificity(True_Negatives_0,False_Positive_0))\n",
    "print(\"Sensitivity: \"+sensitivity(True_Positive_0,False_Negative_0))\n",
    "\n",
    "print(class_names[1])\n",
    "print(\"Specificity: \"+specificity(True_Negatives_1,False_Positive_1))\n",
    "print(\"Sensitivity: \"+sensitivity(True_Positive_1,False_Negative_1))\n",
    "\n",
    "print(class_names[2])\n",
    "print(\"Specificity: \"+specificity(True_Negatives_2,False_Positive_2))\n",
    "print(\"Sensitivity: \"+sensitivity(True_Positive_2,False_Negative_2))\n",
    "\n",
    "print(class_names[3])\n",
    "print(\"Specificity: \"+specificity(True_Negatives_3,False_Positive_3))\n",
    "print(\"Sensitivity: \"+sensitivity(True_Positive_3,False_Negative_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 427.9555555555555, 'Predicted label')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "cnf_matrix_n = metrics.confusion_matrix(clabels[cidx_valid], cout[cidx_valid])\n",
    "#class_names=[\"Not_Related\", \"Related\"]\n",
    "class_names=[\"Not_Related\", \"1\",\"2\",\"3\"] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "sns.heatmap(cnf_matrix_n, annot=True, cmap=\"YlGnBu\" ,fmt='d', xticklabels=class_names, yticklabels=class_names)\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Graph Sage Upsample')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#True Positive\n",
    "True_Positive_0=cnf_matrix_n[0][0]\n",
    "True_Positive_1=cnf_matrix_n[1][1]\n",
    "True_Positive_2=cnf_matrix_n[2][2]\n",
    "True_Positive_3=cnf_matrix_n[3][3]\n",
    "#True Negative\n",
    "True_Negatives_0=cnf_matrix_n[1][1]+cnf_matrix_n[1][2]+cnf_matrix_n[1][3]+cnf_matrix_n[2][1]+cnf_matrix_n[2][2]+cnf_matrix_n[2][3]+cnf_matrix_n[3][1]+cnf_matrix_n[3][2]+cnf_matrix_n[3][3]\n",
    "True_Negatives_1=cnf_matrix_n[0][0]+cnf_matrix_n[0][2]+cnf_matrix_n[0][3]+cnf_matrix_n[2][0]+cnf_matrix_n[2][2]+cnf_matrix_n[2][3]+cnf_matrix_n[3][0]+cnf_matrix_n[3][2]+cnf_matrix_n[3][3]\n",
    "True_Negatives_2=cnf_matrix_n[0][0]+cnf_matrix_n[0][1]+cnf_matrix_n[0][3]+cnf_matrix_n[2][0]+cnf_matrix_n[2][1]+cnf_matrix_n[2][3]+cnf_matrix_n[3][0]+cnf_matrix_n[3][1]+cnf_matrix_n[3][3]\n",
    "True_Negatives_3=cnf_matrix_n[0][0]+cnf_matrix_n[0][1]+cnf_matrix_n[0][2]+cnf_matrix_n[1][0]+cnf_matrix_n[1][1]+cnf_matrix_n[1][2]+cnf_matrix_n[2][0]+cnf_matrix_n[2][1]+cnf_matrix_n[2][2]\n",
    "#False Positive\n",
    "False_Positive_0=cnf_matrix_n[1][0]+cnf_matrix_n[2][0]+cnf_matrix_n[3][0]\n",
    "False_Positive_1=cnf_matrix_n[0][1]+cnf_matrix_n[2][1]+cnf_matrix_n[3][1]\n",
    "False_Positive_2=cnf_matrix_n[0][2]+cnf_matrix_n[1][2]+cnf_matrix_n[3][2]\n",
    "False_Positive_3=cnf_matrix_n[0][3]+cnf_matrix_n[1][3]+cnf_matrix_n[2][3]\n",
    "#False Negative\n",
    "False_Negative_0=cnf_matrix_n[0][1]+cnf_matrix_n[0][2]+cnf_matrix_n[0][3]\n",
    "False_Negative_1=cnf_matrix_n[1][0]+cnf_matrix_n[1][2]+cnf_matrix_n[1][3]\n",
    "False_Negative_2=cnf_matrix_n[2][0]+cnf_matrix_n[2][1]+cnf_matrix_n[2][3]\n",
    "False_Negative_3=cnf_matrix_n[3][0]+cnf_matrix_n[3][1]+cnf_matrix_n[3][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not_Related\n",
      "Specificity: 0.8\n",
      "Sensitivity: 0.38\n",
      "1\n",
      "Specificity: 0.88\n",
      "Sensitivity: 0.51\n",
      "2\n",
      "Specificity: 0.88\n",
      "Sensitivity: 0.31\n",
      "3\n",
      "Specificity: 0.77\n",
      "Sensitivity: 0.8\n"
     ]
    }
   ],
   "source": [
    "print(class_names[0])\n",
    "print(\"Specificity: \"+ specificity(True_Negatives_0,False_Positive_0))\n",
    "print(\"Sensitivity: \"+sensitivity(True_Positive_0,False_Negative_0))\n",
    "\n",
    "print(class_names[1])\n",
    "print(\"Specificity: \"+specificity(True_Negatives_1,False_Positive_1))\n",
    "print(\"Sensitivity: \"+sensitivity(True_Positive_1,False_Negative_1))\n",
    "\n",
    "print(class_names[2])\n",
    "print(\"Specificity: \"+specificity(True_Negatives_2,False_Positive_2))\n",
    "print(\"Sensitivity: \"+sensitivity(True_Positive_2,False_Negative_2))\n",
    "\n",
    "print(class_names[3])\n",
    "print(\"Specificity: \"+specificity(True_Negatives_3,False_Positive_3))\n",
    "print(\"Sensitivity: \"+sensitivity(True_Positive_3,False_Negative_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_specificity_0/valid_sensitivity_0: 0.8/0.38   valid_specificity_1/valid_sensitivity_1: 0.88/0.51   valid_specificity_2/valid_sensitivity_2: 0.88/0.31   valid_specificity_3/valid_sensitivity_3: 0.77/0.72\n"
     ]
    }
   ],
   "source": [
    "print(\"valid_specificity_0/valid_sensitivity_0: \" + specificity(True_Negatives_0,False_Positive_0) +\"/\" +sensitivity(True_Positive_0,False_Negative_0)+\n",
    "     \"   valid_specificity_1/valid_sensitivity_1: \" + specificity(True_Negatives_1,False_Positive_1) +\"/\" +sensitivity(True_Positive_1,False_Negative_1)+\n",
    "    \"   valid_specificity_2/valid_sensitivity_2: \" + specificity(True_Negatives_2,False_Positive_2) +\"/\" +sensitivity(True_Positive_2,False_Negative_2)+\n",
    "     \"   valid_specificity_3/valid_sensitivity_3: \" + specificity(True_Negatives_3,False_Positive_3) +\"/\" +sensitivity(True_Positive_1,False_Negative_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels=clabels[cidx_test]\n",
    "test_output=cout[cidx_test]\n",
    "valid_labels=clabels[cidx_valid]\n",
    "valid_output=cout[cidx_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_enc=np.eye(4)[test_labels]\n",
    "test_output_enc=np.eye(4)[test_output]\n",
    "valid_labels_enc=np.eye(4)[valid_labels]\n",
    "valid_output_enc=np.eye(4)[valid_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6557531426669697"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(test_labels_enc, test_output_enc,multi_class='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(valid_labels_enc, valid_output_enc,multi_class='ovr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
