{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "import ipdb\n",
    "from scipy.io import loadmat\n",
    "import networkx as nx\n",
    "import multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from copy import deepcopy\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_arti(labels, c_train_num):\n",
    "    #labels: n-dim Longtensor, each element in [0,...,m-1].\n",
    "    #cora: m=7\n",
    "    num_classes = len(set(labels.tolist()))\n",
    "    c_idxs = [] # class-wise index\n",
    "    train_idx = []\n",
    "    val_idx = []\n",
    "    test_idx = []\n",
    "    c_num_mat = np.zeros((num_classes,3)).astype(int)\n",
    "    c_num_mat[:,1] = 25\n",
    "    c_num_mat[:,2] = 55\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        c_idx = (labels==i).nonzero()[:,-1].tolist()\n",
    "        print('{:d}-th class sample number: {:d}'.format(i,len(c_idx)))\n",
    "        random.shuffle(c_idx)\n",
    "        c_idxs.append(c_idx)\n",
    "\n",
    "        train_idx = train_idx + c_idx[:c_train_num[i]]\n",
    "        c_num_mat[i,0] = c_train_num[i]\n",
    "\n",
    "        val_idx = val_idx + c_idx[c_train_num[i]:(c_train_num[i]+int(c_train_num[i]*.2))]\n",
    "        test_idx = test_idx + c_idx[int(c_train_num[i]+(c_train_num[i]*.2)):]\n",
    "\n",
    "    random.shuffle(train_idx)\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "\n",
    "    train_idx = torch.LongTensor(train_idx)\n",
    "    val_idx = torch.LongTensor(val_idx)\n",
    "    test_idx = torch.LongTensor(test_idx)\n",
    "    #c_num_mat = torch.LongTensor(c_num_mat)\n",
    "\n",
    "\n",
    "    return train_idx, val_idx, test_idx, c_num_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_upsample(adj,features,labels,idx_train, portion=1.0, im_class_num=3):\n",
    "    c_largest = labels.max().item()\n",
    "    adj_back = adj\n",
    "    chosen = None\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    avg_number = int(idx_train.shape[0]/(c_largest+1))\n",
    "\n",
    "    for i in range(im_class_num):\n",
    "        new_chosen = idx_train[(labels==(c_largest-i))[idx_train]]\n",
    "        if portion == 0:#refers to even distribution\n",
    "            c_portion = int(avg_number/new_chosen.shape[0])\n",
    "\n",
    "            for j in range(c_portion):\n",
    "                if chosen is None:\n",
    "                    chosen = new_chosen\n",
    "                else:\n",
    "                    chosen = torch.cat((chosen, new_chosen), 0)\n",
    "\n",
    "        else:\n",
    "            c_portion = int(portion)\n",
    "            portion_rest = portion-c_portion\n",
    "            for j in range(c_portion):\n",
    "                num = int(new_chosen.shape[0])\n",
    "                new_chosen = new_chosen[:num]\n",
    "\n",
    "                if chosen is None:\n",
    "                    chosen = new_chosen\n",
    "                else:\n",
    "                    chosen = torch.cat((chosen, new_chosen), 0)\n",
    "            \n",
    "            num = int(new_chosen.shape[0]*portion_rest)\n",
    "            new_chosen = new_chosen[:num]\n",
    "\n",
    "            if chosen is None:\n",
    "                chosen = new_chosen\n",
    "            else:\n",
    "                chosen = torch.cat((chosen, new_chosen), 0)\n",
    "            \n",
    "\n",
    "    add_num = chosen.shape[0]\n",
    "    new_adj = adj_back.new(torch.Size((adj_back.shape[0]+add_num, adj_back.shape[0]+add_num)))\n",
    "    new_adj[:adj_back.shape[0], :adj_back.shape[0]] = adj_back[:,:]\n",
    "    new_adj[adj_back.shape[0]:, :adj_back.shape[0]] = adj_back[chosen,:]\n",
    "    new_adj[:adj_back.shape[0], adj_back.shape[0]:] = adj_back[:,chosen]\n",
    "    new_adj[adj_back.shape[0]:, adj_back.shape[0]:] = adj_back[chosen,:][:,chosen]\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    features_append = deepcopy(features[chosen,:])\n",
    "    labels_append = deepcopy(labels[chosen])\n",
    "    idx_new = np.arange(adj_back.shape[0], adj_back.shape[0]+add_num)\n",
    "    idx_train_append = idx_train.new(idx_new)\n",
    "\n",
    "    features = torch.cat((features,features_append), 0)\n",
    "    labels = torch.cat((labels,labels_append), 0)\n",
    "    idx_train = torch.cat((idx_train,idx_train_append), 0)\n",
    "    adj = new_adj\n",
    "\n",
    "    return adj, features, labels, idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_smote(adj,features,labels,idx_train, portion=1.0, im_class_num=3):\n",
    "    c_largest = labels.max().item()\n",
    "    adj_back = adj\n",
    "    chosen = None\n",
    "    new_features = None\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    avg_number = int(idx_train.shape[0]/(c_largest+1))\n",
    "\n",
    "    for i in range(im_class_num):\n",
    "        new_chosen = idx_train[(labels==(c_largest-i))[idx_train]]\n",
    "        if portion == 0:#refers to even distribution\n",
    "            c_portion = int(avg_number/new_chosen.shape[0])\n",
    "\n",
    "            portion_rest = (avg_number/new_chosen.shape[0]) - c_portion\n",
    "\n",
    "        else:\n",
    "            c_portion = int(portion)\n",
    "            portion_rest = portion-c_portion\n",
    "            \n",
    "        for j in range(c_portion):\n",
    "            num = int(new_chosen.shape[0])\n",
    "            new_chosen = new_chosen[:num]\n",
    "\n",
    "            chosen_embed = features[new_chosen,:]\n",
    "            distance = squareform(pdist(chosen_embed.detach()))\n",
    "            np.fill_diagonal(distance,distance.max()+100)\n",
    "\n",
    "            idx_neighbor = distance.argmin(axis=-1)\n",
    "            \n",
    "            interp_place = random.random()\n",
    "            embed = chosen_embed + (chosen_embed[idx_neighbor,:]-chosen_embed)*interp_place\n",
    "\n",
    "            if chosen is None:\n",
    "                chosen = new_chosen\n",
    "                new_features = embed\n",
    "            else:\n",
    "                chosen = torch.cat((chosen, new_chosen), 0)\n",
    "                new_features = torch.cat((new_features, embed),0)\n",
    "            \n",
    "        num = int(new_chosen.shape[0]*portion_rest)\n",
    "        new_chosen = new_chosen[:num]\n",
    "\n",
    "        chosen_embed = features[new_chosen,:]\n",
    "        distance = squareform(pdist(chosen_embed.detach()))\n",
    "        np.fill_diagonal(distance,distance.max()+100)\n",
    "\n",
    "        idx_neighbor = distance.argmin(axis=-1)\n",
    "            \n",
    "        interp_place = random.random()\n",
    "        embed = chosen_embed + (chosen_embed[idx_neighbor,:]-chosen_embed)*interp_place\n",
    "\n",
    "        if chosen is None:\n",
    "            chosen = new_chosen\n",
    "            new_features = embed\n",
    "        else:\n",
    "            chosen = torch.cat((chosen, new_chosen), 0)\n",
    "            new_features = torch.cat((new_features, embed),0)\n",
    "            \n",
    "\n",
    "    add_num = chosen.shape[0]\n",
    "    new_adj = adj_back.new(torch.Size((adj_back.shape[0]+add_num, adj_back.shape[0]+add_num)))\n",
    "    new_adj[:adj_back.shape[0], :adj_back.shape[0]] = adj_back[:,:]\n",
    "    new_adj[adj_back.shape[0]:, :adj_back.shape[0]] = adj_back[chosen,:]\n",
    "    new_adj[:adj_back.shape[0], adj_back.shape[0]:] = adj_back[:,chosen]\n",
    "    new_adj[adj_back.shape[0]:, adj_back.shape[0]:] = adj_back[chosen,:][:,chosen]\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "    features_append = deepcopy(new_features)\n",
    "    labels_append = deepcopy(labels[chosen])\n",
    "    idx_new = np.arange(adj_back.shape[0], adj_back.shape[0]+add_num)\n",
    "    idx_train_append = idx_train.new(idx_new)\n",
    "\n",
    "    features = torch.cat((features,features_append), 0)\n",
    "    labels = torch.cat((labels,labels_append), 0)\n",
    "    idx_train = torch.cat((idx_train,idx_train_append), 0)\n",
    "    adj = new_adj\n",
    "\n",
    "    return adj, features, labels, idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ffn(hidden_units, dropout_rate, name=None):\n",
    "    fnn_layers = []\n",
    "\n",
    "    for units in hidden_units:\n",
    "        fnn_layers.append(layers.BatchNormalization())\n",
    "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
    "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
    "\n",
    "    return keras.Sequential(fnn_layers, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        #for 3_D batch, need a loop!!!\n",
    "\n",
    "\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "#Multihead attention layer\n",
    "class MultiHead(Module):#currently, allowed for only one sample each time. As no padding mask is required.\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        num_heads,\n",
    "        kdim=None,\n",
    "        vdim=None,\n",
    "        embed_dim = 128,#should equal num_heads*head dim\n",
    "        v_embed_dim = None,\n",
    "        dropout=0.1,\n",
    "        bias=True,\n",
    "    ):\n",
    "        super(MultiHead, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.kdim = kdim if kdim is not None else input_dim\n",
    "        self.vdim = vdim if vdim is not None else input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.v_embed_dim = v_embed_dim if v_embed_dim is not None else embed_dim\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.bias = bias\n",
    "        assert (\n",
    "            self.head_dim * num_heads == self.embed_dim\n",
    "        ), \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        assert self.v_embed_dim % num_heads ==0, \"v_embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "\n",
    "\n",
    "        self.q_proj = nn.Linear(self.input_dim, self.embed_dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(self.kdim, self.embed_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(self.vdim, self.v_embed_dim, bias=bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.v_embed_dim, self.v_embed_dim//self.num_heads, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if True:\n",
    "            # Empirically observed the convergence to be much better with\n",
    "            # the scaled initialization\n",
    "            nn.init.normal_(self.k_proj.weight)\n",
    "            nn.init.normal_(self.v_proj.weight)\n",
    "            nn.init.normal_(self.q_proj.weight)\n",
    "        else:\n",
    "            nn.init.normal_(self.k_proj.weight)\n",
    "            nn.init.normal_(self.v_proj.weight)\n",
    "            nn.init.normal_(self.q_proj.weight)\n",
    "\n",
    "        nn.init.normal_(self.out_proj.weight)\n",
    "\n",
    "        if self.out_proj.bias is not None:\n",
    "            nn.init.constant_(self.out_proj.bias, 0.)\n",
    "\n",
    "        if self.bias:\n",
    "            nn.init.constant_(self.k_proj.bias, 0.)\n",
    "            nn.init.constant_(self.v_proj.bias, 0.)\n",
    "            nn.init.constant_(self.q_proj.bias, 0.)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query,\n",
    "        key,\n",
    "        value,\n",
    "        need_weights: bool = False,\n",
    "        need_head_weights: bool = False,\n",
    "    ):\n",
    "        \"\"\"Input shape: Time x Batch x Channel\n",
    "        Args:\n",
    "            need_weights (bool, optional): return the attention weights,\n",
    "                averaged over heads (default: False).\n",
    "            need_head_weights (bool, optional): return the attention\n",
    "                weights for each head. Implies *need_weights*. Default:\n",
    "                return the average attention weights over all heads.\n",
    "        \"\"\"\n",
    "        if need_head_weights:\n",
    "            need_weights = True\n",
    "\n",
    "        batch_num, node_num, input_dim = query.size()\n",
    "\n",
    "        assert key is not None and value is not None\n",
    "\n",
    "        #project input\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(key)\n",
    "        v = self.v_proj(value)\n",
    "        q = q * self.scaling\n",
    "\n",
    "        #compute attention\n",
    "        q = q.view(batch_num, node_num, self.num_heads, self.head_dim).transpose(-2,-3).contiguous().view(batch_num*self.num_heads, node_num, self.head_dim)\n",
    "        k = k.view(batch_num, node_num, self.num_heads, self.head_dim).transpose(-2,-3).contiguous().view(batch_num*self.num_heads, node_num, self.head_dim)\n",
    "        v = v.view(batch_num, node_num, self.num_heads, self.vdim).transpose(-2,-3).contiguous().view(batch_num*self.num_heads, node_num, self.vdim)\n",
    "        attn_output_weights = torch.bmm(q, k.transpose(-1,-2))\n",
    "        attn_output_weights = F.softmax(attn_output_weights, dim=-1)\n",
    "\n",
    "        #drop out\n",
    "        attn_output_weights = F.dropout(attn_output_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        #collect output\n",
    "        attn_output = torch.bmm(attn_output_weights, v)\n",
    "        attn_output = attn_output.view(batch_num, self.num_heads, node_num, self.vdim).transpose(-2,-3).contiguous().view(batch_num, node_num, self.v_embed_dim)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "\n",
    "        if need_weights:\n",
    "            attn_output_weights = attn_output_weights #view: (batch_num, num_heads, node_num, node_num)\n",
    "            return attn_output, attn_output_weights.sum(dim=1) / self.num_heads\n",
    "        else:\n",
    "            return attn_output\n",
    "\n",
    "\n",
    "#Graphsage layer\n",
    "class SageConv(Module):\n",
    "    \"\"\"\n",
    "    Simple Graphsage layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super(SageConv, self).__init__()\n",
    "\n",
    "        self.proj = nn.Linear(in_features*2, out_features, bias=bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "        #print(\"note: for dense graph in graphsage, require it normalized.\")\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        nn.init.normal_(self.proj.weight)\n",
    "\n",
    "        if self.proj.bias is not None:\n",
    "            nn.init.constant_(self.proj.bias, 0.)\n",
    "\n",
    "    def forward(self, features, adj):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            adj: can be sparse or dense matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        #fuse info from neighbors. to be added:\n",
    "        if not isinstance(adj, torch.sparse.FloatTensor):\n",
    "            if len(adj.shape) == 3:\n",
    "                neigh_feature = torch.bmm(adj, features) / (adj.sum(dim=1).reshape((adj.shape[0], adj.shape[1],-1))+1)\n",
    "            else:\n",
    "                neigh_feature = torch.mm(adj, features) / (adj.sum(dim=1).reshape(adj.shape[0], -1)+1)\n",
    "        else:\n",
    "            #print(\"spmm not implemented for batch training. Note!\")\n",
    "            \n",
    "            neigh_feature = torch.spmm(adj, features) / (adj.to_dense().sum(dim=1).reshape(adj.shape[0], -1)+1)\n",
    "\n",
    "        #perform conv\n",
    "        data = torch.cat([features,neigh_feature], dim=-1)\n",
    "        combined = self.proj(data)\n",
    "\n",
    "        return combined\n",
    "\n",
    "#GraphAT layers\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        if isinstance(adj, torch.sparse.FloatTensor):\n",
    "            adj = adj.to_dense()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        N = h.size()[0]\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b\n",
    "\n",
    "\n",
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)\n",
    "\n",
    "    \n",
    "class SpGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(SpGraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
    "                \n",
    "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm = SpecialSpmm()\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        dv = 'cuda' if input.is_cuda else 'cpu'\n",
    "\n",
    "        N = input.size()[0]\n",
    "        edge = adj.nonzero().t()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        # h: N x out\n",
    "        assert not torch.isnan(h).any()\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
    "        # edge: 2*D x E\n",
    "\n",
    "        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "\n",
    "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n",
    "        # e_rowsum: N x 1\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "        \n",
    "        h_prime = h_prime.div(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------\n",
    "### models ###\n",
    "#--------------\n",
    "\n",
    "#gcn_encode\n",
    "class GCN_En(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(GCN_En, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GCN_En2(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(GCN_En2, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc2(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "class GCN_Classifier(nn.Module):\n",
    "    def __init__(self, nembed, nhid, nclass, dropout):\n",
    "        super(GCN_Classifier, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nembed, nhid)\n",
    "        self.mlp = nn.Linear(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "#sage model\n",
    "\n",
    "class Sage_En(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(Sage_En, self).__init__()\n",
    "\n",
    "        self.sage1 = SageConv(nfeat, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.sage1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "class Sage_En2(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout):\n",
    "        super(Sage_En2, self).__init__()\n",
    "\n",
    "        self.sage1 = SageConv(nfeat, nhid)\n",
    "        self.sage2 = SageConv(nhid, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.sage1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.relu(self.sage2(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Sage_Classifier(nn.Module):\n",
    "    def __init__(self, nembed, nhid, nclass, dropout):\n",
    "        super(Sage_Classifier, self).__init__()\n",
    "\n",
    "        self.sage1 = SageConv(nembed, nhid)\n",
    "        self.mlp = nn.Linear(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.sage1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "#GAT model\n",
    "\n",
    "class GAT_En(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout, alpha=0.2, nheads=8):\n",
    "        super(GAT_En, self).__init__()\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_proj = nn.Linear(nhid * nheads, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.out_proj.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_proj(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class GAT_En2(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nembed, dropout, alpha=0.2, nheads=8):\n",
    "        super(GAT_En2, self).__init__()\n",
    "\n",
    "        \n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_proj = nn.Linear(nhid * nheads, nembed)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions_2 = [GraphAttentionLayer(nembed, nembed, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions_2):\n",
    "            self.add_module('attention2_{}'.format(i), attention)\n",
    "\n",
    "        self.out_proj_2 = nn.Linear(nembed * nheads, nembed)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.out_proj.weight,std=0.05)\n",
    "        nn.init.normal_(self.out_proj_2.weight,std=0.05)\n",
    "\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_proj(x))\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions_2], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_proj_2(x))\n",
    "        return x\n",
    "\n",
    "class GAT_Classifier(nn.Module):\n",
    "    def __init__(self, nembed, nhid, nclass, dropout, alpha=0.2, nheads=8):\n",
    "        super(GAT_Classifier, self).__init__()\n",
    "\n",
    "        \n",
    "        self.attentions = [GraphAttentionLayer(nembed, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_proj = nn.Linear(nhid * nheads, nhid)\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.mlp = nn.Linear(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp.weight,std=0.05)\n",
    "        nn.init.normal_(self.out_proj.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_proj(x))\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, nembed, nhid, nclass, dropout):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.mlp = nn.Linear(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.mlp.weight,std=0.05)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(Module):\n",
    "    \"\"\"\n",
    "    Simple Graphsage layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nembed, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.de_weight = Parameter(torch.FloatTensor(nembed, nembed))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.de_weight.size(1))\n",
    "        self.de_weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "    def forward(self, node_embed):\n",
    "        \n",
    "        combine = F.linear(node_embed, self.de_weight)\n",
    "        adj_out = torch.sigmoid(torch.mm(combine, combine.transpose(-1,-2)))\n",
    "\n",
    "        return adj_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_genuine(labels):\n",
    "    #labels: n-dim Longtensor, each element in [0,...,m-1].\n",
    "    #cora: m=7\n",
    "    num_classes = len(set(labels.tolist()))\n",
    "    c_idxs = [] # class-wise index\n",
    "    train_idx = []\n",
    "    val_idx = []\n",
    "    test_idx = []\n",
    "    c_num_mat = np.zeros((num_classes,3)).astype(int)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        c_idx = (labels==i).nonzero()[:,-1].tolist()\n",
    "        c_num = len(c_idx)\n",
    "        print('{:d}-th class sample number: {:d}'.format(i,len(c_idx)))\n",
    "        random.shuffle(c_idx)\n",
    "        c_idxs.append(c_idx)\n",
    "\n",
    "        if c_num <4:\n",
    "            if c_num < 3:\n",
    "                print(\"too small class type\")\n",
    "                ipdb.set_trace()\n",
    "            c_num_mat[i,0] = 1\n",
    "            c_num_mat[i,1] = 1\n",
    "            c_num_mat[i,2] = 1\n",
    "        else:\n",
    "            c_num_mat[i,0] = int(c_num/4)\n",
    "            c_num_mat[i,1] = int(c_num/4)\n",
    "            c_num_mat[i,2] = int(c_num/2)\n",
    "\n",
    "\n",
    "        train_idx = train_idx + c_idx[:c_num_mat[i,0]]\n",
    "\n",
    "        val_idx = val_idx + c_idx[c_num_mat[i,0]:c_num_mat[i,0]+c_num_mat[i,1]]\n",
    "        test_idx = test_idx + c_idx[c_num_mat[i,0]+c_num_mat[i,1]:c_num_mat[i,0]+c_num_mat[i,1]+c_num_mat[i,2]]\n",
    "\n",
    "    random.shuffle(train_idx)\n",
    "\n",
    "    #ipdb.set_trace()\n",
    "\n",
    "    train_idx = torch.LongTensor(train_idx)\n",
    "    val_idx = torch.LongTensor(val_idx)\n",
    "    test_idx = torch.LongTensor(test_idx)\n",
    "    #c_num_mat = torch.LongTensor(c_num_mat)\n",
    "\n",
    "\n",
    "    return train_idx, val_idx, test_idx, c_num_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"interactions_smote.csv\")\n",
    "data['edge']=data['Gene Symbol']+',' +data['Interactor Symbol']\n",
    "Graphtype = nx.Graph()\n",
    "data['edge']=data['edge'].astype(str)\n",
    "g = nx.parse_edgelist(data['edge'], delimiter=',', create_using=Graphtype,)\n",
    "adj=nx.adjacency_matrix(g,weight=None)\n",
    "adj=adj.toarray()\n",
    "#node_features = np.loadtxt('node_features_smote.txt')\n",
    "node_features= np.ones((adj.shape[0],1))\n",
    "#labels = np.loadtxt('Multi-Labels.txt')\n",
    "labels = np.loadtxt('Labels.txt')\n",
    "#labels = np.loadtxt('syndromic-Labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=torch.LongTensor(labels)\n",
    "features=torch.LongTensor(node_features)\n",
    "adj=torch.LongTensor(adj)\n",
    "class_sample_num = 6000\n",
    "c_train_num = []\n",
    "for i in range(labels.max().item() + 1):\n",
    "    if i > labels.max().item()-1: #only imbalance the last classes\n",
    "        c_train_num.append(int(class_sample_num))\n",
    "\n",
    "    else:\n",
    "        c_train_num.append(class_sample_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 813\n",
      "-----------------------------------\n",
      "1\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 1626\n",
      "-----------------------------------\n",
      "2\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 3252\n",
      "-----------------------------------\n",
      "3\n",
      "-----------------------------------\n",
      "0-th class sample number: 11403\n",
      "1-th class sample number: 6504\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "#for i in range(6):\n",
    "#for i in range(7):\n",
    "        print(i)\n",
    "        print(\"-----------------------------------\")\n",
    "        idx_train, idx_val, idx_test, class_num_mat= split_arti(labels, c_train_num)\n",
    "        #adj,features,labels,idx_train = src_upsample(adj,features,labels,idx_train,portion=1, im_class_num=3)\n",
    "        adj,features,labels,idx_train = src_upsample(adj,features,labels,idx_train,portion=1, im_class_num=1)\n",
    "        print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th class sample number: 11403\n",
      "1-th class sample number: 12504\n"
     ]
    }
   ],
   "source": [
    "idx_train, idx_val, idx_test, class_num_mat= split_arti(labels, c_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = GCN_En(nfeat=features.shape[1],\n",
    "        nhid=128,\n",
    "        nembed=128,\n",
    "        dropout=0)\n",
    "classifier = GCN_Classifier(nembed=128, \n",
    "        nhid=128, \n",
    "        nclass=labels.max().item() + 1, \n",
    "        dropout=0)\n",
    "decoder = Decoder(nembed=128,\n",
    "        dropout=0)\n",
    "\n",
    "\n",
    "optimizer_en = optim.AdamW(encoder.parameters(),\n",
    "                       lr=0.0005,weight_decay=5e-4,eps=1e-04)\n",
    "optimizer_cls = optim.AdamW(classifier.parameters(),\n",
    "                       lr=0.0005,weight_decay=5e-4,eps=1e-04)\n",
    "optimizer_de = optim.AdamW(decoder.parameters(),\n",
    "                       lr=0.0005,weight_decay=5e-4,eps=1e-04)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_upsample(embed, labels, idx_train, adj=None, portion=1.0, im_class_num=3):\n",
    "    c_largest = labels.max().item()\n",
    "    avg_number = int(idx_train.shape[0]/(c_largest+1))\n",
    "    #ipdb.set_trace()\n",
    "    adj_new = None\n",
    "\n",
    "    for i in range(im_class_num):\n",
    "        chosen = idx_train[(labels==(c_largest-i))[idx_train]]\n",
    "        num = int(chosen.shape[0]*portion)\n",
    "        if portion == 0:\n",
    "            c_portion = int(avg_number/chosen.shape[0])\n",
    "            num = chosen.shape[0]\n",
    "        else:\n",
    "            c_portion = 1\n",
    "\n",
    "        for j in range(c_portion):\n",
    "            chosen = chosen[:num]\n",
    "\n",
    "            chosen_embed = embed[chosen,:]\n",
    "            distance = squareform(pdist(chosen_embed.detach()))\n",
    "            np.fill_diagonal(distance,distance.max()+100)\n",
    "\n",
    "            idx_neighbor = distance.argmin(axis=-1)\n",
    "            \n",
    "            interp_place = random.random()\n",
    "            new_embed = embed[chosen,:] + (chosen_embed[idx_neighbor[:],:]-embed[chosen,:])*interp_place\n",
    "\n",
    "\n",
    "            new_labels = labels.new(torch.Size((chosen.shape[0],1))).reshape(-1).fill_(c_largest-i)\n",
    "            idx_new = np.arange(embed.shape[0], embed.shape[0]+chosen.shape[0])\n",
    "            idx_train_append = idx_train.new(idx_new)\n",
    "\n",
    "            embed = torch.cat((embed,new_embed), 0)\n",
    "            labels = torch.cat((labels,new_labels), 0)\n",
    "            idx_train = torch.cat((idx_train,idx_train_append), 0)\n",
    "\n",
    "            if adj is not None:\n",
    "                if adj_new is None:\n",
    "                    adj_new = adj.new(torch.clamp(adj[chosen,:] + adj[idx_neighbor,:], min=0.0, max = 1.0))\n",
    "                else:\n",
    "                    temp = adj.new(torch.clamp(adj[chosen,:] + adj[idx_neighbor,:], min=0.0, max = 1.0))\n",
    "                    adj_new = torch.cat((adj_new, temp), 0)\n",
    "\n",
    "    if adj is not None:\n",
    "        add_num = adj_new.shape[0]\n",
    "        new_adj = adj.new(torch.Size((adj.shape[0]+add_num, adj.shape[0]+add_num))).fill_(0.0)\n",
    "        new_adj[:adj.shape[0], :adj.shape[0]] = adj[:,:]\n",
    "        new_adj[adj.shape[0]:, :adj.shape[0]] = adj_new[:,:]\n",
    "        new_adj[:adj.shape[0], adj.shape[0]:] = torch.transpose(adj_new, 0, 1)[:,:]\n",
    "\n",
    "        return embed, labels, idx_train, new_adj.detach()\n",
    "\n",
    "    else:\n",
    "        return embed, labels, idx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "    decoder.train()\n",
    "    optimizer_en.zero_grad()\n",
    "    optimizer_cls.zero_grad()\n",
    "    optimizer_de.zero_grad()\n",
    "\n",
    "    embed = encoder(features, adj)\n",
    "\n",
    "    #perform SMOTE in embedding space\n",
    "    labels_new = labels\n",
    "    idx_train_new = idx_train\n",
    "    adj_new = adj\n",
    "\n",
    "   \n",
    "    #ipdb.set_trace()\n",
    "    output = classifier(embed, adj_new)\n",
    "    output_log=torch.nn.functional.log_softmax(output)\n",
    "    loss_train = F.cross_entropy(output_log[idx_train_new], labels_new[idx_train_new])\n",
    "    \n",
    "    acc_train = accuracy(output_log[idx_train], labels_new[idx_train])\n",
    "    loss = loss_train\n",
    "    loss_rec = loss_train\n",
    "    loss.backward()\n",
    "    optimizer_en.step()\n",
    "\n",
    "    optimizer_cls.step()\n",
    "    loss_val = F.cross_entropy(output_log[idx_val], labels[idx_val])\n",
    "    \n",
    "    acc_val = accuracy(output_log[idx_val], labels[idx_val])\n",
    "\n",
    "    print('Epoch: {:05d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'loss_rec: {:.4f}'.format(loss_rec.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test(epoch = 0):\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    decoder.eval()\n",
    "    embed = encoder(features, adj)\n",
    "    output = classifier(embed, adj)\n",
    "    output_log=torch.nn.functional.log_softmax(output)\n",
    "    loss_test = F.cross_entropy(output_log[idx_test], labels[idx_test])\n",
    "    \n",
    "    acc_test = accuracy(output_log[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "  \n",
    "\n",
    "    '''\n",
    "    if epoch==40:\n",
    "        torch\n",
    "    '''\n",
    "    return output\n",
    "\n",
    "\n",
    "def save_model(epoch):\n",
    "    saved_content = {}\n",
    "\n",
    "    saved_content['encoder'] = encoder.state_dict()\n",
    "    saved_content['decoder'] = decoder.state_dict()\n",
    "    saved_content['classifier'] = classifier.state_dict()\n",
    "\n",
    "    torch.save(saved_content, 'model_checkpoint.pth')\n",
    "\n",
    "    return\n",
    "\n",
    "def load_model(filename):\n",
    "    loaded_content = torch.load('checkpoint/{}/{}.pth')\n",
    "\n",
    "    encoder.load_state_dict(loaded_content['encoder'])\n",
    "    decoder.load_state_dict(loaded_content['decoder'])\n",
    "    classifier.load_state_dict(loaded_content['classifier'])\n",
    "\n",
    "    print(\"successfully loaded: \"+ filename)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.cuda()\n",
    "classifier = classifier.cuda()\n",
    "decoder = decoder.cuda()\n",
    "labels = labels.cuda()\n",
    "idx_train = idx_train.cuda()\n",
    "idx_val = idx_val.cuda()\n",
    "idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kyle\\AppData\\Local\\Temp\\ipykernel_83308\\4161052455.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  adj = torch.tensor(adj, dtype=torch.float)\n",
      "C:\\Users\\Kyle\\AppData\\Local\\Temp\\ipykernel_83308\\4161052455.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  features = torch.tensor(features, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "adj = torch.tensor(adj, dtype=torch.float)\n",
    "features = torch.tensor(features, dtype=torch.float)\n",
    "features = features.cuda()\n",
    "adj = adj.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kyle\\AppData\\Local\\Temp\\ipykernel_83308\\343820884.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output_log=torch.nn.functional.log_softmax(output)\n",
      "C:\\Users\\Kyle\\AppData\\Local\\Temp\\ipykernel_83308\\343820884.py:49: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output_log=torch.nn.functional.log_softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00001 loss_train: 424.8289 loss_rec: 424.8289 acc_train: 0.4992 loss_val: 431.2959 acc_val: 0.5000 time: 0.6586s\n",
      "Test set results: loss= 222.4456 accuracy= 0.4421\n",
      "Epoch: 00002 loss_train: 195.9108 loss_rec: 195.9108 acc_train: 0.5000 loss_val: 194.4079 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 00003 loss_train: 166.6815 loss_rec: 166.6815 acc_train: 0.5000 loss_val: 165.3843 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 00004 loss_train: 77.5822 loss_rec: 77.5822 acc_train: 0.4936 loss_val: 78.7621 acc_val: 0.4967 time: 0.0278s\n",
      "Epoch: 00005 loss_train: 60.2277 loss_rec: 60.2277 acc_train: 0.4899 loss_val: 61.1435 acc_val: 0.4942 time: 0.0258s\n",
      "Epoch: 00006 loss_train: 107.9614 loss_rec: 107.9614 acc_train: 0.5000 loss_val: 107.0632 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 00007 loss_train: 110.2623 loss_rec: 110.2623 acc_train: 0.5000 loss_val: 109.3448 acc_val: 0.5000 time: 0.0258s\n",
      "Epoch: 00008 loss_train: 3.0473 loss_rec: 3.0473 acc_train: 0.4808 loss_val: 3.0948 acc_val: 0.4871 time: 0.0308s\n",
      "Epoch: 00009 loss_train: 12.6197 loss_rec: 12.6197 acc_train: 0.5216 loss_val: 12.4639 acc_val: 0.5238 time: 0.0268s\n",
      "Epoch: 00010 loss_train: 74.0730 loss_rec: 74.0730 acc_train: 0.4936 loss_val: 75.1992 acc_val: 0.4967 time: 0.0263s\n",
      "Epoch: 00011 loss_train: 47.3075 loss_rec: 47.3075 acc_train: 0.4899 loss_val: 48.0265 acc_val: 0.4942 time: 0.0258s\n",
      "Test set results: loss= 76.1413 accuracy= 0.4408\n",
      "Epoch: 00012 loss_train: 67.0739 loss_rec: 67.0739 acc_train: 0.4987 loss_val: 66.4409 acc_val: 0.4979 time: 0.0253s\n",
      "Epoch: 00013 loss_train: 80.9479 loss_rec: 80.9479 acc_train: 0.4991 loss_val: 80.2151 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 00014 loss_train: 13.0941 loss_rec: 13.0941 acc_train: 0.5220 loss_val: 12.9327 acc_val: 0.5238 time: 0.0273s\n",
      "Epoch: 00015 loss_train: 128.0662 loss_rec: 128.0662 acc_train: 0.4961 loss_val: 130.0142 acc_val: 0.4983 time: 0.0268s\n",
      "Epoch: 00016 loss_train: 169.7125 loss_rec: 169.7125 acc_train: 0.4961 loss_val: 172.2943 acc_val: 0.4983 time: 0.0283s\n",
      "Epoch: 00017 loss_train: 126.1087 loss_rec: 126.1087 acc_train: 0.4961 loss_val: 128.0271 acc_val: 0.4983 time: 0.0263s\n",
      "Epoch: 00018 loss_train: 10.0543 loss_rec: 10.0543 acc_train: 0.4867 loss_val: 10.2087 acc_val: 0.4933 time: 0.0248s\n",
      "Epoch: 00019 loss_train: 160.2890 loss_rec: 160.2890 acc_train: 0.4997 loss_val: 158.9867 acc_val: 0.4996 time: 0.0253s\n",
      "Epoch: 00020 loss_train: 242.5329 loss_rec: 242.5329 acc_train: 0.5000 loss_val: 240.6564 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 00021 loss_train: 248.1021 loss_rec: 248.1021 acc_train: 0.5000 loss_val: 246.1833 acc_val: 0.5000 time: 0.0273s\n",
      "Test set results: loss= 212.0459 accuracy= 0.4420\n",
      "Epoch: 00022 loss_train: 186.7567 loss_rec: 186.7567 acc_train: 0.4999 loss_val: 185.2613 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 00023 loss_train: 66.7859 loss_rec: 66.7859 acc_train: 0.4987 loss_val: 66.1216 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 00024 loss_train: 109.0231 loss_rec: 109.0231 acc_train: 0.4949 loss_val: 110.6811 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 00025 loss_train: 198.5300 loss_rec: 198.5300 acc_train: 0.4977 loss_val: 201.5505 acc_val: 0.4983 time: 0.0268s\n",
      "Epoch: 00026 loss_train: 208.8636 loss_rec: 208.8636 acc_train: 0.4977 loss_val: 212.0416 acc_val: 0.4983 time: 0.0258s\n",
      "Epoch: 00027 loss_train: 149.4112 loss_rec: 149.4112 acc_train: 0.4961 loss_val: 151.6839 acc_val: 0.4983 time: 0.0268s\n",
      "Epoch: 00028 loss_train: 28.6104 loss_rec: 28.6104 acc_train: 0.4885 loss_val: 29.0451 acc_val: 0.4942 time: 0.0258s\n",
      "Epoch: 00029 loss_train: 139.3358 loss_rec: 139.3358 acc_train: 0.4992 loss_val: 138.1484 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 00030 loss_train: 227.7619 loss_rec: 227.7619 acc_train: 0.5000 loss_val: 225.9563 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 00031 loss_train: 246.2806 loss_rec: 246.2806 acc_train: 0.5000 loss_val: 244.3432 acc_val: 0.5000 time: 0.0263s\n",
      "Test set results: loss= 230.0105 accuracy= 0.4421\n",
      "Epoch: 00032 loss_train: 202.5795 loss_rec: 202.5795 acc_train: 0.5000 loss_val: 200.9427 acc_val: 0.5000 time: 0.0288s\n",
      "Epoch: 00033 loss_train: 103.4671 loss_rec: 103.4671 acc_train: 0.4987 loss_val: 102.5162 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 00034 loss_train: 46.4609 loss_rec: 46.4609 acc_train: 0.4910 loss_val: 47.1667 acc_val: 0.4950 time: 0.0333s\n",
      "Epoch: 00035 loss_train: 120.5233 loss_rec: 120.5233 acc_train: 0.4961 loss_val: 122.3561 acc_val: 0.4983 time: 0.0323s\n",
      "Epoch: 00036 loss_train: 122.1433 loss_rec: 122.1433 acc_train: 0.4961 loss_val: 124.0008 acc_val: 0.4983 time: 0.0377s\n",
      "Epoch: 00037 loss_train: 59.2577 loss_rec: 59.2577 acc_train: 0.4927 loss_val: 60.1580 acc_val: 0.4971 time: 0.0353s\n",
      "Epoch: 00038 loss_train: 57.7267 loss_rec: 57.7267 acc_train: 0.4981 loss_val: 57.0791 acc_val: 0.4988 time: 0.0323s\n",
      "Epoch: 00039 loss_train: 102.6394 loss_rec: 102.6394 acc_train: 0.4987 loss_val: 101.6758 acc_val: 0.4979 time: 0.0353s\n",
      "Epoch: 00040 loss_train: 84.8217 loss_rec: 84.8217 acc_train: 0.4983 loss_val: 83.9792 acc_val: 0.4979 time: 0.0338s\n",
      "Epoch: 00041 loss_train: 11.1425 loss_rec: 11.1425 acc_train: 0.5317 loss_val: 11.0043 acc_val: 0.5383 time: 0.0338s\n",
      "Test set results: loss= 104.8629 accuracy= 0.5539\n",
      "Epoch: 00042 loss_train: 116.7625 loss_rec: 116.7625 acc_train: 0.4961 loss_val: 118.5379 acc_val: 0.4983 time: 0.0318s\n",
      "Epoch: 00043 loss_train: 169.1317 loss_rec: 169.1317 acc_train: 0.4961 loss_val: 171.7045 acc_val: 0.4983 time: 0.0243s\n",
      "Epoch: 00044 loss_train: 153.0291 loss_rec: 153.0291 acc_train: 0.4961 loss_val: 155.3568 acc_val: 0.4983 time: 0.0283s\n",
      "Epoch: 00045 loss_train: 75.8370 loss_rec: 75.8370 acc_train: 0.4942 loss_val: 76.9893 acc_val: 0.4971 time: 0.0338s\n",
      "Epoch: 00046 loss_train: 52.8945 loss_rec: 52.8945 acc_train: 0.5008 loss_val: 52.2741 acc_val: 0.4988 time: 0.0392s\n",
      "Epoch: 00047 loss_train: 109.0516 loss_rec: 109.0516 acc_train: 0.4987 loss_val: 108.0339 acc_val: 0.4979 time: 0.0372s\n",
      "Epoch: 00048 loss_train: 102.5713 loss_rec: 102.5713 acc_train: 0.4985 loss_val: 101.5930 acc_val: 0.4979 time: 0.0372s\n",
      "Epoch: 00049 loss_train: 39.8676 loss_rec: 39.8676 acc_train: 0.5095 loss_val: 39.3279 acc_val: 0.5104 time: 0.0377s\n",
      "Epoch: 00050 loss_train: 75.7121 loss_rec: 75.7121 acc_train: 0.4942 loss_val: 76.8625 acc_val: 0.4971 time: 0.0363s\n",
      "Epoch: 00051 loss_train: 119.7791 loss_rec: 119.7791 acc_train: 0.4961 loss_val: 121.6003 acc_val: 0.4983 time: 0.0258s\n",
      "Test set results: loss= 87.4637 accuracy= 0.5525\n",
      "Epoch: 00052 loss_train: 97.3919 loss_rec: 97.3919 acc_train: 0.4949 loss_val: 98.8723 acc_val: 0.4975 time: 0.0263s\n",
      "Epoch: 00053 loss_train: 15.5284 loss_rec: 15.5284 acc_train: 0.4897 loss_val: 15.7644 acc_val: 0.4950 time: 0.0298s\n",
      "Epoch: 00054 loss_train: 113.6073 loss_rec: 113.6073 acc_train: 0.4985 loss_val: 112.5242 acc_val: 0.4979 time: 0.0268s\n",
      "Epoch: 00055 loss_train: 173.4701 loss_rec: 173.4701 acc_train: 0.4987 loss_val: 171.9682 acc_val: 0.4979 time: 0.0258s\n",
      "Epoch: 00056 loss_train: 171.0499 loss_rec: 171.0499 acc_train: 0.4987 loss_val: 169.5630 acc_val: 0.4979 time: 0.0268s\n",
      "Epoch: 00057 loss_train: 112.7504 loss_rec: 112.7504 acc_train: 0.4984 loss_val: 111.6650 acc_val: 0.4979 time: 0.0268s\n",
      "Epoch: 00058 loss_train: 4.8636 loss_rec: 4.8636 acc_train: 0.5472 loss_val: 4.8055 acc_val: 0.5538 time: 0.0328s\n",
      "Epoch: 00059 loss_train: 153.2982 loss_rec: 153.2982 acc_train: 0.4961 loss_val: 155.6296 acc_val: 0.4983 time: 0.0387s\n",
      "Epoch: 00060 loss_train: 234.5177 loss_rec: 234.5177 acc_train: 0.4982 loss_val: 238.0860 acc_val: 0.4988 time: 0.0363s\n",
      "Epoch: 00061 loss_train: 246.2407 loss_rec: 246.2407 acc_train: 0.4982 loss_val: 249.9873 acc_val: 0.4988 time: 0.0368s\n",
      "Test set results: loss= 175.6790 accuracy= 0.5546\n",
      "Epoch: 00062 loss_train: 195.6112 loss_rec: 195.6112 acc_train: 0.4977 loss_val: 198.5869 acc_val: 0.4983 time: 0.0378s\n",
      "Epoch: 00063 loss_train: 89.2323 loss_rec: 89.2323 acc_train: 0.4949 loss_val: 90.5882 acc_val: 0.4975 time: 0.0358s\n",
      "Epoch: 00064 loss_train: 63.1958 loss_rec: 63.1958 acc_train: 0.5008 loss_val: 62.4378 acc_val: 0.5004 time: 0.0258s\n",
      "Epoch: 00065 loss_train: 141.9487 loss_rec: 141.9487 acc_train: 0.4987 loss_val: 140.6386 acc_val: 0.4979 time: 0.0268s\n",
      "Epoch: 00066 loss_train: 157.4932 loss_rec: 157.4932 acc_train: 0.4987 loss_val: 156.0730 acc_val: 0.4979 time: 0.0279s\n",
      "Epoch: 00067 loss_train: 116.2106 loss_rec: 116.2106 acc_train: 0.4982 loss_val: 115.0745 acc_val: 0.4979 time: 0.0257s\n",
      "Epoch: 00068 loss_train: 24.2028 loss_rec: 24.2028 acc_train: 0.5267 loss_val: 23.8979 acc_val: 0.5350 time: 0.0273s\n",
      "Epoch: 00069 loss_train: 117.6283 loss_rec: 117.6283 acc_train: 0.4961 loss_val: 119.4165 acc_val: 0.4983 time: 0.0313s\n",
      "Epoch: 00070 loss_train: 186.5403 loss_rec: 186.5403 acc_train: 0.4977 loss_val: 189.3774 acc_val: 0.4983 time: 0.0397s\n",
      "Epoch: 00071 loss_train: 188.2660 loss_rec: 188.2660 acc_train: 0.4977 loss_val: 191.1295 acc_val: 0.4983 time: 0.0372s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 116.5755 accuracy= 0.5539\n",
      "Epoch: 00072 loss_train: 129.8073 loss_rec: 129.8073 acc_train: 0.4961 loss_val: 131.7806 acc_val: 0.4983 time: 0.0363s\n",
      "Epoch: 00073 loss_train: 17.4773 loss_rec: 17.4773 acc_train: 0.4903 loss_val: 17.7423 acc_val: 0.4954 time: 0.0343s\n",
      "Epoch: 00074 loss_train: 135.8453 loss_rec: 135.8453 acc_train: 0.4984 loss_val: 134.5553 acc_val: 0.4979 time: 0.0258s\n",
      "Epoch: 00075 loss_train: 218.1856 loss_rec: 218.1856 acc_train: 0.4987 loss_val: 216.3256 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 00076 loss_train: 238.1050 loss_rec: 238.1050 acc_train: 0.4987 loss_val: 236.1039 acc_val: 0.4979 time: 0.0263s\n",
      "Epoch: 00077 loss_train: 202.2255 loss_rec: 202.2255 acc_train: 0.4987 loss_val: 200.4708 acc_val: 0.4979 time: 0.0263s\n",
      "Epoch: 00078 loss_train: 116.3788 loss_rec: 116.3788 acc_train: 0.4982 loss_val: 115.2184 acc_val: 0.4983 time: 0.0264s\n",
      "Epoch: 00079 loss_train: 13.9177 loss_rec: 13.9177 acc_train: 0.4903 loss_val: 14.1290 acc_val: 0.4954 time: 0.0288s\n",
      "Epoch: 00080 loss_train: 78.2353 loss_rec: 78.2353 acc_train: 0.4949 loss_val: 79.4236 acc_val: 0.4975 time: 0.0268s\n",
      "Epoch: 00081 loss_train: 77.6542 loss_rec: 77.6542 acc_train: 0.4949 loss_val: 78.8336 acc_val: 0.4975 time: 0.0273s\n",
      "Test set results: loss= 16.9616 accuracy= 0.5479\n",
      "Epoch: 00082 loss_train: 18.8984 loss_rec: 18.8984 acc_train: 0.4909 loss_val: 19.1849 acc_val: 0.4958 time: 0.0397s\n",
      "Epoch: 00083 loss_train: 86.9430 loss_rec: 86.9430 acc_train: 0.5001 loss_val: 85.9802 acc_val: 0.4992 time: 0.0382s\n",
      "Epoch: 00084 loss_train: 129.1512 loss_rec: 129.1512 acc_train: 0.4984 loss_val: 127.8897 acc_val: 0.4983 time: 0.0382s\n",
      "Epoch: 00085 loss_train: 114.1170 loss_rec: 114.1170 acc_train: 0.4988 loss_val: 112.9556 acc_val: 0.4996 time: 0.0298s\n",
      "Epoch: 00086 loss_train: 47.6989 loss_rec: 47.6989 acc_train: 0.5134 loss_val: 47.0675 acc_val: 0.5167 time: 0.0273s\n",
      "Epoch: 00087 loss_train: 66.6378 loss_rec: 66.6378 acc_train: 0.4949 loss_val: 67.6495 acc_val: 0.4975 time: 0.0253s\n",
      "Epoch: 00088 loss_train: 114.5219 loss_rec: 114.5219 acc_train: 0.4961 loss_val: 116.2623 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 00089 loss_train: 99.8156 loss_rec: 99.8156 acc_train: 0.4956 loss_val: 101.3322 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 00090 loss_train: 28.9861 loss_rec: 28.9861 acc_train: 0.4922 loss_val: 29.4253 acc_val: 0.4958 time: 0.0268s\n",
      "Epoch: 00091 loss_train: 87.0284 loss_rec: 87.0284 acc_train: 0.5001 loss_val: 86.0362 acc_val: 0.4983 time: 0.0293s\n",
      "Test set results: loss= 157.4305 accuracy= 0.4408\n",
      "Epoch: 00092 loss_train: 138.6755 loss_rec: 138.6755 acc_train: 0.4984 loss_val: 137.3212 acc_val: 0.4983 time: 0.0417s\n",
      "Epoch: 00093 loss_train: 132.5870 loss_rec: 132.5870 acc_train: 0.4982 loss_val: 131.2717 acc_val: 0.4983 time: 0.0343s\n",
      "Epoch: 00094 loss_train: 74.7593 loss_rec: 74.7593 acc_train: 0.5031 loss_val: 73.8450 acc_val: 0.5013 time: 0.0382s\n",
      "Epoch: 00095 loss_train: 29.9122 loss_rec: 29.9122 acc_train: 0.4927 loss_val: 30.3653 acc_val: 0.4971 time: 0.0353s\n",
      "Epoch: 00096 loss_train: 70.8396 loss_rec: 70.8396 acc_train: 0.4949 loss_val: 71.9150 acc_val: 0.4975 time: 0.0372s\n",
      "Epoch: 00097 loss_train: 50.4688 loss_rec: 50.4688 acc_train: 0.4927 loss_val: 51.2342 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 00098 loss_train: 23.0663 loss_rec: 23.0663 acc_train: 0.5331 loss_val: 22.7807 acc_val: 0.5421 time: 0.0263s\n",
      "Epoch: 00099 loss_train: 36.5164 loss_rec: 36.5164 acc_train: 0.5244 loss_val: 36.0496 acc_val: 0.5317 time: 0.0273s\n",
      "Epoch: 00100 loss_train: 1.9980 loss_rec: 1.9980 acc_train: 0.4886 loss_val: 2.0255 acc_val: 0.4933 time: 0.0273s\n",
      "Epoch: 00101 loss_train: 14.6945 loss_rec: 14.6945 acc_train: 0.5375 loss_val: 14.5053 acc_val: 0.5446 time: 0.0253s\n",
      "Test set results: loss= 18.9973 accuracy= 0.5486\n",
      "Epoch: 00102 loss_train: 21.1677 loss_rec: 21.1677 acc_train: 0.4917 loss_val: 21.4878 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 00103 loss_train: 2.1600 loss_rec: 2.1600 acc_train: 0.5585 loss_val: 2.1453 acc_val: 0.5613 time: 0.0273s\n",
      "Epoch: 00104 loss_train: 25.9681 loss_rec: 25.9681 acc_train: 0.4922 loss_val: 26.3612 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 00105 loss_train: 4.8963 loss_rec: 4.8963 acc_train: 0.5500 loss_val: 4.8429 acc_val: 0.5546 time: 0.0273s\n",
      "Epoch: 00106 loss_train: 18.5946 loss_rec: 18.5946 acc_train: 0.4909 loss_val: 18.8759 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 00107 loss_train: 15.3462 loss_rec: 15.3462 acc_train: 0.5384 loss_val: 15.1494 acc_val: 0.5450 time: 0.0288s\n",
      "Epoch: 00108 loss_train: 4.8280 loss_rec: 4.8280 acc_train: 0.4892 loss_val: 4.9008 acc_val: 0.4938 time: 0.0268s\n",
      "Epoch: 00109 loss_train: 30.3285 loss_rec: 30.3285 acc_train: 0.5282 loss_val: 29.9463 acc_val: 0.5367 time: 0.0258s\n",
      "Epoch: 00110 loss_train: 11.4912 loss_rec: 11.4912 acc_train: 0.5432 loss_val: 11.3400 acc_val: 0.5463 time: 0.0273s\n",
      "Epoch: 00111 loss_train: 58.5464 loss_rec: 58.5464 acc_train: 0.4949 loss_val: 59.4346 acc_val: 0.4975 time: 0.0258s\n",
      "Test set results: loss= 58.9491 accuracy= 0.5525\n",
      "Epoch: 00112 loss_train: 65.6512 loss_rec: 65.6512 acc_train: 0.4949 loss_val: 66.6473 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 00113 loss_train: 15.1487 loss_rec: 15.1487 acc_train: 0.4909 loss_val: 15.3779 acc_val: 0.4958 time: 0.0407s\n",
      "Epoch: 00114 loss_train: 82.0619 loss_rec: 82.0619 acc_train: 0.5041 loss_val: 81.0565 acc_val: 0.5017 time: 0.0368s\n",
      "Epoch: 00115 loss_train: 118.0510 loss_rec: 118.0510 acc_train: 0.4979 loss_val: 116.7925 acc_val: 0.4992 time: 0.0353s\n",
      "Epoch: 00116 loss_train: 98.5167 loss_rec: 98.5167 acc_train: 0.5003 loss_val: 97.3925 acc_val: 0.4992 time: 0.0377s\n",
      "Epoch: 00117 loss_train: 29.3594 loss_rec: 29.3594 acc_train: 0.5292 loss_val: 28.9911 acc_val: 0.5392 time: 0.0313s\n",
      "Epoch: 00118 loss_train: 87.4082 loss_rec: 87.4082 acc_train: 0.4949 loss_val: 88.7355 acc_val: 0.4975 time: 0.0258s\n",
      "Epoch: 00119 loss_train: 137.7213 loss_rec: 137.7213 acc_train: 0.4961 loss_val: 139.8146 acc_val: 0.4983 time: 0.0253s\n",
      "Epoch: 00120 loss_train: 126.1627 loss_rec: 126.1627 acc_train: 0.4961 loss_val: 128.0802 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 00121 loss_train: 59.0719 loss_rec: 59.0719 acc_train: 0.4949 loss_val: 59.9680 acc_val: 0.4975 time: 0.0273s\n",
      "Test set results: loss= 61.4402 accuracy= 0.4616\n",
      "Epoch: 00122 loss_train: 54.1237 loss_rec: 54.1237 acc_train: 0.5184 loss_val: 53.4182 acc_val: 0.5242 time: 0.0263s\n",
      "Epoch: 00123 loss_train: 103.0949 loss_rec: 103.0949 acc_train: 0.5004 loss_val: 101.9280 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00124 loss_train: 95.5597 loss_rec: 95.5597 acc_train: 0.5008 loss_val: 94.4412 acc_val: 0.5004 time: 0.0293s\n",
      "Epoch: 00125 loss_train: 37.3455 loss_rec: 37.3455 acc_train: 0.5265 loss_val: 36.8701 acc_val: 0.5317 time: 0.0402s\n",
      "Epoch: 00126 loss_train: 68.2401 loss_rec: 68.2401 acc_train: 0.4949 loss_val: 69.2756 acc_val: 0.4975 time: 0.0353s\n",
      "Epoch: 00127 loss_train: 109.2647 loss_rec: 109.2647 acc_train: 0.4961 loss_val: 110.9246 acc_val: 0.4983 time: 0.0373s\n",
      "Epoch: 00128 loss_train: 89.6965 loss_rec: 89.6965 acc_train: 0.4949 loss_val: 91.0586 acc_val: 0.4975 time: 0.0377s\n",
      "Epoch: 00129 loss_train: 15.7779 loss_rec: 15.7779 acc_train: 0.4909 loss_val: 16.0163 acc_val: 0.4958 time: 0.0333s\n",
      "Epoch: 00130 loss_train: 100.9176 loss_rec: 100.9176 acc_train: 0.5000 loss_val: 99.7473 acc_val: 0.4983 time: 0.0238s\n",
      "Epoch: 00131 loss_train: 155.0024 loss_rec: 155.0024 acc_train: 0.4987 loss_val: 153.4526 acc_val: 0.4996 time: 0.0258s\n",
      "Test set results: loss= 172.8752 accuracy= 0.4407\n",
      "Epoch: 00132 loss_train: 152.2831 loss_rec: 152.2831 acc_train: 0.4987 loss_val: 150.7495 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 00133 loss_train: 98.5726 loss_rec: 98.5726 acc_train: 0.5018 loss_val: 97.4117 acc_val: 0.5008 time: 0.0258s\n",
      "Epoch: 00134 loss_train: 0.6793 loss_rec: 0.6793 acc_train: 0.6235 loss_val: 0.6782 acc_val: 0.6096 time: 0.0273s\n",
      "Epoch: 00135 loss_train: 106.4295 loss_rec: 106.4295 acc_train: 0.4961 loss_val: 108.0464 acc_val: 0.4983 time: 0.0293s\n",
      "Epoch: 00136 loss_train: 146.1523 loss_rec: 146.1523 acc_train: 0.4977 loss_val: 148.3740 acc_val: 0.4983 time: 0.0253s\n",
      "Epoch: 00137 loss_train: 125.6459 loss_rec: 125.6459 acc_train: 0.4961 loss_val: 127.5553 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 00138 loss_train: 51.1204 loss_rec: 51.1204 acc_train: 0.4943 loss_val: 51.8952 acc_val: 0.4975 time: 0.0258s\n",
      "Epoch: 00139 loss_train: 67.3480 loss_rec: 67.3480 acc_train: 0.5132 loss_val: 66.4411 acc_val: 0.5167 time: 0.0272s\n",
      "Epoch: 00140 loss_train: 122.1094 loss_rec: 122.1094 acc_train: 0.5001 loss_val: 120.7679 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00141 loss_train: 120.2932 loss_rec: 120.2932 acc_train: 0.5001 loss_val: 118.9619 acc_val: 0.4983 time: 0.0258s\n",
      "Test set results: loss= 76.8029 accuracy= 0.4576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00142 loss_train: 67.6706 loss_rec: 67.6706 acc_train: 0.5133 loss_val: 66.7619 acc_val: 0.5179 time: 0.0293s\n",
      "Epoch: 00143 loss_train: 30.4877 loss_rec: 30.4877 acc_train: 0.4927 loss_val: 30.9488 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 00144 loss_train: 67.0686 loss_rec: 67.0686 acc_train: 0.4949 loss_val: 68.0860 acc_val: 0.4975 time: 0.0268s\n",
      "Epoch: 00145 loss_train: 44.2490 loss_rec: 44.2490 acc_train: 0.4934 loss_val: 44.9193 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 00146 loss_train: 29.4633 loss_rec: 29.4633 acc_train: 0.5317 loss_val: 29.0983 acc_val: 0.5408 time: 0.0258s\n",
      "Epoch: 00147 loss_train: 44.5730 loss_rec: 44.5730 acc_train: 0.5251 loss_val: 44.0034 acc_val: 0.5312 time: 0.0263s\n",
      "Epoch: 00148 loss_train: 8.8253 loss_rec: 8.8253 acc_train: 0.5490 loss_val: 8.7124 acc_val: 0.5525 time: 0.0273s\n",
      "Epoch: 00149 loss_train: 75.7439 loss_rec: 75.7439 acc_train: 0.4949 loss_val: 76.8933 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 00150 loss_train: 97.2046 loss_rec: 97.2046 acc_train: 0.4961 loss_val: 98.6805 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 00151 loss_train: 61.0044 loss_rec: 61.0044 acc_train: 0.4949 loss_val: 61.9293 acc_val: 0.4975 time: 0.0288s\n",
      "Test set results: loss= 28.1653 accuracy= 0.4831\n",
      "Epoch: 00152 loss_train: 24.8367 loss_rec: 24.8367 acc_train: 0.5328 loss_val: 24.5239 acc_val: 0.5383 time: 0.0258s\n",
      "Epoch: 00153 loss_train: 50.5801 loss_rec: 50.5801 acc_train: 0.5232 loss_val: 49.9305 acc_val: 0.5300 time: 0.0278s\n",
      "Epoch: 00154 loss_train: 24.3745 loss_rec: 24.3745 acc_train: 0.5347 loss_val: 24.0667 acc_val: 0.5400 time: 0.0268s\n",
      "Epoch: 00155 loss_train: 50.3005 loss_rec: 50.3005 acc_train: 0.4949 loss_val: 51.0627 acc_val: 0.4975 time: 0.0263s\n",
      "Epoch: 00156 loss_train: 64.0469 loss_rec: 64.0469 acc_train: 0.4949 loss_val: 65.0184 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 00157 loss_train: 21.2366 loss_rec: 21.2366 acc_train: 0.4927 loss_val: 21.5572 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 00158 loss_train: 67.8607 loss_rec: 67.8607 acc_train: 0.5160 loss_val: 66.9603 acc_val: 0.5221 time: 0.0278s\n",
      "Epoch: 00159 loss_train: 98.1801 loss_rec: 98.1801 acc_train: 0.5065 loss_val: 96.9678 acc_val: 0.5046 time: 0.0268s\n",
      "Epoch: 00160 loss_train: 75.1418 loss_rec: 75.1418 acc_train: 0.5123 loss_val: 74.1245 acc_val: 0.5167 time: 0.0258s\n",
      "Epoch: 00161 loss_train: 5.2697 loss_rec: 5.2697 acc_train: 0.5546 loss_val: 5.2157 acc_val: 0.5613 time: 0.0273s\n",
      "Test set results: loss= 99.9605 accuracy= 0.5539\n",
      "Epoch: 00162 loss_train: 111.3183 loss_rec: 111.3183 acc_train: 0.4961 loss_val: 113.0093 acc_val: 0.4983 time: 0.0268s\n",
      "Epoch: 00163 loss_train: 161.7068 loss_rec: 161.7068 acc_train: 0.4982 loss_val: 164.1649 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 00164 loss_train: 151.9310 loss_rec: 151.9310 acc_train: 0.4977 loss_val: 154.2402 acc_val: 0.4983 time: 0.0263s\n",
      "Epoch: 00165 loss_train: 88.1269 loss_rec: 88.1269 acc_train: 0.4961 loss_val: 89.4646 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 00166 loss_train: 22.1924 loss_rec: 22.1924 acc_train: 0.5384 loss_val: 21.9087 acc_val: 0.5442 time: 0.0303s\n",
      "Epoch: 00167 loss_train: 65.8819 loss_rec: 65.8819 acc_train: 0.5182 loss_val: 65.0160 acc_val: 0.5238 time: 0.0273s\n",
      "Epoch: 00168 loss_train: 52.6469 loss_rec: 52.6469 acc_train: 0.5242 loss_val: 51.9698 acc_val: 0.5292 time: 0.0253s\n",
      "Epoch: 00169 loss_train: 11.4089 loss_rec: 11.4089 acc_train: 0.4917 loss_val: 11.5807 acc_val: 0.4958 time: 0.0259s\n",
      "Epoch: 00170 loss_train: 11.3780 loss_rec: 11.3780 acc_train: 0.4917 loss_val: 11.5493 acc_val: 0.4958 time: 0.0277s\n",
      "Epoch: 00171 loss_train: 45.9184 loss_rec: 45.9184 acc_train: 0.5251 loss_val: 45.3318 acc_val: 0.5308 time: 0.0273s\n",
      "Test set results: loss= 47.7434 accuracy= 0.4752\n",
      "Epoch: 00172 loss_train: 42.0670 loss_rec: 42.0670 acc_train: 0.5268 loss_val: 41.5325 acc_val: 0.5346 time: 0.0273s\n",
      "Epoch: 00173 loss_train: 11.0500 loss_rec: 11.0500 acc_train: 0.4917 loss_val: 11.2165 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 00174 loss_train: 4.0234 loss_rec: 4.0234 acc_train: 0.4892 loss_val: 4.0829 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 00175 loss_train: 55.3583 loss_rec: 55.3583 acc_train: 0.5246 loss_val: 54.6453 acc_val: 0.5296 time: 0.0273s\n",
      "Epoch: 00176 loss_train: 58.2330 loss_rec: 58.2330 acc_train: 0.5246 loss_val: 57.4822 acc_val: 0.5312 time: 0.0268s\n",
      "Epoch: 00177 loss_train: 10.2213 loss_rec: 10.2213 acc_train: 0.5511 loss_val: 10.0883 acc_val: 0.5517 time: 0.0263s\n",
      "Epoch: 00178 loss_train: 87.1375 loss_rec: 87.1375 acc_train: 0.4961 loss_val: 88.4603 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 00179 loss_train: 118.7493 loss_rec: 118.7493 acc_train: 0.4961 loss_val: 120.5533 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 00180 loss_train: 90.4032 loss_rec: 90.4032 acc_train: 0.4961 loss_val: 91.7758 acc_val: 0.4983 time: 0.0258s\n",
      "Epoch: 00181 loss_train: 8.4987 loss_rec: 8.4987 acc_train: 0.4917 loss_val: 8.6264 acc_val: 0.4958 time: 0.0278s\n",
      "Test set results: loss= 129.6579 accuracy= 0.4444\n",
      "Epoch: 00182 loss_train: 114.2297 loss_rec: 114.2297 acc_train: 0.5027 loss_val: 112.8871 acc_val: 0.5013 time: 0.0288s\n",
      "Epoch: 00183 loss_train: 174.2379 loss_rec: 174.2379 acc_train: 0.4982 loss_val: 172.4702 acc_val: 0.4992 time: 0.0268s\n",
      "Epoch: 00184 loss_train: 177.1094 loss_rec: 177.1094 acc_train: 0.4982 loss_val: 175.3148 acc_val: 0.4992 time: 0.0268s\n",
      "Epoch: 00185 loss_train: 128.7508 loss_rec: 128.7508 acc_train: 0.5000 loss_val: 127.2897 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 00186 loss_train: 34.7515 loss_rec: 34.7515 acc_train: 0.5330 loss_val: 34.3166 acc_val: 0.5404 time: 0.0258s\n",
      "Epoch: 00187 loss_train: 103.8839 loss_rec: 103.8839 acc_train: 0.4961 loss_val: 105.4613 acc_val: 0.4983 time: 0.0268s\n",
      "Epoch: 00188 loss_train: 175.2649 loss_rec: 175.2649 acc_train: 0.4982 loss_val: 177.9296 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 00189 loss_train: 183.7133 loss_rec: 183.7133 acc_train: 0.4982 loss_val: 186.5065 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 00190 loss_train: 135.7108 loss_rec: 135.7108 acc_train: 0.4977 loss_val: 137.7728 acc_val: 0.4983 time: 0.0258s\n",
      "Epoch: 00191 loss_train: 37.2280 loss_rec: 37.2280 acc_train: 0.4934 loss_val: 37.7910 acc_val: 0.4975 time: 0.0268s\n",
      "Test set results: loss= 113.4490 accuracy= 0.4498\n",
      "Epoch: 00192 loss_train: 99.9609 loss_rec: 99.9609 acc_train: 0.5080 loss_val: 98.6855 acc_val: 0.5075 time: 0.0258s\n",
      "Epoch: 00193 loss_train: 172.8727 loss_rec: 172.8727 acc_train: 0.4981 loss_val: 171.0869 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00194 loss_train: 188.2558 loss_rec: 188.2558 acc_train: 0.4983 loss_val: 186.3605 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00195 loss_train: 152.0484 loss_rec: 152.0484 acc_train: 0.5002 loss_val: 150.4028 acc_val: 0.4992 time: 0.0278s\n",
      "Epoch: 00196 loss_train: 69.5952 loss_rec: 69.5952 acc_train: 0.5212 loss_val: 68.6904 acc_val: 0.5258 time: 0.0258s\n",
      "Epoch: 00197 loss_train: 55.1356 loss_rec: 55.1356 acc_train: 0.4949 loss_val: 55.9710 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 00198 loss_train: 117.0144 loss_rec: 117.0144 acc_train: 0.4969 loss_val: 118.7919 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 00199 loss_train: 118.1079 loss_rec: 118.1079 acc_train: 0.4977 loss_val: 119.9019 acc_val: 0.4983 time: 0.0268s\n",
      "Epoch: 00200 loss_train: 64.6224 loss_rec: 64.6224 acc_train: 0.4949 loss_val: 65.6021 acc_val: 0.4975 time: 0.0283s\n",
      "Epoch: 00201 loss_train: 34.7978 loss_rec: 34.7978 acc_train: 0.5338 loss_val: 34.3656 acc_val: 0.5417 time: 0.0268s\n",
      "Test set results: loss= 83.3204 accuracy= 0.4624\n",
      "Epoch: 00202 loss_train: 73.3961 loss_rec: 73.3961 acc_train: 0.5197 loss_val: 72.4349 acc_val: 0.5254 time: 0.0397s\n",
      "Epoch: 00203 loss_train: 59.4086 loss_rec: 59.4086 acc_train: 0.5247 loss_val: 58.6449 acc_val: 0.5308 time: 0.0377s\n",
      "Epoch: 00204 loss_train: 0.8896 loss_rec: 0.8896 acc_train: 0.4892 loss_val: 0.8926 acc_val: 0.4938 time: 0.0382s\n",
      "Epoch: 00205 loss_train: 26.0408 loss_rec: 26.0408 acc_train: 0.4934 loss_val: 26.4336 acc_val: 0.4975 time: 0.0338s\n",
      "Epoch: 00206 loss_train: 4.4844 loss_rec: 4.4844 acc_train: 0.5536 loss_val: 4.4456 acc_val: 0.5592 time: 0.0373s\n",
      "Epoch: 00207 loss_train: 15.7487 loss_rec: 15.7487 acc_train: 0.4929 loss_val: 15.9855 acc_val: 0.4975 time: 0.0348s\n",
      "Epoch: 00208 loss_train: 17.6891 loss_rec: 17.6891 acc_train: 0.5438 loss_val: 17.4559 acc_val: 0.5475 time: 0.0258s\n",
      "Epoch: 00209 loss_train: 0.8753 loss_rec: 0.8753 acc_train: 0.6138 loss_val: 0.8762 acc_val: 0.6125 time: 0.0273s\n",
      "Epoch: 00210 loss_train: 43.3593 loss_rec: 43.3593 acc_train: 0.4949 loss_val: 44.0155 acc_val: 0.4975 time: 0.0263s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00211 loss_train: 29.1615 loss_rec: 29.1615 acc_train: 0.4934 loss_val: 29.6017 acc_val: 0.4975 time: 0.0283s\n",
      "Test set results: loss= 39.2164 accuracy= 0.4865\n",
      "Epoch: 00212 loss_train: 34.5675 loss_rec: 34.5675 acc_train: 0.5368 loss_val: 34.1369 acc_val: 0.5429 time: 0.0273s\n",
      "Epoch: 00213 loss_train: 43.4134 loss_rec: 43.4134 acc_train: 0.5306 loss_val: 42.8643 acc_val: 0.5392 time: 0.0268s\n",
      "Epoch: 00214 loss_train: 3.9780 loss_rec: 3.9780 acc_train: 0.5552 loss_val: 3.9468 acc_val: 0.5600 time: 0.0273s\n",
      "Epoch: 00215 loss_train: 81.7440 loss_rec: 81.7440 acc_train: 0.4961 loss_val: 82.9844 acc_val: 0.4983 time: 0.0308s\n",
      "Epoch: 00216 loss_train: 105.6428 loss_rec: 105.6428 acc_train: 0.4961 loss_val: 107.2469 acc_val: 0.4983 time: 0.0402s\n",
      "Epoch: 00217 loss_train: 73.3755 loss_rec: 73.3755 acc_train: 0.4956 loss_val: 74.4883 acc_val: 0.4975 time: 0.0387s\n",
      "Epoch: 00218 loss_train: 8.3336 loss_rec: 8.3336 acc_train: 0.5498 loss_val: 8.2340 acc_val: 0.5554 time: 0.0372s\n",
      "Epoch: 00219 loss_train: 31.6794 loss_rec: 31.6794 acc_train: 0.5342 loss_val: 31.2819 acc_val: 0.5396 time: 0.0382s\n",
      "Epoch: 00220 loss_train: 5.9320 loss_rec: 5.9320 acc_train: 0.5502 loss_val: 5.8727 acc_val: 0.5546 time: 0.0283s\n",
      "Epoch: 00221 loss_train: 67.2789 loss_rec: 67.2789 acc_train: 0.4949 loss_val: 68.2990 acc_val: 0.4975 time: 0.0268s\n",
      "Test set results: loss= 72.0324 accuracy= 0.5539\n",
      "Epoch: 00222 loss_train: 80.2281 loss_rec: 80.2281 acc_train: 0.4961 loss_val: 81.4454 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 00223 loss_train: 38.3536 loss_rec: 38.3536 acc_train: 0.4934 loss_val: 38.9335 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 00224 loss_train: 49.0118 loss_rec: 49.0118 acc_train: 0.5280 loss_val: 48.3889 acc_val: 0.5358 time: 0.0293s\n",
      "Epoch: 00225 loss_train: 78.7977 loss_rec: 78.7977 acc_train: 0.5185 loss_val: 77.7607 acc_val: 0.5246 time: 0.0258s\n",
      "Epoch: 00226 loss_train: 57.4702 loss_rec: 57.4702 acc_train: 0.5257 loss_val: 56.7342 acc_val: 0.5312 time: 0.0328s\n",
      "Epoch: 00227 loss_train: 9.0654 loss_rec: 9.0654 acc_train: 0.4930 loss_val: 9.2009 acc_val: 0.4975 time: 0.0397s\n",
      "Epoch: 00228 loss_train: 19.3482 loss_rec: 19.3482 acc_train: 0.4935 loss_val: 19.6395 acc_val: 0.4975 time: 0.0373s\n",
      "Epoch: 00229 loss_train: 22.5844 loss_rec: 22.5844 acc_train: 0.5403 loss_val: 22.2912 acc_val: 0.5454 time: 0.0348s\n",
      "Epoch: 00230 loss_train: 13.0567 loss_rec: 13.0567 acc_train: 0.5494 loss_val: 12.8842 acc_val: 0.5513 time: 0.0377s\n",
      "Epoch: 00231 loss_train: 44.2084 loss_rec: 44.2084 acc_train: 0.4950 loss_val: 44.8774 acc_val: 0.4975 time: 0.0273s\n",
      "Test set results: loss= 39.2138 accuracy= 0.5526\n",
      "Epoch: 00232 loss_train: 43.6887 loss_rec: 43.6887 acc_train: 0.4950 loss_val: 44.3500 acc_val: 0.4975 time: 0.0263s\n",
      "Epoch: 00233 loss_train: 8.9137 loss_rec: 8.9137 acc_train: 0.5499 loss_val: 8.8054 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 00234 loss_train: 8.2845 loss_rec: 8.2845 acc_train: 0.5513 loss_val: 8.1869 acc_val: 0.5567 time: 0.0258s\n",
      "Epoch: 00235 loss_train: 40.2950 loss_rec: 40.2950 acc_train: 0.4944 loss_val: 40.9045 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 00236 loss_train: 31.7153 loss_rec: 31.7153 acc_train: 0.4935 loss_val: 32.1945 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 00237 loss_train: 26.7662 loss_rec: 26.7662 acc_train: 0.5390 loss_val: 26.4227 acc_val: 0.5438 time: 0.0258s\n",
      "Epoch: 00238 loss_train: 31.4921 loss_rec: 31.4921 acc_train: 0.5342 loss_val: 31.0946 acc_val: 0.5400 time: 0.0293s\n",
      "Epoch: 00239 loss_train: 11.0130 loss_rec: 11.0130 acc_train: 0.4930 loss_val: 11.1781 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 00240 loss_train: 1.3192 loss_rec: 1.3192 acc_train: 0.5910 loss_val: 1.3194 acc_val: 0.5913 time: 0.0368s\n",
      "Epoch: 00241 loss_train: 25.6157 loss_rec: 25.6157 acc_train: 0.4936 loss_val: 26.0021 acc_val: 0.4983 time: 0.0402s\n",
      "Test set results: loss= 3.1107 accuracy= 0.5315\n",
      "Epoch: 00242 loss_train: 2.7704 loss_rec: 2.7704 acc_train: 0.5649 loss_val: 2.7547 acc_val: 0.5700 time: 0.0377s\n",
      "Epoch: 00243 loss_train: 17.0495 loss_rec: 17.0495 acc_train: 0.4932 loss_val: 17.3060 acc_val: 0.4983 time: 0.0377s\n",
      "Epoch: 00244 loss_train: 16.2258 loss_rec: 16.2258 acc_train: 0.5463 loss_val: 16.0084 acc_val: 0.5508 time: 0.0308s\n",
      "Epoch: 00245 loss_train: 0.7061 loss_rec: 0.7061 acc_train: 0.6102 loss_val: 0.7057 acc_val: 0.6050 time: 0.0258s\n",
      "Epoch: 00246 loss_train: 6.7143 loss_rec: 6.7143 acc_train: 0.4932 loss_val: 6.8143 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 00247 loss_train: 37.4860 loss_rec: 37.4860 acc_train: 0.5354 loss_val: 37.0199 acc_val: 0.5429 time: 0.0273s\n",
      "Epoch: 00248 loss_train: 30.5087 loss_rec: 30.5087 acc_train: 0.5367 loss_val: 30.1221 acc_val: 0.5421 time: 0.0273s\n",
      "Epoch: 00249 loss_train: 23.3443 loss_rec: 23.3443 acc_train: 0.4937 loss_val: 23.6963 acc_val: 0.4983 time: 0.0258s\n",
      "Epoch: 00250 loss_train: 21.0320 loss_rec: 21.0320 acc_train: 0.4937 loss_val: 21.3489 acc_val: 0.4983 time: 0.0263s\n",
      "Epoch: 00251 loss_train: 31.5210 loss_rec: 31.5210 acc_train: 0.5351 loss_val: 31.1225 acc_val: 0.5404 time: 0.0273s\n",
      "Test set results: loss= 35.7238 accuracy= 0.4847\n",
      "Epoch: 00252 loss_train: 31.4980 loss_rec: 31.4980 acc_train: 0.5351 loss_val: 31.0999 acc_val: 0.5404 time: 0.0278s\n",
      "Epoch: 00253 loss_train: 15.6377 loss_rec: 15.6377 acc_train: 0.4932 loss_val: 15.8728 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 00254 loss_train: 7.3378 loss_rec: 7.3378 acc_train: 0.4932 loss_val: 7.4472 acc_val: 0.4983 time: 0.0268s\n",
      "Epoch: 00255 loss_train: 49.4324 loss_rec: 49.4324 acc_train: 0.5310 loss_val: 48.8059 acc_val: 0.5383 time: 0.0263s\n",
      "Epoch: 00256 loss_train: 53.4502 loss_rec: 53.4502 acc_train: 0.5276 loss_val: 52.7693 acc_val: 0.5346 time: 0.0278s\n",
      "Epoch: 00257 loss_train: 9.9768 loss_rec: 9.9768 acc_train: 0.5527 loss_val: 9.8516 acc_val: 0.5583 time: 0.0263s\n",
      "Epoch: 00258 loss_train: 79.5456 loss_rec: 79.5456 acc_train: 0.4964 loss_val: 80.7527 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00259 loss_train: 108.0014 loss_rec: 108.0014 acc_train: 0.4964 loss_val: 109.6417 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00260 loss_train: 80.4227 loss_rec: 80.4227 acc_train: 0.4964 loss_val: 81.6432 acc_val: 0.4992 time: 0.0263s\n",
      "Epoch: 00261 loss_train: 2.5806 loss_rec: 2.5806 acc_train: 0.4916 loss_val: 2.6166 acc_val: 0.4967 time: 0.0293s\n",
      "Test set results: loss= 127.6772 accuracy= 0.4521\n",
      "Epoch: 00262 loss_train: 112.4912 loss_rec: 112.4912 acc_train: 0.5110 loss_val: 111.0626 acc_val: 0.5104 time: 0.0273s\n",
      "Epoch: 00263 loss_train: 169.1558 loss_rec: 169.1558 acc_train: 0.5012 loss_val: 167.3128 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 00264 loss_train: 171.7325 loss_rec: 171.7325 acc_train: 0.5012 loss_val: 169.8599 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 00265 loss_train: 125.7452 loss_rec: 125.7452 acc_train: 0.5074 loss_val: 124.1915 acc_val: 0.5067 time: 0.0258s\n",
      "Epoch: 00266 loss_train: 36.6003 loss_rec: 36.6003 acc_train: 0.5328 loss_val: 36.1426 acc_val: 0.5367 time: 0.0259s\n",
      "Epoch: 00267 loss_train: 94.3155 loss_rec: 94.3155 acc_train: 0.4962 loss_val: 95.7471 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 00268 loss_train: 161.9685 loss_rec: 161.9685 acc_train: 0.4983 loss_val: 164.4304 acc_val: 0.4988 time: 0.0293s\n",
      "Epoch: 00269 loss_train: 169.9716 loss_rec: 169.9716 acc_train: 0.4983 loss_val: 172.5553 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 00270 loss_train: 124.3461 loss_rec: 124.3461 acc_train: 0.4977 loss_val: 126.2350 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 00271 loss_train: 30.6625 loss_rec: 30.6625 acc_train: 0.4935 loss_val: 31.1253 acc_val: 0.4975 time: 0.0253s\n",
      "Test set results: loss= 112.8544 accuracy= 0.4565\n",
      "Epoch: 00272 loss_train: 99.4397 loss_rec: 99.4397 acc_train: 0.5142 loss_val: 98.0874 acc_val: 0.5175 time: 0.0273s\n",
      "Epoch: 00273 loss_train: 169.0874 loss_rec: 169.0874 acc_train: 0.5014 loss_val: 167.2159 acc_val: 0.5004 time: 0.0268s\n",
      "Epoch: 00274 loss_train: 183.8541 loss_rec: 183.8541 acc_train: 0.5011 loss_val: 181.8777 acc_val: 0.5004 time: 0.0263s\n",
      "Epoch: 00275 loss_train: 149.3111 loss_rec: 149.3111 acc_train: 0.5031 loss_val: 147.5731 acc_val: 0.5025 time: 0.0273s\n",
      "Epoch: 00276 loss_train: 70.7155 loss_rec: 70.7155 acc_train: 0.5241 loss_val: 69.8049 acc_val: 0.5296 time: 0.0288s\n",
      "Epoch: 00277 loss_train: 47.6855 loss_rec: 47.6855 acc_train: 0.4950 loss_val: 48.4070 acc_val: 0.4975 time: 0.0253s\n",
      "Epoch: 00278 loss_train: 106.5291 loss_rec: 106.5291 acc_train: 0.4962 loss_val: 108.1466 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 00279 loss_train: 107.3127 loss_rec: 107.3127 acc_train: 0.4962 loss_val: 108.9420 acc_val: 0.4983 time: 0.0263s\n",
      "Epoch: 00280 loss_train: 55.9067 loss_rec: 55.9067 acc_train: 0.4950 loss_val: 56.7533 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 00281 loss_train: 38.9420 loss_rec: 38.9420 acc_train: 0.5321 loss_val: 38.4559 acc_val: 0.5358 time: 0.0273s\n",
      "Test set results: loss= 86.5795 accuracy= 0.4689\n",
      "Epoch: 00282 loss_train: 76.2624 loss_rec: 76.2624 acc_train: 0.5250 loss_val: 75.2783 acc_val: 0.5312 time: 0.0263s\n",
      "Epoch: 00283 loss_train: 63.4881 loss_rec: 63.4881 acc_train: 0.5253 loss_val: 62.6750 acc_val: 0.5308 time: 0.0258s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00284 loss_train: 6.3012 loss_rec: 6.3012 acc_train: 0.5519 loss_val: 6.2401 acc_val: 0.5554 time: 0.0293s\n",
      "Epoch: 00285 loss_train: 95.0902 loss_rec: 95.0902 acc_train: 0.4962 loss_val: 96.5334 acc_val: 0.4983 time: 0.0348s\n",
      "Epoch: 00286 loss_train: 135.2846 loss_rec: 135.2846 acc_train: 0.4983 loss_val: 137.3400 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 00287 loss_train: 119.6977 loss_rec: 119.6977 acc_train: 0.4977 loss_val: 121.5154 acc_val: 0.4983 time: 0.0323s\n",
      "Epoch: 00288 loss_train: 54.0770 loss_rec: 54.0770 acc_train: 0.4950 loss_val: 54.8958 acc_val: 0.4975 time: 0.0353s\n",
      "Epoch: 00289 loss_train: 52.2926 loss_rec: 52.2926 acc_train: 0.5304 loss_val: 51.6297 acc_val: 0.5379 time: 0.0343s\n",
      "Epoch: 00290 loss_train: 100.3651 loss_rec: 100.3651 acc_train: 0.5147 loss_val: 99.0107 acc_val: 0.5204 time: 0.0328s\n",
      "Epoch: 00291 loss_train: 97.1737 loss_rec: 97.1737 acc_train: 0.5155 loss_val: 95.8716 acc_val: 0.5212 time: 0.0358s\n",
      "Test set results: loss= 54.5495 accuracy= 0.4799\n",
      "Epoch: 00292 loss_train: 48.0679 loss_rec: 48.0679 acc_train: 0.5314 loss_val: 47.4624 acc_val: 0.5396 time: 0.0353s\n",
      "Epoch: 00293 loss_train: 43.1783 loss_rec: 43.1783 acc_train: 0.4950 loss_val: 43.8312 acc_val: 0.4975 time: 0.0348s\n",
      "Epoch: 00294 loss_train: 77.4532 loss_rec: 77.4532 acc_train: 0.4962 loss_val: 78.6278 acc_val: 0.4983 time: 0.0348s\n",
      "Epoch: 00295 loss_train: 57.0153 loss_rec: 57.0153 acc_train: 0.4950 loss_val: 57.8787 acc_val: 0.4975 time: 0.0328s\n",
      "Epoch: 00296 loss_train: 11.1907 loss_rec: 11.1907 acc_train: 0.5524 loss_val: 11.0489 acc_val: 0.5575 time: 0.0328s\n",
      "Epoch: 00297 loss_train: 25.5669 loss_rec: 25.5669 acc_train: 0.5428 loss_val: 25.2335 acc_val: 0.5475 time: 0.0363s\n",
      "Epoch: 00298 loss_train: 5.9227 loss_rec: 5.9227 acc_train: 0.4930 loss_val: 6.0099 acc_val: 0.4975 time: 0.0343s\n",
      "Epoch: 00299 loss_train: 13.1711 loss_rec: 13.1711 acc_train: 0.5488 loss_val: 12.9988 acc_val: 0.5525 time: 0.0298s\n",
      "Epoch: 00300 loss_train: 14.0578 loss_rec: 14.0578 acc_train: 0.4930 loss_val: 14.2682 acc_val: 0.4975 time: 0.0308s\n",
      "Epoch: 00301 loss_train: 9.9447 loss_rec: 9.9447 acc_train: 0.5513 loss_val: 9.8243 acc_val: 0.5558 time: 0.0258s\n",
      "Test set results: loss= 11.7742 accuracy= 0.5501\n",
      "Epoch: 00302 loss_train: 13.1420 loss_rec: 13.1420 acc_train: 0.4930 loss_val: 13.3385 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 00303 loss_train: 14.3879 loss_rec: 14.3879 acc_train: 0.5502 loss_val: 14.1981 acc_val: 0.5517 time: 0.0278s\n",
      "Epoch: 00304 loss_train: 5.1176 loss_rec: 5.1176 acc_train: 0.4912 loss_val: 5.1925 acc_val: 0.4954 time: 0.0283s\n",
      "Epoch: 00305 loss_train: 24.4391 loss_rec: 24.4391 acc_train: 0.5432 loss_val: 24.1190 acc_val: 0.5467 time: 0.0263s\n",
      "Epoch: 00306 loss_train: 6.9493 loss_rec: 6.9493 acc_train: 0.5518 loss_val: 6.8792 acc_val: 0.5550 time: 0.0273s\n",
      "Epoch: 00307 loss_train: 55.7124 loss_rec: 55.7124 acc_train: 0.4950 loss_val: 56.5560 acc_val: 0.4975 time: 0.0373s\n",
      "Epoch: 00308 loss_train: 62.0768 loss_rec: 62.0768 acc_train: 0.4950 loss_val: 63.0172 acc_val: 0.4975 time: 0.0338s\n",
      "Epoch: 00309 loss_train: 16.9246 loss_rec: 16.9246 acc_train: 0.4935 loss_val: 17.1785 acc_val: 0.4975 time: 0.0348s\n",
      "Epoch: 00310 loss_train: 69.3331 loss_rec: 69.3331 acc_train: 0.5251 loss_val: 68.4435 acc_val: 0.5317 time: 0.0328s\n",
      "Epoch: 00311 loss_train: 101.7935 loss_rec: 101.7935 acc_train: 0.5160 loss_val: 100.4265 acc_val: 0.5204 time: 0.0363s\n",
      "Test set results: loss= 96.5815 accuracy= 0.4666\n",
      "Epoch: 00312 loss_train: 85.0690 loss_rec: 85.0690 acc_train: 0.5238 loss_val: 83.9685 acc_val: 0.5308 time: 0.0283s\n",
      "Epoch: 00313 loss_train: 24.9121 loss_rec: 24.9121 acc_train: 0.5433 loss_val: 24.5858 acc_val: 0.5475 time: 0.0268s\n",
      "Epoch: 00314 loss_train: 77.5812 loss_rec: 77.5812 acc_train: 0.4962 loss_val: 78.7575 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 00315 loss_train: 121.1518 loss_rec: 121.1518 acc_train: 0.4977 loss_val: 122.9916 acc_val: 0.4983 time: 0.0258s\n",
      "Epoch: 00316 loss_train: 109.5980 loss_rec: 109.5980 acc_train: 0.4977 loss_val: 111.2619 acc_val: 0.4983 time: 0.0258s\n",
      "Epoch: 00317 loss_train: 48.4939 loss_rec: 48.4939 acc_train: 0.4950 loss_val: 49.2274 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 00318 loss_train: 52.8736 loss_rec: 52.8736 acc_train: 0.5308 loss_val: 52.2053 acc_val: 0.5400 time: 0.0283s\n",
      "Epoch: 00319 loss_train: 97.5059 loss_rec: 97.5059 acc_train: 0.5179 loss_val: 96.2124 acc_val: 0.5242 time: 0.0283s\n",
      "Epoch: 00320 loss_train: 92.0757 loss_rec: 92.0757 acc_train: 0.5222 loss_val: 90.8717 acc_val: 0.5267 time: 0.0263s\n",
      "Epoch: 00321 loss_train: 41.7587 loss_rec: 41.7587 acc_train: 0.5330 loss_val: 41.2368 acc_val: 0.5367 time: 0.0263s\n",
      "Test set results: loss= 44.8346 accuracy= 0.5528\n",
      "Epoch: 00322 loss_train: 49.9529 loss_rec: 49.9529 acc_train: 0.4951 loss_val: 50.7086 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 00323 loss_train: 85.1767 loss_rec: 85.1767 acc_train: 0.4962 loss_val: 86.4687 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00324 loss_train: 66.3843 loss_rec: 66.3843 acc_train: 0.4952 loss_val: 67.3903 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 00325 loss_train: 0.9844 loss_rec: 0.9844 acc_train: 0.6178 loss_val: 0.9874 acc_val: 0.6175 time: 0.0258s\n",
      "Epoch: 00326 loss_train: 31.3734 loss_rec: 31.3734 acc_train: 0.5396 loss_val: 30.9695 acc_val: 0.5450 time: 0.0288s\n",
      "Epoch: 00327 loss_train: 15.4406 loss_rec: 15.4406 acc_train: 0.5505 loss_val: 15.2361 acc_val: 0.5525 time: 0.0253s\n",
      "Epoch: 00328 loss_train: 44.9833 loss_rec: 44.9833 acc_train: 0.4952 loss_val: 45.6635 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 00329 loss_train: 50.5217 loss_rec: 50.5217 acc_train: 0.4952 loss_val: 51.2861 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 00330 loss_train: 5.2748 loss_rec: 5.2748 acc_train: 0.4915 loss_val: 5.3518 acc_val: 0.4963 time: 0.0253s\n",
      "Epoch: 00331 loss_train: 80.2333 loss_rec: 80.2333 acc_train: 0.5251 loss_val: 79.1983 acc_val: 0.5317 time: 0.0273s\n",
      "Test set results: loss= 127.4274 accuracy= 0.4577\n",
      "Epoch: 00332 loss_train: 112.2791 loss_rec: 112.2791 acc_train: 0.5148 loss_val: 110.7518 acc_val: 0.5204 time: 0.0259s\n",
      "Epoch: 00333 loss_train: 96.0099 loss_rec: 96.0099 acc_train: 0.5198 loss_val: 94.7479 acc_val: 0.5267 time: 0.0258s\n",
      "Epoch: 00334 loss_train: 36.4189 loss_rec: 36.4189 acc_train: 0.5367 loss_val: 35.9572 acc_val: 0.5421 time: 0.0288s\n",
      "Epoch: 00335 loss_train: 64.1186 loss_rec: 64.1186 acc_train: 0.4952 loss_val: 65.0899 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 00336 loss_train: 107.2721 loss_rec: 107.2721 acc_train: 0.4980 loss_val: 108.9007 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00337 loss_train: 96.0057 loss_rec: 96.0057 acc_train: 0.4964 loss_val: 97.4626 acc_val: 0.4992 time: 0.0258s\n",
      "Epoch: 00338 loss_train: 35.7570 loss_rec: 35.7570 acc_train: 0.4937 loss_val: 36.2969 acc_val: 0.4983 time: 0.0263s\n",
      "Epoch: 00339 loss_train: 63.5021 loss_rec: 63.5021 acc_train: 0.5286 loss_val: 62.6925 acc_val: 0.5350 time: 0.0273s\n",
      "Epoch: 00340 loss_train: 107.5067 loss_rec: 107.5067 acc_train: 0.5160 loss_val: 106.0606 acc_val: 0.5204 time: 0.0273s\n",
      "Epoch: 00341 loss_train: 102.0406 loss_rec: 102.0406 acc_train: 0.5173 loss_val: 100.6846 acc_val: 0.5238 time: 0.0273s\n",
      "Test set results: loss= 59.2677 accuracy= 0.4800\n",
      "Epoch: 00342 loss_train: 52.2242 loss_rec: 52.2242 acc_train: 0.5315 loss_val: 51.5665 acc_val: 0.5400 time: 0.0283s\n",
      "Epoch: 00343 loss_train: 38.0254 loss_rec: 38.0254 acc_train: 0.4947 loss_val: 38.5996 acc_val: 0.4983 time: 0.0258s\n",
      "Epoch: 00344 loss_train: 73.1531 loss_rec: 73.1531 acc_train: 0.4964 loss_val: 74.2620 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00345 loss_train: 54.9341 loss_rec: 54.9341 acc_train: 0.4952 loss_val: 55.7657 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 00346 loss_train: 9.9372 loss_rec: 9.9372 acc_train: 0.5529 loss_val: 9.8198 acc_val: 0.5575 time: 0.0268s\n",
      "Epoch: 00347 loss_train: 22.6624 loss_rec: 22.6624 acc_train: 0.5436 loss_val: 22.3608 acc_val: 0.5483 time: 0.0263s\n",
      "Epoch: 00348 loss_train: 9.0758 loss_rec: 9.0758 acc_train: 0.4932 loss_val: 9.2106 acc_val: 0.4983 time: 0.0268s\n",
      "Epoch: 00349 loss_train: 8.8089 loss_rec: 8.8089 acc_train: 0.5547 loss_val: 8.7109 acc_val: 0.5621 time: 0.0258s\n",
      "Epoch: 00350 loss_train: 18.5503 loss_rec: 18.5503 acc_train: 0.4937 loss_val: 18.8287 acc_val: 0.4983 time: 0.0268s\n",
      "Epoch: 00351 loss_train: 4.6577 loss_rec: 4.6577 acc_train: 0.5603 loss_val: 4.6230 acc_val: 0.5625 time: 0.0303s\n",
      "Test set results: loss= 15.5185 accuracy= 0.5513\n",
      "Epoch: 00352 loss_train: 17.3140 loss_rec: 17.3140 acc_train: 0.4937 loss_val: 17.5735 acc_val: 0.4983 time: 0.0274s\n",
      "Epoch: 00353 loss_train: 10.5064 loss_rec: 10.5064 acc_train: 0.5530 loss_val: 10.3797 acc_val: 0.5583 time: 0.0263s\n",
      "Epoch: 00354 loss_train: 7.5794 loss_rec: 7.5794 acc_train: 0.4932 loss_val: 7.6915 acc_val: 0.4983 time: 0.0268s\n",
      "Epoch: 00355 loss_train: 22.2396 loss_rec: 22.2396 acc_train: 0.5442 loss_val: 21.9430 acc_val: 0.5492 time: 0.0258s\n",
      "Epoch: 00356 loss_train: 6.1772 loss_rec: 6.1772 acc_train: 0.5543 loss_val: 6.1216 acc_val: 0.5588 time: 0.0278s\n",
      "Epoch: 00357 loss_train: 53.5696 loss_rec: 53.5696 acc_train: 0.4952 loss_val: 54.3804 acc_val: 0.4983 time: 0.0278s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00358 loss_train: 58.6029 loss_rec: 58.6029 acc_train: 0.4952 loss_val: 59.4902 acc_val: 0.4983 time: 0.0288s\n",
      "Epoch: 00359 loss_train: 13.6189 loss_rec: 13.6189 acc_train: 0.4932 loss_val: 13.8225 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 00360 loss_train: 70.9554 loss_rec: 70.9554 acc_train: 0.5275 loss_val: 70.0479 acc_val: 0.5333 time: 0.0258s\n",
      "Epoch: 00361 loss_train: 103.2689 loss_rec: 103.2689 acc_train: 0.5195 loss_val: 101.9024 acc_val: 0.5258 time: 0.0273s\n",
      "Test set results: loss= 99.6192 accuracy= 0.4692\n",
      "Epoch: 00362 loss_train: 87.7463 loss_rec: 87.7463 acc_train: 0.5252 loss_val: 86.6131 acc_val: 0.5325 time: 0.0273s\n",
      "Epoch: 00363 loss_train: 29.8354 loss_rec: 29.8354 acc_train: 0.5405 loss_val: 29.4470 acc_val: 0.5467 time: 0.0263s\n",
      "Epoch: 00364 loss_train: 68.8577 loss_rec: 68.8577 acc_train: 0.4965 loss_val: 69.9014 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 00365 loss_train: 110.7338 loss_rec: 110.7338 acc_train: 0.4980 loss_val: 112.4151 acc_val: 0.4992 time: 0.0318s\n",
      "Epoch: 00366 loss_train: 98.9908 loss_rec: 98.9908 acc_train: 0.4965 loss_val: 100.4934 acc_val: 0.4996 time: 0.0303s\n",
      "Epoch: 00367 loss_train: 39.1036 loss_rec: 39.1036 acc_train: 0.4947 loss_val: 39.6944 acc_val: 0.4988 time: 0.0318s\n",
      "Epoch: 00368 loss_train: 59.3528 loss_rec: 59.3528 acc_train: 0.5311 loss_val: 58.6008 acc_val: 0.5392 time: 0.0372s\n",
      "Epoch: 00369 loss_train: 103.1055 loss_rec: 103.1055 acc_train: 0.5195 loss_val: 101.7454 acc_val: 0.5258 time: 0.0358s\n",
      "Epoch: 00370 loss_train: 98.1029 loss_rec: 98.1029 acc_train: 0.5222 loss_val: 96.8252 acc_val: 0.5271 time: 0.0353s\n",
      "Epoch: 00371 loss_train: 49.3590 loss_rec: 49.3590 acc_train: 0.5327 loss_val: 48.7420 acc_val: 0.5408 time: 0.0343s\n",
      "Test set results: loss= 35.2719 accuracy= 0.5521\n",
      "Epoch: 00372 loss_train: 39.3086 loss_rec: 39.3086 acc_train: 0.4947 loss_val: 39.9024 acc_val: 0.4988 time: 0.0343s\n",
      "Epoch: 00373 loss_train: 73.6289 loss_rec: 73.6289 acc_train: 0.4965 loss_val: 74.7451 acc_val: 0.4996 time: 0.0323s\n",
      "Epoch: 00374 loss_train: 55.3621 loss_rec: 55.3621 acc_train: 0.4953 loss_val: 56.2004 acc_val: 0.4988 time: 0.0343s\n",
      "Epoch: 00375 loss_train: 8.9242 loss_rec: 8.9242 acc_train: 0.5557 loss_val: 8.8264 acc_val: 0.5633 time: 0.0293s\n",
      "Epoch: 00376 loss_train: 21.7199 loss_rec: 21.7199 acc_train: 0.5467 loss_val: 21.4278 acc_val: 0.5517 time: 0.0268s\n",
      "Epoch: 00377 loss_train: 9.2347 loss_rec: 9.2347 acc_train: 0.4933 loss_val: 9.3719 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 00378 loss_train: 8.7034 loss_rec: 8.7034 acc_train: 0.5563 loss_val: 8.6090 acc_val: 0.5642 time: 0.0263s\n",
      "Epoch: 00379 loss_train: 17.9243 loss_rec: 17.9243 acc_train: 0.4938 loss_val: 18.1932 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 00380 loss_train: 5.1846 loss_rec: 5.1846 acc_train: 0.5584 loss_val: 5.1445 acc_val: 0.5625 time: 0.0273s\n",
      "Epoch: 00381 loss_train: 16.3831 loss_rec: 16.3831 acc_train: 0.4938 loss_val: 16.6286 acc_val: 0.4988 time: 0.0273s\n",
      "Test set results: loss= 12.4451 accuracy= 0.5156\n",
      "Epoch: 00382 loss_train: 11.0021 loss_rec: 11.0021 acc_train: 0.5536 loss_val: 10.8692 acc_val: 0.5604 time: 0.0268s\n",
      "Epoch: 00383 loss_train: 6.7505 loss_rec: 6.7505 acc_train: 0.4933 loss_val: 6.8500 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 00384 loss_train: 22.5839 loss_rec: 22.5839 acc_train: 0.5455 loss_val: 22.2812 acc_val: 0.5500 time: 0.0258s\n",
      "Epoch: 00385 loss_train: 6.8212 loss_rec: 6.8212 acc_train: 0.5540 loss_val: 6.7578 acc_val: 0.5583 time: 0.0273s\n",
      "Epoch: 00386 loss_train: 52.0528 loss_rec: 52.0528 acc_train: 0.4953 loss_val: 52.8408 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 00387 loss_train: 57.0824 loss_rec: 57.0824 acc_train: 0.4953 loss_val: 57.9468 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 00388 loss_train: 12.7558 loss_rec: 12.7558 acc_train: 0.4933 loss_val: 12.9464 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 00389 loss_train: 81.4138 loss_rec: 81.4138 acc_train: 0.5252 loss_val: 80.3669 acc_val: 0.5317 time: 0.0273s\n",
      "Epoch: 00390 loss_train: 102.3435 loss_rec: 102.3435 acc_train: 0.5223 loss_val: 101.0055 acc_val: 0.5271 time: 0.0253s\n",
      "Epoch: 00391 loss_train: 87.0455 loss_rec: 87.0455 acc_train: 0.5247 loss_val: 85.9239 acc_val: 0.5312 time: 0.0268s\n",
      "Test set results: loss= 33.9101 accuracy= 0.4963\n",
      "Epoch: 00392 loss_train: 29.9154 loss_rec: 29.9154 acc_train: 0.5431 loss_val: 29.5247 acc_val: 0.5479 time: 0.0273s\n",
      "Epoch: 00393 loss_train: 67.3655 loss_rec: 67.3655 acc_train: 0.4953 loss_val: 68.3866 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 00394 loss_train: 108.6660 loss_rec: 108.6660 acc_train: 0.4981 loss_val: 110.3161 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 00395 loss_train: 97.1211 loss_rec: 97.1211 acc_train: 0.4965 loss_val: 98.5955 acc_val: 0.4996 time: 0.0263s\n",
      "Epoch: 00396 loss_train: 38.0578 loss_rec: 38.0578 acc_train: 0.4947 loss_val: 38.6329 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 00397 loss_train: 58.9342 loss_rec: 58.9342 acc_train: 0.5327 loss_val: 58.1891 acc_val: 0.5408 time: 0.0273s\n",
      "Epoch: 00398 loss_train: 102.0868 loss_rec: 102.0868 acc_train: 0.5223 loss_val: 100.7573 acc_val: 0.5271 time: 0.0273s\n",
      "Epoch: 00399 loss_train: 97.1277 loss_rec: 97.1277 acc_train: 0.5241 loss_val: 95.8708 acc_val: 0.5312 time: 0.0273s\n",
      "Epoch: 00400 loss_train: 49.3757 loss_rec: 49.3757 acc_train: 0.5371 loss_val: 48.7607 acc_val: 0.5442 time: 0.0263s\n",
      "Epoch: 00401 loss_train: 37.4990 loss_rec: 37.4990 acc_train: 0.4947 loss_val: 38.0657 acc_val: 0.4988 time: 0.0258s\n",
      "Test set results: loss= 63.6802 accuracy= 0.5545\n",
      "Epoch: 00402 loss_train: 70.9394 loss_rec: 70.9394 acc_train: 0.4965 loss_val: 72.0151 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 00403 loss_train: 52.5677 loss_rec: 52.5677 acc_train: 0.4953 loss_val: 53.3638 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 00404 loss_train: 10.9479 loss_rec: 10.9479 acc_train: 0.5536 loss_val: 10.8180 acc_val: 0.5604 time: 0.0253s\n",
      "Epoch: 00405 loss_train: 23.8089 loss_rec: 23.8089 acc_train: 0.5452 loss_val: 23.4908 acc_val: 0.5500 time: 0.0273s\n",
      "Epoch: 00406 loss_train: 6.3864 loss_rec: 6.3864 acc_train: 0.4933 loss_val: 6.4807 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 00407 loss_train: 11.1445 loss_rec: 11.1445 acc_train: 0.5537 loss_val: 11.0109 acc_val: 0.5604 time: 0.0278s\n",
      "Epoch: 00408 loss_train: 14.8285 loss_rec: 14.8285 acc_train: 0.4933 loss_val: 15.0509 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 00409 loss_train: 7.6578 loss_rec: 7.6578 acc_train: 0.5536 loss_val: 7.5828 acc_val: 0.5579 time: 0.0258s\n",
      "Epoch: 00410 loss_train: 14.1317 loss_rec: 14.1317 acc_train: 0.4933 loss_val: 14.3435 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 00411 loss_train: 12.0022 loss_rec: 12.0022 acc_train: 0.5523 loss_val: 11.8546 acc_val: 0.5588 time: 0.0288s\n",
      "Test set results: loss= 5.4135 accuracy= 0.5506\n",
      "Epoch: 00412 loss_train: 6.0654 loss_rec: 6.0654 acc_train: 0.4933 loss_val: 6.1548 acc_val: 0.4988 time: 0.0402s\n",
      "Epoch: 00413 loss_train: 22.1904 loss_rec: 22.1904 acc_train: 0.5480 loss_val: 21.8907 acc_val: 0.5521 time: 0.0392s\n",
      "Epoch: 00414 loss_train: 6.2107 loss_rec: 6.2107 acc_train: 0.5566 loss_val: 6.1571 acc_val: 0.5625 time: 0.0382s\n",
      "Epoch: 00415 loss_train: 52.1023 loss_rec: 52.1023 acc_train: 0.4953 loss_val: 52.8914 acc_val: 0.4988 time: 0.0377s\n",
      "Epoch: 00416 loss_train: 57.2648 loss_rec: 57.2648 acc_train: 0.4953 loss_val: 58.1325 acc_val: 0.4988 time: 0.0338s\n",
      "Epoch: 00417 loss_train: 13.7164 loss_rec: 13.7164 acc_train: 0.4937 loss_val: 13.9221 acc_val: 0.4992 time: 0.0328s\n",
      "Epoch: 00418 loss_train: 68.2328 loss_rec: 68.2328 acc_train: 0.5307 loss_val: 67.3645 acc_val: 0.5383 time: 0.0253s\n",
      "Epoch: 00419 loss_train: 99.4908 loss_rec: 99.4908 acc_train: 0.5247 loss_val: 98.2041 acc_val: 0.5317 time: 0.0273s\n",
      "Epoch: 00420 loss_train: 84.5634 loss_rec: 84.5634 acc_train: 0.5274 loss_val: 83.4765 acc_val: 0.5342 time: 0.0278s\n",
      "Epoch: 00421 loss_train: 28.4177 loss_rec: 28.4177 acc_train: 0.5437 loss_val: 28.0438 acc_val: 0.5471 time: 0.0258s\n",
      "Test set results: loss= 60.5158 accuracy= 0.5535\n",
      "Epoch: 00422 loss_train: 67.4170 loss_rec: 67.4170 acc_train: 0.4957 loss_val: 68.4392 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00423 loss_train: 108.0156 loss_rec: 108.0156 acc_train: 0.4981 loss_val: 109.6562 acc_val: 0.4996 time: 0.0283s\n",
      "Epoch: 00424 loss_train: 96.4874 loss_rec: 96.4874 acc_train: 0.4965 loss_val: 97.9524 acc_val: 0.4996 time: 0.0288s\n",
      "Epoch: 00425 loss_train: 38.0895 loss_rec: 38.0895 acc_train: 0.4952 loss_val: 38.6655 acc_val: 0.4992 time: 0.0253s\n",
      "Epoch: 00426 loss_train: 57.6661 loss_rec: 57.6661 acc_train: 0.5336 loss_val: 56.9404 acc_val: 0.5413 time: 0.0258s\n",
      "Epoch: 00427 loss_train: 100.2670 loss_rec: 100.2670 acc_train: 0.5247 loss_val: 98.9698 acc_val: 0.5317 time: 0.0278s\n",
      "Epoch: 00428 loss_train: 95.8088 loss_rec: 95.8088 acc_train: 0.5261 loss_val: 94.5720 acc_val: 0.5325 time: 0.0288s\n",
      "Epoch: 00429 loss_train: 49.0135 loss_rec: 49.0135 acc_train: 0.5342 loss_val: 48.4009 acc_val: 0.5375 time: 0.0417s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00430 loss_train: 36.2723 loss_rec: 36.2723 acc_train: 0.4942 loss_val: 36.8206 acc_val: 0.4992 time: 0.0392s\n",
      "Epoch: 00431 loss_train: 68.9325 loss_rec: 68.9325 acc_train: 0.4964 loss_val: 69.9780 acc_val: 0.4992 time: 0.0338s\n",
      "Test set results: loss= 45.3219 accuracy= 0.5535\n",
      "Epoch: 00432 loss_train: 50.5006 loss_rec: 50.5006 acc_train: 0.4957 loss_val: 51.2656 acc_val: 0.4992 time: 0.0377s\n",
      "Epoch: 00433 loss_train: 12.2751 loss_rec: 12.2751 acc_train: 0.5535 loss_val: 12.1249 acc_val: 0.5596 time: 0.0372s\n",
      "Epoch: 00434 loss_train: 25.1726 loss_rec: 25.1726 acc_train: 0.5451 loss_val: 24.8371 acc_val: 0.5496 time: 0.0253s\n",
      "Epoch: 00435 loss_train: 4.2898 loss_rec: 4.2898 acc_train: 0.4921 loss_val: 4.3522 acc_val: 0.4971 time: 0.0258s\n",
      "Epoch: 00436 loss_train: 12.9251 loss_rec: 12.9251 acc_train: 0.5532 loss_val: 12.7642 acc_val: 0.5592 time: 0.0283s\n",
      "Epoch: 00437 loss_train: 12.5017 loss_rec: 12.5017 acc_train: 0.4938 loss_val: 12.6889 acc_val: 0.4992 time: 0.0293s\n",
      "Epoch: 00438 loss_train: 9.5477 loss_rec: 9.5477 acc_train: 0.5575 loss_val: 9.4433 acc_val: 0.5642 time: 0.0253s\n",
      "Epoch: 00439 loss_train: 11.9511 loss_rec: 11.9511 acc_train: 0.4941 loss_val: 12.1299 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00440 loss_train: 13.5062 loss_rec: 13.5062 acc_train: 0.5543 loss_val: 13.3352 acc_val: 0.5604 time: 0.0273s\n",
      "Epoch: 00441 loss_train: 4.3185 loss_rec: 4.3185 acc_train: 0.4923 loss_val: 4.3814 acc_val: 0.4971 time: 0.0258s\n",
      "Test set results: loss= 26.3079 accuracy= 0.5054\n",
      "Epoch: 00442 loss_train: 23.2294 loss_rec: 23.2294 acc_train: 0.5484 loss_val: 22.9164 acc_val: 0.5525 time: 0.0278s\n",
      "Epoch: 00443 loss_train: 7.2635 loss_rec: 7.2635 acc_train: 0.5549 loss_val: 7.1966 acc_val: 0.5592 time: 0.0363s\n",
      "Epoch: 00444 loss_train: 50.5331 loss_rec: 50.5331 acc_train: 0.4960 loss_val: 51.2988 acc_val: 0.4992 time: 0.0377s\n",
      "Epoch: 00445 loss_train: 55.9678 loss_rec: 55.9678 acc_train: 0.4960 loss_val: 56.8162 acc_val: 0.4992 time: 0.0358s\n",
      "Epoch: 00446 loss_train: 13.3351 loss_rec: 13.3351 acc_train: 0.4941 loss_val: 13.5352 acc_val: 0.4992 time: 0.0377s\n",
      "Epoch: 00447 loss_train: 67.1952 loss_rec: 67.1952 acc_train: 0.5327 loss_val: 66.3423 acc_val: 0.5388 time: 0.0377s\n",
      "Epoch: 00448 loss_train: 97.7660 loss_rec: 97.7660 acc_train: 0.5262 loss_val: 96.5035 acc_val: 0.5329 time: 0.0338s\n",
      "Epoch: 00449 loss_train: 82.8701 loss_rec: 82.8701 acc_train: 0.5287 loss_val: 81.8076 acc_val: 0.5346 time: 0.0243s\n",
      "Epoch: 00450 loss_train: 27.3294 loss_rec: 27.3294 acc_train: 0.5461 loss_val: 26.9677 acc_val: 0.5483 time: 0.0278s\n",
      "Epoch: 00451 loss_train: 67.3765 loss_rec: 67.3765 acc_train: 0.4960 loss_val: 68.3987 acc_val: 0.4992 time: 0.0273s\n",
      "Test set results: loss= 96.6043 accuracy= 0.5553\n",
      "Epoch: 00452 loss_train: 107.5987 loss_rec: 107.5987 acc_train: 0.4977 loss_val: 109.2333 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 00453 loss_train: 96.3639 loss_rec: 96.3639 acc_train: 0.4969 loss_val: 97.8274 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 00454 loss_train: 38.8531 loss_rec: 38.8531 acc_train: 0.4953 loss_val: 39.4410 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00455 loss_train: 55.5618 loss_rec: 55.5618 acc_train: 0.5338 loss_val: 54.8666 acc_val: 0.5413 time: 0.0258s\n",
      "Epoch: 00456 loss_train: 97.5236 loss_rec: 97.5236 acc_train: 0.5260 loss_val: 96.2652 acc_val: 0.5321 time: 0.0278s\n",
      "Epoch: 00457 loss_train: 93.0350 loss_rec: 93.0350 acc_train: 0.5262 loss_val: 91.8363 acc_val: 0.5325 time: 0.0288s\n",
      "Epoch: 00458 loss_train: 46.8651 loss_rec: 46.8651 acc_train: 0.5349 loss_val: 46.2754 acc_val: 0.5396 time: 0.0253s\n",
      "Epoch: 00459 loss_train: 37.2716 loss_rec: 37.2716 acc_train: 0.4942 loss_val: 37.8353 acc_val: 0.4992 time: 0.0278s\n",
      "Epoch: 00460 loss_train: 69.4154 loss_rec: 69.4154 acc_train: 0.4964 loss_val: 70.4685 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00461 loss_train: 51.1958 loss_rec: 51.1958 acc_train: 0.4957 loss_val: 51.9715 acc_val: 0.4992 time: 0.0258s\n",
      "Test set results: loss= 12.2955 accuracy= 0.5174\n",
      "Epoch: 00462 loss_train: 10.8693 loss_rec: 10.8693 acc_train: 0.5556 loss_val: 10.7440 acc_val: 0.5604 time: 0.0273s\n",
      "Epoch: 00463 loss_train: 23.6735 loss_rec: 23.6735 acc_train: 0.5484 loss_val: 23.3544 acc_val: 0.5521 time: 0.0273s\n",
      "Epoch: 00464 loss_train: 5.2412 loss_rec: 5.2412 acc_train: 0.4921 loss_val: 5.3182 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 00465 loss_train: 12.0185 loss_rec: 12.0185 acc_train: 0.5546 loss_val: 11.8742 acc_val: 0.5608 time: 0.0288s\n",
      "Epoch: 00466 loss_train: 12.8710 loss_rec: 12.8710 acc_train: 0.4938 loss_val: 13.0639 acc_val: 0.4992 time: 0.0258s\n",
      "Epoch: 00467 loss_train: 9.1172 loss_rec: 9.1172 acc_train: 0.5531 loss_val: 9.0218 acc_val: 0.5583 time: 0.0258s\n",
      "Epoch: 00468 loss_train: 11.8235 loss_rec: 11.8235 acc_train: 0.4941 loss_val: 12.0008 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00469 loss_train: 13.4472 loss_rec: 13.4472 acc_train: 0.5533 loss_val: 13.2792 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 00470 loss_train: 3.9494 loss_rec: 3.9494 acc_train: 0.4926 loss_val: 4.0065 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 00471 loss_train: 23.3312 loss_rec: 23.3312 acc_train: 0.5484 loss_val: 23.0158 acc_val: 0.5521 time: 0.0268s\n",
      "Test set results: loss= 8.7052 accuracy= 0.5198\n",
      "Epoch: 00472 loss_train: 7.6989 loss_rec: 7.6989 acc_train: 0.5551 loss_val: 7.6264 acc_val: 0.5592 time: 0.0258s\n",
      "Epoch: 00473 loss_train: 49.1754 loss_rec: 49.1754 acc_train: 0.4960 loss_val: 49.9205 acc_val: 0.4992 time: 0.0283s\n",
      "Epoch: 00474 loss_train: 54.4385 loss_rec: 54.4385 acc_train: 0.4960 loss_val: 55.2638 acc_val: 0.4992 time: 0.0258s\n",
      "Epoch: 00475 loss_train: 12.3104 loss_rec: 12.3104 acc_train: 0.4942 loss_val: 12.4950 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00476 loss_train: 67.1363 loss_rec: 67.1363 acc_train: 0.5327 loss_val: 66.2860 acc_val: 0.5413 time: 0.0273s\n",
      "Epoch: 00477 loss_train: 97.3643 loss_rec: 97.3643 acc_train: 0.5260 loss_val: 96.1090 acc_val: 0.5321 time: 0.0263s\n",
      "Epoch: 00478 loss_train: 82.6937 loss_rec: 82.6937 acc_train: 0.5283 loss_val: 81.6349 acc_val: 0.5342 time: 0.0278s\n",
      "Epoch: 00479 loss_train: 27.9278 loss_rec: 27.9278 acc_train: 0.5467 loss_val: 27.5573 acc_val: 0.5504 time: 0.0278s\n",
      "Epoch: 00480 loss_train: 65.3798 loss_rec: 65.3798 acc_train: 0.4960 loss_val: 66.3717 acc_val: 0.4992 time: 0.0263s\n",
      "Epoch: 00481 loss_train: 104.9906 loss_rec: 104.9906 acc_train: 0.4972 loss_val: 106.5860 acc_val: 0.5000 time: 0.0273s\n",
      "Test set results: loss= 84.2648 accuracy= 0.5551\n",
      "Epoch: 00482 loss_train: 93.8612 loss_rec: 93.8612 acc_train: 0.4972 loss_val: 95.2869 acc_val: 0.5000 time: 0.0368s\n",
      "Epoch: 00483 loss_train: 37.0714 loss_rec: 37.0714 acc_train: 0.4946 loss_val: 37.6324 acc_val: 0.4992 time: 0.0377s\n",
      "Epoch: 00484 loss_train: 55.9574 loss_rec: 55.9574 acc_train: 0.5347 loss_val: 55.2579 acc_val: 0.5417 time: 0.0363s\n",
      "Epoch: 00485 loss_train: 97.4234 loss_rec: 97.4234 acc_train: 0.5257 loss_val: 96.1674 acc_val: 0.5321 time: 0.0377s\n",
      "Epoch: 00486 loss_train: 93.0416 loss_rec: 93.0416 acc_train: 0.5270 loss_val: 91.8446 acc_val: 0.5338 time: 0.0377s\n",
      "Epoch: 00487 loss_train: 47.5179 loss_rec: 47.5179 acc_train: 0.5363 loss_val: 46.9189 acc_val: 0.5413 time: 0.0288s\n",
      "Epoch: 00488 loss_train: 35.3655 loss_rec: 35.3655 acc_train: 0.4947 loss_val: 35.9004 acc_val: 0.4992 time: 0.0263s\n",
      "Epoch: 00489 loss_train: 67.0652 loss_rec: 67.0652 acc_train: 0.4960 loss_val: 68.0828 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00490 loss_train: 49.0492 loss_rec: 49.0492 acc_train: 0.4961 loss_val: 49.7926 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00491 loss_train: 12.0757 loss_rec: 12.0757 acc_train: 0.5547 loss_val: 11.9319 acc_val: 0.5608 time: 0.0263s\n",
      "Test set results: loss= 27.9729 accuracy= 0.5045\n",
      "Epoch: 00492 loss_train: 24.6982 loss_rec: 24.6982 acc_train: 0.5482 loss_val: 24.3654 acc_val: 0.5517 time: 0.0258s\n",
      "Epoch: 00493 loss_train: 3.7034 loss_rec: 3.7034 acc_train: 0.4926 loss_val: 3.7569 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 00494 loss_train: 13.0569 loss_rec: 13.0569 acc_train: 0.5539 loss_val: 12.8971 acc_val: 0.5596 time: 0.0263s\n",
      "Epoch: 00495 loss_train: 11.5096 loss_rec: 11.5096 acc_train: 0.4943 loss_val: 11.6821 acc_val: 0.4992 time: 0.0288s\n",
      "Epoch: 00496 loss_train: 9.9323 loss_rec: 9.9323 acc_train: 0.5583 loss_val: 9.8247 acc_val: 0.5650 time: 0.0273s\n",
      "Epoch: 00497 loss_train: 10.7785 loss_rec: 10.7785 acc_train: 0.4943 loss_val: 10.9400 acc_val: 0.4992 time: 0.0268s\n",
      "Epoch: 00498 loss_train: 13.9684 loss_rec: 13.9684 acc_train: 0.5535 loss_val: 13.7936 acc_val: 0.5592 time: 0.0258s\n",
      "Epoch: 00499 loss_train: 3.2160 loss_rec: 3.2160 acc_train: 0.4926 loss_val: 3.2619 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 00500 loss_train: 23.3499 loss_rec: 23.3499 acc_train: 0.5493 loss_val: 23.0338 acc_val: 0.5521 time: 0.0268s\n",
      "Epoch: 00501 loss_train: 7.7252 loss_rec: 7.7252 acc_train: 0.5551 loss_val: 7.6538 acc_val: 0.5592 time: 0.0273s\n",
      "Test set results: loss= 43.5438 accuracy= 0.5537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00502 loss_train: 48.5237 loss_rec: 48.5237 acc_train: 0.4962 loss_val: 49.2593 acc_val: 0.4992 time: 0.0258s\n",
      "Epoch: 00503 loss_train: 53.9407 loss_rec: 53.9407 acc_train: 0.4962 loss_val: 54.7586 acc_val: 0.4992 time: 0.0263s\n",
      "Epoch: 00504 loss_train: 12.5026 loss_rec: 12.5026 acc_train: 0.4945 loss_val: 12.6902 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 00505 loss_train: 65.7457 loss_rec: 65.7457 acc_train: 0.5337 loss_val: 64.9148 acc_val: 0.5417 time: 0.0273s\n",
      "Epoch: 00506 loss_train: 95.4812 loss_rec: 95.4812 acc_train: 0.5270 loss_val: 94.2521 acc_val: 0.5338 time: 0.0273s\n",
      "Epoch: 00507 loss_train: 80.9408 loss_rec: 80.9408 acc_train: 0.5292 loss_val: 79.9072 acc_val: 0.5358 time: 0.0268s\n",
      "Epoch: 00508 loss_train: 26.8773 loss_rec: 26.8773 acc_train: 0.5467 loss_val: 26.5189 acc_val: 0.5508 time: 0.0263s\n",
      "Epoch: 00509 loss_train: 65.2696 loss_rec: 65.2696 acc_train: 0.4962 loss_val: 66.2602 acc_val: 0.4992 time: 0.0258s\n",
      "Epoch: 00510 loss_train: 104.4508 loss_rec: 104.4508 acc_train: 0.4972 loss_val: 106.0381 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 00511 loss_train: 93.5038 loss_rec: 93.5038 acc_train: 0.4972 loss_val: 94.9243 acc_val: 0.5000 time: 0.0283s\n",
      "Test set results: loss= 33.6376 accuracy= 0.5522\n",
      "Epoch: 00512 loss_train: 37.4943 loss_rec: 37.4943 acc_train: 0.4947 loss_val: 38.0619 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00513 loss_train: 54.3563 loss_rec: 54.3563 acc_train: 0.5341 loss_val: 53.6781 acc_val: 0.5379 time: 0.0258s\n",
      "Epoch: 00514 loss_train: 95.4270 loss_rec: 95.4270 acc_train: 0.5267 loss_val: 94.1992 acc_val: 0.5333 time: 0.0278s\n",
      "Epoch: 00515 loss_train: 91.2764 loss_rec: 91.2764 acc_train: 0.5272 loss_val: 90.1041 acc_val: 0.5338 time: 0.0273s\n",
      "Epoch: 00516 loss_train: 46.5423 loss_rec: 46.5423 acc_train: 0.5373 loss_val: 45.9526 acc_val: 0.5421 time: 0.0273s\n",
      "Epoch: 00517 loss_train: 35.0784 loss_rec: 35.0784 acc_train: 0.4947 loss_val: 35.6091 acc_val: 0.4992 time: 0.0258s\n",
      "Epoch: 00518 loss_train: 66.2009 loss_rec: 66.2009 acc_train: 0.4962 loss_val: 67.2057 acc_val: 0.4992 time: 0.0288s\n",
      "Epoch: 00519 loss_train: 48.2668 loss_rec: 48.2668 acc_train: 0.4962 loss_val: 48.9986 acc_val: 0.4992 time: 0.0258s\n",
      "Epoch: 00520 loss_train: 12.1507 loss_rec: 12.1507 acc_train: 0.5548 loss_val: 12.0075 acc_val: 0.5600 time: 0.0273s\n",
      "Epoch: 00521 loss_train: 24.7280 loss_rec: 24.7280 acc_train: 0.5486 loss_val: 24.3944 acc_val: 0.5521 time: 0.0278s\n",
      "Test set results: loss= 2.7820 accuracy= 0.5494\n",
      "Epoch: 00522 loss_train: 3.1398 loss_rec: 3.1398 acc_train: 0.4927 loss_val: 3.1846 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 00523 loss_train: 13.2796 loss_rec: 13.2796 acc_train: 0.5541 loss_val: 13.1175 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 00524 loss_train: 10.9565 loss_rec: 10.9565 acc_train: 0.4943 loss_val: 11.1208 acc_val: 0.4992 time: 0.0263s\n",
      "Epoch: 00525 loss_train: 10.1261 loss_rec: 10.1261 acc_train: 0.5585 loss_val: 10.0172 acc_val: 0.5654 time: 0.0258s\n",
      "Epoch: 00526 loss_train: 10.3136 loss_rec: 10.3136 acc_train: 0.4943 loss_val: 10.4682 acc_val: 0.4992 time: 0.0293s\n",
      "Epoch: 00527 loss_train: 14.0243 loss_rec: 14.0243 acc_train: 0.5537 loss_val: 13.8500 acc_val: 0.5596 time: 0.0268s\n",
      "Epoch: 00528 loss_train: 2.9020 loss_rec: 2.9020 acc_train: 0.4929 loss_val: 2.9431 acc_val: 0.4975 time: 0.0268s\n",
      "Epoch: 00529 loss_train: 23.0646 loss_rec: 23.0646 acc_train: 0.5501 loss_val: 22.7525 acc_val: 0.5546 time: 0.0268s\n",
      "Epoch: 00530 loss_train: 7.5131 loss_rec: 7.5131 acc_train: 0.5559 loss_val: 7.4460 acc_val: 0.5604 time: 0.0263s\n",
      "Epoch: 00531 loss_train: 48.0817 loss_rec: 48.0817 acc_train: 0.4963 loss_val: 48.8109 acc_val: 0.4996 time: 0.0258s\n",
      "Test set results: loss= 47.9685 accuracy= 0.5538\n",
      "Epoch: 00532 loss_train: 53.4510 loss_rec: 53.4510 acc_train: 0.4963 loss_val: 54.2619 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00533 loss_train: 12.6095 loss_rec: 12.6095 acc_train: 0.4945 loss_val: 12.7991 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00534 loss_train: 64.5095 loss_rec: 64.5095 acc_train: 0.5335 loss_val: 63.6966 acc_val: 0.5417 time: 0.0283s\n",
      "Epoch: 00535 loss_train: 93.8300 loss_rec: 93.8300 acc_train: 0.5273 loss_val: 92.6245 acc_val: 0.5338 time: 0.0263s\n",
      "Epoch: 00536 loss_train: 79.4429 loss_rec: 79.4429 acc_train: 0.5300 loss_val: 78.4303 acc_val: 0.5367 time: 0.0273s\n",
      "Epoch: 00537 loss_train: 27.1450 loss_rec: 27.1450 acc_train: 0.5466 loss_val: 26.7819 acc_val: 0.5508 time: 0.0268s\n",
      "Epoch: 00538 loss_train: 64.8438 loss_rec: 64.8438 acc_train: 0.4963 loss_val: 65.8283 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00539 loss_train: 103.4871 loss_rec: 103.4871 acc_train: 0.4972 loss_val: 105.0602 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 00540 loss_train: 92.6740 loss_rec: 92.6740 acc_train: 0.4973 loss_val: 94.0822 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 00541 loss_train: 37.3751 loss_rec: 37.3751 acc_train: 0.4950 loss_val: 37.9411 acc_val: 0.4996 time: 0.0258s\n",
      "Test set results: loss= 60.4430 accuracy= 0.4845\n",
      "Epoch: 00542 loss_train: 53.2746 loss_rec: 53.2746 acc_train: 0.5357 loss_val: 52.6073 acc_val: 0.5408 time: 0.0288s\n",
      "Epoch: 00543 loss_train: 93.8843 loss_rec: 93.8843 acc_train: 0.5273 loss_val: 92.6789 acc_val: 0.5338 time: 0.0268s\n",
      "Epoch: 00544 loss_train: 89.8336 loss_rec: 89.8336 acc_train: 0.5285 loss_val: 88.6821 acc_val: 0.5346 time: 0.0273s\n",
      "Epoch: 00545 loss_train: 45.7718 loss_rec: 45.7718 acc_train: 0.5388 loss_val: 45.1895 acc_val: 0.5433 time: 0.0273s\n",
      "Epoch: 00546 loss_train: 34.7180 loss_rec: 34.7180 acc_train: 0.4950 loss_val: 35.2436 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00547 loss_train: 65.3766 loss_rec: 65.3766 acc_train: 0.4963 loss_val: 66.3693 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00548 loss_train: 47.6628 loss_rec: 47.6628 acc_train: 0.4965 loss_val: 48.3856 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00549 loss_train: 12.0274 loss_rec: 12.0274 acc_train: 0.5563 loss_val: 11.8880 acc_val: 0.5608 time: 0.0278s\n",
      "Epoch: 00550 loss_train: 24.4739 loss_rec: 24.4739 acc_train: 0.5496 loss_val: 24.1426 acc_val: 0.5525 time: 0.0273s\n",
      "Epoch: 00551 loss_train: 2.9307 loss_rec: 2.9307 acc_train: 0.4931 loss_val: 2.9723 acc_val: 0.4975 time: 0.0283s\n",
      "Test set results: loss= 14.9382 accuracy= 0.5166\n",
      "Epoch: 00552 loss_train: 13.2007 loss_rec: 13.2007 acc_train: 0.5552 loss_val: 13.0416 acc_val: 0.5613 time: 0.0258s\n",
      "Epoch: 00553 loss_train: 10.7268 loss_rec: 10.7268 acc_train: 0.4947 loss_val: 10.8880 acc_val: 0.4996 time: 0.0283s\n",
      "Epoch: 00554 loss_train: 9.9912 loss_rec: 9.9912 acc_train: 0.5586 loss_val: 9.8860 acc_val: 0.5654 time: 0.0278s\n",
      "Epoch: 00555 loss_train: 10.2131 loss_rec: 10.2131 acc_train: 0.4947 loss_val: 10.3663 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00556 loss_train: 13.7885 loss_rec: 13.7885 acc_train: 0.5543 loss_val: 13.6196 acc_val: 0.5600 time: 0.0268s\n",
      "Epoch: 00557 loss_train: 2.9126 loss_rec: 2.9126 acc_train: 0.4932 loss_val: 2.9541 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 00558 loss_train: 22.6993 loss_rec: 22.6993 acc_train: 0.5512 loss_val: 22.3931 acc_val: 0.5538 time: 0.0273s\n",
      "Epoch: 00559 loss_train: 7.3481 loss_rec: 7.3481 acc_train: 0.5574 loss_val: 7.2844 acc_val: 0.5621 time: 0.0273s\n",
      "Epoch: 00560 loss_train: 47.4639 loss_rec: 47.4639 acc_train: 0.4965 loss_val: 48.1838 acc_val: 0.4996 time: 0.0263s\n",
      "Epoch: 00561 loss_train: 52.6904 loss_rec: 52.6904 acc_train: 0.4965 loss_val: 53.4900 acc_val: 0.4996 time: 0.0268s\n",
      "Test set results: loss= 11.0373 accuracy= 0.5515\n",
      "Epoch: 00562 loss_train: 12.3328 loss_rec: 12.3328 acc_train: 0.4947 loss_val: 12.5183 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00563 loss_train: 63.8239 loss_rec: 63.8239 acc_train: 0.5352 loss_val: 63.0222 acc_val: 0.5421 time: 0.0263s\n",
      "Epoch: 00564 loss_train: 92.7976 loss_rec: 92.7976 acc_train: 0.5287 loss_val: 91.6075 acc_val: 0.5346 time: 0.0253s\n",
      "Epoch: 00565 loss_train: 78.6391 loss_rec: 78.6391 acc_train: 0.5322 loss_val: 77.6386 acc_val: 0.5392 time: 0.0278s\n",
      "Epoch: 00566 loss_train: 26.0193 loss_rec: 26.0193 acc_train: 0.5489 loss_val: 25.6680 acc_val: 0.5525 time: 0.0283s\n",
      "Epoch: 00567 loss_train: 63.5656 loss_rec: 63.5656 acc_train: 0.4964 loss_val: 64.5309 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00568 loss_train: 101.6247 loss_rec: 101.6247 acc_train: 0.4975 loss_val: 103.1697 acc_val: 0.5004 time: 0.0258s\n",
      "Epoch: 00569 loss_train: 90.8171 loss_rec: 90.8171 acc_train: 0.4975 loss_val: 92.1976 acc_val: 0.5004 time: 0.0253s\n",
      "Epoch: 00570 loss_train: 36.1401 loss_rec: 36.1401 acc_train: 0.4950 loss_val: 36.6878 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 00571 loss_train: 53.3475 loss_rec: 53.3475 acc_train: 0.5357 loss_val: 52.6780 acc_val: 0.5408 time: 0.0268s\n",
      "Test set results: loss= 106.1557 accuracy= 0.4733\n",
      "Epoch: 00572 loss_train: 93.5096 loss_rec: 93.5096 acc_train: 0.5287 loss_val: 92.3101 acc_val: 0.5346 time: 0.0273s\n",
      "Epoch: 00573 loss_train: 89.6493 loss_rec: 89.6493 acc_train: 0.5287 loss_val: 88.5019 acc_val: 0.5346 time: 0.0278s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00574 loss_train: 46.2301 loss_rec: 46.2301 acc_train: 0.5406 loss_val: 45.6406 acc_val: 0.5454 time: 0.0348s\n",
      "Epoch: 00575 loss_train: 33.0066 loss_rec: 33.0066 acc_train: 0.4950 loss_val: 33.5065 acc_val: 0.4996 time: 0.0368s\n",
      "Epoch: 00576 loss_train: 63.2006 loss_rec: 63.2006 acc_train: 0.4965 loss_val: 64.1603 acc_val: 0.4996 time: 0.0363s\n",
      "Epoch: 00577 loss_train: 45.6366 loss_rec: 45.6366 acc_train: 0.4965 loss_val: 46.3288 acc_val: 0.4996 time: 0.0303s\n",
      "Epoch: 00578 loss_train: 13.2405 loss_rec: 13.2405 acc_train: 0.5552 loss_val: 13.0827 acc_val: 0.5613 time: 0.0288s\n",
      "Epoch: 00579 loss_train: 25.5534 loss_rec: 25.5534 acc_train: 0.5487 loss_val: 25.2077 acc_val: 0.5525 time: 0.0268s\n",
      "Epoch: 00580 loss_train: 1.4246 loss_rec: 1.4246 acc_train: 0.4936 loss_val: 1.4390 acc_val: 0.4983 time: 0.0283s\n",
      "Epoch: 00581 loss_train: 8.2254 loss_rec: 8.2254 acc_train: 0.5563 loss_val: 8.1503 acc_val: 0.5596 time: 0.0288s\n",
      "Test set results: loss= 19.2763 accuracy= 0.5525\n",
      "Epoch: 00582 loss_train: 21.5072 loss_rec: 21.5072 acc_train: 0.4951 loss_val: 21.8322 acc_val: 0.4996 time: 0.0288s\n",
      "Epoch: 00583 loss_train: 4.7782 loss_rec: 4.7782 acc_train: 0.4930 loss_val: 4.8488 acc_val: 0.4975 time: 0.0288s\n",
      "Epoch: 00584 loss_train: 50.4192 loss_rec: 50.4192 acc_train: 0.5362 loss_val: 49.7818 acc_val: 0.5413 time: 0.0268s\n",
      "Epoch: 00585 loss_train: 61.5575 loss_rec: 61.5575 acc_train: 0.5350 loss_val: 60.7879 acc_val: 0.5417 time: 0.0293s\n",
      "Epoch: 00586 loss_train: 32.1458 loss_rec: 32.1458 acc_train: 0.5464 loss_val: 31.7209 acc_val: 0.5492 time: 0.0278s\n",
      "Epoch: 00587 loss_train: 34.5371 loss_rec: 34.5371 acc_train: 0.4950 loss_val: 35.0604 acc_val: 0.4996 time: 0.0288s\n",
      "Epoch: 00588 loss_train: 52.8435 loss_rec: 52.8435 acc_train: 0.4965 loss_val: 53.6457 acc_val: 0.4996 time: 0.0293s\n",
      "Epoch: 00589 loss_train: 24.7920 loss_rec: 24.7920 acc_train: 0.4951 loss_val: 25.1670 acc_val: 0.4996 time: 0.0283s\n",
      "Epoch: 00590 loss_train: 41.1239 loss_rec: 41.1239 acc_train: 0.5417 loss_val: 40.5916 acc_val: 0.5467 time: 0.0273s\n",
      "Epoch: 00591 loss_train: 60.9497 loss_rec: 60.9497 acc_train: 0.5360 loss_val: 60.1890 acc_val: 0.5438 time: 0.0288s\n",
      "Test set results: loss= 44.6993 accuracy= 0.4949\n",
      "Epoch: 00592 loss_train: 39.4220 loss_rec: 39.4220 acc_train: 0.5426 loss_val: 38.9098 acc_val: 0.5475 time: 0.0293s\n",
      "Epoch: 00593 loss_train: 19.1107 loss_rec: 19.1107 acc_train: 0.4952 loss_val: 19.3993 acc_val: 0.4996 time: 0.0288s\n",
      "Epoch: 00594 loss_train: 30.6544 loss_rec: 30.6544 acc_train: 0.4951 loss_val: 31.1186 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 00595 loss_train: 2.8190 loss_rec: 2.8190 acc_train: 0.5828 loss_val: 2.8166 acc_val: 0.5850 time: 0.0293s\n",
      "Epoch: 00596 loss_train: 2.7782 loss_rec: 2.7782 acc_train: 0.4932 loss_val: 2.8177 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 00597 loss_train: 32.2948 loss_rec: 32.2948 acc_train: 0.5464 loss_val: 31.8680 acc_val: 0.5488 time: 0.0273s\n",
      "Epoch: 00598 loss_train: 25.7578 loss_rec: 25.7578 acc_train: 0.5489 loss_val: 25.4094 acc_val: 0.5525 time: 0.0288s\n",
      "Epoch: 00599 loss_train: 19.1433 loss_rec: 19.1433 acc_train: 0.4952 loss_val: 19.4326 acc_val: 0.4996 time: 0.0288s\n",
      "Epoch: 00600 loss_train: 17.6155 loss_rec: 17.6155 acc_train: 0.4947 loss_val: 17.8817 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00601 loss_train: 25.3935 loss_rec: 25.3935 acc_train: 0.5497 loss_val: 25.0496 acc_val: 0.5525 time: 0.0278s\n",
      "Test set results: loss= 28.6316 accuracy= 0.5060\n",
      "Epoch: 00602 loss_train: 25.2821 loss_rec: 25.2821 acc_train: 0.5497 loss_val: 24.9392 acc_val: 0.5525 time: 0.0258s\n",
      "Epoch: 00603 loss_train: 13.3230 loss_rec: 13.3230 acc_train: 0.4947 loss_val: 13.5239 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00604 loss_train: 6.1911 loss_rec: 6.1911 acc_train: 0.4931 loss_val: 6.2837 acc_val: 0.4975 time: 0.0258s\n",
      "Epoch: 00605 loss_train: 40.7032 loss_rec: 40.7032 acc_train: 0.5428 loss_val: 40.1752 acc_val: 0.5483 time: 0.0263s\n",
      "Epoch: 00606 loss_train: 44.6159 loss_rec: 44.6159 acc_train: 0.5414 loss_val: 44.0426 acc_val: 0.5458 time: 0.0273s\n",
      "Epoch: 00607 loss_train: 9.6412 loss_rec: 9.6412 acc_train: 0.5552 loss_val: 9.5455 acc_val: 0.5583 time: 0.0273s\n",
      "Epoch: 00608 loss_train: 63.4739 loss_rec: 63.4739 acc_train: 0.4965 loss_val: 64.4382 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00609 loss_train: 86.2415 loss_rec: 86.2415 acc_train: 0.4976 loss_val: 87.5526 acc_val: 0.5004 time: 0.0258s\n",
      "Epoch: 00610 loss_train: 62.4399 loss_rec: 62.4399 acc_train: 0.4965 loss_val: 63.3884 acc_val: 0.4996 time: 0.0283s\n",
      "Epoch: 00611 loss_train: 2.7168 loss_rec: 2.7168 acc_train: 0.5842 loss_val: 2.7160 acc_val: 0.5863 time: 0.0268s\n",
      "Test set results: loss= 26.6323 accuracy= 0.5089\n",
      "Epoch: 00612 loss_train: 23.5192 loss_rec: 23.5192 acc_train: 0.5520 loss_val: 23.2020 acc_val: 0.5546 time: 0.0313s\n",
      "Epoch: 00613 loss_train: 4.6882 loss_rec: 4.6882 acc_train: 0.5680 loss_val: 4.6622 acc_val: 0.5725 time: 0.0278s\n",
      "Epoch: 00614 loss_train: 51.8316 loss_rec: 51.8316 acc_train: 0.4965 loss_val: 52.6186 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 00615 loss_train: 59.2079 loss_rec: 59.2079 acc_train: 0.4965 loss_val: 60.1074 acc_val: 0.4996 time: 0.0293s\n",
      "Epoch: 00616 loss_train: 21.7020 loss_rec: 21.7020 acc_train: 0.4952 loss_val: 22.0303 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00617 loss_train: 51.6538 loss_rec: 51.6538 acc_train: 0.5363 loss_val: 51.0004 acc_val: 0.5413 time: 0.0288s\n",
      "Epoch: 00618 loss_train: 78.7579 loss_rec: 78.7579 acc_train: 0.5329 loss_val: 77.7578 acc_val: 0.5392 time: 0.0288s\n",
      "Epoch: 00619 loss_train: 63.8191 loss_rec: 63.8191 acc_train: 0.5343 loss_val: 63.0204 acc_val: 0.5417 time: 0.0283s\n",
      "Epoch: 00620 loss_train: 11.7139 loss_rec: 11.7139 acc_train: 0.5571 loss_val: 11.5836 acc_val: 0.5638 time: 0.0273s\n",
      "Epoch: 00621 loss_train: 77.5115 loss_rec: 77.5115 acc_train: 0.4965 loss_val: 78.6897 acc_val: 0.4996 time: 0.0273s\n",
      "Test set results: loss= 103.3392 accuracy= 0.5552\n",
      "Epoch: 00622 loss_train: 115.1027 loss_rec: 115.1027 acc_train: 0.4975 loss_val: 116.8537 acc_val: 0.5004 time: 0.0288s\n",
      "Epoch: 00623 loss_train: 104.8143 loss_rec: 104.8143 acc_train: 0.4975 loss_val: 106.4085 acc_val: 0.5004 time: 0.0278s\n",
      "Epoch: 00624 loss_train: 51.5068 loss_rec: 51.5068 acc_train: 0.4965 loss_val: 52.2889 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 00625 loss_train: 36.8943 loss_rec: 36.8943 acc_train: 0.5451 loss_val: 36.4114 acc_val: 0.5496 time: 0.0268s\n",
      "Epoch: 00626 loss_train: 75.9581 loss_rec: 75.9581 acc_train: 0.5329 loss_val: 74.9955 acc_val: 0.5417 time: 0.0253s\n",
      "Epoch: 00627 loss_train: 72.1256 loss_rec: 72.1256 acc_train: 0.5338 loss_val: 71.2146 acc_val: 0.5421 time: 0.0273s\n",
      "Epoch: 00628 loss_train: 29.7786 loss_rec: 29.7786 acc_train: 0.5469 loss_val: 29.3806 acc_val: 0.5508 time: 0.0258s\n",
      "Epoch: 00629 loss_train: 48.7297 loss_rec: 48.7297 acc_train: 0.4966 loss_val: 49.4695 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 00630 loss_train: 78.2089 loss_rec: 78.2089 acc_train: 0.4965 loss_val: 79.3980 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00631 loss_train: 60.9071 loss_rec: 60.9071 acc_train: 0.4965 loss_val: 61.8324 acc_val: 0.4996 time: 0.0273s\n",
      "Test set results: loss= 1.4283 accuracy= 0.5502\n",
      "Epoch: 00632 loss_train: 1.6303 loss_rec: 1.6303 acc_train: 0.4937 loss_val: 1.6497 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 00633 loss_train: 84.1497 loss_rec: 84.1497 acc_train: 0.5302 loss_val: 83.0781 acc_val: 0.5379 time: 0.0273s\n",
      "Epoch: 00634 loss_train: 123.5018 loss_rec: 123.5018 acc_train: 0.5251 loss_val: 121.9029 acc_val: 0.5321 time: 0.0278s\n",
      "Epoch: 00635 loss_train: 119.8367 loss_rec: 119.8367 acc_train: 0.5247 loss_val: 118.2874 acc_val: 0.5317 time: 0.0268s\n",
      "Epoch: 00636 loss_train: 77.4701 loss_rec: 77.4701 acc_train: 0.5329 loss_val: 76.4879 acc_val: 0.5417 time: 0.0263s\n",
      "Epoch: 00637 loss_train: 1.8396 loss_rec: 1.8396 acc_train: 0.6011 loss_val: 1.8494 acc_val: 0.6008 time: 0.0268s\n",
      "Epoch: 00638 loss_train: 103.2856 loss_rec: 103.2856 acc_train: 0.4977 loss_val: 104.8567 acc_val: 0.5004 time: 0.0263s\n",
      "Epoch: 00639 loss_train: 154.4875 loss_rec: 154.4875 acc_train: 0.4997 loss_val: 156.8387 acc_val: 0.5008 time: 0.0278s\n",
      "Epoch: 00640 loss_train: 156.8636 loss_rec: 156.8636 acc_train: 0.4997 loss_val: 159.2509 acc_val: 0.5008 time: 0.0278s\n",
      "Epoch: 00641 loss_train: 115.4392 loss_rec: 115.4392 acc_train: 0.4976 loss_val: 117.1958 acc_val: 0.5004 time: 0.0273s\n",
      "Test set results: loss= 31.1153 accuracy= 0.5525\n",
      "Epoch: 00642 loss_train: 34.6905 loss_rec: 34.6905 acc_train: 0.4952 loss_val: 35.2168 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00643 loss_train: 75.2014 loss_rec: 75.2014 acc_train: 0.5334 loss_val: 74.2497 acc_val: 0.5425 time: 0.0258s\n",
      "Epoch: 00644 loss_train: 135.1246 loss_rec: 135.1246 acc_train: 0.5205 loss_val: 133.3433 acc_val: 0.5267 time: 0.0278s\n",
      "Epoch: 00645 loss_train: 150.0456 loss_rec: 150.0456 acc_train: 0.5161 loss_val: 148.0951 acc_val: 0.5208 time: 0.0378s\n",
      "Epoch: 00646 loss_train: 124.1137 loss_rec: 124.1137 acc_train: 0.5250 loss_val: 122.5070 acc_val: 0.5321 time: 0.0348s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00647 loss_train: 62.3444 loss_rec: 62.3444 acc_train: 0.5384 loss_val: 61.5673 acc_val: 0.5450 time: 0.0328s\n",
      "Epoch: 00648 loss_train: 31.7255 loss_rec: 31.7255 acc_train: 0.4948 loss_val: 32.2059 acc_val: 0.4992 time: 0.0363s\n",
      "Epoch: 00649 loss_train: 77.9264 loss_rec: 77.9264 acc_train: 0.4961 loss_val: 79.1104 acc_val: 0.4992 time: 0.0343s\n",
      "Epoch: 00650 loss_train: 76.3878 loss_rec: 76.3878 acc_train: 0.4960 loss_val: 77.5482 acc_val: 0.4992 time: 0.0343s\n",
      "Epoch: 00651 loss_train: 31.9538 loss_rec: 31.9538 acc_train: 0.4946 loss_val: 32.4374 acc_val: 0.4992 time: 0.0348s\n",
      "Test set results: loss= 52.9610 accuracy= 0.4927\n",
      "Epoch: 00652 loss_train: 46.6965 loss_rec: 46.6965 acc_train: 0.5414 loss_val: 46.0978 acc_val: 0.5458 time: 0.0338s\n",
      "Epoch: 00653 loss_train: 79.0179 loss_rec: 79.0179 acc_train: 0.5324 loss_val: 78.0162 acc_val: 0.5413 time: 0.0258s\n",
      "Epoch: 00654 loss_train: 69.9336 loss_rec: 69.9336 acc_train: 0.5335 loss_val: 69.0539 acc_val: 0.5408 time: 0.0268s\n",
      "Epoch: 00655 loss_train: 23.9661 loss_rec: 23.9661 acc_train: 0.5517 loss_val: 23.6432 acc_val: 0.5533 time: 0.0273s\n",
      "Epoch: 00656 loss_train: 57.3329 loss_rec: 57.3329 acc_train: 0.4956 loss_val: 58.2024 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 00657 loss_train: 90.0009 loss_rec: 90.0009 acc_train: 0.4967 loss_val: 91.3680 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00658 loss_train: 76.6365 loss_rec: 76.6365 acc_train: 0.4956 loss_val: 77.7999 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 00659 loss_train: 21.8766 loss_rec: 21.8766 acc_train: 0.4942 loss_val: 22.2062 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 00660 loss_train: 64.5394 loss_rec: 64.5394 acc_train: 0.5349 loss_val: 63.7337 acc_val: 0.5429 time: 0.0288s\n",
      "Epoch: 00661 loss_train: 104.5680 loss_rec: 104.5680 acc_train: 0.5262 loss_val: 103.2242 acc_val: 0.5325 time: 0.0303s\n",
      "Test set results: loss= 116.4277 accuracy= 0.4722\n",
      "Epoch: 00662 loss_train: 102.5567 loss_rec: 102.5567 acc_train: 0.5269 loss_val: 101.2403 acc_val: 0.5329 time: 0.0288s\n",
      "Epoch: 00663 loss_train: 62.7322 loss_rec: 62.7322 acc_train: 0.5326 loss_val: 61.9501 acc_val: 0.5367 time: 0.0268s\n",
      "Epoch: 00664 loss_train: 9.7440 loss_rec: 9.7440 acc_train: 0.4937 loss_val: 9.8893 acc_val: 0.4988 time: 0.0308s\n",
      "Epoch: 00665 loss_train: 37.2894 loss_rec: 37.2894 acc_train: 0.4941 loss_val: 37.8533 acc_val: 0.4988 time: 0.0288s\n",
      "Epoch: 00666 loss_train: 19.5740 loss_rec: 19.5740 acc_train: 0.4942 loss_val: 19.8685 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 00667 loss_train: 35.2523 loss_rec: 35.2523 acc_train: 0.5442 loss_val: 34.7878 acc_val: 0.5463 time: 0.0288s\n",
      "Epoch: 00668 loss_train: 47.2647 loss_rec: 47.2647 acc_train: 0.5408 loss_val: 46.6582 acc_val: 0.5454 time: 0.0348s\n",
      "Epoch: 00669 loss_train: 21.0876 loss_rec: 21.0876 acc_train: 0.5515 loss_val: 20.8070 acc_val: 0.5525 time: 0.0348s\n",
      "Epoch: 00670 loss_train: 40.9822 loss_rec: 40.9822 acc_train: 0.4941 loss_val: 41.6024 acc_val: 0.4988 time: 0.0412s\n",
      "Epoch: 00671 loss_train: 56.6755 loss_rec: 56.6755 acc_train: 0.4956 loss_val: 57.5347 acc_val: 0.4988 time: 0.0353s\n",
      "Test set results: loss= 25.5237 accuracy= 0.5517\n",
      "Epoch: 00672 loss_train: 28.4682 loss_rec: 28.4682 acc_train: 0.4942 loss_val: 28.8980 acc_val: 0.4988 time: 0.0348s\n",
      "Epoch: 00673 loss_train: 35.5939 loss_rec: 35.5939 acc_train: 0.5444 loss_val: 35.1248 acc_val: 0.5471 time: 0.0353s\n",
      "Epoch: 00674 loss_train: 39.7429 loss_rec: 39.7429 acc_train: 0.5427 loss_val: 39.2245 acc_val: 0.5463 time: 0.0363s\n",
      "Epoch: 00675 loss_train: 1.2114 loss_rec: 1.2114 acc_train: 0.4927 loss_val: 1.2199 acc_val: 0.4979 time: 0.0308s\n",
      "Epoch: 00676 loss_train: 1.9147 loss_rec: 1.9147 acc_train: 0.5992 loss_val: 1.9237 acc_val: 0.5996 time: 0.0268s\n",
      "Epoch: 00677 loss_train: 34.7091 loss_rec: 34.7091 acc_train: 0.4942 loss_val: 35.2338 acc_val: 0.4988 time: 0.0288s\n",
      "Epoch: 00678 loss_train: 12.1767 loss_rec: 12.1767 acc_train: 0.4937 loss_val: 12.3588 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 00679 loss_train: 54.7993 loss_rec: 54.7993 acc_train: 0.5353 loss_val: 54.1074 acc_val: 0.5404 time: 0.0293s\n",
      "Epoch: 00680 loss_train: 70.5835 loss_rec: 70.5835 acc_train: 0.5333 loss_val: 69.6964 acc_val: 0.5413 time: 0.0268s\n",
      "Epoch: 00681 loss_train: 40.9461 loss_rec: 40.9461 acc_train: 0.5414 loss_val: 40.4128 acc_val: 0.5467 time: 0.0288s\n",
      "Test set results: loss= 26.5612 accuracy= 0.5518\n",
      "Epoch: 00682 loss_train: 29.6231 loss_rec: 29.6231 acc_train: 0.4942 loss_val: 30.0707 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 00683 loss_train: 46.5078 loss_rec: 46.5078 acc_train: 0.4941 loss_val: 47.2125 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 00684 loss_train: 12.4642 loss_rec: 12.4642 acc_train: 0.4937 loss_val: 12.6508 acc_val: 0.4988 time: 0.0288s\n",
      "Epoch: 00685 loss_train: 60.9297 loss_rec: 60.9297 acc_train: 0.5335 loss_val: 60.1678 acc_val: 0.5371 time: 0.0253s\n",
      "Epoch: 00686 loss_train: 84.9554 loss_rec: 84.9554 acc_train: 0.5326 loss_val: 83.8760 acc_val: 0.5379 time: 0.0363s\n",
      "Epoch: 00687 loss_train: 64.2406 loss_rec: 64.2406 acc_train: 0.5373 loss_val: 63.4400 acc_val: 0.5442 time: 0.0368s\n",
      "Epoch: 00688 loss_train: 4.7305 loss_rec: 4.7305 acc_train: 0.5677 loss_val: 4.7057 acc_val: 0.5725 time: 0.0313s\n",
      "Epoch: 00689 loss_train: 93.6092 loss_rec: 93.6092 acc_train: 0.4967 loss_val: 95.0315 acc_val: 0.4996 time: 0.0382s\n",
      "Epoch: 00690 loss_train: 135.1535 loss_rec: 135.1535 acc_train: 0.4982 loss_val: 137.2087 acc_val: 0.4996 time: 0.0372s\n",
      "Epoch: 00691 loss_train: 124.9443 loss_rec: 124.9443 acc_train: 0.4966 loss_val: 126.8439 acc_val: 0.4996 time: 0.0363s\n",
      "Test set results: loss= 61.7235 accuracy= 0.5532\n",
      "Epoch: 00692 loss_train: 68.7726 loss_rec: 68.7726 acc_train: 0.4956 loss_val: 69.8166 acc_val: 0.4988 time: 0.0308s\n",
      "Epoch: 00693 loss_train: 25.1757 loss_rec: 25.1757 acc_train: 0.5503 loss_val: 24.8355 acc_val: 0.5529 time: 0.0274s\n",
      "Epoch: 00694 loss_train: 66.0126 loss_rec: 66.0126 acc_train: 0.5353 loss_val: 65.1886 acc_val: 0.5433 time: 0.0272s\n",
      "Epoch: 00695 loss_train: 62.1219 loss_rec: 62.1219 acc_train: 0.5343 loss_val: 61.3456 acc_val: 0.5375 time: 0.0278s\n",
      "Epoch: 00696 loss_train: 18.5729 loss_rec: 18.5729 acc_train: 0.5537 loss_val: 18.3306 acc_val: 0.5579 time: 0.0293s\n",
      "Epoch: 00697 loss_train: 63.3395 loss_rec: 63.3395 acc_train: 0.4961 loss_val: 64.3010 acc_val: 0.4992 time: 0.0293s\n",
      "Epoch: 00698 loss_train: 93.1955 loss_rec: 93.1955 acc_train: 0.4972 loss_val: 94.6119 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 00699 loss_train: 74.2675 loss_rec: 74.2675 acc_train: 0.4960 loss_val: 75.3956 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00700 loss_train: 11.8964 loss_rec: 11.8964 acc_train: 0.4942 loss_val: 12.0749 acc_val: 0.4992 time: 0.0288s\n",
      "Epoch: 00701 loss_train: 82.5455 loss_rec: 82.5455 acc_train: 0.5318 loss_val: 81.4985 acc_val: 0.5396 time: 0.0278s\n",
      "Test set results: loss= 145.3668 accuracy= 0.4673\n",
      "Epoch: 00702 loss_train: 128.0332 loss_rec: 128.0332 acc_train: 0.5245 loss_val: 126.3765 acc_val: 0.5317 time: 0.0263s\n",
      "Epoch: 00703 loss_train: 128.8804 loss_rec: 128.8804 acc_train: 0.5245 loss_val: 127.2123 acc_val: 0.5317 time: 0.0263s\n",
      "Epoch: 00704 loss_train: 89.7566 loss_rec: 89.7566 acc_train: 0.5297 loss_val: 88.6135 acc_val: 0.5371 time: 0.0328s\n",
      "Epoch: 00705 loss_train: 15.4319 loss_rec: 15.4319 acc_train: 0.5539 loss_val: 15.2434 acc_val: 0.5596 time: 0.0363s\n",
      "Epoch: 00706 loss_train: 95.1064 loss_rec: 95.1064 acc_train: 0.4972 loss_val: 96.5521 acc_val: 0.5000 time: 0.0363s\n",
      "Epoch: 00707 loss_train: 151.5785 loss_rec: 151.5785 acc_train: 0.4986 loss_val: 153.8845 acc_val: 0.5000 time: 0.0353s\n",
      "Epoch: 00708 loss_train: 157.6023 loss_rec: 157.6023 acc_train: 0.4986 loss_val: 159.9999 acc_val: 0.5000 time: 0.0353s\n",
      "Epoch: 00709 loss_train: 118.3831 loss_rec: 118.3831 acc_train: 0.4972 loss_val: 120.1834 acc_val: 0.5000 time: 0.0338s\n",
      "Epoch: 00710 loss_train: 38.9110 loss_rec: 38.9110 acc_train: 0.4946 loss_val: 39.5007 acc_val: 0.4992 time: 0.0293s\n",
      "Epoch: 00711 loss_train: 70.4996 loss_rec: 70.4996 acc_train: 0.5341 loss_val: 69.6153 acc_val: 0.5413 time: 0.0278s\n",
      "Test set results: loss= 146.5460 accuracy= 0.4674\n",
      "Epoch: 00712 loss_train: 129.0714 loss_rec: 129.0714 acc_train: 0.5247 loss_val: 127.4018 acc_val: 0.5317 time: 0.0253s\n",
      "Epoch: 00713 loss_train: 142.6450 loss_rec: 142.6450 acc_train: 0.5202 loss_val: 140.7645 acc_val: 0.5262 time: 0.0258s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00714 loss_train: 115.5028 loss_rec: 115.5028 acc_train: 0.5267 loss_val: 114.0151 acc_val: 0.5325 time: 0.0278s\n",
      "Epoch: 00715 loss_train: 52.4045 loss_rec: 52.4045 acc_train: 0.5384 loss_val: 51.7376 acc_val: 0.5429 time: 0.0273s\n",
      "Epoch: 00716 loss_train: 43.7819 loss_rec: 43.7819 acc_train: 0.4946 loss_val: 44.4460 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00717 loss_train: 90.9755 loss_rec: 90.9755 acc_train: 0.4972 loss_val: 92.3585 acc_val: 0.5000 time: 0.0258s\n",
      "Epoch: 00718 loss_train: 89.8763 loss_rec: 89.8763 acc_train: 0.4972 loss_val: 91.2426 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 00719 loss_train: 45.5096 loss_rec: 45.5096 acc_train: 0.4946 loss_val: 46.1999 acc_val: 0.4992 time: 0.0258s\n",
      "Epoch: 00720 loss_train: 34.0871 loss_rec: 34.0871 acc_train: 0.5467 loss_val: 33.6348 acc_val: 0.5500 time: 0.0273s\n",
      "Epoch: 00721 loss_train: 66.2013 loss_rec: 66.2013 acc_train: 0.5373 loss_val: 65.3765 acc_val: 0.5442 time: 0.0268s\n",
      "Test set results: loss= 64.8315 accuracy= 0.4854\n",
      "Epoch: 00722 loss_train: 57.1473 loss_rec: 57.1473 acc_train: 0.5359 loss_val: 56.4263 acc_val: 0.5408 time: 0.0273s\n",
      "Epoch: 00723 loss_train: 11.6814 loss_rec: 11.6814 acc_train: 0.5584 loss_val: 11.5572 acc_val: 0.5650 time: 0.0278s\n",
      "Epoch: 00724 loss_train: 70.0426 loss_rec: 70.0426 acc_train: 0.4963 loss_val: 71.1069 acc_val: 0.4992 time: 0.0258s\n",
      "Epoch: 00725 loss_train: 102.0893 loss_rec: 102.0893 acc_train: 0.4975 loss_val: 103.6419 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 00726 loss_train: 87.9798 loss_rec: 87.9798 acc_train: 0.4975 loss_val: 89.3175 acc_val: 0.5000 time: 0.0343s\n",
      "Epoch: 00727 loss_train: 32.5468 loss_rec: 32.5468 acc_train: 0.4951 loss_val: 33.0400 acc_val: 0.4992 time: 0.0338s\n",
      "Epoch: 00728 loss_train: 54.9716 loss_rec: 54.9716 acc_train: 0.5373 loss_val: 54.2751 acc_val: 0.5417 time: 0.0303s\n",
      "Epoch: 00729 loss_train: 95.6608 loss_rec: 95.6608 acc_train: 0.5302 loss_val: 94.4400 acc_val: 0.5367 time: 0.0333s\n",
      "Epoch: 00730 loss_train: 94.4359 loss_rec: 94.4359 acc_train: 0.5302 loss_val: 93.2314 acc_val: 0.5367 time: 0.0293s\n",
      "Epoch: 00731 loss_train: 55.5944 loss_rec: 55.5944 acc_train: 0.5374 loss_val: 54.8904 acc_val: 0.5421 time: 0.0313s\n",
      "Test set results: loss= 14.5266 accuracy= 0.5516\n",
      "Epoch: 00732 loss_train: 16.2245 loss_rec: 16.2245 acc_train: 0.4949 loss_val: 16.4695 acc_val: 0.4996 time: 0.0293s\n",
      "Epoch: 00733 loss_train: 42.8674 loss_rec: 42.8674 acc_train: 0.4952 loss_val: 43.5180 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 00734 loss_train: 24.5205 loss_rec: 24.5205 acc_train: 0.4952 loss_val: 24.8918 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00735 loss_train: 30.9814 loss_rec: 30.9814 acc_train: 0.5473 loss_val: 30.5654 acc_val: 0.5508 time: 0.0263s\n",
      "Epoch: 00736 loss_train: 43.5634 loss_rec: 43.5634 acc_train: 0.5424 loss_val: 42.9971 acc_val: 0.5475 time: 0.0318s\n",
      "Epoch: 00737 loss_train: 18.1944 loss_rec: 18.1944 acc_train: 0.5548 loss_val: 17.9605 acc_val: 0.5600 time: 0.0293s\n",
      "Epoch: 00738 loss_train: 42.9095 loss_rec: 42.9095 acc_train: 0.4952 loss_val: 43.5610 acc_val: 0.4996 time: 0.0283s\n",
      "Epoch: 00739 loss_train: 57.7754 loss_rec: 57.7754 acc_train: 0.4966 loss_val: 58.6534 acc_val: 0.4996 time: 0.0253s\n",
      "Epoch: 00740 loss_train: 29.0778 loss_rec: 29.0778 acc_train: 0.4952 loss_val: 29.5187 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00741 loss_train: 35.1671 loss_rec: 35.1671 acc_train: 0.5467 loss_val: 34.7011 acc_val: 0.5488 time: 0.0273s\n",
      "Test set results: loss= 62.9761 accuracy= 0.4867\n",
      "Epoch: 00742 loss_train: 55.5151 loss_rec: 55.5151 acc_train: 0.5376 loss_val: 54.8112 acc_val: 0.5421 time: 0.0263s\n",
      "Epoch: 00743 loss_train: 37.1871 loss_rec: 37.1871 acc_train: 0.5456 loss_val: 36.6968 acc_val: 0.5479 time: 0.0278s\n",
      "Epoch: 00744 loss_train: 15.8960 loss_rec: 15.8960 acc_train: 0.4949 loss_val: 16.1365 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 00745 loss_train: 25.0784 loss_rec: 25.0784 acc_train: 0.4954 loss_val: 25.4586 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00746 loss_train: 7.2236 loss_rec: 7.2236 acc_train: 0.5603 loss_val: 7.1699 acc_val: 0.5638 time: 0.0278s\n",
      "Epoch: 00747 loss_train: 1.1606 loss_rec: 1.1606 acc_train: 0.6286 loss_val: 1.1740 acc_val: 0.6258 time: 0.0313s\n",
      "Epoch: 00748 loss_train: 27.4772 loss_rec: 27.4772 acc_train: 0.4954 loss_val: 27.8939 acc_val: 0.4996 time: 0.0348s\n",
      "Epoch: 00749 loss_train: 11.3469 loss_rec: 11.3469 acc_train: 0.4950 loss_val: 11.5184 acc_val: 0.4996 time: 0.0368s\n",
      "Epoch: 00750 loss_train: 40.9297 loss_rec: 40.9297 acc_train: 0.5451 loss_val: 40.3947 acc_val: 0.5496 time: 0.0323s\n",
      "Epoch: 00751 loss_train: 51.7423 loss_rec: 51.7423 acc_train: 0.5411 loss_val: 51.0808 acc_val: 0.5450 time: 0.0343s\n",
      "Test set results: loss= 28.4371 accuracy= 0.5100\n",
      "Epoch: 00752 loss_train: 25.1129 loss_rec: 25.1129 acc_train: 0.5528 loss_val: 24.7750 acc_val: 0.5542 time: 0.0338s\n",
      "Epoch: 00753 loss_train: 36.4759 loss_rec: 36.4759 acc_train: 0.4952 loss_val: 37.0298 acc_val: 0.4996 time: 0.0368s\n",
      "Epoch: 00754 loss_train: 52.7232 loss_rec: 52.7232 acc_train: 0.4967 loss_val: 53.5246 acc_val: 0.4996 time: 0.0323s\n",
      "Epoch: 00755 loss_train: 25.7217 loss_rec: 25.7217 acc_train: 0.4955 loss_val: 26.1117 acc_val: 0.4996 time: 0.0358s\n",
      "Epoch: 00756 loss_train: 36.4988 loss_rec: 36.4988 acc_train: 0.5466 loss_val: 36.0163 acc_val: 0.5492 time: 0.0263s\n",
      "Epoch: 00757 loss_train: 55.6223 loss_rec: 55.6223 acc_train: 0.5382 loss_val: 54.9168 acc_val: 0.5429 time: 0.0288s\n",
      "Epoch: 00758 loss_train: 36.4768 loss_rec: 36.4768 acc_train: 0.5466 loss_val: 35.9946 acc_val: 0.5492 time: 0.0288s\n",
      "Epoch: 00759 loss_train: 17.0790 loss_rec: 17.0790 acc_train: 0.4950 loss_val: 17.3376 acc_val: 0.4996 time: 0.0293s\n",
      "Epoch: 00760 loss_train: 27.0347 loss_rec: 27.0347 acc_train: 0.4955 loss_val: 27.4447 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 00761 loss_train: 4.5964 loss_rec: 4.5964 acc_train: 0.5703 loss_val: 4.5769 acc_val: 0.5742 time: 0.0298s\n",
      "Test set results: loss= 1.3374 accuracy= 0.5506\n",
      "Epoch: 00762 loss_train: 1.5313 loss_rec: 1.5313 acc_train: 0.4943 loss_val: 1.5493 acc_val: 0.4992 time: 0.0442s\n",
      "Epoch: 00763 loss_train: 24.7902 loss_rec: 24.7902 acc_train: 0.5527 loss_val: 24.4571 acc_val: 0.5542 time: 0.0348s\n",
      "Epoch: 00764 loss_train: 13.3487 loss_rec: 13.3487 acc_train: 0.5571 loss_val: 13.1982 acc_val: 0.5629 time: 0.0263s\n",
      "Epoch: 00765 loss_train: 34.0403 loss_rec: 34.0403 acc_train: 0.4955 loss_val: 34.5572 acc_val: 0.4996 time: 0.0353s\n",
      "Epoch: 00766 loss_train: 36.9349 loss_rec: 36.9349 acc_train: 0.4955 loss_val: 37.4957 acc_val: 0.4996 time: 0.0353s\n",
      "Epoch: 00767 loss_train: 1.6663 loss_rec: 1.6663 acc_train: 0.6111 loss_val: 1.6802 acc_val: 0.6071 time: 0.0333s\n",
      "Epoch: 00768 loss_train: 7.2149 loss_rec: 7.2149 acc_train: 0.5603 loss_val: 7.1620 acc_val: 0.5638 time: 0.0348s\n",
      "Epoch: 00769 loss_train: 22.8061 loss_rec: 22.8061 acc_train: 0.4955 loss_val: 23.1520 acc_val: 0.4996 time: 0.0353s\n",
      "Epoch: 00770 loss_train: 9.5751 loss_rec: 9.5751 acc_train: 0.4950 loss_val: 9.7200 acc_val: 0.5000 time: 0.0368s\n",
      "Epoch: 00771 loss_train: 39.7579 loss_rec: 39.7579 acc_train: 0.5450 loss_val: 39.2358 acc_val: 0.5483 time: 0.0373s\n",
      "Test set results: loss= 54.9336 accuracy= 0.4942\n",
      "Epoch: 00772 loss_train: 48.4380 loss_rec: 48.4380 acc_train: 0.5423 loss_val: 47.8131 acc_val: 0.5467 time: 0.0372s\n",
      "Epoch: 00773 loss_train: 20.2545 loss_rec: 20.2545 acc_train: 0.5546 loss_val: 19.9873 acc_val: 0.5588 time: 0.0407s\n",
      "Epoch: 00774 loss_train: 42.6988 loss_rec: 42.6988 acc_train: 0.4955 loss_val: 43.3479 acc_val: 0.4996 time: 0.0402s\n",
      "Epoch: 00775 loss_train: 60.1973 loss_rec: 60.1973 acc_train: 0.4967 loss_val: 61.1131 acc_val: 0.4996 time: 0.0387s\n",
      "Epoch: 00776 loss_train: 34.6598 loss_rec: 34.6598 acc_train: 0.4955 loss_val: 35.1865 acc_val: 0.4996 time: 0.0338s\n",
      "Epoch: 00777 loss_train: 26.6152 loss_rec: 26.6152 acc_train: 0.5515 loss_val: 26.2553 acc_val: 0.5538 time: 0.0353s\n",
      "Epoch: 00778 loss_train: 44.7813 loss_rec: 44.7813 acc_train: 0.5427 loss_val: 44.1989 acc_val: 0.5475 time: 0.0298s\n",
      "Epoch: 00779 loss_train: 25.2315 loss_rec: 25.2315 acc_train: 0.5528 loss_val: 24.8918 acc_val: 0.5542 time: 0.0258s\n",
      "Epoch: 00780 loss_train: 29.0736 loss_rec: 29.0736 acc_train: 0.4955 loss_val: 29.5152 acc_val: 0.4996 time: 0.0288s\n",
      "Epoch: 00781 loss_train: 39.2637 loss_rec: 39.2637 acc_train: 0.4955 loss_val: 39.8607 acc_val: 0.4996 time: 0.0278s\n",
      "Test set results: loss= 6.5186 accuracy= 0.5498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00782 loss_train: 7.3091 loss_rec: 7.3091 acc_train: 0.4935 loss_val: 7.4200 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 00783 loss_train: 57.4414 loss_rec: 57.4414 acc_train: 0.5372 loss_val: 56.7141 acc_val: 0.5417 time: 0.0288s\n",
      "Epoch: 00784 loss_train: 80.3523 loss_rec: 80.3523 acc_train: 0.5338 loss_val: 79.3377 acc_val: 0.5421 time: 0.0278s\n",
      "Epoch: 00785 loss_train: 64.4392 loss_rec: 64.4392 acc_train: 0.5353 loss_val: 63.6321 acc_val: 0.5392 time: 0.0288s\n",
      "Epoch: 00786 loss_train: 14.4100 loss_rec: 14.4100 acc_train: 0.5560 loss_val: 14.2426 acc_val: 0.5608 time: 0.0268s\n",
      "Epoch: 00787 loss_train: 69.7596 loss_rec: 69.7596 acc_train: 0.4967 loss_val: 70.8213 acc_val: 0.4996 time: 0.0263s\n",
      "Epoch: 00788 loss_train: 106.1335 loss_rec: 106.1335 acc_train: 0.4978 loss_val: 107.7495 acc_val: 0.5004 time: 0.0263s\n",
      "Epoch: 00789 loss_train: 97.7680 loss_rec: 97.7680 acc_train: 0.4979 loss_val: 99.2565 acc_val: 0.5004 time: 0.0283s\n",
      "Epoch: 00790 loss_train: 49.2478 loss_rec: 49.2478 acc_train: 0.4954 loss_val: 49.9968 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 00791 loss_train: 31.9682 loss_rec: 31.9682 acc_train: 0.5475 loss_val: 31.5382 acc_val: 0.5508 time: 0.0273s\n",
      "Test set results: loss= 76.5353 accuracy= 0.4835\n",
      "Epoch: 00792 loss_train: 67.4463 loss_rec: 67.4463 acc_train: 0.5344 loss_val: 66.6042 acc_val: 0.5383 time: 0.0263s\n",
      "Epoch: 00793 loss_train: 63.4507 loss_rec: 63.4507 acc_train: 0.5363 loss_val: 62.6545 acc_val: 0.5413 time: 0.0278s\n",
      "Epoch: 00794 loss_train: 24.2039 loss_rec: 24.2039 acc_train: 0.5534 loss_val: 23.8796 acc_val: 0.5542 time: 0.0283s\n",
      "Epoch: 00795 loss_train: 48.7847 loss_rec: 48.7847 acc_train: 0.4954 loss_val: 49.5266 acc_val: 0.4996 time: 0.0293s\n",
      "Epoch: 00796 loss_train: 76.0817 loss_rec: 76.0817 acc_train: 0.4967 loss_val: 77.2398 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00797 loss_train: 59.8410 loss_rec: 59.8410 acc_train: 0.4968 loss_val: 60.7515 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 00798 loss_train: 4.5241 loss_rec: 4.5241 acc_train: 0.4936 loss_val: 4.5922 acc_val: 0.4979 time: 0.0268s\n",
      "Epoch: 00799 loss_train: 79.3049 loss_rec: 79.3049 acc_train: 0.5332 loss_val: 78.3048 acc_val: 0.5404 time: 0.0273s\n",
      "Epoch: 00800 loss_train: 119.7353 loss_rec: 119.7353 acc_train: 0.5262 loss_val: 118.1939 acc_val: 0.5325 time: 0.0278s\n",
      "Epoch: 00801 loss_train: 119.8458 loss_rec: 119.8458 acc_train: 0.5262 loss_val: 118.3029 acc_val: 0.5325 time: 0.0343s\n",
      "Test set results: loss= 95.0503 accuracy= 0.4816\n",
      "Epoch: 00802 loss_train: 83.7423 loss_rec: 83.7423 acc_train: 0.5340 loss_val: 82.6830 acc_val: 0.5421 time: 0.0338s\n",
      "Epoch: 00803 loss_train: 15.8004 loss_rec: 15.8004 acc_train: 0.5557 loss_val: 15.6100 acc_val: 0.5613 time: 0.0338s\n",
      "Epoch: 00804 loss_train: 84.8188 loss_rec: 84.8188 acc_train: 0.4967 loss_val: 86.1099 acc_val: 0.4996 time: 0.0263s\n",
      "Epoch: 00805 loss_train: 136.6335 loss_rec: 136.6335 acc_train: 0.4977 loss_val: 138.7144 acc_val: 0.5004 time: 0.0273s\n",
      "Epoch: 00806 loss_train: 142.7122 loss_rec: 142.7122 acc_train: 0.4977 loss_val: 144.8858 acc_val: 0.5004 time: 0.0263s\n",
      "Epoch: 00807 loss_train: 107.6990 loss_rec: 107.6990 acc_train: 0.4979 loss_val: 109.3390 acc_val: 0.5004 time: 0.0278s\n",
      "Epoch: 00808 loss_train: 35.8399 loss_rec: 35.8399 acc_train: 0.4955 loss_val: 36.3847 acc_val: 0.4996 time: 0.0253s\n",
      "Epoch: 00809 loss_train: 63.4289 loss_rec: 63.4289 acc_train: 0.5358 loss_val: 62.6322 acc_val: 0.5413 time: 0.0283s\n",
      "Epoch: 00810 loss_train: 116.8302 loss_rec: 116.8302 acc_train: 0.5263 loss_val: 115.3281 acc_val: 0.5338 time: 0.0283s\n",
      "Epoch: 00811 loss_train: 129.0036 loss_rec: 129.0036 acc_train: 0.5262 loss_val: 127.3387 acc_val: 0.5325 time: 0.0268s\n",
      "Test set results: loss= 118.1469 accuracy= 0.4728\n",
      "Epoch: 00812 loss_train: 104.0746 loss_rec: 104.0746 acc_train: 0.5286 loss_val: 102.7436 acc_val: 0.5342 time: 0.0283s\n",
      "Epoch: 00813 loss_train: 46.1226 loss_rec: 46.1226 acc_train: 0.5429 loss_val: 45.5229 acc_val: 0.5479 time: 0.0268s\n",
      "Epoch: 00814 loss_train: 42.3293 loss_rec: 42.3293 acc_train: 0.4955 loss_val: 42.9730 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 00815 loss_train: 85.7627 loss_rec: 85.7627 acc_train: 0.4967 loss_val: 87.0685 acc_val: 0.4996 time: 0.0283s\n",
      "Epoch: 00816 loss_train: 84.7770 loss_rec: 84.7770 acc_train: 0.4967 loss_val: 86.0678 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00817 loss_train: 43.9918 loss_rec: 43.9918 acc_train: 0.4955 loss_val: 44.6609 acc_val: 0.4996 time: 0.0284s\n",
      "Epoch: 00818 loss_train: 29.4068 loss_rec: 29.4068 acc_train: 0.5499 loss_val: 29.0077 acc_val: 0.5525 time: 0.0272s\n",
      "Epoch: 00819 loss_train: 59.1115 loss_rec: 59.1115 acc_train: 0.5364 loss_val: 58.3636 acc_val: 0.5413 time: 0.0278s\n",
      "Epoch: 00820 loss_train: 51.0171 loss_rec: 51.0171 acc_train: 0.5417 loss_val: 50.3607 acc_val: 0.5463 time: 0.0258s\n",
      "Epoch: 00821 loss_train: 9.2894 loss_rec: 9.2894 acc_train: 0.5565 loss_val: 9.2073 acc_val: 0.5604 time: 0.0268s\n",
      "Test set results: loss= 59.0318 accuracy= 0.5540\n",
      "Epoch: 00822 loss_train: 65.7780 loss_rec: 65.7780 acc_train: 0.4968 loss_val: 66.7792 acc_val: 0.4996 time: 0.0283s\n",
      "Epoch: 00823 loss_train: 94.9194 loss_rec: 94.9194 acc_train: 0.4979 loss_val: 96.3649 acc_val: 0.5004 time: 0.0268s\n",
      "Epoch: 00824 loss_train: 81.4392 loss_rec: 81.4392 acc_train: 0.4967 loss_val: 82.6793 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00825 loss_train: 29.6696 loss_rec: 29.6696 acc_train: 0.4955 loss_val: 30.1206 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 00826 loss_train: 51.6593 loss_rec: 51.6593 acc_train: 0.5416 loss_val: 50.9953 acc_val: 0.5458 time: 0.0278s\n",
      "Epoch: 00827 loss_train: 89.6417 loss_rec: 89.6417 acc_train: 0.5328 loss_val: 88.5035 acc_val: 0.5408 time: 0.0278s\n",
      "Epoch: 00828 loss_train: 77.8992 loss_rec: 77.8992 acc_train: 0.5343 loss_val: 76.9179 acc_val: 0.5421 time: 0.0293s\n",
      "Epoch: 00829 loss_train: 26.3327 loss_rec: 26.3327 acc_train: 0.5525 loss_val: 25.9775 acc_val: 0.5550 time: 0.0263s\n",
      "Epoch: 00830 loss_train: 67.4423 loss_rec: 67.4423 acc_train: 0.4969 loss_val: 68.4691 acc_val: 0.4996 time: 0.0253s\n",
      "Epoch: 00831 loss_train: 94.8198 loss_rec: 94.8198 acc_train: 0.4967 loss_val: 96.2640 acc_val: 0.4996 time: 0.0283s\n",
      "Test set results: loss= 66.2515 accuracy= 0.5540\n",
      "Epoch: 00832 loss_train: 73.8153 loss_rec: 73.8153 acc_train: 0.4967 loss_val: 74.9393 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00833 loss_train: 9.6402 loss_rec: 9.6402 acc_train: 0.4952 loss_val: 9.7868 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 00834 loss_train: 84.1150 loss_rec: 84.1150 acc_train: 0.5327 loss_val: 83.0508 acc_val: 0.5417 time: 0.0283s\n",
      "Epoch: 00835 loss_train: 129.5882 loss_rec: 129.5882 acc_train: 0.5260 loss_val: 127.9156 acc_val: 0.5317 time: 0.0263s\n",
      "Epoch: 00836 loss_train: 132.0914 loss_rec: 132.0914 acc_train: 0.5254 loss_val: 130.3854 acc_val: 0.5325 time: 0.0273s\n",
      "Epoch: 00837 loss_train: 96.6442 loss_rec: 96.6442 acc_train: 0.5301 loss_val: 95.4129 acc_val: 0.5375 time: 0.0273s\n",
      "Epoch: 00838 loss_train: 27.6615 loss_rec: 27.6615 acc_train: 0.5516 loss_val: 27.2867 acc_val: 0.5538 time: 0.0263s\n",
      "Epoch: 00839 loss_train: 74.6139 loss_rec: 74.6139 acc_train: 0.4967 loss_val: 75.7501 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 00840 loss_train: 126.8003 loss_rec: 126.8003 acc_train: 0.4979 loss_val: 128.7321 acc_val: 0.5004 time: 0.0283s\n",
      "Epoch: 00841 loss_train: 131.2079 loss_rec: 131.2079 acc_train: 0.4978 loss_val: 133.2067 acc_val: 0.5004 time: 0.0283s\n",
      "Test set results: loss= 83.6257 accuracy= 0.5540\n",
      "Epoch: 00842 loss_train: 93.1600 loss_rec: 93.1600 acc_train: 0.4967 loss_val: 94.5790 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 00843 loss_train: 17.7697 loss_rec: 17.7697 acc_train: 0.4952 loss_val: 18.0397 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 00844 loss_train: 83.6156 loss_rec: 83.6156 acc_train: 0.5339 loss_val: 82.5586 acc_val: 0.5421 time: 0.0268s\n",
      "Epoch: 00845 loss_train: 138.6985 loss_rec: 138.6985 acc_train: 0.5250 loss_val: 136.9045 acc_val: 0.5321 time: 0.0402s\n",
      "Epoch: 00846 loss_train: 152.0485 loss_rec: 152.0485 acc_train: 0.5205 loss_val: 150.0442 acc_val: 0.5267 time: 0.0377s\n",
      "Epoch: 00847 loss_train: 127.8074 loss_rec: 127.8074 acc_train: 0.5262 loss_val: 126.1590 acc_val: 0.5321 time: 0.0338s\n",
      "Epoch: 00848 loss_train: 70.3697 loss_rec: 70.3697 acc_train: 0.5364 loss_val: 69.4927 acc_val: 0.5442 time: 0.0353s\n",
      "Epoch: 00849 loss_train: 16.0192 loss_rec: 16.0192 acc_train: 0.4952 loss_val: 16.2626 acc_val: 0.5000 time: 0.0333s\n",
      "Epoch: 00850 loss_train: 59.0215 loss_rec: 59.0215 acc_train: 0.4970 loss_val: 59.9200 acc_val: 0.4996 time: 0.0353s\n",
      "Epoch: 00851 loss_train: 57.7332 loss_rec: 57.7332 acc_train: 0.4970 loss_val: 58.6119 acc_val: 0.4996 time: 0.0358s\n",
      "Test set results: loss= 15.0128 accuracy= 0.5517\n",
      "Epoch: 00852 loss_train: 16.7682 loss_rec: 16.7682 acc_train: 0.4952 loss_val: 17.0229 acc_val: 0.5000 time: 0.0353s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00853 loss_train: 54.5697 loss_rec: 54.5697 acc_train: 0.5406 loss_val: 53.8724 acc_val: 0.5450 time: 0.0343s\n",
      "Epoch: 00854 loss_train: 84.1959 loss_rec: 84.1959 acc_train: 0.5338 loss_val: 83.1316 acc_val: 0.5421 time: 0.0363s\n",
      "Epoch: 00855 loss_train: 75.8677 loss_rec: 75.8677 acc_train: 0.5347 loss_val: 74.9163 acc_val: 0.5421 time: 0.0348s\n",
      "Epoch: 00856 loss_train: 33.7034 loss_rec: 33.7034 acc_train: 0.5470 loss_val: 33.2516 acc_val: 0.5508 time: 0.0358s\n",
      "Epoch: 00857 loss_train: 39.8608 loss_rec: 39.8608 acc_train: 0.4955 loss_val: 40.4671 acc_val: 0.4996 time: 0.0293s\n",
      "Epoch: 00858 loss_train: 69.9320 loss_rec: 69.9320 acc_train: 0.4969 loss_val: 70.9968 acc_val: 0.4996 time: 0.0293s\n",
      "Epoch: 00859 loss_train: 57.7593 loss_rec: 57.7593 acc_train: 0.4970 loss_val: 58.6385 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00860 loss_train: 7.7514 loss_rec: 7.7514 acc_train: 0.4953 loss_val: 7.8693 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 00861 loss_train: 70.0946 loss_rec: 70.0946 acc_train: 0.5337 loss_val: 69.2205 acc_val: 0.5379 time: 0.0278s\n",
      "Test set results: loss= 121.1377 accuracy= 0.4728\n",
      "Epoch: 00862 loss_train: 106.7087 loss_rec: 106.7087 acc_train: 0.5286 loss_val: 105.3440 acc_val: 0.5346 time: 0.0278s\n",
      "Epoch: 00863 loss_train: 105.0643 loss_rec: 105.0643 acc_train: 0.5292 loss_val: 103.7220 acc_val: 0.5346 time: 0.0258s\n",
      "Epoch: 00864 loss_train: 69.0444 loss_rec: 69.0444 acc_train: 0.5345 loss_val: 68.1822 acc_val: 0.5388 time: 0.0278s\n",
      "Epoch: 00865 loss_train: 3.6167 loss_rec: 3.6167 acc_train: 0.5794 loss_val: 3.6134 acc_val: 0.5813 time: 0.0268s\n",
      "Epoch: 00866 loss_train: 91.7441 loss_rec: 91.7441 acc_train: 0.4967 loss_val: 93.1416 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 00867 loss_train: 139.5011 loss_rec: 139.5011 acc_train: 0.4978 loss_val: 141.6265 acc_val: 0.5004 time: 0.0278s\n",
      "Epoch: 00868 loss_train: 143.6785 loss_rec: 143.6785 acc_train: 0.4978 loss_val: 145.8676 acc_val: 0.5004 time: 0.0258s\n",
      "Epoch: 00869 loss_train: 108.7280 loss_rec: 108.7280 acc_train: 0.4979 loss_val: 110.3846 acc_val: 0.5004 time: 0.0283s\n",
      "Epoch: 00870 loss_train: 38.8288 loss_rec: 38.8288 acc_train: 0.4956 loss_val: 39.4195 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 00871 loss_train: 57.1756 loss_rec: 57.1756 acc_train: 0.5392 loss_val: 56.4479 acc_val: 0.5438 time: 0.0263s\n",
      "Test set results: loss= 123.7107 accuracy= 0.4728\n",
      "Epoch: 00872 loss_train: 108.9740 loss_rec: 108.9740 acc_train: 0.5287 loss_val: 107.5794 acc_val: 0.5346 time: 0.0278s\n",
      "Epoch: 00873 loss_train: 121.5649 loss_rec: 121.5649 acc_train: 0.5262 loss_val: 120.0018 acc_val: 0.5329 time: 0.0263s\n",
      "Epoch: 00874 loss_train: 98.7949 loss_rec: 98.7949 acc_train: 0.5303 loss_val: 97.5366 acc_val: 0.5379 time: 0.0268s\n",
      "Epoch: 00875 loss_train: 44.5898 loss_rec: 44.5898 acc_train: 0.5445 loss_val: 44.0070 acc_val: 0.5475 time: 0.0283s\n",
      "Epoch: 00876 loss_train: 38.5304 loss_rec: 38.5304 acc_train: 0.4956 loss_val: 39.1167 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00877 loss_train: 79.0687 loss_rec: 79.0687 acc_train: 0.4969 loss_val: 80.2731 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00878 loss_train: 77.4885 loss_rec: 77.4885 acc_train: 0.4969 loss_val: 78.6688 acc_val: 0.4996 time: 0.0283s\n",
      "Epoch: 00879 loss_train: 38.0786 loss_rec: 38.0786 acc_train: 0.4956 loss_val: 38.6580 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00880 loss_train: 31.8157 loss_rec: 31.8157 acc_train: 0.5492 loss_val: 31.3849 acc_val: 0.5525 time: 0.0258s\n",
      "Epoch: 00881 loss_train: 60.4636 loss_rec: 60.4636 acc_train: 0.5377 loss_val: 59.6979 acc_val: 0.5425 time: 0.0278s\n",
      "Test set results: loss= 60.3389 accuracy= 0.4929\n",
      "Epoch: 00882 loss_train: 53.1997 loss_rec: 53.1997 acc_train: 0.5416 loss_val: 52.5164 acc_val: 0.5458 time: 0.0273s\n",
      "Epoch: 00883 loss_train: 13.8900 loss_rec: 13.8900 acc_train: 0.5572 loss_val: 13.7357 acc_val: 0.5629 time: 0.0268s\n",
      "Epoch: 00884 loss_train: 56.8771 loss_rec: 56.8771 acc_train: 0.4970 loss_val: 57.7431 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00885 loss_train: 84.5320 loss_rec: 84.5320 acc_train: 0.4967 loss_val: 85.8198 acc_val: 0.4996 time: 0.0263s\n",
      "Epoch: 00886 loss_train: 71.7400 loss_rec: 71.7400 acc_train: 0.4970 loss_val: 72.8326 acc_val: 0.4996 time: 0.0293s\n",
      "Epoch: 00887 loss_train: 22.6075 loss_rec: 22.6075 acc_train: 0.4957 loss_val: 22.9513 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 00888 loss_train: 54.1550 loss_rec: 54.1550 acc_train: 0.5422 loss_val: 53.4607 acc_val: 0.5463 time: 0.0298s\n",
      "Epoch: 00889 loss_train: 90.2288 loss_rec: 90.2288 acc_train: 0.5329 loss_val: 89.0860 acc_val: 0.5417 time: 0.0412s\n",
      "Epoch: 00890 loss_train: 89.3637 loss_rec: 89.3637 acc_train: 0.5329 loss_val: 88.2316 acc_val: 0.5417 time: 0.0248s\n",
      "Epoch: 00891 loss_train: 55.4127 loss_rec: 55.4127 acc_train: 0.5412 loss_val: 54.7039 acc_val: 0.5446 time: 0.0253s\n",
      "Test set results: loss= 6.2721 accuracy= 0.5499\n",
      "Epoch: 00892 loss_train: 7.0379 loss_rec: 7.0379 acc_train: 0.4936 loss_val: 7.1452 acc_val: 0.4979 time: 0.0323s\n",
      "Epoch: 00893 loss_train: 30.3684 loss_rec: 30.3684 acc_train: 0.4957 loss_val: 30.8304 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00894 loss_train: 14.0588 loss_rec: 14.0588 acc_train: 0.4952 loss_val: 14.2727 acc_val: 0.5000 time: 0.0258s\n",
      "Epoch: 00895 loss_train: 34.3800 loss_rec: 34.3800 acc_train: 0.5470 loss_val: 33.9189 acc_val: 0.5508 time: 0.0298s\n",
      "Epoch: 00896 loss_train: 45.5620 loss_rec: 45.5620 acc_train: 0.5438 loss_val: 44.9671 acc_val: 0.5471 time: 0.0298s\n",
      "Epoch: 00897 loss_train: 23.1860 loss_rec: 23.1860 acc_train: 0.5515 loss_val: 22.8762 acc_val: 0.5521 time: 0.0268s\n",
      "Epoch: 00898 loss_train: 30.1187 loss_rec: 30.1187 acc_train: 0.4957 loss_val: 30.5769 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00899 loss_train: 43.3173 loss_rec: 43.3173 acc_train: 0.4956 loss_val: 43.9766 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00900 loss_train: 17.9904 loss_rec: 17.9904 acc_train: 0.4952 loss_val: 18.2642 acc_val: 0.5000 time: 0.0258s\n",
      "Epoch: 00901 loss_train: 38.0900 loss_rec: 38.0900 acc_train: 0.5469 loss_val: 37.5847 acc_val: 0.5488 time: 0.0258s\n",
      "Test set results: loss= 63.5610 accuracy= 0.4908\n",
      "Epoch: 00902 loss_train: 56.0366 loss_rec: 56.0366 acc_train: 0.5406 loss_val: 55.3196 acc_val: 0.5454 time: 0.0273s\n",
      "Epoch: 00903 loss_train: 39.7720 loss_rec: 39.7720 acc_train: 0.5463 loss_val: 39.2463 acc_val: 0.5488 time: 0.0268s\n",
      "Epoch: 00904 loss_train: 6.5939 loss_rec: 6.5939 acc_train: 0.4936 loss_val: 6.6945 acc_val: 0.4979 time: 0.0278s\n",
      "Epoch: 00905 loss_train: 14.7865 loss_rec: 14.7865 acc_train: 0.4952 loss_val: 15.0117 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 00906 loss_train: 12.9979 loss_rec: 12.9979 acc_train: 0.5590 loss_val: 12.8594 acc_val: 0.5654 time: 0.0258s\n",
      "Epoch: 00907 loss_train: 6.3439 loss_rec: 6.3439 acc_train: 0.5637 loss_val: 6.3050 acc_val: 0.5650 time: 0.0273s\n",
      "Epoch: 00908 loss_train: 31.9471 loss_rec: 31.9471 acc_train: 0.4957 loss_val: 32.4331 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 00909 loss_train: 30.4082 loss_rec: 30.4082 acc_train: 0.4957 loss_val: 30.8709 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00910 loss_train: 6.7474 loss_rec: 6.7474 acc_train: 0.5633 loss_val: 6.7040 acc_val: 0.5646 time: 0.0273s\n",
      "Epoch: 00911 loss_train: 8.1879 loss_rec: 8.1879 acc_train: 0.5597 loss_val: 8.1253 acc_val: 0.5642 time: 0.0258s\n",
      "Test set results: loss= 20.0003 accuracy= 0.5517\n",
      "Epoch: 00912 loss_train: 22.3231 loss_rec: 22.3231 acc_train: 0.4952 loss_val: 22.6628 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 00913 loss_train: 14.0514 loss_rec: 14.0514 acc_train: 0.4952 loss_val: 14.2655 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 00914 loss_train: 27.2972 loss_rec: 27.2972 acc_train: 0.5529 loss_val: 26.9290 acc_val: 0.5542 time: 0.0273s\n",
      "Epoch: 00915 loss_train: 32.5259 loss_rec: 32.5259 acc_train: 0.5494 loss_val: 32.0853 acc_val: 0.5525 time: 0.0278s\n",
      "Epoch: 00916 loss_train: 5.6123 loss_rec: 5.6123 acc_train: 0.5684 loss_val: 5.5831 acc_val: 0.5725 time: 0.0273s\n",
      "Epoch: 00917 loss_train: 52.1742 loss_rec: 52.1742 acc_train: 0.4956 loss_val: 52.9688 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00918 loss_train: 68.2044 loss_rec: 68.2044 acc_train: 0.4970 loss_val: 69.2436 acc_val: 0.4996 time: 0.0263s\n",
      "Epoch: 00919 loss_train: 45.7029 loss_rec: 45.7029 acc_train: 0.4957 loss_val: 46.3988 acc_val: 0.4996 time: 0.0263s\n",
      "Epoch: 00920 loss_train: 9.9110 loss_rec: 9.9110 acc_train: 0.5571 loss_val: 9.8230 acc_val: 0.5608 time: 0.0268s\n",
      "Epoch: 00921 loss_train: 26.3541 loss_rec: 26.3541 acc_train: 0.5527 loss_val: 25.9999 acc_val: 0.5538 time: 0.0268s\n",
      "Test set results: loss= 10.8926 accuracy= 0.5209\n",
      "Epoch: 00922 loss_train: 9.6266 loss_rec: 9.6266 acc_train: 0.5566 loss_val: 9.5423 acc_val: 0.5604 time: 0.0278s\n",
      "Epoch: 00923 loss_train: 38.6772 loss_rec: 38.6772 acc_train: 0.4957 loss_val: 39.2662 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00924 loss_train: 46.6748 loss_rec: 46.6748 acc_train: 0.4957 loss_val: 47.3856 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00925 loss_train: 17.0258 loss_rec: 17.0258 acc_train: 0.4952 loss_val: 17.2853 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 00926 loss_train: 42.3275 loss_rec: 42.3275 acc_train: 0.5456 loss_val: 41.7709 acc_val: 0.5488 time: 0.0268s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00927 loss_train: 63.5932 loss_rec: 63.5932 acc_train: 0.5364 loss_val: 62.7906 acc_val: 0.5417 time: 0.0268s\n",
      "Epoch: 00928 loss_train: 50.4989 loss_rec: 50.4989 acc_train: 0.5419 loss_val: 49.8444 acc_val: 0.5471 time: 0.0263s\n",
      "Epoch: 00929 loss_train: 7.0064 loss_rec: 7.0064 acc_train: 0.5627 loss_val: 6.9603 acc_val: 0.5642 time: 0.0273s\n",
      "Epoch: 00930 loss_train: 66.7761 loss_rec: 66.7761 acc_train: 0.4971 loss_val: 67.7935 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00931 loss_train: 97.5146 loss_rec: 97.5146 acc_train: 0.4968 loss_val: 99.0009 acc_val: 0.4996 time: 0.0273s\n",
      "Test set results: loss= 79.3337 accuracy= 0.5540\n",
      "Epoch: 00932 loss_train: 88.3839 loss_rec: 88.3839 acc_train: 0.4969 loss_val: 89.7308 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00933 loss_train: 43.3971 loss_rec: 43.3971 acc_train: 0.4957 loss_val: 44.0580 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00934 loss_train: 30.4622 loss_rec: 30.4622 acc_train: 0.5503 loss_val: 30.0482 acc_val: 0.5538 time: 0.0258s\n",
      "Epoch: 00935 loss_train: 63.4524 loss_rec: 63.4524 acc_train: 0.5364 loss_val: 62.6511 acc_val: 0.5417 time: 0.0278s\n",
      "Epoch: 00936 loss_train: 61.0704 loss_rec: 61.0704 acc_train: 0.5377 loss_val: 60.2957 acc_val: 0.5425 time: 0.0273s\n",
      "Epoch: 00937 loss_train: 26.9461 loss_rec: 26.9461 acc_train: 0.5528 loss_val: 26.5833 acc_val: 0.5542 time: 0.0273s\n",
      "Epoch: 00938 loss_train: 36.9199 loss_rec: 36.9199 acc_train: 0.4957 loss_val: 37.4820 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00939 loss_train: 60.4708 loss_rec: 60.4708 acc_train: 0.4971 loss_val: 61.3921 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00940 loss_train: 45.0774 loss_rec: 45.0774 acc_train: 0.4957 loss_val: 45.7639 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00941 loss_train: 4.3986 loss_rec: 4.3986 acc_train: 0.5737 loss_val: 4.3873 acc_val: 0.5767 time: 0.0278s\n",
      "Test set results: loss= 19.2397 accuracy= 0.5167\n",
      "Epoch: 00942 loss_train: 16.9961 loss_rec: 16.9961 acc_train: 0.5557 loss_val: 16.7904 acc_val: 0.5613 time: 0.0258s\n",
      "Epoch: 00943 loss_train: 1.9954 loss_rec: 1.9954 acc_train: 0.4952 loss_val: 2.0252 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 00944 loss_train: 11.2775 loss_rec: 11.2775 acc_train: 0.5556 loss_val: 11.1697 acc_val: 0.5592 time: 0.0278s\n",
      "Epoch: 00945 loss_train: 7.0008 loss_rec: 7.0008 acc_train: 0.4936 loss_val: 7.1081 acc_val: 0.4979 time: 0.0258s\n",
      "Epoch: 00946 loss_train: 9.8896 loss_rec: 9.8896 acc_train: 0.5571 loss_val: 9.8020 acc_val: 0.5608 time: 0.0273s\n",
      "Epoch: 00947 loss_train: 5.0795 loss_rec: 5.0795 acc_train: 0.4939 loss_val: 5.1575 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 00948 loss_train: 14.4535 loss_rec: 14.4535 acc_train: 0.5566 loss_val: 14.2918 acc_val: 0.5629 time: 0.0258s\n",
      "Epoch: 00949 loss_train: 1.9516 loss_rec: 1.9516 acc_train: 0.6061 loss_val: 1.9684 acc_val: 0.6063 time: 0.0273s\n",
      "Epoch: 00950 loss_train: 36.5092 loss_rec: 36.5092 acc_train: 0.4957 loss_val: 37.0653 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00951 loss_train: 34.9994 loss_rec: 34.9994 acc_train: 0.4957 loss_val: 35.5324 acc_val: 0.4996 time: 0.0258s\n",
      "Test set results: loss= 2.6111 accuracy= 0.5719\n",
      "Epoch: 00952 loss_train: 2.3493 loss_rec: 2.3493 acc_train: 0.5971 loss_val: 2.3636 acc_val: 0.5967 time: 0.0278s\n",
      "Epoch: 00953 loss_train: 7.7318 loss_rec: 7.7318 acc_train: 0.5612 loss_val: 7.6766 acc_val: 0.5638 time: 0.0273s\n",
      "Epoch: 00954 loss_train: 17.9012 loss_rec: 17.9012 acc_train: 0.4952 loss_val: 18.1745 acc_val: 0.5000 time: 0.0288s\n",
      "Epoch: 00955 loss_train: 5.8067 loss_rec: 5.8067 acc_train: 0.4938 loss_val: 5.8959 acc_val: 0.4979 time: 0.0258s\n",
      "Epoch: 00956 loss_train: 37.6192 loss_rec: 37.6192 acc_train: 0.5464 loss_val: 37.1179 acc_val: 0.5496 time: 0.0263s\n",
      "Epoch: 00957 loss_train: 45.6041 loss_rec: 45.6041 acc_train: 0.5450 loss_val: 45.0075 acc_val: 0.5492 time: 0.0278s\n",
      "Epoch: 00958 loss_train: 21.1247 loss_rec: 21.1247 acc_train: 0.5547 loss_val: 20.8481 acc_val: 0.5583 time: 0.0273s\n",
      "Epoch: 00959 loss_train: 33.5210 loss_rec: 33.5210 acc_train: 0.4957 loss_val: 34.0316 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 00960 loss_train: 48.6374 loss_rec: 48.6374 acc_train: 0.4957 loss_val: 49.3783 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00961 loss_train: 25.8756 loss_rec: 25.8756 acc_train: 0.4957 loss_val: 26.2698 acc_val: 0.5000 time: 0.0253s\n",
      "Test set results: loss= 31.5465 accuracy= 0.5100\n",
      "Epoch: 00962 loss_train: 27.8572 loss_rec: 27.8572 acc_train: 0.5530 loss_val: 27.4810 acc_val: 0.5542 time: 0.0278s\n",
      "Epoch: 00963 loss_train: 44.0637 loss_rec: 44.0637 acc_train: 0.5454 loss_val: 43.4854 acc_val: 0.5496 time: 0.0283s\n",
      "Epoch: 00964 loss_train: 27.0187 loss_rec: 27.0187 acc_train: 0.5528 loss_val: 26.6547 acc_val: 0.5542 time: 0.0258s\n",
      "Epoch: 00965 loss_train: 20.1071 loss_rec: 20.1071 acc_train: 0.4952 loss_val: 20.4136 acc_val: 0.5000 time: 0.0288s\n",
      "Epoch: 00966 loss_train: 28.9352 loss_rec: 28.9352 acc_train: 0.4957 loss_val: 29.3759 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 00967 loss_train: 0.9875 loss_rec: 0.9875 acc_train: 0.5877 loss_val: 0.9957 acc_val: 0.5817 time: 0.0258s\n",
      "Epoch: 00968 loss_train: 35.7444 loss_rec: 35.7444 acc_train: 0.5463 loss_val: 35.2650 acc_val: 0.5504 time: 0.0278s\n",
      "Epoch: 00969 loss_train: 37.7886 loss_rec: 37.7886 acc_train: 0.5464 loss_val: 37.2854 acc_val: 0.5496 time: 0.0278s\n",
      "Epoch: 00970 loss_train: 8.2688 loss_rec: 8.2688 acc_train: 0.5590 loss_val: 8.2063 acc_val: 0.5642 time: 0.0258s\n",
      "Epoch: 00971 loss_train: 51.7463 loss_rec: 51.7463 acc_train: 0.4957 loss_val: 52.5346 acc_val: 0.4996 time: 0.0273s\n",
      "Test set results: loss= 63.4743 accuracy= 0.5541\n",
      "Epoch: 00972 loss_train: 70.7271 loss_rec: 70.7271 acc_train: 0.4971 loss_val: 71.8052 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00973 loss_train: 51.5463 loss_rec: 51.5463 acc_train: 0.4957 loss_val: 52.3316 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 00974 loss_train: 1.7651 loss_rec: 1.7651 acc_train: 0.6114 loss_val: 1.7835 acc_val: 0.6075 time: 0.0278s\n",
      "Epoch: 00975 loss_train: 22.7278 loss_rec: 22.7278 acc_train: 0.5538 loss_val: 22.4252 acc_val: 0.5571 time: 0.0273s\n",
      "Epoch: 00976 loss_train: 11.3436 loss_rec: 11.3436 acc_train: 0.5556 loss_val: 11.2355 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 00977 loss_train: 31.2257 loss_rec: 31.2257 acc_train: 0.4957 loss_val: 31.7014 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 00978 loss_train: 34.8452 loss_rec: 34.8452 acc_train: 0.4957 loss_val: 35.3760 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00979 loss_train: 1.9481 loss_rec: 1.9481 acc_train: 0.4952 loss_val: 1.9774 acc_val: 0.4992 time: 0.0258s\n",
      "Epoch: 00980 loss_train: 56.0031 loss_rec: 56.0031 acc_train: 0.5422 loss_val: 55.2846 acc_val: 0.5467 time: 0.0278s\n",
      "Epoch: 00981 loss_train: 77.4436 loss_rec: 77.4436 acc_train: 0.5343 loss_val: 76.4754 acc_val: 0.5421 time: 0.0273s\n",
      "Test set results: loss= 73.3858 accuracy= 0.4855\n",
      "Epoch: 00982 loss_train: 64.6854 loss_rec: 64.6854 acc_train: 0.5364 loss_val: 63.8685 acc_val: 0.5417 time: 0.0273s\n",
      "Epoch: 00983 loss_train: 21.7399 loss_rec: 21.7399 acc_train: 0.5547 loss_val: 21.4534 acc_val: 0.5588 time: 0.0263s\n",
      "Epoch: 00984 loss_train: 50.3977 loss_rec: 50.3977 acc_train: 0.4957 loss_val: 51.1658 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 00985 loss_train: 81.5448 loss_rec: 81.5448 acc_train: 0.4971 loss_val: 82.7879 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00986 loss_train: 73.4161 loss_rec: 73.4161 acc_train: 0.4971 loss_val: 74.5353 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00987 loss_train: 29.9837 loss_rec: 29.9837 acc_train: 0.4957 loss_val: 30.4407 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 00988 loss_train: 41.0375 loss_rec: 41.0375 acc_train: 0.5453 loss_val: 40.4950 acc_val: 0.5471 time: 0.0273s\n",
      "Epoch: 00989 loss_train: 72.7462 loss_rec: 72.7462 acc_train: 0.5344 loss_val: 71.8373 acc_val: 0.5383 time: 0.0263s\n",
      "Epoch: 00990 loss_train: 69.6534 loss_rec: 69.6534 acc_train: 0.5364 loss_val: 68.7807 acc_val: 0.5417 time: 0.0273s\n",
      "Epoch: 00991 loss_train: 35.3073 loss_rec: 35.3073 acc_train: 0.5463 loss_val: 34.8328 acc_val: 0.5504 time: 0.0268s\n",
      "Test set results: loss= 24.7211 accuracy= 0.5528\n",
      "Epoch: 00992 loss_train: 27.5803 loss_rec: 27.5803 acc_train: 0.4957 loss_val: 28.0009 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 00993 loss_train: 51.5176 loss_rec: 51.5176 acc_train: 0.4957 loss_val: 52.3029 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 00994 loss_train: 37.1081 loss_rec: 37.1081 acc_train: 0.4957 loss_val: 37.6737 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 00995 loss_train: 10.1784 loss_rec: 10.1784 acc_train: 0.5571 loss_val: 10.0880 acc_val: 0.5608 time: 0.0258s\n",
      "Epoch: 00996 loss_train: 20.5755 loss_rec: 20.5755 acc_train: 0.5547 loss_val: 20.3093 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 00997 loss_train: 0.9022 loss_rec: 0.9022 acc_train: 0.6319 loss_val: 0.9139 acc_val: 0.6163 time: 0.0278s\n",
      "Epoch: 00998 loss_train: 25.7121 loss_rec: 25.7121 acc_train: 0.4952 loss_val: 26.1044 acc_val: 0.5000 time: 0.0268s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00999 loss_train: 13.1569 loss_rec: 13.1569 acc_train: 0.4953 loss_val: 13.3585 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01000 loss_train: 30.6939 loss_rec: 30.6939 acc_train: 0.5499 loss_val: 30.2767 acc_val: 0.5542 time: 0.0263s\n",
      "Epoch: 01001 loss_train: 39.1676 loss_rec: 39.1676 acc_train: 0.5467 loss_val: 38.6468 acc_val: 0.5496 time: 0.0258s\n",
      "Test set results: loss= 17.8488 accuracy= 0.5167\n",
      "Epoch: 01002 loss_train: 15.7682 loss_rec: 15.7682 acc_train: 0.5557 loss_val: 15.5844 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 01003 loss_train: 37.7972 loss_rec: 37.7972 acc_train: 0.4957 loss_val: 38.3733 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 01004 loss_train: 52.0608 loss_rec: 52.0608 acc_train: 0.4957 loss_val: 52.8543 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 01005 loss_train: 29.0889 loss_rec: 29.0889 acc_train: 0.4957 loss_val: 29.5326 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01006 loss_train: 24.4893 loss_rec: 24.4893 acc_train: 0.5521 loss_val: 24.1615 acc_val: 0.5500 time: 0.0263s\n",
      "Epoch: 01007 loss_train: 40.9451 loss_rec: 40.9451 acc_train: 0.5461 loss_val: 40.4034 acc_val: 0.5492 time: 0.0273s\n",
      "Epoch: 01008 loss_train: 24.6756 loss_rec: 24.6756 acc_train: 0.5515 loss_val: 24.3457 acc_val: 0.5496 time: 0.0258s\n",
      "Epoch: 01009 loss_train: 21.3046 loss_rec: 21.3046 acc_train: 0.4952 loss_val: 21.6300 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01010 loss_train: 29.4972 loss_rec: 29.4972 acc_train: 0.4957 loss_val: 29.9471 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01011 loss_train: 1.3311 loss_rec: 1.3311 acc_train: 0.4957 loss_val: 1.3471 acc_val: 0.4996 time: 0.0258s\n",
      "Test set results: loss= 52.3788 accuracy= 0.4971\n",
      "Epoch: 01012 loss_train: 46.1991 loss_rec: 46.1991 acc_train: 0.5450 loss_val: 45.5941 acc_val: 0.5492 time: 0.0268s\n",
      "Epoch: 01013 loss_train: 58.6044 loss_rec: 58.6044 acc_train: 0.5407 loss_val: 57.8553 acc_val: 0.5454 time: 0.0278s\n",
      "Epoch: 01014 loss_train: 38.5689 loss_rec: 38.5689 acc_train: 0.5464 loss_val: 38.0552 acc_val: 0.5504 time: 0.0273s\n",
      "Epoch: 01015 loss_train: 10.1412 loss_rec: 10.1412 acc_train: 0.4955 loss_val: 10.2970 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01016 loss_train: 21.8186 loss_rec: 21.8186 acc_train: 0.4952 loss_val: 22.1518 acc_val: 0.5000 time: 0.0258s\n",
      "Epoch: 01017 loss_train: 2.7376 loss_rec: 2.7376 acc_train: 0.5898 loss_val: 2.7488 acc_val: 0.5933 time: 0.0258s\n",
      "Epoch: 01018 loss_train: 2.2777 loss_rec: 2.2777 acc_train: 0.4952 loss_val: 2.3137 acc_val: 0.4992 time: 0.0278s\n",
      "Epoch: 01019 loss_train: 24.0025 loss_rec: 24.0025 acc_train: 0.5515 loss_val: 23.6815 acc_val: 0.5521 time: 0.0278s\n",
      "Epoch: 01020 loss_train: 17.7230 loss_rec: 17.7230 acc_train: 0.5538 loss_val: 17.5063 acc_val: 0.5583 time: 0.0313s\n",
      "Epoch: 01021 loss_train: 18.9265 loss_rec: 18.9265 acc_train: 0.4952 loss_val: 19.2158 acc_val: 0.5000 time: 0.0338s\n",
      "Test set results: loss= 16.4324 accuracy= 0.5517\n",
      "Epoch: 01022 loss_train: 18.3524 loss_rec: 18.3524 acc_train: 0.4952 loss_val: 18.6330 acc_val: 0.5000 time: 0.0293s\n",
      "Epoch: 01023 loss_train: 15.6828 loss_rec: 15.6828 acc_train: 0.5557 loss_val: 15.5014 acc_val: 0.5596 time: 0.0318s\n",
      "Epoch: 01024 loss_train: 15.4494 loss_rec: 15.4494 acc_train: 0.5557 loss_val: 15.2719 acc_val: 0.5596 time: 0.0328s\n",
      "Epoch: 01025 loss_train: 15.4129 loss_rec: 15.4129 acc_train: 0.4952 loss_val: 15.6489 acc_val: 0.5000 time: 0.0323s\n",
      "Epoch: 01026 loss_train: 9.4980 loss_rec: 9.4980 acc_train: 0.4955 loss_val: 9.6440 acc_val: 0.5000 time: 0.0293s\n",
      "Epoch: 01027 loss_train: 28.2420 loss_rec: 28.2420 acc_train: 0.5530 loss_val: 27.8604 acc_val: 0.5542 time: 0.0313s\n",
      "Epoch: 01028 loss_train: 31.7429 loss_rec: 31.7429 acc_train: 0.5493 loss_val: 31.3108 acc_val: 0.5521 time: 0.0318s\n",
      "Epoch: 01029 loss_train: 4.5631 loss_rec: 4.5631 acc_train: 0.5737 loss_val: 4.5518 acc_val: 0.5767 time: 0.0313s\n",
      "Epoch: 01030 loss_train: 51.8893 loss_rec: 51.8893 acc_train: 0.4957 loss_val: 52.6802 acc_val: 0.4996 time: 0.0313s\n",
      "Epoch: 01031 loss_train: 67.9252 loss_rec: 67.9252 acc_train: 0.4972 loss_val: 68.9609 acc_val: 0.4996 time: 0.0298s\n",
      "Test set results: loss= 41.9951 accuracy= 0.5528\n",
      "Epoch: 01032 loss_train: 46.8135 loss_rec: 46.8135 acc_train: 0.4957 loss_val: 47.5270 acc_val: 0.4996 time: 0.0253s\n",
      "Epoch: 01033 loss_train: 6.5386 loss_rec: 6.5386 acc_train: 0.5643 loss_val: 6.4996 acc_val: 0.5658 time: 0.0273s\n",
      "Epoch: 01034 loss_train: 22.6386 loss_rec: 22.6386 acc_train: 0.5547 loss_val: 22.3386 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 01035 loss_train: 7.0989 loss_rec: 7.0989 acc_train: 0.5634 loss_val: 7.0536 acc_val: 0.5646 time: 0.0273s\n",
      "Epoch: 01036 loss_train: 38.5136 loss_rec: 38.5136 acc_train: 0.4957 loss_val: 39.1008 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 01037 loss_train: 45.1914 loss_rec: 45.1914 acc_train: 0.4957 loss_val: 45.8804 acc_val: 0.4996 time: 0.0258s\n",
      "Epoch: 01038 loss_train: 15.7044 loss_rec: 15.7044 acc_train: 0.4953 loss_val: 15.9449 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01039 loss_train: 42.1336 loss_rec: 42.1336 acc_train: 0.5450 loss_val: 41.5768 acc_val: 0.5479 time: 0.0278s\n",
      "Epoch: 01040 loss_train: 63.3256 loss_rec: 63.3256 acc_train: 0.5377 loss_val: 62.5225 acc_val: 0.5425 time: 0.0273s\n",
      "Epoch: 01041 loss_train: 51.3539 loss_rec: 51.3539 acc_train: 0.5424 loss_val: 50.6871 acc_val: 0.5483 time: 0.0273s\n",
      "Test set results: loss= 11.2220 accuracy= 0.5201\n",
      "Epoch: 01042 loss_train: 9.9164 loss_rec: 9.9164 acc_train: 0.5558 loss_val: 9.8307 acc_val: 0.5604 time: 0.0273s\n",
      "Epoch: 01043 loss_train: 60.9236 loss_rec: 60.9236 acc_train: 0.4957 loss_val: 61.8525 acc_val: 0.4996 time: 0.0368s\n",
      "Epoch: 01044 loss_train: 90.6228 loss_rec: 90.6228 acc_train: 0.4971 loss_val: 92.0049 acc_val: 0.4996 time: 0.0328s\n",
      "Epoch: 01045 loss_train: 81.9064 loss_rec: 81.9064 acc_train: 0.4972 loss_val: 83.1555 acc_val: 0.4996 time: 0.0358s\n",
      "Epoch: 01046 loss_train: 38.6294 loss_rec: 38.6294 acc_train: 0.4957 loss_val: 39.2183 acc_val: 0.5000 time: 0.0323s\n",
      "Epoch: 01047 loss_train: 32.2166 loss_rec: 32.2166 acc_train: 0.5493 loss_val: 31.7781 acc_val: 0.5521 time: 0.0328s\n",
      "Epoch: 01048 loss_train: 63.9392 loss_rec: 63.9392 acc_train: 0.5372 loss_val: 63.1291 acc_val: 0.5421 time: 0.0368s\n",
      "Epoch: 01049 loss_train: 61.5664 loss_rec: 61.5664 acc_train: 0.5391 loss_val: 60.7829 acc_val: 0.5438 time: 0.0402s\n",
      "Epoch: 01050 loss_train: 28.5782 loss_rec: 28.5782 acc_train: 0.5523 loss_val: 28.1916 acc_val: 0.5538 time: 0.0323s\n",
      "Epoch: 01051 loss_train: 32.7734 loss_rec: 32.7734 acc_train: 0.4957 loss_val: 33.2734 acc_val: 0.5000 time: 0.0368s\n",
      "Test set results: loss= 49.8472 accuracy= 0.5528\n",
      "Epoch: 01052 loss_train: 55.5561 loss_rec: 55.5561 acc_train: 0.4957 loss_val: 56.4033 acc_val: 0.4996 time: 0.0368s\n",
      "Epoch: 01053 loss_train: 40.7949 loss_rec: 40.7949 acc_train: 0.4957 loss_val: 41.4169 acc_val: 0.4996 time: 0.0353s\n",
      "Epoch: 01054 loss_train: 6.5125 loss_rec: 6.5125 acc_train: 0.5647 loss_val: 6.4740 acc_val: 0.5671 time: 0.0343s\n",
      "Epoch: 01055 loss_train: 17.8592 loss_rec: 17.8592 acc_train: 0.5538 loss_val: 17.6406 acc_val: 0.5583 time: 0.0343s\n",
      "Epoch: 01056 loss_train: 1.2574 loss_rec: 1.2574 acc_train: 0.4957 loss_val: 1.2717 acc_val: 0.5000 time: 0.0353s\n",
      "Epoch: 01057 loss_train: 3.3213 loss_rec: 3.3213 acc_train: 0.5857 loss_val: 3.3253 acc_val: 0.5863 time: 0.0353s\n",
      "Epoch: 01058 loss_train: 20.2986 loss_rec: 20.2986 acc_train: 0.4952 loss_val: 20.6091 acc_val: 0.5000 time: 0.0358s\n",
      "Epoch: 01059 loss_train: 7.0056 loss_rec: 7.0056 acc_train: 0.4939 loss_val: 7.1138 acc_val: 0.4979 time: 0.0318s\n",
      "Epoch: 01060 loss_train: 36.3103 loss_rec: 36.3103 acc_train: 0.5463 loss_val: 35.8227 acc_val: 0.5508 time: 0.0273s\n",
      "Epoch: 01061 loss_train: 45.2459 loss_rec: 45.2459 acc_train: 0.5454 loss_val: 44.6518 acc_val: 0.5500 time: 0.0258s\n",
      "Test set results: loss= 25.6986 accuracy= 0.5135\n",
      "Epoch: 01062 loss_train: 22.6991 loss_rec: 22.6991 acc_train: 0.5540 loss_val: 22.3982 acc_val: 0.5583 time: 0.0273s\n",
      "Epoch: 01063 loss_train: 28.7965 loss_rec: 28.7965 acc_train: 0.4957 loss_val: 29.2362 acc_val: 0.5000 time: 0.0288s\n",
      "Epoch: 01064 loss_train: 42.4528 loss_rec: 42.4528 acc_train: 0.4957 loss_val: 43.1002 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 01065 loss_train: 19.6244 loss_rec: 19.6244 acc_train: 0.4954 loss_val: 19.9248 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01066 loss_train: 32.5069 loss_rec: 32.5069 acc_train: 0.5493 loss_val: 32.0643 acc_val: 0.5521 time: 0.0283s\n",
      "Epoch: 01067 loss_train: 48.7637 loss_rec: 48.7637 acc_train: 0.5426 loss_val: 48.1269 acc_val: 0.5479 time: 0.0288s\n",
      "Epoch: 01068 loss_train: 32.8173 loss_rec: 32.8173 acc_train: 0.5485 loss_val: 32.3709 acc_val: 0.5525 time: 0.0283s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01069 loss_train: 11.6543 loss_rec: 11.6543 acc_train: 0.4955 loss_val: 11.8336 acc_val: 0.5000 time: 0.0288s\n",
      "Epoch: 01070 loss_train: 19.8123 loss_rec: 19.8123 acc_train: 0.4954 loss_val: 20.1155 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01071 loss_train: 6.6802 loss_rec: 6.6802 acc_train: 0.5639 loss_val: 6.6405 acc_val: 0.5650 time: 0.0263s\n",
      "Test set results: loss= 1.7679 accuracy= 0.5959\n",
      "Epoch: 01072 loss_train: 1.6202 loss_rec: 1.6202 acc_train: 0.6163 loss_val: 1.6408 acc_val: 0.6125 time: 0.0273s\n",
      "Epoch: 01073 loss_train: 26.3670 loss_rec: 26.3670 acc_train: 0.4952 loss_val: 26.7698 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01074 loss_train: 16.6409 loss_rec: 16.6409 acc_train: 0.4954 loss_val: 16.8961 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01075 loss_train: 24.2651 loss_rec: 24.2651 acc_train: 0.5507 loss_val: 23.9405 acc_val: 0.5517 time: 0.0258s\n",
      "Epoch: 01076 loss_train: 30.7364 loss_rec: 30.7364 acc_train: 0.5504 loss_val: 30.3184 acc_val: 0.5542 time: 0.0283s\n",
      "Epoch: 01077 loss_train: 6.6595 loss_rec: 6.6595 acc_train: 0.5639 loss_val: 6.6199 acc_val: 0.5650 time: 0.0278s\n",
      "Epoch: 01078 loss_train: 46.7340 loss_rec: 46.7340 acc_train: 0.4957 loss_val: 47.4469 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 01079 loss_train: 60.9074 loss_rec: 60.9074 acc_train: 0.4957 loss_val: 61.8365 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 01080 loss_train: 38.6306 loss_rec: 38.6306 acc_train: 0.4957 loss_val: 39.2201 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01081 loss_train: 14.3305 loss_rec: 14.3305 acc_train: 0.5570 loss_val: 14.1734 acc_val: 0.5629 time: 0.0268s\n",
      "Test set results: loss= 34.3745 accuracy= 0.5078\n",
      "Epoch: 01082 loss_train: 30.3522 loss_rec: 30.3522 acc_train: 0.5509 loss_val: 29.9397 acc_val: 0.5538 time: 0.0293s\n",
      "Epoch: 01083 loss_train: 14.7997 loss_rec: 14.7997 acc_train: 0.5562 loss_val: 14.6347 acc_val: 0.5617 time: 0.0283s\n",
      "Epoch: 01084 loss_train: 30.4315 loss_rec: 30.4315 acc_train: 0.4957 loss_val: 30.8964 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01085 loss_train: 38.0491 loss_rec: 38.0491 acc_train: 0.4957 loss_val: 38.6299 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01086 loss_train: 9.9783 loss_rec: 9.9783 acc_train: 0.4956 loss_val: 10.1322 acc_val: 0.5000 time: 0.0258s\n",
      "Epoch: 01087 loss_train: 45.7241 loss_rec: 45.7241 acc_train: 0.5454 loss_val: 45.1237 acc_val: 0.5500 time: 0.0263s\n",
      "Epoch: 01088 loss_train: 65.8327 loss_rec: 65.8327 acc_train: 0.5364 loss_val: 65.0001 acc_val: 0.5417 time: 0.0278s\n",
      "Epoch: 01089 loss_train: 53.3821 loss_rec: 53.3821 acc_train: 0.5422 loss_val: 52.6904 acc_val: 0.5471 time: 0.0283s\n",
      "Epoch: 01090 loss_train: 11.9834 loss_rec: 11.9834 acc_train: 0.5546 loss_val: 11.8677 acc_val: 0.5579 time: 0.0273s\n",
      "Epoch: 01091 loss_train: 58.2603 loss_rec: 58.2603 acc_train: 0.4957 loss_val: 59.1492 acc_val: 0.4996 time: 0.0283s\n",
      "Test set results: loss= 79.2349 accuracy= 0.5542\n",
      "Epoch: 01092 loss_train: 88.2756 loss_rec: 88.2756 acc_train: 0.4972 loss_val: 89.6223 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 01093 loss_train: 80.3616 loss_rec: 80.3616 acc_train: 0.4972 loss_val: 81.5876 acc_val: 0.4996 time: 0.0283s\n",
      "Epoch: 01094 loss_train: 38.4216 loss_rec: 38.4216 acc_train: 0.4957 loss_val: 39.0080 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01095 loss_train: 30.7240 loss_rec: 30.7240 acc_train: 0.5504 loss_val: 30.3067 acc_val: 0.5546 time: 0.0293s\n",
      "Epoch: 01096 loss_train: 61.4760 loss_rec: 61.4760 acc_train: 0.5383 loss_val: 60.6925 acc_val: 0.5438 time: 0.0263s\n",
      "Epoch: 01097 loss_train: 58.7525 loss_rec: 58.7525 acc_train: 0.5416 loss_val: 57.9997 acc_val: 0.5458 time: 0.0333s\n",
      "Epoch: 01098 loss_train: 25.9950 loss_rec: 25.9950 acc_train: 0.5527 loss_val: 25.6467 acc_val: 0.5538 time: 0.0268s\n",
      "Epoch: 01099 loss_train: 34.7828 loss_rec: 34.7828 acc_train: 0.4957 loss_val: 35.3140 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01100 loss_train: 57.4655 loss_rec: 57.4655 acc_train: 0.4957 loss_val: 58.3421 acc_val: 0.4996 time: 0.0288s\n",
      "Epoch: 01101 loss_train: 43.1711 loss_rec: 43.1711 acc_train: 0.4957 loss_val: 43.8298 acc_val: 0.5000 time: 0.0273s\n",
      "Test set results: loss= 4.0202 accuracy= 0.5519\n",
      "Epoch: 01102 loss_train: 3.5855 loss_rec: 3.5855 acc_train: 0.5842 loss_val: 3.5876 acc_val: 0.5850 time: 0.0258s\n",
      "Epoch: 01103 loss_train: 16.1459 loss_rec: 16.1459 acc_train: 0.5550 loss_val: 15.9583 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 01104 loss_train: 1.2179 loss_rec: 1.2179 acc_train: 0.5742 loss_val: 1.2319 acc_val: 0.5638 time: 0.0278s\n",
      "Epoch: 01105 loss_train: 3.5052 loss_rec: 3.5052 acc_train: 0.5852 loss_val: 3.5082 acc_val: 0.5863 time: 0.0273s\n",
      "Epoch: 01106 loss_train: 19.6971 loss_rec: 19.6971 acc_train: 0.4954 loss_val: 19.9990 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01107 loss_train: 6.6476 loss_rec: 6.6476 acc_train: 0.4940 loss_val: 6.7509 acc_val: 0.4983 time: 0.0263s\n",
      "Epoch: 01108 loss_train: 35.9156 loss_rec: 35.9156 acc_train: 0.5468 loss_val: 35.4318 acc_val: 0.5508 time: 0.0258s\n",
      "Epoch: 01109 loss_train: 44.7197 loss_rec: 44.7197 acc_train: 0.5451 loss_val: 44.1309 acc_val: 0.5488 time: 0.0278s\n",
      "Epoch: 01110 loss_train: 22.5866 loss_rec: 22.5866 acc_train: 0.5540 loss_val: 22.2887 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 01111 loss_train: 28.0148 loss_rec: 28.0148 acc_train: 0.4953 loss_val: 28.4431 acc_val: 0.5000 time: 0.0273s\n",
      "Test set results: loss= 37.1341 accuracy= 0.5528\n",
      "Epoch: 01112 loss_train: 41.4021 loss_rec: 41.4021 acc_train: 0.4957 loss_val: 42.0341 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01113 loss_train: 18.9255 loss_rec: 18.9255 acc_train: 0.4954 loss_val: 19.2158 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01114 loss_train: 32.3064 loss_rec: 32.3064 acc_train: 0.5496 loss_val: 31.8660 acc_val: 0.5538 time: 0.0283s\n",
      "Epoch: 01115 loss_train: 48.3542 loss_rec: 48.3542 acc_train: 0.5420 loss_val: 47.7218 acc_val: 0.5475 time: 0.0273s\n",
      "Epoch: 01116 loss_train: 32.6987 loss_rec: 32.6987 acc_train: 0.5493 loss_val: 32.2538 acc_val: 0.5525 time: 0.0278s\n",
      "Epoch: 01117 loss_train: 10.9664 loss_rec: 10.9664 acc_train: 0.4956 loss_val: 11.1356 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01118 loss_train: 18.9549 loss_rec: 18.9549 acc_train: 0.4954 loss_val: 19.2457 acc_val: 0.5000 time: 0.0282s\n",
      "Epoch: 01119 loss_train: 7.0670 loss_rec: 7.0670 acc_train: 0.5630 loss_val: 7.0238 acc_val: 0.5638 time: 0.0263s\n",
      "Epoch: 01120 loss_train: 1.9467 loss_rec: 1.9467 acc_train: 0.6075 loss_val: 1.9669 acc_val: 0.6079 time: 0.0273s\n",
      "Epoch: 01121 loss_train: 27.4790 loss_rec: 27.4790 acc_train: 0.4953 loss_val: 27.8996 acc_val: 0.5000 time: 0.0278s\n",
      "Test set results: loss= 17.7406 accuracy= 0.5517\n",
      "Epoch: 01122 loss_train: 19.8100 loss_rec: 19.8100 acc_train: 0.4954 loss_val: 20.1140 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01123 loss_train: 19.1196 loss_rec: 19.1196 acc_train: 0.5526 loss_val: 18.8812 acc_val: 0.5579 time: 0.0283s\n",
      "Epoch: 01124 loss_train: 24.2003 loss_rec: 24.2003 acc_train: 0.5514 loss_val: 23.8769 acc_val: 0.5550 time: 0.0258s\n",
      "Epoch: 01125 loss_train: 0.9114 loss_rec: 0.9114 acc_train: 0.6325 loss_val: 0.9247 acc_val: 0.6146 time: 0.0268s\n",
      "Epoch: 01126 loss_train: 28.4026 loss_rec: 28.4026 acc_train: 0.4953 loss_val: 28.8369 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01127 loss_train: 19.6777 loss_rec: 19.6777 acc_train: 0.4954 loss_val: 19.9795 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01128 loss_train: 19.7625 loss_rec: 19.7625 acc_train: 0.5550 loss_val: 19.5106 acc_val: 0.5600 time: 0.0283s\n",
      "Epoch: 01129 loss_train: 25.5974 loss_rec: 25.5974 acc_train: 0.5527 loss_val: 25.2541 acc_val: 0.5542 time: 0.0278s\n",
      "Epoch: 01130 loss_train: 2.2784 loss_rec: 2.2784 acc_train: 0.6011 loss_val: 2.2957 acc_val: 0.5988 time: 0.0258s\n",
      "Epoch: 01131 loss_train: 46.0843 loss_rec: 46.0843 acc_train: 0.4957 loss_val: 46.7877 acc_val: 0.5000 time: 0.0263s\n",
      "Test set results: loss= 50.2091 accuracy= 0.5528\n",
      "Epoch: 01132 loss_train: 55.9585 loss_rec: 55.9585 acc_train: 0.4957 loss_val: 56.8123 acc_val: 0.4996 time: 0.0283s\n",
      "Epoch: 01133 loss_train: 30.7917 loss_rec: 30.7917 acc_train: 0.4953 loss_val: 31.2626 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01134 loss_train: 23.0528 loss_rec: 23.0528 acc_train: 0.5518 loss_val: 22.7455 acc_val: 0.5554 time: 0.0283s\n",
      "Epoch: 01135 loss_train: 41.2352 loss_rec: 41.2352 acc_train: 0.5453 loss_val: 40.6891 acc_val: 0.5475 time: 0.0268s\n",
      "Epoch: 01136 loss_train: 28.0922 loss_rec: 28.0922 acc_train: 0.5523 loss_val: 27.7127 acc_val: 0.5542 time: 0.0278s\n",
      "Epoch: 01137 loss_train: 12.9829 loss_rec: 12.9829 acc_train: 0.4956 loss_val: 13.1830 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01138 loss_train: 18.8328 loss_rec: 18.8328 acc_train: 0.4954 loss_val: 19.1219 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01139 loss_train: 8.3767 loss_rec: 8.3767 acc_train: 0.5578 loss_val: 8.3150 acc_val: 0.5629 time: 0.0258s\n",
      "Epoch: 01140 loss_train: 4.1698 loss_rec: 4.1698 acc_train: 0.5745 loss_val: 4.1642 acc_val: 0.5775 time: 0.0278s\n",
      "Epoch: 01141 loss_train: 28.1729 loss_rec: 28.1729 acc_train: 0.4954 loss_val: 28.6040 acc_val: 0.5000 time: 0.0268s\n",
      "Test set results: loss= 21.6471 accuracy= 0.5517\n",
      "Epoch: 01142 loss_train: 24.1583 loss_rec: 24.1583 acc_train: 0.4954 loss_val: 24.5284 acc_val: 0.5000 time: 0.0268s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01143 loss_train: 11.6000 loss_rec: 11.6000 acc_train: 0.5537 loss_val: 11.4894 acc_val: 0.5575 time: 0.0273s\n",
      "Epoch: 01144 loss_train: 14.0995 loss_rec: 14.0995 acc_train: 0.5563 loss_val: 13.9452 acc_val: 0.5625 time: 0.0278s\n",
      "Epoch: 01145 loss_train: 12.6100 loss_rec: 12.6100 acc_train: 0.4956 loss_val: 12.8045 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01146 loss_train: 4.5882 loss_rec: 4.5882 acc_train: 0.4945 loss_val: 4.6620 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 01147 loss_train: 32.9173 loss_rec: 32.9173 acc_train: 0.5485 loss_val: 32.4694 acc_val: 0.5525 time: 0.0273s\n",
      "Epoch: 01148 loss_train: 37.9470 loss_rec: 37.9470 acc_train: 0.5464 loss_val: 37.4395 acc_val: 0.5500 time: 0.0273s\n",
      "Epoch: 01149 loss_train: 13.2058 loss_rec: 13.2058 acc_train: 0.5573 loss_val: 13.0673 acc_val: 0.5638 time: 0.0273s\n",
      "Epoch: 01150 loss_train: 40.0301 loss_rec: 40.0301 acc_train: 0.4958 loss_val: 40.6417 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01151 loss_train: 55.8243 loss_rec: 55.8243 acc_train: 0.4957 loss_val: 56.6763 acc_val: 0.4996 time: 0.0318s\n",
      "Test set results: loss= 32.4360 accuracy= 0.5528\n",
      "Epoch: 01152 loss_train: 36.1705 loss_rec: 36.1705 acc_train: 0.4958 loss_val: 36.7233 acc_val: 0.5000 time: 0.0298s\n",
      "Epoch: 01153 loss_train: 13.4121 loss_rec: 13.4121 acc_train: 0.5573 loss_val: 13.2702 acc_val: 0.5638 time: 0.0318s\n",
      "Epoch: 01154 loss_train: 27.5037 loss_rec: 27.5037 acc_train: 0.5520 loss_val: 27.1330 acc_val: 0.5542 time: 0.0372s\n",
      "Epoch: 01155 loss_train: 11.2451 loss_rec: 11.2451 acc_train: 0.5544 loss_val: 11.1405 acc_val: 0.5579 time: 0.0353s\n",
      "Epoch: 01156 loss_train: 33.7503 loss_rec: 33.7503 acc_train: 0.4958 loss_val: 34.2664 acc_val: 0.5000 time: 0.0353s\n",
      "Epoch: 01157 loss_train: 42.0376 loss_rec: 42.0376 acc_train: 0.4958 loss_val: 42.6796 acc_val: 0.5000 time: 0.0358s\n",
      "Epoch: 01158 loss_train: 15.6861 loss_rec: 15.6861 acc_train: 0.4956 loss_val: 15.9276 acc_val: 0.5000 time: 0.0353s\n",
      "Epoch: 01159 loss_train: 37.8786 loss_rec: 37.8786 acc_train: 0.5463 loss_val: 37.3718 acc_val: 0.5500 time: 0.0328s\n",
      "Epoch: 01160 loss_train: 56.7425 loss_rec: 56.7425 acc_train: 0.5413 loss_val: 56.0126 acc_val: 0.5467 time: 0.0243s\n",
      "Epoch: 01161 loss_train: 44.2525 loss_rec: 44.2525 acc_train: 0.5454 loss_val: 43.6701 acc_val: 0.5496 time: 0.0268s\n",
      "Test set results: loss= 4.6899 accuracy= 0.5400\n",
      "Epoch: 01162 loss_train: 4.1713 loss_rec: 4.1713 acc_train: 0.5744 loss_val: 4.1665 acc_val: 0.5775 time: 0.0283s\n",
      "Epoch: 01163 loss_train: 62.9062 loss_rec: 62.9062 acc_train: 0.4957 loss_val: 63.8661 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 01164 loss_train: 90.3126 loss_rec: 90.3126 acc_train: 0.4972 loss_val: 91.6907 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 01165 loss_train: 81.1745 loss_rec: 81.1745 acc_train: 0.4967 loss_val: 82.4133 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 01166 loss_train: 39.2631 loss_rec: 39.2631 acc_train: 0.4958 loss_val: 39.8631 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01167 loss_train: 28.8539 loss_rec: 28.8539 acc_train: 0.5523 loss_val: 28.4633 acc_val: 0.5542 time: 0.0273s\n",
      "Epoch: 01168 loss_train: 59.6045 loss_rec: 59.6045 acc_train: 0.5399 loss_val: 58.8423 acc_val: 0.5458 time: 0.0283s\n",
      "Epoch: 01169 loss_train: 57.8957 loss_rec: 57.8957 acc_train: 0.5407 loss_val: 57.1526 acc_val: 0.5458 time: 0.0268s\n",
      "Epoch: 01170 loss_train: 27.0543 loss_rec: 27.0543 acc_train: 0.5519 loss_val: 26.6900 acc_val: 0.5538 time: 0.0268s\n",
      "Epoch: 01171 loss_train: 30.7866 loss_rec: 30.7866 acc_train: 0.4954 loss_val: 31.2576 acc_val: 0.5000 time: 0.0283s\n",
      "Test set results: loss= 46.6737 accuracy= 0.5528\n",
      "Epoch: 01172 loss_train: 52.0226 loss_rec: 52.0226 acc_train: 0.4957 loss_val: 52.8166 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01173 loss_train: 37.5357 loss_rec: 37.5357 acc_train: 0.4958 loss_val: 38.1094 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01174 loss_train: 7.6479 loss_rec: 7.6479 acc_train: 0.5599 loss_val: 7.5973 acc_val: 0.5625 time: 0.0273s\n",
      "Epoch: 01175 loss_train: 18.5508 loss_rec: 18.5508 acc_train: 0.5525 loss_val: 18.3203 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 01176 loss_train: 1.0885 loss_rec: 1.0885 acc_train: 0.6286 loss_val: 1.1074 acc_val: 0.6154 time: 0.0263s\n",
      "Epoch: 01177 loss_train: 31.4115 loss_rec: 31.4115 acc_train: 0.4954 loss_val: 31.8922 acc_val: 0.5000 time: 0.0258s\n",
      "Epoch: 01178 loss_train: 26.9483 loss_rec: 26.9483 acc_train: 0.4954 loss_val: 27.3611 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01179 loss_train: 9.0613 loss_rec: 9.0613 acc_train: 0.5562 loss_val: 8.9901 acc_val: 0.5617 time: 0.0278s\n",
      "Epoch: 01180 loss_train: 12.1922 loss_rec: 12.1922 acc_train: 0.5536 loss_val: 12.0724 acc_val: 0.5571 time: 0.0273s\n",
      "Epoch: 01181 loss_train: 13.5760 loss_rec: 13.5760 acc_train: 0.4956 loss_val: 13.7855 acc_val: 0.5000 time: 0.0278s\n",
      "Test set results: loss= 4.4069 accuracy= 0.5503\n",
      "Epoch: 01182 loss_train: 4.9641 loss_rec: 4.9641 acc_train: 0.4945 loss_val: 5.0441 acc_val: 0.4988 time: 0.0269s\n",
      "Epoch: 01183 loss_train: 32.6337 loss_rec: 32.6337 acc_train: 0.5493 loss_val: 32.1894 acc_val: 0.5525 time: 0.0288s\n",
      "Epoch: 01184 loss_train: 38.1302 loss_rec: 38.1302 acc_train: 0.5463 loss_val: 37.6203 acc_val: 0.5500 time: 0.0273s\n",
      "Epoch: 01185 loss_train: 14.1618 loss_rec: 14.1618 acc_train: 0.5563 loss_val: 14.0074 acc_val: 0.5629 time: 0.0283s\n",
      "Epoch: 01186 loss_train: 37.8495 loss_rec: 37.8495 acc_train: 0.4958 loss_val: 38.4281 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01187 loss_train: 52.9853 loss_rec: 52.9853 acc_train: 0.4957 loss_val: 53.7944 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01188 loss_train: 33.2356 loss_rec: 33.2356 acc_train: 0.4954 loss_val: 33.7442 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01189 loss_train: 15.7623 loss_rec: 15.7623 acc_train: 0.5550 loss_val: 15.5804 acc_val: 0.5596 time: 0.0283s\n",
      "Epoch: 01190 loss_train: 29.9172 loss_rec: 29.9172 acc_train: 0.5509 loss_val: 29.5111 acc_val: 0.5538 time: 0.0273s\n",
      "Epoch: 01191 loss_train: 14.0783 loss_rec: 14.0783 acc_train: 0.5567 loss_val: 13.9259 acc_val: 0.5633 time: 0.0273s\n",
      "Test set results: loss= 26.9178 accuracy= 0.5517\n",
      "Epoch: 01192 loss_train: 30.0274 loss_rec: 30.0274 acc_train: 0.4954 loss_val: 30.4873 acc_val: 0.5000 time: 0.0279s\n",
      "Epoch: 01193 loss_train: 38.1791 loss_rec: 38.1791 acc_train: 0.4958 loss_val: 38.7628 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01194 loss_train: 12.1872 loss_rec: 12.1872 acc_train: 0.4957 loss_val: 12.3755 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01195 loss_train: 40.4375 loss_rec: 40.4375 acc_train: 0.5464 loss_val: 39.8999 acc_val: 0.5488 time: 0.0278s\n",
      "Epoch: 01196 loss_train: 59.0323 loss_rec: 59.0323 acc_train: 0.5403 loss_val: 58.2758 acc_val: 0.5450 time: 0.0278s\n",
      "Epoch: 01197 loss_train: 46.6833 loss_rec: 46.6833 acc_train: 0.5447 loss_val: 46.0709 acc_val: 0.5496 time: 0.0278s\n",
      "Epoch: 01198 loss_train: 6.9552 loss_rec: 6.9552 acc_train: 0.5614 loss_val: 6.9132 acc_val: 0.5633 time: 0.0263s\n",
      "Epoch: 01199 loss_train: 60.0063 loss_rec: 60.0063 acc_train: 0.4957 loss_val: 60.9223 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 01200 loss_train: 88.1476 loss_rec: 88.1476 acc_train: 0.4972 loss_val: 89.4929 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 01201 loss_train: 80.1402 loss_rec: 80.1402 acc_train: 0.4957 loss_val: 81.3634 acc_val: 0.4996 time: 0.0278s\n",
      "Test set results: loss= 35.6015 accuracy= 0.5528\n",
      "Epoch: 01202 loss_train: 39.6959 loss_rec: 39.6959 acc_train: 0.4958 loss_val: 40.3028 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01203 loss_train: 26.7954 loss_rec: 26.7954 acc_train: 0.5527 loss_val: 26.4354 acc_val: 0.5542 time: 0.0263s\n",
      "Epoch: 01204 loss_train: 56.4243 loss_rec: 56.4243 acc_train: 0.5419 loss_val: 55.6973 acc_val: 0.5467 time: 0.0278s\n",
      "Epoch: 01205 loss_train: 54.1624 loss_rec: 54.1624 acc_train: 0.5423 loss_val: 53.4614 acc_val: 0.5471 time: 0.0273s\n",
      "Epoch: 01206 loss_train: 23.2489 loss_rec: 23.2489 acc_train: 0.5530 loss_val: 22.9394 acc_val: 0.5563 time: 0.0278s\n",
      "Epoch: 01207 loss_train: 34.3841 loss_rec: 34.3841 acc_train: 0.4954 loss_val: 34.9103 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01208 loss_train: 55.6196 loss_rec: 55.6196 acc_train: 0.4958 loss_val: 56.4689 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01209 loss_train: 41.6084 loss_rec: 41.6084 acc_train: 0.4958 loss_val: 42.2445 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01210 loss_train: 3.3444 loss_rec: 3.3444 acc_train: 0.5837 loss_val: 3.3501 acc_val: 0.5842 time: 0.0273s\n",
      "Epoch: 01211 loss_train: 15.8811 loss_rec: 15.8811 acc_train: 0.5550 loss_val: 15.6976 acc_val: 0.5596 time: 0.0268s\n",
      "Test set results: loss= 0.8913 accuracy= 0.6315\n",
      "Epoch: 01212 loss_train: 0.9139 loss_rec: 0.9139 acc_train: 0.6312 loss_val: 0.9284 acc_val: 0.6146 time: 0.0278s\n",
      "Epoch: 01213 loss_train: 20.7555 loss_rec: 20.7555 acc_train: 0.4955 loss_val: 21.0747 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01214 loss_train: 6.5788 loss_rec: 6.5788 acc_train: 0.4942 loss_val: 6.6829 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 01215 loss_train: 35.4307 loss_rec: 35.4307 acc_train: 0.5467 loss_val: 34.9515 acc_val: 0.5508 time: 0.0263s\n",
      "Epoch: 01216 loss_train: 45.1330 loss_rec: 45.1330 acc_train: 0.5454 loss_val: 44.5387 acc_val: 0.5496 time: 0.0283s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01217 loss_train: 25.1496 loss_rec: 25.1496 acc_train: 0.5537 loss_val: 24.8118 acc_val: 0.5546 time: 0.0273s\n",
      "Epoch: 01218 loss_train: 21.7181 loss_rec: 21.7181 acc_train: 0.4955 loss_val: 22.0521 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01219 loss_train: 33.5697 loss_rec: 33.5697 acc_train: 0.4954 loss_val: 34.0839 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01220 loss_train: 11.2804 loss_rec: 11.2804 acc_train: 0.4939 loss_val: 11.4554 acc_val: 0.4979 time: 0.0258s\n",
      "Epoch: 01221 loss_train: 37.7994 loss_rec: 37.7994 acc_train: 0.5457 loss_val: 37.2923 acc_val: 0.5500 time: 0.0263s\n",
      "Test set results: loss= 60.8189 accuracy= 0.4942\n",
      "Epoch: 01222 loss_train: 53.6292 loss_rec: 53.6292 acc_train: 0.5422 loss_val: 52.9336 acc_val: 0.5471 time: 0.0283s\n",
      "Epoch: 01223 loss_train: 39.1075 loss_rec: 39.1075 acc_train: 0.5464 loss_val: 38.5850 acc_val: 0.5500 time: 0.0263s\n",
      "Epoch: 01224 loss_train: 1.6887 loss_rec: 1.6887 acc_train: 0.4958 loss_val: 1.7152 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01225 loss_train: 13.3227 loss_rec: 13.3227 acc_train: 0.4957 loss_val: 13.5287 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01226 loss_train: 7.6840 loss_rec: 7.6840 acc_train: 0.5605 loss_val: 7.6341 acc_val: 0.5629 time: 0.0278s\n",
      "Epoch: 01227 loss_train: 0.9097 loss_rec: 0.9097 acc_train: 0.6100 loss_val: 0.9214 acc_val: 0.6021 time: 0.0273s\n",
      "Epoch: 01228 loss_train: 4.2521 loss_rec: 4.2521 acc_train: 0.4947 loss_val: 4.3220 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01229 loss_train: 22.1887 loss_rec: 22.1887 acc_train: 0.5535 loss_val: 21.8974 acc_val: 0.5571 time: 0.0268s\n",
      "Epoch: 01230 loss_train: 18.4121 loss_rec: 18.4121 acc_train: 0.5525 loss_val: 18.1855 acc_val: 0.5583 time: 0.0273s\n",
      "Epoch: 01231 loss_train: 13.1821 loss_rec: 13.1821 acc_train: 0.4957 loss_val: 13.3861 acc_val: 0.5000 time: 0.0263s\n",
      "Test set results: loss= 9.7323 accuracy= 0.5499\n",
      "Epoch: 01232 loss_train: 10.8945 loss_rec: 10.8945 acc_train: 0.4939 loss_val: 11.0635 acc_val: 0.4979 time: 0.0283s\n",
      "Epoch: 01233 loss_train: 21.3733 loss_rec: 21.3733 acc_train: 0.5539 loss_val: 21.0960 acc_val: 0.5592 time: 0.0278s\n",
      "Epoch: 01234 loss_train: 22.4144 loss_rec: 22.4144 acc_train: 0.5535 loss_val: 22.1196 acc_val: 0.5571 time: 0.0268s\n",
      "Epoch: 01235 loss_train: 4.1657 loss_rec: 4.1657 acc_train: 0.4947 loss_val: 4.2344 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 01236 loss_train: 1.8907 loss_rec: 1.8907 acc_train: 0.6097 loss_val: 1.9122 acc_val: 0.6054 time: 0.0258s\n",
      "Epoch: 01237 loss_train: 14.9364 loss_rec: 14.9364 acc_train: 0.4957 loss_val: 15.1670 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01238 loss_train: 2.1378 loss_rec: 2.1378 acc_train: 0.6017 loss_val: 2.1579 acc_val: 0.6000 time: 0.0278s\n",
      "Epoch: 01239 loss_train: 5.7153 loss_rec: 5.7153 acc_train: 0.4944 loss_val: 5.8073 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01240 loss_train: 17.3520 loss_rec: 17.3520 acc_train: 0.5539 loss_val: 17.1439 acc_val: 0.5596 time: 0.0283s\n",
      "Epoch: 01241 loss_train: 10.5409 loss_rec: 10.5409 acc_train: 0.5553 loss_val: 10.4482 acc_val: 0.5596 time: 0.0283s\n",
      "Test set results: loss= 21.8152 accuracy= 0.5517\n",
      "Epoch: 01242 loss_train: 24.3471 loss_rec: 24.3471 acc_train: 0.4955 loss_val: 24.7212 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01243 loss_train: 24.4366 loss_rec: 24.4366 acc_train: 0.4955 loss_val: 24.8120 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01244 loss_train: 7.0455 loss_rec: 7.0455 acc_train: 0.5614 loss_val: 7.0031 acc_val: 0.5638 time: 0.0283s\n",
      "Epoch: 01245 loss_train: 7.3118 loss_rec: 7.3118 acc_train: 0.5607 loss_val: 7.2663 acc_val: 0.5633 time: 0.0283s\n",
      "Epoch: 01246 loss_train: 20.3219 loss_rec: 20.3219 acc_train: 0.4956 loss_val: 20.6349 acc_val: 0.5000 time: 0.0258s\n",
      "Epoch: 01247 loss_train: 13.5605 loss_rec: 13.5605 acc_train: 0.4957 loss_val: 13.7705 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01248 loss_train: 22.5883 loss_rec: 22.5883 acc_train: 0.5535 loss_val: 22.2904 acc_val: 0.5575 time: 0.0273s\n",
      "Epoch: 01249 loss_train: 26.9538 loss_rec: 26.9538 acc_train: 0.5527 loss_val: 26.5914 acc_val: 0.5542 time: 0.0268s\n",
      "Epoch: 01250 loss_train: 3.1640 loss_rec: 3.1640 acc_train: 0.5871 loss_val: 3.1728 acc_val: 0.5892 time: 0.0273s\n",
      "Epoch: 01251 loss_train: 46.3300 loss_rec: 46.3300 acc_train: 0.4958 loss_val: 47.0385 acc_val: 0.5000 time: 0.0278s\n",
      "Test set results: loss= 52.6718 accuracy= 0.5528\n",
      "Epoch: 01252 loss_train: 58.7018 loss_rec: 58.7018 acc_train: 0.4958 loss_val: 59.5984 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01253 loss_train: 36.9506 loss_rec: 36.9506 acc_train: 0.4960 loss_val: 37.5165 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01254 loss_train: 13.4478 loss_rec: 13.4478 acc_train: 0.5573 loss_val: 13.3075 acc_val: 0.5642 time: 0.0273s\n",
      "Epoch: 01255 loss_train: 29.1048 loss_rec: 29.1048 acc_train: 0.5523 loss_val: 28.7110 acc_val: 0.5542 time: 0.0273s\n",
      "Epoch: 01256 loss_train: 15.1024 loss_rec: 15.1024 acc_train: 0.5554 loss_val: 14.9329 acc_val: 0.5617 time: 0.0273s\n",
      "Epoch: 01257 loss_train: 26.5642 loss_rec: 26.5642 acc_train: 0.4955 loss_val: 26.9723 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01258 loss_train: 33.1826 loss_rec: 33.1826 acc_train: 0.4955 loss_val: 33.6913 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01259 loss_train: 6.3856 loss_rec: 6.3856 acc_train: 0.4943 loss_val: 6.4878 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 01260 loss_train: 45.9259 loss_rec: 45.9259 acc_train: 0.5448 loss_val: 45.3213 acc_val: 0.5492 time: 0.0269s\n",
      "Epoch: 01261 loss_train: 65.1411 loss_rec: 65.1411 acc_train: 0.5372 loss_val: 64.3143 acc_val: 0.5421 time: 0.0278s\n",
      "Test set results: loss= 60.9932 accuracy= 0.4941\n",
      "Epoch: 01262 loss_train: 53.7834 loss_rec: 53.7834 acc_train: 0.5422 loss_val: 53.0855 acc_val: 0.5471 time: 0.0283s\n",
      "Epoch: 01263 loss_train: 15.1850 loss_rec: 15.1850 acc_train: 0.5549 loss_val: 15.0145 acc_val: 0.5596 time: 0.0263s\n",
      "Epoch: 01264 loss_train: 50.1524 loss_rec: 50.1524 acc_train: 0.4958 loss_val: 50.9191 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01265 loss_train: 78.1341 loss_rec: 78.1341 acc_train: 0.4957 loss_val: 79.3271 acc_val: 0.4996 time: 0.0283s\n",
      "Epoch: 01266 loss_train: 70.5470 loss_rec: 70.5470 acc_train: 0.4958 loss_val: 71.6243 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 01267 loss_train: 31.0495 loss_rec: 31.0495 acc_train: 0.4955 loss_val: 31.5257 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01268 loss_train: 33.5251 loss_rec: 33.5251 acc_train: 0.5485 loss_val: 33.0682 acc_val: 0.5525 time: 0.0278s\n",
      "Epoch: 01269 loss_train: 62.4777 loss_rec: 62.4777 acc_train: 0.5393 loss_val: 61.6808 acc_val: 0.5438 time: 0.0253s\n",
      "Epoch: 01270 loss_train: 60.0166 loss_rec: 60.0166 acc_train: 0.5403 loss_val: 59.2479 acc_val: 0.5450 time: 0.0268s\n",
      "Epoch: 01271 loss_train: 29.3118 loss_rec: 29.3118 acc_train: 0.5523 loss_val: 28.9150 acc_val: 0.5542 time: 0.0283s\n",
      "Test set results: loss= 24.3803 accuracy= 0.5517\n",
      "Epoch: 01272 loss_train: 27.2036 loss_rec: 27.2036 acc_train: 0.4955 loss_val: 27.6214 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01273 loss_train: 48.4515 loss_rec: 48.4515 acc_train: 0.4959 loss_val: 49.1923 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01274 loss_train: 35.0120 loss_rec: 35.0120 acc_train: 0.4955 loss_val: 35.5485 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01275 loss_train: 8.1921 loss_rec: 8.1921 acc_train: 0.5584 loss_val: 8.1366 acc_val: 0.5625 time: 0.0263s\n",
      "Epoch: 01276 loss_train: 18.2501 loss_rec: 18.2501 acc_train: 0.5525 loss_val: 18.0271 acc_val: 0.5583 time: 0.0273s\n",
      "Epoch: 01277 loss_train: 1.0141 loss_rec: 1.0141 acc_train: 0.6276 loss_val: 1.0329 acc_val: 0.6146 time: 0.0273s\n",
      "Epoch: 01278 loss_train: 28.2327 loss_rec: 28.2327 acc_train: 0.4955 loss_val: 28.6662 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01279 loss_train: 21.6718 loss_rec: 21.6718 acc_train: 0.4955 loss_val: 22.0057 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01280 loss_train: 14.6131 loss_rec: 14.6131 acc_train: 0.5563 loss_val: 14.4526 acc_val: 0.5629 time: 0.0263s\n",
      "Epoch: 01281 loss_train: 18.9341 loss_rec: 18.9341 acc_train: 0.5525 loss_val: 18.6995 acc_val: 0.5583 time: 0.0283s\n",
      "Test set results: loss= 3.8398 accuracy= 0.5488\n",
      "Epoch: 01282 loss_train: 4.3346 loss_rec: 4.3346 acc_train: 0.4929 loss_val: 4.4066 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 01283 loss_train: 4.0319 loss_rec: 4.0319 acc_train: 0.5762 loss_val: 4.0311 acc_val: 0.5783 time: 0.0273s\n",
      "Epoch: 01284 loss_train: 14.2237 loss_rec: 14.2237 acc_train: 0.4957 loss_val: 14.4441 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01285 loss_train: 1.1879 loss_rec: 1.1879 acc_train: 0.6249 loss_val: 1.2100 acc_val: 0.6158 time: 0.0268s\n",
      "Epoch: 01286 loss_train: 1.1128 loss_rec: 1.1128 acc_train: 0.5639 loss_val: 1.1271 acc_val: 0.5542 time: 0.0258s\n",
      "Epoch: 01287 loss_train: 13.2101 loss_rec: 13.2101 acc_train: 0.5573 loss_val: 13.0746 acc_val: 0.5642 time: 0.0283s\n",
      "Epoch: 01288 loss_train: 0.9263 loss_rec: 0.9263 acc_train: 0.6139 loss_val: 0.9389 acc_val: 0.6079 time: 0.0278s\n",
      "Epoch: 01289 loss_train: 8.4685 loss_rec: 8.4685 acc_train: 0.4933 loss_val: 8.6016 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 01290 loss_train: 14.5409 loss_rec: 14.5409 acc_train: 0.5563 loss_val: 14.3821 acc_val: 0.5629 time: 0.0278s\n",
      "Epoch: 01291 loss_train: 7.9423 loss_rec: 7.9423 acc_train: 0.5601 loss_val: 7.8906 acc_val: 0.5629 time: 0.0258s\n",
      "Test set results: loss= 23.5088 accuracy= 0.5517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01292 loss_train: 26.2333 loss_rec: 26.2333 acc_train: 0.4955 loss_val: 26.6369 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01293 loss_train: 25.8052 loss_rec: 25.8052 acc_train: 0.4955 loss_val: 26.2023 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01294 loss_train: 5.8862 loss_rec: 5.8862 acc_train: 0.5660 loss_val: 5.8599 acc_val: 0.5717 time: 0.0273s\n",
      "Epoch: 01295 loss_train: 6.7888 loss_rec: 6.7888 acc_train: 0.5615 loss_val: 6.7502 acc_val: 0.5646 time: 0.0263s\n",
      "Epoch: 01296 loss_train: 19.7362 loss_rec: 19.7362 acc_train: 0.4956 loss_val: 20.0412 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01297 loss_train: 12.3677 loss_rec: 12.3677 acc_train: 0.4939 loss_val: 12.5601 acc_val: 0.4979 time: 0.0268s\n",
      "Epoch: 01298 loss_train: 23.8344 loss_rec: 23.8344 acc_train: 0.5524 loss_val: 23.5168 acc_val: 0.5558 time: 0.0273s\n",
      "Epoch: 01299 loss_train: 28.6422 loss_rec: 28.6422 acc_train: 0.5521 loss_val: 28.2552 acc_val: 0.5542 time: 0.0279s\n",
      "Epoch: 01300 loss_train: 5.3991 loss_rec: 5.3991 acc_train: 0.5687 loss_val: 5.3796 acc_val: 0.5733 time: 0.0273s\n",
      "Epoch: 01301 loss_train: 44.6604 loss_rec: 44.6604 acc_train: 0.4960 loss_val: 45.3444 acc_val: 0.5000 time: 0.0273s\n",
      "Test set results: loss= 52.3214 accuracy= 0.5528\n",
      "Epoch: 01302 loss_train: 58.3122 loss_rec: 58.3122 acc_train: 0.4958 loss_val: 59.2038 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01303 loss_train: 38.1288 loss_rec: 38.1288 acc_train: 0.4955 loss_val: 38.7135 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01304 loss_train: 10.6969 loss_rec: 10.6969 acc_train: 0.5553 loss_val: 10.6024 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 01305 loss_train: 25.3965 loss_rec: 25.3965 acc_train: 0.5527 loss_val: 25.0557 acc_val: 0.5538 time: 0.0273s\n",
      "Epoch: 01306 loss_train: 11.0700 loss_rec: 11.0700 acc_train: 0.5543 loss_val: 10.9698 acc_val: 0.5583 time: 0.0273s\n",
      "Epoch: 01307 loss_train: 30.6924 loss_rec: 30.6924 acc_train: 0.4955 loss_val: 31.1640 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01308 loss_train: 37.4846 loss_rec: 37.4846 acc_train: 0.4955 loss_val: 38.0595 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01309 loss_train: 11.2191 loss_rec: 11.2191 acc_train: 0.4932 loss_val: 11.3941 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 01310 loss_train: 40.5842 loss_rec: 40.5842 acc_train: 0.5466 loss_val: 40.0428 acc_val: 0.5496 time: 0.0263s\n",
      "Epoch: 01311 loss_train: 59.4326 loss_rec: 59.4326 acc_train: 0.5413 loss_val: 58.6694 acc_val: 0.5467 time: 0.0283s\n",
      "Test set results: loss= 54.5370 accuracy= 0.4964\n",
      "Epoch: 01312 loss_train: 48.1033 loss_rec: 48.1033 acc_train: 0.5444 loss_val: 47.4723 acc_val: 0.5488 time: 0.0268s\n",
      "Epoch: 01313 loss_train: 10.0458 loss_rec: 10.0458 acc_train: 0.5555 loss_val: 9.9609 acc_val: 0.5596 time: 0.0263s\n",
      "Epoch: 01314 loss_train: 54.6204 loss_rec: 54.6204 acc_train: 0.4960 loss_val: 55.4558 acc_val: 0.5000 time: 0.0288s\n",
      "Epoch: 01315 loss_train: 82.0055 loss_rec: 82.0055 acc_train: 0.4958 loss_val: 83.2581 acc_val: 0.4996 time: 0.0283s\n",
      "Epoch: 01316 loss_train: 74.2758 loss_rec: 74.2758 acc_train: 0.4958 loss_val: 75.4106 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 01317 loss_train: 35.0629 loss_rec: 35.0629 acc_train: 0.4955 loss_val: 35.6008 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01318 loss_train: 29.1772 loss_rec: 29.1772 acc_train: 0.5523 loss_val: 28.7829 acc_val: 0.5542 time: 0.0253s\n",
      "Epoch: 01319 loss_train: 57.9467 loss_rec: 57.9467 acc_train: 0.5410 loss_val: 57.2004 acc_val: 0.5467 time: 0.0268s\n",
      "Epoch: 01320 loss_train: 55.6787 loss_rec: 55.6787 acc_train: 0.5417 loss_val: 54.9577 acc_val: 0.5467 time: 0.0278s\n",
      "Epoch: 01321 loss_train: 25.6066 loss_rec: 25.6066 acc_train: 0.5537 loss_val: 25.2627 acc_val: 0.5546 time: 0.0274s\n",
      "Test set results: loss= 27.0308 accuracy= 0.5517\n",
      "Epoch: 01322 loss_train: 30.1551 loss_rec: 30.1551 acc_train: 0.4955 loss_val: 30.6184 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01323 loss_train: 50.8922 loss_rec: 50.8922 acc_train: 0.4960 loss_val: 51.6709 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01324 loss_train: 37.3729 loss_rec: 37.3729 acc_train: 0.4955 loss_val: 37.9459 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01325 loss_train: 5.7767 loss_rec: 5.7767 acc_train: 0.5666 loss_val: 5.7527 acc_val: 0.5721 time: 0.0278s\n",
      "Epoch: 01326 loss_train: 16.3671 loss_rec: 16.3671 acc_train: 0.5543 loss_val: 16.1771 acc_val: 0.5592 time: 0.0278s\n",
      "Epoch: 01327 loss_train: 0.9597 loss_rec: 0.9597 acc_train: 0.5817 loss_val: 0.9725 acc_val: 0.5729 time: 0.0283s\n",
      "Epoch: 01328 loss_train: 8.7185 loss_rec: 8.7185 acc_train: 0.4928 loss_val: 8.8557 acc_val: 0.4971 time: 0.0258s\n",
      "Epoch: 01329 loss_train: 13.8572 loss_rec: 13.8572 acc_train: 0.5573 loss_val: 13.7110 acc_val: 0.5642 time: 0.0278s\n",
      "Epoch: 01330 loss_train: 7.2049 loss_rec: 7.2049 acc_train: 0.5615 loss_val: 7.1624 acc_val: 0.5638 time: 0.0268s\n",
      "Epoch: 01331 loss_train: 26.6141 loss_rec: 26.6141 acc_train: 0.4955 loss_val: 27.0238 acc_val: 0.5000 time: 0.0278s\n",
      "Test set results: loss= 23.4526 accuracy= 0.5517\n",
      "Epoch: 01332 loss_train: 26.1714 loss_rec: 26.1714 acc_train: 0.4955 loss_val: 26.5743 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01333 loss_train: 5.2072 loss_rec: 5.2072 acc_train: 0.5697 loss_val: 5.1916 acc_val: 0.5737 time: 0.0283s\n",
      "Epoch: 01334 loss_train: 6.3489 loss_rec: 6.3489 acc_train: 0.5635 loss_val: 6.3166 acc_val: 0.5683 time: 0.0283s\n",
      "Epoch: 01335 loss_train: 19.4996 loss_rec: 19.4996 acc_train: 0.4956 loss_val: 19.8010 acc_val: 0.5000 time: 0.0258s\n",
      "Epoch: 01336 loss_train: 11.8642 loss_rec: 11.8642 acc_train: 0.4932 loss_val: 12.0491 acc_val: 0.4979 time: 0.0263s\n",
      "Epoch: 01337 loss_train: 24.1564 loss_rec: 24.1564 acc_train: 0.5517 loss_val: 23.8344 acc_val: 0.5554 time: 0.0288s\n",
      "Epoch: 01338 loss_train: 29.1687 loss_rec: 29.1687 acc_train: 0.5523 loss_val: 28.7746 acc_val: 0.5542 time: 0.0273s\n",
      "Epoch: 01339 loss_train: 6.4101 loss_rec: 6.4101 acc_train: 0.5635 loss_val: 6.3773 acc_val: 0.5683 time: 0.0278s\n",
      "Epoch: 01340 loss_train: 42.9545 loss_rec: 42.9545 acc_train: 0.4960 loss_val: 43.6129 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01341 loss_train: 56.4979 loss_rec: 56.4979 acc_train: 0.4960 loss_val: 57.3623 acc_val: 0.5000 time: 0.0273s\n",
      "Test set results: loss= 32.8851 accuracy= 0.5517\n",
      "Epoch: 01342 loss_train: 36.6733 loss_rec: 36.6733 acc_train: 0.4955 loss_val: 37.2362 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01343 loss_train: 11.3751 loss_rec: 11.3751 acc_train: 0.5543 loss_val: 11.2713 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 01344 loss_train: 25.7966 loss_rec: 25.7966 acc_train: 0.5537 loss_val: 25.4505 acc_val: 0.5546 time: 0.0263s\n",
      "Epoch: 01345 loss_train: 11.5622 loss_rec: 11.5622 acc_train: 0.5543 loss_val: 11.4553 acc_val: 0.5583 time: 0.0263s\n",
      "Epoch: 01346 loss_train: 29.7024 loss_rec: 29.7024 acc_train: 0.4955 loss_val: 30.1595 acc_val: 0.5000 time: 0.0288s\n",
      "Epoch: 01347 loss_train: 36.5194 loss_rec: 36.5194 acc_train: 0.4955 loss_val: 37.0800 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01348 loss_train: 10.7065 loss_rec: 10.7065 acc_train: 0.4927 loss_val: 10.8740 acc_val: 0.4967 time: 0.0268s\n",
      "Epoch: 01349 loss_train: 40.2911 loss_rec: 40.2911 acc_train: 0.5466 loss_val: 39.7530 acc_val: 0.5492 time: 0.0278s\n",
      "Epoch: 01350 loss_train: 58.7877 loss_rec: 58.7877 acc_train: 0.5413 loss_val: 58.0309 acc_val: 0.5467 time: 0.0268s\n",
      "Epoch: 01351 loss_train: 19.6282 loss_rec: 19.6282 acc_train: 0.5534 loss_val: 19.3752 acc_val: 0.5579 time: 0.0263s\n",
      "Test set results: loss= 47.5137 accuracy= 0.5521\n",
      "Epoch: 01352 loss_train: 52.9542 loss_rec: 52.9542 acc_train: 0.4952 loss_val: 53.7649 acc_val: 0.4996 time: 0.0273s\n",
      "Epoch: 01353 loss_train: 53.5997 loss_rec: 53.5997 acc_train: 0.4952 loss_val: 54.4204 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 01354 loss_train: 4.5304 loss_rec: 4.5304 acc_train: 0.4931 loss_val: 4.6072 acc_val: 0.4979 time: 0.0263s\n",
      "Epoch: 01355 loss_train: 75.9705 loss_rec: 75.9705 acc_train: 0.5354 loss_val: 75.0206 acc_val: 0.5442 time: 0.0258s\n",
      "Epoch: 01356 loss_train: 104.2003 loss_rec: 104.2003 acc_train: 0.5311 loss_val: 102.8773 acc_val: 0.5392 time: 0.0278s\n",
      "Epoch: 01357 loss_train: 93.4527 loss_rec: 93.4527 acc_train: 0.5333 loss_val: 92.2736 acc_val: 0.5417 time: 0.0278s\n",
      "Epoch: 01358 loss_train: 55.6278 loss_rec: 55.6278 acc_train: 0.5422 loss_val: 54.9067 acc_val: 0.5471 time: 0.0273s\n",
      "Epoch: 01359 loss_train: 5.1496 loss_rec: 5.1496 acc_train: 0.4927 loss_val: 5.2352 acc_val: 0.4975 time: 0.0263s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01360 loss_train: 32.8047 loss_rec: 32.8047 acc_train: 0.4955 loss_val: 33.3087 acc_val: 0.5000 time: 0.0284s\n",
      "Epoch: 01361 loss_train: 25.9005 loss_rec: 25.9005 acc_train: 0.4955 loss_val: 26.2995 acc_val: 0.5000 time: 0.0258s\n",
      "Test set results: loss= 11.7391 accuracy= 0.5199\n",
      "Epoch: 01362 loss_train: 10.3723 loss_rec: 10.3723 acc_train: 0.5560 loss_val: 10.2836 acc_val: 0.5600 time: 0.0258s\n",
      "Epoch: 01363 loss_train: 15.2173 loss_rec: 15.2173 acc_train: 0.5557 loss_val: 15.0482 acc_val: 0.5625 time: 0.0293s\n",
      "Epoch: 01364 loss_train: 7.0579 loss_rec: 7.0579 acc_train: 0.4923 loss_val: 7.1718 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 01365 loss_train: 2.2227 loss_rec: 2.2227 acc_train: 0.6007 loss_val: 2.2449 acc_val: 0.5983 time: 0.0273s\n",
      "Epoch: 01366 loss_train: 11.9016 loss_rec: 11.9016 acc_train: 0.4927 loss_val: 12.0871 acc_val: 0.4967 time: 0.0268s\n",
      "Epoch: 01367 loss_train: 5.6011 loss_rec: 5.6011 acc_train: 0.5674 loss_val: 5.5807 acc_val: 0.5729 time: 0.0283s\n",
      "Epoch: 01368 loss_train: 4.2734 loss_rec: 4.2734 acc_train: 0.4931 loss_val: 4.3459 acc_val: 0.4979 time: 0.0263s\n",
      "Epoch: 01369 loss_train: 15.2748 loss_rec: 15.2748 acc_train: 0.5554 loss_val: 15.1051 acc_val: 0.5617 time: 0.0278s\n",
      "Epoch: 01370 loss_train: 6.5112 loss_rec: 6.5112 acc_train: 0.5629 loss_val: 6.4778 acc_val: 0.5683 time: 0.0278s\n",
      "Epoch: 01371 loss_train: 28.9454 loss_rec: 28.9454 acc_train: 0.4955 loss_val: 29.3910 acc_val: 0.5000 time: 0.0268s\n",
      "Test set results: loss= 27.1393 accuracy= 0.5517\n",
      "Epoch: 01372 loss_train: 30.2768 loss_rec: 30.2768 acc_train: 0.4955 loss_val: 30.7427 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01373 loss_train: 0.9317 loss_rec: 0.9317 acc_train: 0.6298 loss_val: 0.9489 acc_val: 0.6138 time: 0.0288s\n",
      "Epoch: 01374 loss_train: 20.3978 loss_rec: 20.3978 acc_train: 0.5542 loss_val: 20.1399 acc_val: 0.5596 time: 0.0283s\n",
      "Epoch: 01375 loss_train: 12.3920 loss_rec: 12.3920 acc_train: 0.5545 loss_val: 12.2723 acc_val: 0.5575 time: 0.0273s\n",
      "Epoch: 01376 loss_train: 22.5457 loss_rec: 22.5457 acc_train: 0.4948 loss_val: 22.8941 acc_val: 0.4996 time: 0.0278s\n",
      "Epoch: 01377 loss_train: 24.0300 loss_rec: 24.0300 acc_train: 0.4956 loss_val: 24.4010 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01378 loss_train: 5.2354 loss_rec: 5.2354 acc_train: 0.5688 loss_val: 5.2208 acc_val: 0.5737 time: 0.0258s\n",
      "Epoch: 01379 loss_train: 5.0481 loss_rec: 5.0481 acc_train: 0.5702 loss_val: 5.0364 acc_val: 0.5737 time: 0.0283s\n",
      "Epoch: 01380 loss_train: 21.5261 loss_rec: 21.5261 acc_train: 0.4948 loss_val: 21.8592 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01381 loss_train: 14.7411 loss_rec: 14.7411 acc_train: 0.4949 loss_val: 14.9707 acc_val: 0.5000 time: 0.0268s\n",
      "Test set results: loss= 23.0976 accuracy= 0.5146\n",
      "Epoch: 01382 loss_train: 20.4022 loss_rec: 20.4022 acc_train: 0.5542 loss_val: 20.1442 acc_val: 0.5596 time: 0.0263s\n",
      "Epoch: 01383 loss_train: 24.8494 loss_rec: 24.8494 acc_train: 0.5508 loss_val: 24.5169 acc_val: 0.5542 time: 0.0283s\n",
      "Epoch: 01384 loss_train: 2.4406 loss_rec: 2.4406 acc_train: 0.5988 loss_val: 2.4618 acc_val: 0.5971 time: 0.0278s\n",
      "Epoch: 01385 loss_train: 43.3251 loss_rec: 43.3251 acc_train: 0.4960 loss_val: 43.9901 acc_val: 0.5000 time: 0.0258s\n",
      "Epoch: 01386 loss_train: 53.3439 loss_rec: 53.3439 acc_train: 0.4960 loss_val: 54.1612 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01387 loss_train: 30.6940 loss_rec: 30.6940 acc_train: 0.4955 loss_val: 31.1669 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01388 loss_train: 18.8373 loss_rec: 18.8373 acc_train: 0.5525 loss_val: 18.6064 acc_val: 0.5583 time: 0.0263s\n",
      "Epoch: 01389 loss_train: 35.2094 loss_rec: 35.2094 acc_train: 0.5485 loss_val: 34.7309 acc_val: 0.5525 time: 0.0288s\n",
      "Epoch: 01390 loss_train: 22.7182 loss_rec: 22.7182 acc_train: 0.5542 loss_val: 22.4211 acc_val: 0.5588 time: 0.0278s\n",
      "Epoch: 01391 loss_train: 15.7561 loss_rec: 15.7561 acc_train: 0.4949 loss_val: 16.0015 acc_val: 0.5000 time: 0.0273s\n",
      "Test set results: loss= 19.0806 accuracy= 0.5512\n",
      "Epoch: 01392 loss_train: 21.3050 loss_rec: 21.3050 acc_train: 0.4949 loss_val: 21.6350 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01393 loss_train: 4.3239 loss_rec: 4.3239 acc_train: 0.5755 loss_val: 4.3217 acc_val: 0.5783 time: 0.0268s\n",
      "Epoch: 01394 loss_train: 2.0134 loss_rec: 2.0134 acc_train: 0.6063 loss_val: 2.0378 acc_val: 0.6071 time: 0.0263s\n",
      "Epoch: 01395 loss_train: 22.5431 loss_rec: 22.5431 acc_train: 0.4948 loss_val: 22.8922 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01396 loss_train: 13.4062 loss_rec: 13.4062 acc_train: 0.4927 loss_val: 13.6159 acc_val: 0.4967 time: 0.0278s\n",
      "Epoch: 01397 loss_train: 23.5152 loss_rec: 23.5152 acc_train: 0.5535 loss_val: 23.2046 acc_val: 0.5575 time: 0.0273s\n",
      "Epoch: 01398 loss_train: 29.7188 loss_rec: 29.7188 acc_train: 0.5523 loss_val: 29.3173 acc_val: 0.5542 time: 0.0273s\n",
      "Epoch: 01399 loss_train: 8.3275 loss_rec: 8.3275 acc_train: 0.5595 loss_val: 8.2712 acc_val: 0.5629 time: 0.0258s\n",
      "Epoch: 01400 loss_train: 39.3843 loss_rec: 39.3843 acc_train: 0.4955 loss_val: 39.9895 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01401 loss_train: 52.1062 loss_rec: 52.1062 acc_train: 0.4960 loss_val: 52.9050 acc_val: 0.5000 time: 0.0278s\n",
      "Test set results: loss= 28.7068 accuracy= 0.5517\n",
      "Epoch: 01402 loss_train: 32.0222 loss_rec: 32.0222 acc_train: 0.4955 loss_val: 32.5156 acc_val: 0.5004 time: 0.0273s\n",
      "Epoch: 01403 loss_train: 15.3963 loss_rec: 15.3963 acc_train: 0.5554 loss_val: 15.2249 acc_val: 0.5617 time: 0.0273s\n",
      "Epoch: 01404 loss_train: 29.8390 loss_rec: 29.8390 acc_train: 0.5523 loss_val: 29.4356 acc_val: 0.5542 time: 0.0268s\n",
      "Epoch: 01405 loss_train: 15.8471 loss_rec: 15.8471 acc_train: 0.5549 loss_val: 15.6676 acc_val: 0.5596 time: 0.0259s\n",
      "Epoch: 01406 loss_train: 24.5219 loss_rec: 24.5219 acc_train: 0.4948 loss_val: 24.9012 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01407 loss_train: 31.3734 loss_rec: 31.3734 acc_train: 0.4955 loss_val: 31.8569 acc_val: 0.5004 time: 0.0283s\n",
      "Epoch: 01408 loss_train: 6.0578 loss_rec: 6.0578 acc_train: 0.4927 loss_val: 6.1585 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 01409 loss_train: 43.7161 loss_rec: 43.7161 acc_train: 0.5455 loss_val: 43.1365 acc_val: 0.5483 time: 0.0263s\n",
      "Epoch: 01410 loss_train: 61.8264 loss_rec: 61.8264 acc_train: 0.5407 loss_val: 61.0343 acc_val: 0.5463 time: 0.0273s\n",
      "Epoch: 01411 loss_train: 50.5975 loss_rec: 50.5975 acc_train: 0.5420 loss_val: 49.9355 acc_val: 0.5475 time: 0.0273s\n",
      "Test set results: loss= 15.1290 accuracy= 0.5160\n",
      "Epoch: 01412 loss_train: 13.3656 loss_rec: 13.3656 acc_train: 0.5523 loss_val: 13.2302 acc_val: 0.5575 time: 0.0273s\n",
      "Epoch: 01413 loss_train: 49.6406 loss_rec: 49.6406 acc_train: 0.4960 loss_val: 50.4018 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01414 loss_train: 76.7105 loss_rec: 76.7105 acc_train: 0.4959 loss_val: 77.8834 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01415 loss_train: 69.6058 loss_rec: 69.6058 acc_train: 0.4960 loss_val: 70.6706 acc_val: 0.5000 time: 0.0258s\n",
      "Epoch: 01416 loss_train: 31.8148 loss_rec: 31.8148 acc_train: 0.4955 loss_val: 32.3048 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01417 loss_train: 30.1324 loss_rec: 30.1324 acc_train: 0.5523 loss_val: 29.7250 acc_val: 0.5542 time: 0.0278s\n",
      "Epoch: 01418 loss_train: 57.8213 loss_rec: 57.8213 acc_train: 0.5408 loss_val: 57.0744 acc_val: 0.5467 time: 0.0273s\n",
      "Epoch: 01419 loss_train: 55.4003 loss_rec: 55.4003 acc_train: 0.5426 loss_val: 54.6812 acc_val: 0.5483 time: 0.0268s\n",
      "Epoch: 01420 loss_train: 25.9643 loss_rec: 25.9643 acc_train: 0.5527 loss_val: 25.6161 acc_val: 0.5538 time: 0.0278s\n",
      "Epoch: 01421 loss_train: 28.3452 loss_rec: 28.3452 acc_train: 0.4947 loss_val: 28.7825 acc_val: 0.5000 time: 0.0263s\n",
      "Test set results: loss= 43.6580 accuracy= 0.5528\n",
      "Epoch: 01422 loss_train: 48.6687 loss_rec: 48.6687 acc_train: 0.4960 loss_val: 49.4150 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01423 loss_train: 35.6695 loss_rec: 35.6695 acc_train: 0.4955 loss_val: 36.2181 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01424 loss_train: 6.0544 loss_rec: 6.0544 acc_train: 0.5661 loss_val: 6.0282 acc_val: 0.5717 time: 0.0268s\n",
      "Epoch: 01425 loss_train: 16.1922 loss_rec: 16.1922 acc_train: 0.5543 loss_val: 16.0075 acc_val: 0.5592 time: 0.0278s\n",
      "Epoch: 01426 loss_train: 0.9313 loss_rec: 0.9313 acc_train: 0.6100 loss_val: 0.9463 acc_val: 0.6025 time: 0.0258s\n",
      "Epoch: 01427 loss_train: 12.8310 loss_rec: 12.8310 acc_train: 0.4920 loss_val: 13.0317 acc_val: 0.4967 time: 0.0268s\n",
      "Epoch: 01428 loss_train: 5.8440 loss_rec: 5.8440 acc_train: 0.5665 loss_val: 5.8212 acc_val: 0.5717 time: 0.0273s\n",
      "Epoch: 01429 loss_train: 2.5377 loss_rec: 2.5377 acc_train: 0.4937 loss_val: 2.5835 acc_val: 0.4983 time: 0.0283s\n",
      "Epoch: 01430 loss_train: 16.8392 loss_rec: 16.8392 acc_train: 0.5543 loss_val: 16.6436 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 01431 loss_train: 8.4013 loss_rec: 8.4013 acc_train: 0.5585 loss_val: 8.3446 acc_val: 0.5625 time: 0.0278s\n",
      "Test set results: loss= 23.6014 accuracy= 0.5511\n",
      "Epoch: 01432 loss_train: 26.3387 loss_rec: 26.3387 acc_train: 0.4948 loss_val: 26.7457 acc_val: 0.5000 time: 0.0268s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01433 loss_train: 27.7922 loss_rec: 27.7922 acc_train: 0.4947 loss_val: 28.2214 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01434 loss_train: 1.8048 loss_rec: 1.8048 acc_train: 0.6134 loss_val: 1.8309 acc_val: 0.6100 time: 0.0278s\n",
      "Epoch: 01435 loss_train: 6.7759 loss_rec: 6.7759 acc_train: 0.5618 loss_val: 6.7395 acc_val: 0.5658 time: 0.0273s\n",
      "Epoch: 01436 loss_train: 14.0747 loss_rec: 14.0747 acc_train: 0.4927 loss_val: 14.2946 acc_val: 0.4967 time: 0.0263s\n",
      "Epoch: 01437 loss_train: 2.9246 loss_rec: 2.9246 acc_train: 0.4937 loss_val: 2.9771 acc_val: 0.4983 time: 0.0258s\n",
      "Epoch: 01438 loss_train: 34.0408 loss_rec: 34.0408 acc_train: 0.5497 loss_val: 33.5769 acc_val: 0.5538 time: 0.0283s\n",
      "Epoch: 01439 loss_train: 41.1412 loss_rec: 41.1412 acc_train: 0.5466 loss_val: 40.5919 acc_val: 0.5496 time: 0.0278s\n",
      "Epoch: 01440 loss_train: 20.5613 loss_rec: 20.5613 acc_train: 0.5542 loss_val: 20.3017 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 01441 loss_train: 25.4485 loss_rec: 25.4485 acc_train: 0.4948 loss_val: 25.8420 acc_val: 0.5000 time: 0.0283s\n",
      "Test set results: loss= 34.1633 accuracy= 0.5517\n",
      "Epoch: 01442 loss_train: 38.0979 loss_rec: 38.0979 acc_train: 0.4955 loss_val: 38.6838 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01443 loss_train: 18.3535 loss_rec: 18.3535 acc_train: 0.4944 loss_val: 18.6387 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01444 loss_train: 27.3007 loss_rec: 27.3007 acc_train: 0.5527 loss_val: 26.9346 acc_val: 0.5542 time: 0.0268s\n",
      "Epoch: 01445 loss_train: 41.3776 loss_rec: 41.3776 acc_train: 0.5466 loss_val: 40.8255 acc_val: 0.5496 time: 0.0273s\n",
      "Epoch: 01446 loss_train: 27.0550 loss_rec: 27.0550 acc_train: 0.5533 loss_val: 26.6921 acc_val: 0.5542 time: 0.0278s\n",
      "Epoch: 01447 loss_train: 12.4211 loss_rec: 12.4211 acc_train: 0.4920 loss_val: 12.6157 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 01448 loss_train: 19.7270 loss_rec: 19.7270 acc_train: 0.4949 loss_val: 20.0332 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01449 loss_train: 3.9063 loss_rec: 3.9063 acc_train: 0.5791 loss_val: 3.9107 acc_val: 0.5808 time: 0.0283s\n",
      "Epoch: 01450 loss_train: 1.1124 loss_rec: 1.1124 acc_train: 0.6272 loss_val: 1.1362 acc_val: 0.6138 time: 0.0273s\n",
      "Epoch: 01451 loss_train: 15.4266 loss_rec: 15.4266 acc_train: 0.4944 loss_val: 15.6673 acc_val: 0.4988 time: 0.0268s\n",
      "Test set results: loss= 1.2502 accuracy= 0.6133\n",
      "Epoch: 01452 loss_train: 1.1867 loss_rec: 1.1867 acc_train: 0.6274 loss_val: 1.2119 acc_val: 0.6163 time: 0.0278s\n",
      "Epoch: 01453 loss_train: 1.4017 loss_rec: 1.4017 acc_train: 0.6261 loss_val: 1.4288 acc_val: 0.6233 time: 0.0268s\n",
      "Epoch: 01454 loss_train: 15.7309 loss_rec: 15.7309 acc_train: 0.4944 loss_val: 15.9766 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 01455 loss_train: 0.9242 loss_rec: 0.9242 acc_train: 0.6165 loss_val: 0.9405 acc_val: 0.6117 time: 0.0283s\n",
      "Epoch: 01456 loss_train: 14.0477 loss_rec: 14.0477 acc_train: 0.5573 loss_val: 13.9008 acc_val: 0.5642 time: 0.0288s\n",
      "Epoch: 01457 loss_train: 1.9053 loss_rec: 1.9053 acc_train: 0.6101 loss_val: 1.9315 acc_val: 0.6067 time: 0.0273s\n",
      "Epoch: 01458 loss_train: 31.6203 loss_rec: 31.6203 acc_train: 0.4947 loss_val: 32.1086 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01459 loss_train: 30.9947 loss_rec: 30.9947 acc_train: 0.4947 loss_val: 31.4735 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01460 loss_train: 1.0391 loss_rec: 1.0391 acc_train: 0.6276 loss_val: 1.0621 acc_val: 0.6150 time: 0.0258s\n",
      "Epoch: 01461 loss_train: 16.3203 loss_rec: 16.3203 acc_train: 0.5543 loss_val: 16.1335 acc_val: 0.5592 time: 0.0283s\n",
      "Test set results: loss= 5.7812 accuracy= 0.5357\n",
      "Epoch: 01462 loss_train: 5.1286 loss_rec: 5.1286 acc_train: 0.5694 loss_val: 5.1165 acc_val: 0.5737 time: 0.0273s\n",
      "Epoch: 01463 loss_train: 31.8934 loss_rec: 31.8934 acc_train: 0.4947 loss_val: 32.3861 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01464 loss_train: 35.0996 loss_rec: 35.0996 acc_train: 0.4947 loss_val: 35.6412 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01465 loss_train: 6.9470 loss_rec: 6.9470 acc_train: 0.4921 loss_val: 7.0627 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 01466 loss_train: 44.8900 loss_rec: 44.8900 acc_train: 0.5437 loss_val: 44.2958 acc_val: 0.5471 time: 0.0283s\n",
      "Epoch: 01467 loss_train: 65.1805 loss_rec: 65.1805 acc_train: 0.5387 loss_val: 64.3498 acc_val: 0.5433 time: 0.0278s\n",
      "Epoch: 01468 loss_train: 56.2779 loss_rec: 56.2779 acc_train: 0.5407 loss_val: 55.5480 acc_val: 0.5467 time: 0.0273s\n",
      "Epoch: 01469 loss_train: 21.3138 loss_rec: 21.3138 acc_train: 0.5545 loss_val: 21.0413 acc_val: 0.5592 time: 0.0263s\n",
      "Epoch: 01470 loss_train: 38.3627 loss_rec: 38.3627 acc_train: 0.4947 loss_val: 38.9535 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01471 loss_train: 63.4740 loss_rec: 63.4740 acc_train: 0.4960 loss_val: 64.4466 acc_val: 0.5000 time: 0.0283s\n",
      "Test set results: loss= 49.3948 accuracy= 0.5528\n",
      "Epoch: 01472 loss_train: 55.0562 loss_rec: 55.0562 acc_train: 0.4960 loss_val: 55.9007 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01473 loss_train: 16.4646 loss_rec: 16.4646 acc_train: 0.4937 loss_val: 16.7217 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01474 loss_train: 44.6906 loss_rec: 44.6906 acc_train: 0.5450 loss_val: 44.0988 acc_val: 0.5479 time: 0.0273s\n",
      "Epoch: 01475 loss_train: 72.8960 loss_rec: 72.8960 acc_train: 0.5372 loss_val: 71.9781 acc_val: 0.5425 time: 0.0263s\n",
      "Epoch: 01476 loss_train: 71.1817 loss_rec: 71.1817 acc_train: 0.5368 loss_val: 70.2828 acc_val: 0.5425 time: 0.0278s\n",
      "Epoch: 01477 loss_train: 42.6058 loss_rec: 42.6058 acc_train: 0.5457 loss_val: 42.0389 acc_val: 0.5483 time: 0.0258s\n",
      "Epoch: 01478 loss_train: 9.4237 loss_rec: 9.4237 acc_train: 0.4923 loss_val: 9.5747 acc_val: 0.4975 time: 0.0283s\n",
      "Epoch: 01479 loss_train: 29.2584 loss_rec: 29.2584 acc_train: 0.4947 loss_val: 29.7104 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01480 loss_train: 16.2621 loss_rec: 16.2621 acc_train: 0.4937 loss_val: 16.5156 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 01481 loss_train: 23.4373 loss_rec: 23.4373 acc_train: 0.5535 loss_val: 23.1290 acc_val: 0.5571 time: 0.0268s\n",
      "Test set results: loss= 36.8750 accuracy= 0.5069\n",
      "Epoch: 01482 loss_train: 32.5587 loss_rec: 32.5587 acc_train: 0.5499 loss_val: 32.1160 acc_val: 0.5546 time: 0.0283s\n",
      "Epoch: 01483 loss_train: 14.3071 loss_rec: 14.3071 acc_train: 0.5573 loss_val: 14.1561 acc_val: 0.5642 time: 0.0283s\n",
      "Epoch: 01484 loss_train: 29.7213 loss_rec: 29.7213 acc_train: 0.4947 loss_val: 30.1801 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01485 loss_train: 40.3881 loss_rec: 40.3881 acc_train: 0.4947 loss_val: 41.0091 acc_val: 0.4996 time: 0.0268s\n",
      "Epoch: 01486 loss_train: 19.1643 loss_rec: 19.1643 acc_train: 0.4944 loss_val: 19.4623 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 01487 loss_train: 27.4385 loss_rec: 27.4385 acc_train: 0.5527 loss_val: 27.0704 acc_val: 0.5542 time: 0.0268s\n",
      "Epoch: 01488 loss_train: 42.6159 loss_rec: 42.6159 acc_train: 0.5462 loss_val: 42.0487 acc_val: 0.5492 time: 0.0283s\n",
      "Epoch: 01489 loss_train: 29.5731 loss_rec: 29.5731 acc_train: 0.5521 loss_val: 29.1742 acc_val: 0.5542 time: 0.0273s\n",
      "Epoch: 01490 loss_train: 8.2069 loss_rec: 8.2069 acc_train: 0.4924 loss_val: 8.3403 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 01491 loss_train: 14.5375 loss_rec: 14.5375 acc_train: 0.4919 loss_val: 14.7648 acc_val: 0.4967 time: 0.0263s\n",
      "Test set results: loss= 10.0425 accuracy= 0.5225\n",
      "Epoch: 01492 loss_train: 8.8753 loss_rec: 8.8753 acc_train: 0.5579 loss_val: 8.8121 acc_val: 0.5629 time: 0.0263s\n",
      "Epoch: 01493 loss_train: 4.3746 loss_rec: 4.3746 acc_train: 0.5755 loss_val: 4.3740 acc_val: 0.5783 time: 0.0273s\n",
      "Epoch: 01494 loss_train: 25.3846 loss_rec: 25.3846 acc_train: 0.4948 loss_val: 25.7777 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01495 loss_train: 22.3316 loss_rec: 22.3316 acc_train: 0.4944 loss_val: 22.6781 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01496 loss_train: 9.4404 loss_rec: 9.4404 acc_train: 0.5573 loss_val: 9.3683 acc_val: 0.5633 time: 0.0268s\n",
      "Epoch: 01497 loss_train: 11.5662 loss_rec: 11.5662 acc_train: 0.5543 loss_val: 11.4613 acc_val: 0.5583 time: 0.0263s\n",
      "Epoch: 01498 loss_train: 12.4572 loss_rec: 12.4572 acc_train: 0.4920 loss_val: 12.6530 acc_val: 0.4971 time: 0.0283s\n",
      "Epoch: 01499 loss_train: 5.0515 loss_rec: 5.0515 acc_train: 0.4933 loss_val: 5.1386 acc_val: 0.4979 time: 0.0278s\n",
      "Epoch: 01500 loss_train: 28.7941 loss_rec: 28.7941 acc_train: 0.5519 loss_val: 28.4069 acc_val: 0.5538 time: 0.0263s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01501 loss_train: 33.4177 loss_rec: 33.4177 acc_train: 0.5495 loss_val: 32.9627 acc_val: 0.5538 time: 0.0283s\n",
      "Test set results: loss= 12.8309 accuracy= 0.5195\n",
      "Epoch: 01502 loss_train: 11.3361 loss_rec: 11.3361 acc_train: 0.5553 loss_val: 11.2348 acc_val: 0.5596 time: 0.0253s\n",
      "Epoch: 01503 loss_train: 36.2981 loss_rec: 36.2981 acc_train: 0.4947 loss_val: 36.8576 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01504 loss_train: 50.1704 loss_rec: 50.1704 acc_train: 0.4960 loss_val: 50.9409 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01505 loss_train: 32.0369 loss_rec: 32.0369 acc_train: 0.4947 loss_val: 32.5318 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01506 loss_train: 12.9311 loss_rec: 12.9311 acc_train: 0.5536 loss_val: 12.8049 acc_val: 0.5575 time: 0.0278s\n",
      "Epoch: 01507 loss_train: 25.9636 loss_rec: 25.9636 acc_train: 0.5515 loss_val: 25.6152 acc_val: 0.5521 time: 0.0278s\n",
      "Epoch: 01508 loss_train: 11.6938 loss_rec: 11.6938 acc_train: 0.5543 loss_val: 11.5872 acc_val: 0.5583 time: 0.0268s\n",
      "Epoch: 01509 loss_train: 28.2829 loss_rec: 28.2829 acc_train: 0.4948 loss_val: 28.7209 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01510 loss_train: 35.3668 loss_rec: 35.3668 acc_train: 0.4947 loss_val: 35.9125 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01511 loss_train: 11.1862 loss_rec: 11.1862 acc_train: 0.4922 loss_val: 11.3639 acc_val: 0.4971 time: 0.0268s\n",
      "Test set results: loss= 42.0666 accuracy= 0.5045\n",
      "Epoch: 01512 loss_train: 37.1347 loss_rec: 37.1347 acc_train: 0.5485 loss_val: 36.6312 acc_val: 0.5533 time: 0.0283s\n",
      "Epoch: 01513 loss_train: 54.5278 loss_rec: 54.5278 acc_train: 0.5415 loss_val: 53.8179 acc_val: 0.5483 time: 0.0263s\n",
      "Epoch: 01514 loss_train: 43.5590 loss_rec: 43.5590 acc_train: 0.5461 loss_val: 42.9805 acc_val: 0.5488 time: 0.0253s\n",
      "Epoch: 01515 loss_train: 7.4783 loss_rec: 7.4783 acc_train: 0.5615 loss_val: 7.4335 acc_val: 0.5638 time: 0.0283s\n",
      "Epoch: 01516 loss_train: 53.4135 loss_rec: 53.4135 acc_train: 0.4960 loss_val: 54.2337 acc_val: 0.5004 time: 0.0278s\n",
      "Epoch: 01517 loss_train: 78.9717 loss_rec: 78.9717 acc_train: 0.4960 loss_val: 80.1805 acc_val: 0.5000 time: 0.0258s\n",
      "Epoch: 01518 loss_train: 71.4206 loss_rec: 71.4206 acc_train: 0.4960 loss_val: 72.5146 acc_val: 0.5000 time: 0.0288s\n",
      "Epoch: 01519 loss_train: 34.1443 loss_rec: 34.1443 acc_train: 0.4947 loss_val: 34.6716 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01520 loss_train: 26.6788 loss_rec: 26.6788 acc_train: 0.5537 loss_val: 26.3209 acc_val: 0.5546 time: 0.0273s\n",
      "Epoch: 01521 loss_train: 54.0548 loss_rec: 54.0548 acc_train: 0.5424 loss_val: 53.3506 acc_val: 0.5483 time: 0.0278s\n",
      "Test set results: loss= 59.2114 accuracy= 0.4941\n",
      "Epoch: 01522 loss_train: 52.2208 loss_rec: 52.2208 acc_train: 0.5420 loss_val: 51.5386 acc_val: 0.5475 time: 0.0268s\n",
      "Epoch: 01523 loss_train: 24.1382 loss_rec: 24.1382 acc_train: 0.5535 loss_val: 23.8193 acc_val: 0.5575 time: 0.0263s\n",
      "Epoch: 01524 loss_train: 28.0983 loss_rec: 28.0983 acc_train: 0.4948 loss_val: 28.5336 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01525 loss_train: 47.3511 loss_rec: 47.3511 acc_train: 0.4955 loss_val: 48.0792 acc_val: 0.5004 time: 0.0278s\n",
      "Epoch: 01526 loss_train: 34.3147 loss_rec: 34.3147 acc_train: 0.4947 loss_val: 34.8447 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01527 loss_train: 6.4488 loss_rec: 6.4488 acc_train: 0.5651 loss_val: 6.4176 acc_val: 0.5713 time: 0.0273s\n",
      "Epoch: 01528 loss_train: 16.5345 loss_rec: 16.5345 acc_train: 0.5543 loss_val: 16.3453 acc_val: 0.5592 time: 0.0263s\n",
      "Epoch: 01529 loss_train: 1.1688 loss_rec: 1.1688 acc_train: 0.6268 loss_val: 1.1952 acc_val: 0.6133 time: 0.0263s\n",
      "Epoch: 01530 loss_train: 28.6291 loss_rec: 28.6291 acc_train: 0.4948 loss_val: 29.0726 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01531 loss_train: 24.9521 loss_rec: 24.9521 acc_train: 0.4944 loss_val: 25.3395 acc_val: 0.4988 time: 0.0283s\n",
      "Test set results: loss= 8.1894 accuracy= 0.5254\n",
      "Epoch: 01532 loss_train: 7.2459 loss_rec: 7.2459 acc_train: 0.5613 loss_val: 7.2045 acc_val: 0.5638 time: 0.0283s\n",
      "Epoch: 01533 loss_train: 10.2238 loss_rec: 10.2238 acc_train: 0.5563 loss_val: 10.1393 acc_val: 0.5621 time: 0.0278s\n",
      "Epoch: 01534 loss_train: 12.5366 loss_rec: 12.5366 acc_train: 0.4920 loss_val: 12.7348 acc_val: 0.4971 time: 0.0258s\n",
      "Epoch: 01535 loss_train: 4.3419 loss_rec: 4.3419 acc_train: 0.4936 loss_val: 4.4190 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 01536 loss_train: 29.7114 loss_rec: 29.7114 acc_train: 0.5521 loss_val: 29.3109 acc_val: 0.5542 time: 0.0263s\n",
      "Epoch: 01537 loss_train: 34.9438 loss_rec: 34.9438 acc_train: 0.5493 loss_val: 34.4676 acc_val: 0.5525 time: 0.0278s\n",
      "Epoch: 01538 loss_train: 13.6725 loss_rec: 13.6725 acc_train: 0.5573 loss_val: 13.5336 acc_val: 0.5642 time: 0.0283s\n",
      "Epoch: 01539 loss_train: 32.7572 loss_rec: 32.7572 acc_train: 0.4947 loss_val: 33.2636 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01540 loss_train: 46.1572 loss_rec: 46.1572 acc_train: 0.4947 loss_val: 46.8673 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01541 loss_train: 27.9907 loss_rec: 27.9907 acc_train: 0.4949 loss_val: 28.4246 acc_val: 0.5000 time: 0.0283s\n",
      "Test set results: loss= 18.3776 accuracy= 0.5152\n",
      "Epoch: 01542 loss_train: 16.2352 loss_rec: 16.2352 acc_train: 0.5543 loss_val: 16.0515 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 01543 loss_train: 29.2438 loss_rec: 29.2438 acc_train: 0.5519 loss_val: 28.8498 acc_val: 0.5538 time: 0.0278s\n",
      "Epoch: 01544 loss_train: 15.1350 loss_rec: 15.1350 acc_train: 0.5563 loss_val: 14.9706 acc_val: 0.5629 time: 0.0278s\n",
      "Epoch: 01545 loss_train: 24.1810 loss_rec: 24.1810 acc_train: 0.4944 loss_val: 24.5566 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 01546 loss_train: 31.3809 loss_rec: 31.3809 acc_train: 0.4948 loss_val: 31.8663 acc_val: 0.5000 time: 0.0258s\n",
      "Epoch: 01547 loss_train: 7.7019 loss_rec: 7.7019 acc_train: 0.4926 loss_val: 7.8298 acc_val: 0.4975 time: 0.0288s\n",
      "Epoch: 01548 loss_train: 39.5360 loss_rec: 39.5360 acc_train: 0.5463 loss_val: 39.0044 acc_val: 0.5508 time: 0.0278s\n",
      "Epoch: 01549 loss_train: 56.4694 loss_rec: 56.4694 acc_train: 0.5416 loss_val: 55.7362 acc_val: 0.5483 time: 0.0273s\n",
      "Epoch: 01550 loss_train: 45.4001 loss_rec: 45.4001 acc_train: 0.5450 loss_val: 44.7987 acc_val: 0.5479 time: 0.0268s\n",
      "Epoch: 01551 loss_train: 9.5770 loss_rec: 9.5770 acc_train: 0.5573 loss_val: 9.5035 acc_val: 0.5633 time: 0.0258s\n",
      "Test set results: loss= 45.6107 accuracy= 0.5521\n",
      "Epoch: 01552 loss_train: 50.8444 loss_rec: 50.8444 acc_train: 0.4952 loss_val: 51.6259 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01553 loss_train: 76.6050 loss_rec: 76.6050 acc_train: 0.4960 loss_val: 77.7782 acc_val: 0.5000 time: 0.0258s\n",
      "Epoch: 01554 loss_train: 69.6690 loss_rec: 69.6690 acc_train: 0.4960 loss_val: 70.7367 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01555 loss_train: 33.3385 loss_rec: 33.3385 acc_train: 0.4947 loss_val: 33.8540 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01556 loss_train: 26.1950 loss_rec: 26.1950 acc_train: 0.5515 loss_val: 25.8433 acc_val: 0.5521 time: 0.0263s\n",
      "Epoch: 01557 loss_train: 52.8840 loss_rec: 52.8840 acc_train: 0.5420 loss_val: 52.1933 acc_val: 0.5475 time: 0.0278s\n",
      "Epoch: 01558 loss_train: 50.7667 loss_rec: 50.7667 acc_train: 0.5426 loss_val: 50.1011 acc_val: 0.5479 time: 0.0283s\n",
      "Epoch: 01559 loss_train: 22.8513 loss_rec: 22.8513 acc_train: 0.5542 loss_val: 22.5547 acc_val: 0.5588 time: 0.0258s\n",
      "Epoch: 01560 loss_train: 28.9406 loss_rec: 28.9406 acc_train: 0.4944 loss_val: 29.3892 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 01561 loss_train: 48.1472 loss_rec: 48.1472 acc_train: 0.4947 loss_val: 48.8878 acc_val: 0.5000 time: 0.0283s\n",
      "Test set results: loss= 31.8038 accuracy= 0.5511\n",
      "Epoch: 01562 loss_train: 35.4726 loss_rec: 35.4726 acc_train: 0.4947 loss_val: 36.0207 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01563 loss_train: 4.8067 loss_rec: 4.8067 acc_train: 0.5725 loss_val: 4.8010 acc_val: 0.5763 time: 0.0273s\n",
      "Epoch: 01564 loss_train: 15.1277 loss_rec: 15.1277 acc_train: 0.5563 loss_val: 14.9640 acc_val: 0.5629 time: 0.0278s\n",
      "Epoch: 01565 loss_train: 0.9413 loss_rec: 0.9413 acc_train: 0.6316 loss_val: 0.9618 acc_val: 0.6175 time: 0.0283s\n",
      "Epoch: 01566 loss_train: 18.7389 loss_rec: 18.7389 acc_train: 0.4937 loss_val: 19.0320 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 01567 loss_train: 5.8903 loss_rec: 5.8903 acc_train: 0.4932 loss_val: 5.9915 acc_val: 0.4979 time: 0.0288s\n",
      "Epoch: 01568 loss_train: 31.8935 loss_rec: 31.8935 acc_train: 0.5516 loss_val: 31.4616 acc_val: 0.5546 time: 0.0363s\n",
      "Epoch: 01569 loss_train: 40.6825 loss_rec: 40.6825 acc_train: 0.5463 loss_val: 40.1372 acc_val: 0.5500 time: 0.0387s\n",
      "Epoch: 01570 loss_train: 22.7493 loss_rec: 22.7493 acc_train: 0.5534 loss_val: 22.4545 acc_val: 0.5579 time: 0.0318s\n",
      "Epoch: 01571 loss_train: 19.3182 loss_rec: 19.3182 acc_train: 0.4937 loss_val: 19.6202 acc_val: 0.4988 time: 0.0353s\n",
      "Test set results: loss= 26.7837 accuracy= 0.5503\n",
      "Epoch: 01572 loss_train: 29.8837 loss_rec: 29.8837 acc_train: 0.4943 loss_val: 30.3470 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01573 loss_train: 9.5821 loss_rec: 9.5821 acc_train: 0.4924 loss_val: 9.7382 acc_val: 0.4975 time: 0.0387s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01574 loss_train: 34.6397 loss_rec: 34.6397 acc_train: 0.5496 loss_val: 34.1674 acc_val: 0.5538 time: 0.0358s\n",
      "Epoch: 01575 loss_train: 49.1341 loss_rec: 49.1341 acc_train: 0.5445 loss_val: 48.4879 acc_val: 0.5496 time: 0.0333s\n",
      "Epoch: 01576 loss_train: 36.2114 loss_rec: 36.2114 acc_train: 0.5485 loss_val: 35.7185 acc_val: 0.5525 time: 0.0343s\n",
      "Epoch: 01577 loss_train: 0.9312 loss_rec: 0.9312 acc_train: 0.6210 loss_val: 0.9506 acc_val: 0.6150 time: 0.0343s\n",
      "Epoch: 01578 loss_train: 36.6687 loss_rec: 36.6687 acc_train: 0.4947 loss_val: 37.2352 acc_val: 0.5000 time: 0.0333s\n",
      "Epoch: 01579 loss_train: 39.8683 loss_rec: 39.8683 acc_train: 0.4947 loss_val: 40.4835 acc_val: 0.5000 time: 0.0348s\n",
      "Epoch: 01580 loss_train: 13.0146 loss_rec: 13.0146 acc_train: 0.4921 loss_val: 13.2205 acc_val: 0.4971 time: 0.0298s\n",
      "Epoch: 01581 loss_train: 36.9282 loss_rec: 36.9282 acc_train: 0.5490 loss_val: 36.4266 acc_val: 0.5542 time: 0.0248s\n",
      "Test set results: loss= 63.9284 accuracy= 0.4941\n",
      "Epoch: 01582 loss_train: 56.3732 loss_rec: 56.3732 acc_train: 0.5416 loss_val: 55.6405 acc_val: 0.5483 time: 0.0278s\n",
      "Epoch: 01583 loss_train: 47.9304 loss_rec: 47.9304 acc_train: 0.5453 loss_val: 47.2985 acc_val: 0.5496 time: 0.0283s\n",
      "Epoch: 01584 loss_train: 14.7035 loss_rec: 14.7035 acc_train: 0.5573 loss_val: 14.5480 acc_val: 0.5642 time: 0.0263s\n",
      "Epoch: 01585 loss_train: 42.7324 loss_rec: 42.7324 acc_train: 0.4947 loss_val: 43.3911 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01586 loss_train: 66.6342 loss_rec: 66.6342 acc_train: 0.4960 loss_val: 67.6561 acc_val: 0.5004 time: 0.0268s\n",
      "Epoch: 01587 loss_train: 58.3974 loss_rec: 58.3974 acc_train: 0.4960 loss_val: 59.2943 acc_val: 0.5004 time: 0.0268s\n",
      "Epoch: 01588 loss_train: 21.3515 loss_rec: 21.3515 acc_train: 0.4937 loss_val: 21.6848 acc_val: 0.4988 time: 0.0298s\n",
      "Epoch: 01589 loss_train: 37.5765 loss_rec: 37.5765 acc_train: 0.5485 loss_val: 37.0672 acc_val: 0.5533 time: 0.0283s\n",
      "Epoch: 01590 loss_train: 64.7243 loss_rec: 64.7243 acc_train: 0.5397 loss_val: 63.8963 acc_val: 0.5458 time: 0.0253s\n",
      "Epoch: 01591 loss_train: 63.2766 loss_rec: 63.2766 acc_train: 0.5414 loss_val: 62.4648 acc_val: 0.5471 time: 0.0288s\n",
      "Test set results: loss= 40.9381 accuracy= 0.5050\n",
      "Epoch: 01592 loss_train: 36.1416 loss_rec: 36.1416 acc_train: 0.5485 loss_val: 35.6496 acc_val: 0.5525 time: 0.0273s\n",
      "Epoch: 01593 loss_train: 13.5783 loss_rec: 13.5783 acc_train: 0.4920 loss_val: 13.7930 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 01594 loss_train: 32.3067 loss_rec: 32.3067 acc_train: 0.4948 loss_val: 32.8071 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01595 loss_train: 19.5746 loss_rec: 19.5746 acc_train: 0.4937 loss_val: 19.8808 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01596 loss_train: 18.8359 loss_rec: 18.8359 acc_train: 0.5533 loss_val: 18.6086 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 01597 loss_train: 27.7449 loss_rec: 27.7449 acc_train: 0.5533 loss_val: 27.3727 acc_val: 0.5542 time: 0.0253s\n",
      "Epoch: 01598 loss_train: 10.6425 loss_rec: 10.6425 acc_train: 0.5561 loss_val: 10.5527 acc_val: 0.5608 time: 0.0263s\n",
      "Epoch: 01599 loss_train: 31.1173 loss_rec: 31.1173 acc_train: 0.4943 loss_val: 31.5996 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01600 loss_train: 40.7983 loss_rec: 40.7983 acc_train: 0.4947 loss_val: 41.4279 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01601 loss_train: 20.0699 loss_rec: 20.0699 acc_train: 0.4937 loss_val: 20.3837 acc_val: 0.4988 time: 0.0278s\n",
      "Test set results: loss= 28.2947 accuracy= 0.5113\n",
      "Epoch: 01602 loss_train: 24.9911 loss_rec: 24.9911 acc_train: 0.5523 loss_val: 24.6590 acc_val: 0.5558 time: 0.0283s\n",
      "Epoch: 01603 loss_train: 39.9583 loss_rec: 39.9583 acc_train: 0.5463 loss_val: 39.4212 acc_val: 0.5508 time: 0.0258s\n",
      "Epoch: 01604 loss_train: 27.9271 loss_rec: 27.9271 acc_train: 0.5533 loss_val: 27.5523 acc_val: 0.5542 time: 0.0268s\n",
      "Epoch: 01605 loss_train: 7.6253 loss_rec: 7.6253 acc_train: 0.4927 loss_val: 7.7536 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 01606 loss_train: 13.2853 loss_rec: 13.2853 acc_train: 0.4921 loss_val: 13.4959 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 01607 loss_train: 9.3596 loss_rec: 9.3596 acc_train: 0.5572 loss_val: 9.2907 acc_val: 0.5629 time: 0.0258s\n",
      "Epoch: 01608 loss_train: 5.2659 loss_rec: 5.2659 acc_train: 0.5694 loss_val: 5.2543 acc_val: 0.5737 time: 0.0263s\n",
      "Epoch: 01609 loss_train: 23.2305 loss_rec: 23.2305 acc_train: 0.4937 loss_val: 23.5927 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01610 loss_train: 20.5311 loss_rec: 20.5311 acc_train: 0.4937 loss_val: 20.8520 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01611 loss_train: 9.5808 loss_rec: 9.5808 acc_train: 0.5573 loss_val: 9.5084 acc_val: 0.5633 time: 0.0278s\n",
      "Test set results: loss= 12.9862 accuracy= 0.5194\n",
      "Epoch: 01612 loss_train: 11.4734 loss_rec: 11.4734 acc_train: 0.5553 loss_val: 11.3714 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 01613 loss_train: 11.5051 loss_rec: 11.5051 acc_train: 0.4922 loss_val: 11.6898 acc_val: 0.4975 time: 0.0268s\n",
      "Epoch: 01614 loss_train: 4.5679 loss_rec: 4.5679 acc_train: 0.4937 loss_val: 4.6498 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 01615 loss_train: 27.5922 loss_rec: 27.5922 acc_train: 0.5537 loss_val: 27.2219 acc_val: 0.5546 time: 0.0283s\n",
      "Epoch: 01616 loss_train: 31.9853 loss_rec: 31.9853 acc_train: 0.5520 loss_val: 31.5521 acc_val: 0.5550 time: 0.0273s\n",
      "Epoch: 01617 loss_train: 10.8514 loss_rec: 10.8514 acc_train: 0.5560 loss_val: 10.7587 acc_val: 0.5600 time: 0.0258s\n",
      "Epoch: 01618 loss_train: 34.6224 loss_rec: 34.6224 acc_train: 0.4948 loss_val: 35.1586 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01619 loss_train: 47.8824 loss_rec: 47.8824 acc_train: 0.4947 loss_val: 48.6201 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01620 loss_train: 30.5132 loss_rec: 30.5132 acc_train: 0.4944 loss_val: 30.9866 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01621 loss_train: 12.4325 loss_rec: 12.4325 acc_train: 0.5543 loss_val: 12.3160 acc_val: 0.5583 time: 0.0278s\n",
      "Test set results: loss= 28.3241 accuracy= 0.5128\n",
      "Epoch: 01622 loss_train: 25.0172 loss_rec: 25.0172 acc_train: 0.5527 loss_val: 24.6849 acc_val: 0.5563 time: 0.0278s\n",
      "Epoch: 01623 loss_train: 11.5341 loss_rec: 11.5341 acc_train: 0.5553 loss_val: 11.4316 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 01624 loss_train: 26.4618 loss_rec: 26.4618 acc_train: 0.4944 loss_val: 26.8735 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 01625 loss_train: 33.0787 loss_rec: 33.0787 acc_train: 0.4943 loss_val: 33.5914 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 01626 loss_train: 9.8146 loss_rec: 9.8146 acc_train: 0.4924 loss_val: 9.9753 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 01627 loss_train: 36.3846 loss_rec: 36.3846 acc_train: 0.5485 loss_val: 35.8892 acc_val: 0.5525 time: 0.0278s\n",
      "Epoch: 01628 loss_train: 53.1146 loss_rec: 53.1146 acc_train: 0.5420 loss_val: 52.4203 acc_val: 0.5475 time: 0.0273s\n",
      "Epoch: 01629 loss_train: 42.7028 loss_rec: 42.7028 acc_train: 0.5460 loss_val: 42.1328 acc_val: 0.5492 time: 0.0258s\n",
      "Epoch: 01630 loss_train: 8.2990 loss_rec: 8.2990 acc_train: 0.5606 loss_val: 8.2455 acc_val: 0.5633 time: 0.0263s\n",
      "Epoch: 01631 loss_train: 49.7978 loss_rec: 49.7978 acc_train: 0.4947 loss_val: 50.5648 acc_val: 0.5000 time: 0.0278s\n",
      "Test set results: loss= 66.6224 accuracy= 0.5528\n",
      "Epoch: 01632 loss_train: 74.2390 loss_rec: 74.2390 acc_train: 0.4960 loss_val: 75.3776 acc_val: 0.5004 time: 0.0278s\n",
      "Epoch: 01633 loss_train: 67.0459 loss_rec: 67.0459 acc_train: 0.4960 loss_val: 68.0751 acc_val: 0.5004 time: 0.0258s\n",
      "Epoch: 01634 loss_train: 31.3837 loss_rec: 31.3837 acc_train: 0.4944 loss_val: 31.8709 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01635 loss_train: 26.6156 loss_rec: 26.6156 acc_train: 0.5515 loss_val: 26.2584 acc_val: 0.5521 time: 0.0263s\n",
      "Epoch: 01636 loss_train: 52.8048 loss_rec: 52.8048 acc_train: 0.5420 loss_val: 52.1140 acc_val: 0.5475 time: 0.0268s\n",
      "Epoch: 01637 loss_train: 51.0494 loss_rec: 51.0494 acc_train: 0.5447 loss_val: 50.3797 acc_val: 0.5496 time: 0.0278s\n",
      "Epoch: 01638 loss_train: 24.2146 loss_rec: 24.2146 acc_train: 0.5535 loss_val: 23.8960 acc_val: 0.5571 time: 0.0273s\n",
      "Epoch: 01639 loss_train: 25.5644 loss_rec: 25.5644 acc_train: 0.4944 loss_val: 25.9627 acc_val: 0.4988 time: 0.0253s\n",
      "Epoch: 01640 loss_train: 43.9658 loss_rec: 43.9658 acc_train: 0.4947 loss_val: 44.6444 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01641 loss_train: 31.4826 loss_rec: 31.4826 acc_train: 0.4944 loss_val: 31.9712 acc_val: 0.4988 time: 0.0263s\n",
      "Test set results: loss= 8.3159 accuracy= 0.5262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01642 loss_train: 7.3583 loss_rec: 7.3583 acc_train: 0.5616 loss_val: 7.3165 acc_val: 0.5646 time: 0.0278s\n",
      "Epoch: 01643 loss_train: 16.8719 loss_rec: 16.8719 acc_train: 0.5537 loss_val: 16.6793 acc_val: 0.5592 time: 0.0278s\n",
      "Epoch: 01644 loss_train: 1.7495 loss_rec: 1.7495 acc_train: 0.6143 loss_val: 1.7794 acc_val: 0.6108 time: 0.0273s\n",
      "Epoch: 01645 loss_train: 32.3430 loss_rec: 32.3430 acc_train: 0.4944 loss_val: 32.8448 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 01646 loss_train: 34.2179 loss_rec: 34.2179 acc_train: 0.4943 loss_val: 34.7484 acc_val: 0.4988 time: 0.0253s\n",
      "Epoch: 01647 loss_train: 6.9721 loss_rec: 6.9721 acc_train: 0.4927 loss_val: 7.0912 acc_val: 0.4975 time: 0.0263s\n",
      "Epoch: 01648 loss_train: 42.0735 loss_rec: 42.0735 acc_train: 0.5458 loss_val: 41.5105 acc_val: 0.5496 time: 0.0273s\n",
      "Epoch: 01649 loss_train: 61.7293 loss_rec: 61.7293 acc_train: 0.5410 loss_val: 60.9341 acc_val: 0.5467 time: 0.0283s\n",
      "Epoch: 01650 loss_train: 54.1218 loss_rec: 54.1218 acc_train: 0.5413 loss_val: 53.4153 acc_val: 0.5475 time: 0.0273s\n",
      "Epoch: 01651 loss_train: 22.1761 loss_rec: 22.1761 acc_train: 0.5540 loss_val: 21.8923 acc_val: 0.5579 time: 0.0268s\n",
      "Test set results: loss= 29.1562 accuracy= 0.5503\n",
      "Epoch: 01652 loss_train: 32.5265 loss_rec: 32.5265 acc_train: 0.4944 loss_val: 33.0312 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 01653 loss_train: 55.3715 loss_rec: 55.3715 acc_train: 0.4952 loss_val: 56.2236 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01654 loss_train: 47.0264 loss_rec: 47.0264 acc_train: 0.4947 loss_val: 47.7517 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01655 loss_train: 10.6681 loss_rec: 10.6681 acc_train: 0.4924 loss_val: 10.8420 acc_val: 0.4975 time: 0.0258s\n",
      "Epoch: 01656 loss_train: 46.1921 loss_rec: 46.1921 acc_train: 0.5450 loss_val: 45.5803 acc_val: 0.5479 time: 0.0278s\n",
      "Epoch: 01657 loss_train: 72.7548 loss_rec: 72.7548 acc_train: 0.5353 loss_val: 71.8348 acc_val: 0.5408 time: 0.0278s\n",
      "Epoch: 01658 loss_train: 71.4373 loss_rec: 71.4373 acc_train: 0.5358 loss_val: 70.5321 acc_val: 0.5413 time: 0.0258s\n",
      "Epoch: 01659 loss_train: 45.0868 loss_rec: 45.0868 acc_train: 0.5461 loss_val: 44.4882 acc_val: 0.5488 time: 0.0278s\n",
      "Epoch: 01660 loss_train: 2.6364 loss_rec: 2.6364 acc_train: 0.4939 loss_val: 2.6884 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01661 loss_train: 21.6674 loss_rec: 21.6674 acc_train: 0.4937 loss_val: 22.0064 acc_val: 0.4988 time: 0.0263s\n",
      "Test set results: loss= 9.0347 accuracy= 0.5482\n",
      "Epoch: 01662 loss_train: 10.1243 loss_rec: 10.1243 acc_train: 0.4924 loss_val: 10.2904 acc_val: 0.4975 time: 0.0258s\n",
      "Epoch: 01663 loss_train: 25.8359 loss_rec: 25.8359 acc_train: 0.5517 loss_val: 25.4908 acc_val: 0.5554 time: 0.0268s\n",
      "Epoch: 01664 loss_train: 33.8681 loss_rec: 33.8681 acc_train: 0.5499 loss_val: 33.4078 acc_val: 0.5546 time: 0.0278s\n",
      "Epoch: 01665 loss_train: 16.4482 loss_rec: 16.4482 acc_train: 0.5543 loss_val: 16.2638 acc_val: 0.5592 time: 0.0268s\n",
      "Epoch: 01666 loss_train: 24.5913 loss_rec: 24.5913 acc_train: 0.4937 loss_val: 24.9751 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01667 loss_train: 34.9155 loss_rec: 34.9155 acc_train: 0.4943 loss_val: 35.4569 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01668 loss_train: 15.5228 loss_rec: 15.5228 acc_train: 0.4920 loss_val: 15.7682 acc_val: 0.4967 time: 0.0258s\n",
      "Epoch: 01669 loss_train: 27.2819 loss_rec: 27.2819 acc_train: 0.5536 loss_val: 26.9153 acc_val: 0.5546 time: 0.0263s\n",
      "Epoch: 01670 loss_train: 41.1991 loss_rec: 41.1991 acc_train: 0.5453 loss_val: 40.6466 acc_val: 0.5496 time: 0.0283s\n",
      "Epoch: 01671 loss_train: 28.8641 loss_rec: 28.8641 acc_train: 0.5527 loss_val: 28.4766 acc_val: 0.5542 time: 0.0273s\n",
      "Test set results: loss= 5.5284 accuracy= 0.5490\n",
      "Epoch: 01672 loss_train: 6.2204 loss_rec: 6.2204 acc_train: 0.4934 loss_val: 6.3288 acc_val: 0.4979 time: 0.0258s\n",
      "Epoch: 01673 loss_train: 12.3120 loss_rec: 12.3120 acc_train: 0.4922 loss_val: 12.5100 acc_val: 0.4975 time: 0.0283s\n",
      "Epoch: 01674 loss_train: 9.1241 loss_rec: 9.1241 acc_train: 0.5579 loss_val: 9.0599 acc_val: 0.5625 time: 0.0258s\n",
      "Epoch: 01675 loss_train: 4.7800 loss_rec: 4.7800 acc_train: 0.5730 loss_val: 4.7761 acc_val: 0.5775 time: 0.0279s\n",
      "Epoch: 01676 loss_train: 23.0871 loss_rec: 23.0871 acc_train: 0.4937 loss_val: 23.4481 acc_val: 0.4988 time: 0.0272s\n",
      "Epoch: 01677 loss_train: 20.5277 loss_rec: 20.5277 acc_train: 0.4937 loss_val: 20.8497 acc_val: 0.4988 time: 0.0253s\n",
      "Epoch: 01678 loss_train: 8.7242 loss_rec: 8.7242 acc_train: 0.5595 loss_val: 8.6658 acc_val: 0.5629 time: 0.0278s\n",
      "Epoch: 01679 loss_train: 10.6977 loss_rec: 10.6977 acc_train: 0.5563 loss_val: 10.6083 acc_val: 0.5621 time: 0.0278s\n",
      "Epoch: 01680 loss_train: 11.4799 loss_rec: 11.4799 acc_train: 0.4923 loss_val: 11.6662 acc_val: 0.4975 time: 0.0258s\n",
      "Epoch: 01681 loss_train: 4.5565 loss_rec: 4.5565 acc_train: 0.4937 loss_val: 4.6394 acc_val: 0.4983 time: 0.0283s\n",
      "Test set results: loss= 30.4355 accuracy= 0.5107\n",
      "Epoch: 01682 loss_train: 26.8807 loss_rec: 26.8807 acc_train: 0.5530 loss_val: 26.5197 acc_val: 0.5558 time: 0.0278s\n",
      "Epoch: 01683 loss_train: 31.3121 loss_rec: 31.3121 acc_train: 0.5523 loss_val: 30.8892 acc_val: 0.5542 time: 0.0283s\n",
      "Epoch: 01684 loss_train: 10.8837 loss_rec: 10.8837 acc_train: 0.5560 loss_val: 10.7913 acc_val: 0.5600 time: 0.0258s\n",
      "Epoch: 01685 loss_train: 33.1701 loss_rec: 33.1701 acc_train: 0.4944 loss_val: 33.6853 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 01686 loss_train: 45.9232 loss_rec: 45.9232 acc_train: 0.4947 loss_val: 46.6324 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01687 loss_train: 28.8676 loss_rec: 28.8676 acc_train: 0.4944 loss_val: 29.3172 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01688 loss_train: 12.9237 loss_rec: 12.9237 acc_train: 0.5537 loss_val: 12.8008 acc_val: 0.5579 time: 0.0263s\n",
      "Epoch: 01689 loss_train: 25.2239 loss_rec: 25.2239 acc_train: 0.5527 loss_val: 24.8895 acc_val: 0.5563 time: 0.0278s\n",
      "Epoch: 01690 loss_train: 12.1208 loss_rec: 12.1208 acc_train: 0.5543 loss_val: 12.0100 acc_val: 0.5583 time: 0.0263s\n",
      "Epoch: 01691 loss_train: 24.6909 loss_rec: 24.6909 acc_train: 0.4937 loss_val: 25.0768 acc_val: 0.4988 time: 0.0273s\n",
      "Test set results: loss= 27.8972 accuracy= 0.5504\n",
      "Epoch: 01692 loss_train: 31.1255 loss_rec: 31.1255 acc_train: 0.4944 loss_val: 31.6095 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01693 loss_train: 8.4608 loss_rec: 8.4608 acc_train: 0.4927 loss_val: 8.6034 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 01694 loss_train: 36.3819 loss_rec: 36.3819 acc_train: 0.5493 loss_val: 35.8861 acc_val: 0.5525 time: 0.0263s\n",
      "Epoch: 01695 loss_train: 52.6678 loss_rec: 52.6678 acc_train: 0.5427 loss_val: 51.9780 acc_val: 0.5479 time: 0.0278s\n",
      "Epoch: 01696 loss_train: 42.5477 loss_rec: 42.5477 acc_train: 0.5460 loss_val: 41.9787 acc_val: 0.5488 time: 0.0258s\n",
      "Epoch: 01697 loss_train: 9.0505 loss_rec: 9.0505 acc_train: 0.5585 loss_val: 8.9878 acc_val: 0.5625 time: 0.0278s\n",
      "Epoch: 01698 loss_train: 47.4787 loss_rec: 47.4787 acc_train: 0.4947 loss_val: 48.2117 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01699 loss_train: 71.3886 loss_rec: 71.3886 acc_train: 0.4960 loss_val: 72.4850 acc_val: 0.5004 time: 0.0258s\n",
      "Epoch: 01700 loss_train: 64.4712 loss_rec: 64.4712 acc_train: 0.4952 loss_val: 65.4625 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01701 loss_train: 29.8665 loss_rec: 29.8665 acc_train: 0.4944 loss_val: 30.3314 acc_val: 0.4988 time: 0.0283s\n",
      "Test set results: loss= 29.9295 accuracy= 0.5091\n",
      "Epoch: 01702 loss_train: 26.4342 loss_rec: 26.4342 acc_train: 0.5508 loss_val: 26.0805 acc_val: 0.5525 time: 0.0268s\n",
      "Epoch: 01703 loss_train: 51.8571 loss_rec: 51.8571 acc_train: 0.5432 loss_val: 51.1769 acc_val: 0.5492 time: 0.0273s\n",
      "Epoch: 01704 loss_train: 50.1072 loss_rec: 50.1072 acc_train: 0.5445 loss_val: 49.4480 acc_val: 0.5496 time: 0.0278s\n",
      "Epoch: 01705 loss_train: 23.9453 loss_rec: 23.9453 acc_train: 0.5542 loss_val: 23.6325 acc_val: 0.5588 time: 0.0263s\n",
      "Epoch: 01706 loss_train: 24.4485 loss_rec: 24.4485 acc_train: 0.4937 loss_val: 24.8308 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 01707 loss_train: 42.3825 loss_rec: 42.3825 acc_train: 0.4947 loss_val: 43.0386 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01708 loss_train: 30.2791 loss_rec: 30.2791 acc_train: 0.4944 loss_val: 30.7505 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01709 loss_train: 7.4467 loss_rec: 7.4467 acc_train: 0.5616 loss_val: 7.4042 acc_val: 0.5646 time: 0.0278s\n",
      "Epoch: 01710 loss_train: 16.6812 loss_rec: 16.6812 acc_train: 0.5543 loss_val: 16.4931 acc_val: 0.5592 time: 0.0258s\n",
      "Epoch: 01711 loss_train: 1.9399 loss_rec: 1.9399 acc_train: 0.6122 loss_val: 1.9698 acc_val: 0.6096 time: 0.0273s\n",
      "Test set results: loss= 28.7230 accuracy= 0.5504\n",
      "Epoch: 01712 loss_train: 32.0452 loss_rec: 32.0452 acc_train: 0.4944 loss_val: 32.5440 acc_val: 0.4988 time: 0.0283s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01713 loss_train: 34.6861 loss_rec: 34.6861 acc_train: 0.4944 loss_val: 35.2252 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 01714 loss_train: 8.8671 loss_rec: 8.8671 acc_train: 0.4927 loss_val: 9.0166 acc_val: 0.4975 time: 0.0268s\n",
      "Epoch: 01715 loss_train: 38.3787 loss_rec: 38.3787 acc_train: 0.5490 loss_val: 37.8581 acc_val: 0.5546 time: 0.0278s\n",
      "Epoch: 01716 loss_train: 57.0525 loss_rec: 57.0525 acc_train: 0.5415 loss_val: 56.3104 acc_val: 0.5483 time: 0.0278s\n",
      "Epoch: 01717 loss_train: 49.2647 loss_rec: 49.2647 acc_train: 0.5453 loss_val: 48.6156 acc_val: 0.5496 time: 0.0258s\n",
      "Epoch: 01718 loss_train: 17.9114 loss_rec: 17.9114 acc_train: 0.5537 loss_val: 17.7024 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 01719 loss_train: 35.8983 loss_rec: 35.8983 acc_train: 0.4944 loss_val: 36.4560 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 01720 loss_train: 58.3985 loss_rec: 58.3985 acc_train: 0.4952 loss_val: 59.2982 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01721 loss_train: 50.5113 loss_rec: 50.5113 acc_train: 0.4947 loss_val: 51.2911 acc_val: 0.5000 time: 0.0288s\n",
      "Test set results: loss= 13.6741 accuracy= 0.5479\n",
      "Epoch: 01722 loss_train: 15.2903 loss_rec: 15.2903 acc_train: 0.4921 loss_val: 15.5336 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 01723 loss_train: 40.1927 loss_rec: 40.1927 acc_train: 0.5463 loss_val: 39.6508 acc_val: 0.5508 time: 0.0258s\n",
      "Epoch: 01724 loss_train: 65.9938 loss_rec: 65.9938 acc_train: 0.5397 loss_val: 65.1492 acc_val: 0.5450 time: 0.0278s\n",
      "Epoch: 01725 loss_train: 64.6389 loss_rec: 64.6389 acc_train: 0.5407 loss_val: 63.8090 acc_val: 0.5467 time: 0.0283s\n",
      "Epoch: 01726 loss_train: 38.9658 loss_rec: 38.9658 acc_train: 0.5485 loss_val: 38.4382 acc_val: 0.5533 time: 0.0273s\n",
      "Epoch: 01727 loss_train: 7.6892 loss_rec: 7.6892 acc_train: 0.4927 loss_val: 7.8211 acc_val: 0.4975 time: 0.0268s\n",
      "Epoch: 01728 loss_train: 25.4685 loss_rec: 25.4685 acc_train: 0.4937 loss_val: 25.8669 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01729 loss_train: 13.4849 loss_rec: 13.4849 acc_train: 0.4922 loss_val: 13.7016 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 01730 loss_train: 22.3905 loss_rec: 22.3905 acc_train: 0.5545 loss_val: 22.1048 acc_val: 0.5592 time: 0.0283s\n",
      "Epoch: 01731 loss_train: 30.7959 loss_rec: 30.7959 acc_train: 0.5520 loss_val: 30.3809 acc_val: 0.5542 time: 0.0273s\n",
      "Test set results: loss= 16.3185 accuracy= 0.5196\n",
      "Epoch: 01732 loss_train: 14.4165 loss_rec: 14.4165 acc_train: 0.5573 loss_val: 14.2687 acc_val: 0.5642 time: 0.0283s\n",
      "Epoch: 01733 loss_train: 25.0714 loss_rec: 25.0714 acc_train: 0.4937 loss_val: 25.4636 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 01734 loss_train: 34.6211 loss_rec: 34.6211 acc_train: 0.4944 loss_val: 35.1592 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01735 loss_train: 15.2773 loss_rec: 15.2773 acc_train: 0.4921 loss_val: 15.5205 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 01736 loss_train: 26.7440 loss_rec: 26.7440 acc_train: 0.5513 loss_val: 26.3853 acc_val: 0.5521 time: 0.0263s\n",
      "Epoch: 01737 loss_train: 40.6544 loss_rec: 40.6544 acc_train: 0.5463 loss_val: 40.1072 acc_val: 0.5508 time: 0.0278s\n",
      "Epoch: 01738 loss_train: 29.0181 loss_rec: 29.0181 acc_train: 0.5527 loss_val: 28.6287 acc_val: 0.5542 time: 0.0278s\n",
      "Epoch: 01739 loss_train: 4.6628 loss_rec: 4.6628 acc_train: 0.4937 loss_val: 4.7485 acc_val: 0.4983 time: 0.0258s\n",
      "Epoch: 01740 loss_train: 10.3047 loss_rec: 10.3047 acc_train: 0.4926 loss_val: 10.4759 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 01741 loss_train: 10.5770 loss_rec: 10.5770 acc_train: 0.5563 loss_val: 10.4901 acc_val: 0.5621 time: 0.0273s\n",
      "Test set results: loss= 7.0600 accuracy= 0.5326\n",
      "Epoch: 01742 loss_train: 6.2555 loss_rec: 6.2555 acc_train: 0.5667 loss_val: 6.2305 acc_val: 0.5721 time: 0.0278s\n",
      "Epoch: 01743 loss_train: 21.2204 loss_rec: 21.2204 acc_train: 0.4937 loss_val: 21.5541 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01744 loss_train: 19.3195 loss_rec: 19.3195 acc_train: 0.4937 loss_val: 19.6241 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 01745 loss_train: 8.6219 loss_rec: 8.6219 acc_train: 0.5601 loss_val: 8.5655 acc_val: 0.5629 time: 0.0268s\n",
      "Epoch: 01746 loss_train: 10.1687 loss_rec: 10.1687 acc_train: 0.5563 loss_val: 10.0887 acc_val: 0.5621 time: 0.0268s\n",
      "Epoch: 01747 loss_train: 11.7088 loss_rec: 11.7088 acc_train: 0.4924 loss_val: 11.9006 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 01748 loss_train: 5.2469 loss_rec: 5.2469 acc_train: 0.4937 loss_val: 5.3419 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 01749 loss_train: 25.1841 loss_rec: 25.1841 acc_train: 0.5535 loss_val: 24.8515 acc_val: 0.5575 time: 0.0273s\n",
      "Epoch: 01750 loss_train: 29.3528 loss_rec: 29.3528 acc_train: 0.5527 loss_val: 28.9590 acc_val: 0.5542 time: 0.0268s\n",
      "Epoch: 01751 loss_train: 9.4230 loss_rec: 9.4230 acc_train: 0.5579 loss_val: 9.3557 acc_val: 0.5629 time: 0.0268s\n",
      "Test set results: loss= 29.9506 accuracy= 0.5504\n",
      "Epoch: 01752 loss_train: 33.4127 loss_rec: 33.4127 acc_train: 0.4944 loss_val: 33.9328 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01753 loss_train: 45.7139 loss_rec: 45.7139 acc_train: 0.4947 loss_val: 46.4213 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01754 loss_train: 28.9821 loss_rec: 28.9821 acc_train: 0.4937 loss_val: 29.4345 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 01755 loss_train: 11.8650 loss_rec: 11.8650 acc_train: 0.5553 loss_val: 11.7592 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 01756 loss_train: 24.0136 loss_rec: 24.0136 acc_train: 0.5542 loss_val: 23.7006 acc_val: 0.5588 time: 0.0253s\n",
      "Epoch: 01757 loss_train: 11.4604 loss_rec: 11.4604 acc_train: 0.5553 loss_val: 11.3604 acc_val: 0.5596 time: 0.0283s\n",
      "Epoch: 01758 loss_train: 24.1759 loss_rec: 24.1759 acc_train: 0.4937 loss_val: 24.5550 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 01759 loss_train: 30.2170 loss_rec: 30.2170 acc_train: 0.4937 loss_val: 30.6883 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 01760 loss_train: 7.9729 loss_rec: 7.9729 acc_train: 0.4927 loss_val: 8.1096 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 01761 loss_train: 35.7775 loss_rec: 35.7775 acc_train: 0.5496 loss_val: 35.2901 acc_val: 0.5538 time: 0.0253s\n",
      "Test set results: loss= 58.7043 accuracy= 0.4973\n",
      "Epoch: 01762 loss_train: 51.7802 loss_rec: 51.7802 acc_train: 0.5445 loss_val: 51.1005 acc_val: 0.5496 time: 0.0273s\n",
      "Epoch: 01763 loss_train: 42.0507 loss_rec: 42.0507 acc_train: 0.5457 loss_val: 41.4868 acc_val: 0.5496 time: 0.0278s\n",
      "Epoch: 01764 loss_train: 9.5494 loss_rec: 9.5494 acc_train: 0.5581 loss_val: 9.4801 acc_val: 0.5629 time: 0.0273s\n",
      "Epoch: 01765 loss_train: 45.3740 loss_rec: 45.3740 acc_train: 0.4947 loss_val: 46.0763 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01766 loss_train: 68.6383 loss_rec: 68.6383 acc_train: 0.4952 loss_val: 69.6942 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01767 loss_train: 61.8578 loss_rec: 61.8578 acc_train: 0.4952 loss_val: 62.8108 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01768 loss_train: 28.1129 loss_rec: 28.1129 acc_train: 0.4937 loss_val: 28.5522 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 01769 loss_train: 26.6263 loss_rec: 26.6263 acc_train: 0.5508 loss_val: 26.2702 acc_val: 0.5525 time: 0.0283s\n",
      "Epoch: 01770 loss_train: 51.4186 loss_rec: 51.4186 acc_train: 0.5445 loss_val: 50.7429 acc_val: 0.5496 time: 0.0273s\n",
      "Epoch: 01771 loss_train: 49.7376 loss_rec: 49.7376 acc_train: 0.5453 loss_val: 49.0823 acc_val: 0.5496 time: 0.0273s\n",
      "Test set results: loss= 27.4899 accuracy= 0.5133\n",
      "Epoch: 01772 loss_train: 24.2801 loss_rec: 24.2801 acc_train: 0.5542 loss_val: 23.9625 acc_val: 0.5588 time: 0.0258s\n",
      "Epoch: 01773 loss_train: 22.7194 loss_rec: 22.7194 acc_train: 0.4937 loss_val: 23.0764 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 01774 loss_train: 40.1882 loss_rec: 40.1882 acc_train: 0.4943 loss_val: 40.8120 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01775 loss_train: 28.3577 loss_rec: 28.3577 acc_train: 0.4937 loss_val: 28.8009 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 01776 loss_train: 8.2676 loss_rec: 8.2676 acc_train: 0.5603 loss_val: 8.2159 acc_val: 0.5625 time: 0.0278s\n",
      "Epoch: 01777 loss_train: 17.1910 loss_rec: 17.1910 acc_train: 0.5537 loss_val: 16.9954 acc_val: 0.5592 time: 0.0278s\n",
      "Epoch: 01778 loss_train: 2.5788 loss_rec: 2.5788 acc_train: 0.5992 loss_val: 2.6045 acc_val: 0.5975 time: 0.0253s\n",
      "Epoch: 01779 loss_train: 32.3868 loss_rec: 32.3868 acc_train: 0.4937 loss_val: 32.8917 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01780 loss_train: 36.8517 loss_rec: 36.8517 acc_train: 0.4944 loss_val: 37.4249 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 01781 loss_train: 13.4132 loss_rec: 13.4132 acc_train: 0.4922 loss_val: 13.6305 acc_val: 0.4975 time: 0.0273s\n",
      "Test set results: loss= 35.6699 accuracy= 0.5094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01782 loss_train: 31.4972 loss_rec: 31.4972 acc_train: 0.5522 loss_val: 31.0723 acc_val: 0.5542 time: 0.0273s\n",
      "Epoch: 01783 loss_train: 48.4111 loss_rec: 48.4111 acc_train: 0.5463 loss_val: 47.7712 acc_val: 0.5488 time: 0.0263s\n",
      "Epoch: 01784 loss_train: 39.7744 loss_rec: 39.7744 acc_train: 0.5462 loss_val: 39.2363 acc_val: 0.5504 time: 0.0263s\n",
      "Epoch: 01785 loss_train: 8.5719 loss_rec: 8.5719 acc_train: 0.5606 loss_val: 8.5164 acc_val: 0.5633 time: 0.0278s\n",
      "Epoch: 01786 loss_train: 44.8122 loss_rec: 44.8122 acc_train: 0.4947 loss_val: 45.5067 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01787 loss_train: 66.8403 loss_rec: 66.8403 acc_train: 0.4952 loss_val: 67.8697 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01788 loss_train: 59.2617 loss_rec: 59.2617 acc_train: 0.4952 loss_val: 60.1759 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01789 loss_train: 25.0299 loss_rec: 25.0299 acc_train: 0.4937 loss_val: 25.4227 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 01790 loss_train: 29.6301 loss_rec: 29.6301 acc_train: 0.5527 loss_val: 29.2324 acc_val: 0.5542 time: 0.0253s\n",
      "Epoch: 01791 loss_train: 54.7121 loss_rec: 54.7121 acc_train: 0.5411 loss_val: 53.9968 acc_val: 0.5475 time: 0.0278s\n",
      "Test set results: loss= 60.6494 accuracy= 0.4943\n",
      "Epoch: 01792 loss_train: 53.4921 loss_rec: 53.4921 acc_train: 0.5417 loss_val: 52.7916 acc_val: 0.5479 time: 0.0273s\n",
      "Epoch: 01793 loss_train: 28.6415 loss_rec: 28.6415 acc_train: 0.5532 loss_val: 28.2572 acc_val: 0.5542 time: 0.0278s\n",
      "Epoch: 01794 loss_train: 17.2432 loss_rec: 17.2432 acc_train: 0.4921 loss_val: 17.5170 acc_val: 0.4971 time: 0.0253s\n",
      "Epoch: 01795 loss_train: 34.2903 loss_rec: 34.2903 acc_train: 0.4944 loss_val: 34.8244 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 01796 loss_train: 22.3649 loss_rec: 22.3649 acc_train: 0.4937 loss_val: 22.7169 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01797 loss_train: 13.5223 loss_rec: 13.5223 acc_train: 0.5543 loss_val: 13.3909 acc_val: 0.5579 time: 0.0258s\n",
      "Epoch: 01798 loss_train: 22.0053 loss_rec: 22.0053 acc_train: 0.5544 loss_val: 21.7274 acc_val: 0.5592 time: 0.0283s\n",
      "Epoch: 01799 loss_train: 6.7030 loss_rec: 6.7030 acc_train: 0.5658 loss_val: 6.6721 acc_val: 0.5717 time: 0.0278s\n",
      "Epoch: 01800 loss_train: 31.0483 loss_rec: 31.0483 acc_train: 0.4937 loss_val: 31.5330 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 01801 loss_train: 39.0102 loss_rec: 39.0102 acc_train: 0.4944 loss_val: 39.6165 acc_val: 0.4988 time: 0.0278s\n",
      "Test set results: loss= 16.9692 accuracy= 0.5498\n",
      "Epoch: 01802 loss_train: 18.9601 loss_rec: 18.9601 acc_train: 0.4938 loss_val: 19.2602 acc_val: 0.4988 time: 0.0248s\n",
      "Epoch: 01803 loss_train: 23.2914 loss_rec: 23.2914 acc_train: 0.5542 loss_val: 22.9914 acc_val: 0.5579 time: 0.0278s\n",
      "Epoch: 01804 loss_train: 37.7789 loss_rec: 37.7789 acc_train: 0.5488 loss_val: 37.2651 acc_val: 0.5542 time: 0.0278s\n",
      "Epoch: 01805 loss_train: 27.3869 loss_rec: 27.3869 acc_train: 0.5530 loss_val: 27.0192 acc_val: 0.5558 time: 0.0258s\n",
      "Epoch: 01806 loss_train: 4.4962 loss_rec: 4.4962 acc_train: 0.4939 loss_val: 4.5804 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 01807 loss_train: 9.1616 loss_rec: 9.1616 acc_train: 0.4927 loss_val: 9.3176 acc_val: 0.4975 time: 0.0258s\n",
      "Epoch: 01808 loss_train: 11.7770 loss_rec: 11.7770 acc_train: 0.5553 loss_val: 11.6730 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 01809 loss_train: 8.0572 loss_rec: 8.0572 acc_train: 0.5607 loss_val: 8.0083 acc_val: 0.5633 time: 0.0278s\n",
      "Epoch: 01810 loss_train: 18.2787 loss_rec: 18.2787 acc_train: 0.4921 loss_val: 18.5684 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 01811 loss_train: 16.2411 loss_rec: 16.2411 acc_train: 0.4921 loss_val: 16.5006 acc_val: 0.4971 time: 0.0258s\n",
      "Test set results: loss= 12.2388 accuracy= 0.5207\n",
      "Epoch: 01812 loss_train: 10.8139 loss_rec: 10.8139 acc_train: 0.5563 loss_val: 10.7242 acc_val: 0.5621 time: 0.0273s\n",
      "Epoch: 01813 loss_train: 12.0993 loss_rec: 12.0993 acc_train: 0.5550 loss_val: 11.9904 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 01814 loss_train: 9.3517 loss_rec: 9.3517 acc_train: 0.4927 loss_val: 9.5106 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 01815 loss_train: 3.3998 loss_rec: 3.3998 acc_train: 0.4940 loss_val: 3.4667 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01816 loss_train: 25.5597 loss_rec: 25.5597 acc_train: 0.5527 loss_val: 25.2215 acc_val: 0.5563 time: 0.0263s\n",
      "Epoch: 01817 loss_train: 29.1798 loss_rec: 29.1798 acc_train: 0.5533 loss_val: 28.7886 acc_val: 0.5542 time: 0.0253s\n",
      "Epoch: 01818 loss_train: 9.3829 loss_rec: 9.3829 acc_train: 0.5579 loss_val: 9.3170 acc_val: 0.5625 time: 0.0283s\n",
      "Epoch: 01819 loss_train: 32.6457 loss_rec: 32.6457 acc_train: 0.4937 loss_val: 33.1552 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01820 loss_train: 44.9628 loss_rec: 44.9628 acc_train: 0.4942 loss_val: 45.6601 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01821 loss_train: 28.9331 loss_rec: 28.9331 acc_train: 0.4937 loss_val: 29.3859 acc_val: 0.4988 time: 0.0268s\n",
      "Test set results: loss= 12.0589 accuracy= 0.5207\n",
      "Epoch: 01822 loss_train: 10.6553 loss_rec: 10.6553 acc_train: 0.5563 loss_val: 10.5680 acc_val: 0.5621 time: 0.0283s\n",
      "Epoch: 01823 loss_train: 22.3746 loss_rec: 22.3746 acc_train: 0.5545 loss_val: 22.0907 acc_val: 0.5592 time: 0.0278s\n",
      "Epoch: 01824 loss_train: 10.0682 loss_rec: 10.0682 acc_train: 0.5573 loss_val: 9.9909 acc_val: 0.5633 time: 0.0258s\n",
      "Epoch: 01825 loss_train: 24.6508 loss_rec: 24.6508 acc_train: 0.4937 loss_val: 25.0383 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01826 loss_train: 30.4860 loss_rec: 30.4860 acc_train: 0.4937 loss_val: 30.9626 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01827 loss_train: 8.7289 loss_rec: 8.7289 acc_train: 0.4927 loss_val: 8.8787 acc_val: 0.4975 time: 0.0258s\n",
      "Epoch: 01828 loss_train: 33.9697 loss_rec: 33.9697 acc_train: 0.5504 loss_val: 33.5090 acc_val: 0.5546 time: 0.0283s\n",
      "Epoch: 01829 loss_train: 49.6080 loss_rec: 49.6080 acc_train: 0.5461 loss_val: 48.9536 acc_val: 0.5496 time: 0.0278s\n",
      "Epoch: 01830 loss_train: 40.1883 loss_rec: 40.1883 acc_train: 0.5462 loss_val: 39.6449 acc_val: 0.5504 time: 0.0253s\n",
      "Epoch: 01831 loss_train: 8.6996 loss_rec: 8.6996 acc_train: 0.5607 loss_val: 8.6431 acc_val: 0.5633 time: 0.0278s\n",
      "Test set results: loss= 39.9453 accuracy= 0.5503\n",
      "Epoch: 01832 loss_train: 44.5409 loss_rec: 44.5409 acc_train: 0.4942 loss_val: 45.2322 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 01833 loss_train: 66.9576 loss_rec: 66.9576 acc_train: 0.4952 loss_val: 67.9895 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01834 loss_train: 60.1357 loss_rec: 60.1357 acc_train: 0.4952 loss_val: 61.0641 acc_val: 0.5000 time: 0.0273s\n",
      "Epoch: 01835 loss_train: 27.0599 loss_rec: 27.0599 acc_train: 0.4937 loss_val: 27.4844 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 01836 loss_train: 26.3853 loss_rec: 26.3853 acc_train: 0.5516 loss_val: 26.0340 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 01837 loss_train: 50.6738 loss_rec: 50.6738 acc_train: 0.5445 loss_val: 50.0063 acc_val: 0.5492 time: 0.0268s\n",
      "Epoch: 01838 loss_train: 49.2112 loss_rec: 49.2112 acc_train: 0.5461 loss_val: 48.5616 acc_val: 0.5496 time: 0.0263s\n",
      "Epoch: 01839 loss_train: 24.5495 loss_rec: 24.5495 acc_train: 0.5542 loss_val: 24.2283 acc_val: 0.5588 time: 0.0268s\n",
      "Epoch: 01840 loss_train: 20.9715 loss_rec: 20.9715 acc_train: 0.4937 loss_val: 21.3031 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01841 loss_train: 37.8621 loss_rec: 37.8621 acc_train: 0.4944 loss_val: 38.4518 acc_val: 0.4988 time: 0.0278s\n",
      "Test set results: loss= 23.5151 accuracy= 0.5497\n",
      "Epoch: 01842 loss_train: 26.2488 loss_rec: 26.2488 acc_train: 0.4937 loss_val: 26.6609 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01843 loss_train: 9.3432 loss_rec: 9.3432 acc_train: 0.5578 loss_val: 9.2781 acc_val: 0.5625 time: 0.0278s\n",
      "Epoch: 01844 loss_train: 17.9855 loss_rec: 17.9855 acc_train: 0.5537 loss_val: 17.7773 acc_val: 0.5592 time: 0.0258s\n",
      "Epoch: 01845 loss_train: 3.5240 loss_rec: 3.5240 acc_train: 0.5856 loss_val: 3.5379 acc_val: 0.5896 time: 0.0278s\n",
      "Epoch: 01846 loss_train: 31.8923 loss_rec: 31.8923 acc_train: 0.4937 loss_val: 32.3909 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 01847 loss_train: 37.7405 loss_rec: 37.7405 acc_train: 0.4944 loss_val: 38.3285 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01848 loss_train: 16.2409 loss_rec: 16.2409 acc_train: 0.4922 loss_val: 16.5014 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 01849 loss_train: 26.6057 loss_rec: 26.6057 acc_train: 0.5512 loss_val: 26.2507 acc_val: 0.5529 time: 0.0258s\n",
      "Epoch: 01850 loss_train: 42.1700 loss_rec: 42.1700 acc_train: 0.5456 loss_val: 41.6038 acc_val: 0.5496 time: 0.0283s\n",
      "Epoch: 01851 loss_train: 33.0304 loss_rec: 33.0304 acc_train: 0.5520 loss_val: 32.5838 acc_val: 0.5550 time: 0.0283s\n",
      "Test set results: loss= 2.8433 accuracy= 0.5735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01852 loss_train: 2.5572 loss_rec: 2.5572 acc_train: 0.5986 loss_val: 2.5837 acc_val: 0.5992 time: 0.0278s\n",
      "Epoch: 01853 loss_train: 47.1925 loss_rec: 47.1925 acc_train: 0.4942 loss_val: 47.9247 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01854 loss_train: 65.7356 loss_rec: 65.7356 acc_train: 0.4952 loss_val: 66.7497 acc_val: 0.5000 time: 0.0268s\n",
      "Epoch: 01855 loss_train: 55.7099 loss_rec: 55.7099 acc_train: 0.4947 loss_val: 56.5717 acc_val: 0.5000 time: 0.0263s\n",
      "Epoch: 01856 loss_train: 20.0032 loss_rec: 20.0032 acc_train: 0.4938 loss_val: 20.3203 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01857 loss_train: 34.8802 loss_rec: 34.8802 acc_train: 0.5499 loss_val: 34.4063 acc_val: 0.5546 time: 0.0278s\n",
      "Epoch: 01858 loss_train: 61.1042 loss_rec: 61.1042 acc_train: 0.5403 loss_val: 60.3128 acc_val: 0.5463 time: 0.0273s\n",
      "Epoch: 01859 loss_train: 61.4776 loss_rec: 61.4776 acc_train: 0.5403 loss_val: 60.6819 acc_val: 0.5463 time: 0.0273s\n",
      "Epoch: 01860 loss_train: 38.5944 loss_rec: 38.5944 acc_train: 0.5484 loss_val: 38.0702 acc_val: 0.5538 time: 0.0268s\n",
      "Epoch: 01861 loss_train: 4.0626 loss_rec: 4.0626 acc_train: 0.4940 loss_val: 4.1401 acc_val: 0.4988 time: 0.0263s\n",
      "Test set results: loss= 17.6677 accuracy= 0.5498\n",
      "Epoch: 01862 loss_train: 19.7386 loss_rec: 19.7386 acc_train: 0.4938 loss_val: 20.0516 acc_val: 0.4988 time: 0.0288s\n",
      "Epoch: 01863 loss_train: 7.3548 loss_rec: 7.3548 acc_train: 0.4935 loss_val: 7.4845 acc_val: 0.4979 time: 0.0253s\n",
      "Epoch: 01864 loss_train: 26.9951 loss_rec: 26.9951 acc_train: 0.5513 loss_val: 26.6338 acc_val: 0.5521 time: 0.0273s\n",
      "Epoch: 01865 loss_train: 35.6370 loss_rec: 35.6370 acc_train: 0.5495 loss_val: 35.1518 acc_val: 0.5538 time: 0.0263s\n",
      "Epoch: 01866 loss_train: 20.6197 loss_rec: 20.6197 acc_train: 0.5520 loss_val: 20.3663 acc_val: 0.5571 time: 0.0273s\n",
      "Epoch: 01867 loss_train: 15.6719 loss_rec: 15.6719 acc_train: 0.4922 loss_val: 15.9241 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 01868 loss_train: 24.2986 loss_rec: 24.2986 acc_train: 0.4937 loss_val: 24.6813 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 01869 loss_train: 5.5945 loss_rec: 5.5945 acc_train: 0.4937 loss_val: 5.6965 acc_val: 0.4983 time: 0.0258s\n",
      "Epoch: 01870 loss_train: 33.7997 loss_rec: 33.7997 acc_train: 0.5504 loss_val: 33.3415 acc_val: 0.5546 time: 0.0273s\n",
      "Epoch: 01871 loss_train: 47.1037 loss_rec: 47.1037 acc_train: 0.5454 loss_val: 46.4789 acc_val: 0.5483 time: 0.0263s\n",
      "Test set results: loss= 40.8230 accuracy= 0.5063\n",
      "Epoch: 01872 loss_train: 36.0423 loss_rec: 36.0423 acc_train: 0.5495 loss_val: 35.5513 acc_val: 0.5538 time: 0.0278s\n",
      "Epoch: 01873 loss_train: 3.8987 loss_rec: 3.8987 acc_train: 0.5827 loss_val: 3.9075 acc_val: 0.5850 time: 0.0273s\n",
      "Epoch: 01874 loss_train: 48.7303 loss_rec: 48.7303 acc_train: 0.4942 loss_val: 49.4861 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01875 loss_train: 70.4935 loss_rec: 70.4935 acc_train: 0.4952 loss_val: 71.5800 acc_val: 0.5000 time: 0.0258s\n",
      "Epoch: 01876 loss_train: 63.5630 loss_rec: 63.5630 acc_train: 0.4952 loss_val: 64.5445 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01877 loss_train: 30.9331 loss_rec: 30.9331 acc_train: 0.4937 loss_val: 31.4176 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 01878 loss_train: 21.9447 loss_rec: 21.9447 acc_train: 0.5544 loss_val: 21.6687 acc_val: 0.5592 time: 0.0268s\n",
      "Epoch: 01879 loss_train: 45.9276 loss_rec: 45.9276 acc_train: 0.5460 loss_val: 45.3167 acc_val: 0.5488 time: 0.0278s\n",
      "Epoch: 01880 loss_train: 44.6683 loss_rec: 44.6683 acc_train: 0.5460 loss_val: 44.0724 acc_val: 0.5492 time: 0.0278s\n",
      "Epoch: 01881 loss_train: 20.7413 loss_rec: 20.7413 acc_train: 0.5520 loss_val: 20.4861 acc_val: 0.5571 time: 0.0258s\n",
      "Test set results: loss= 21.4783 accuracy= 0.5497\n",
      "Epoch: 01882 loss_train: 23.9815 loss_rec: 23.9815 acc_train: 0.4937 loss_val: 24.3600 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 01883 loss_train: 40.3312 loss_rec: 40.3312 acc_train: 0.4944 loss_val: 40.9595 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01884 loss_train: 28.7760 loss_rec: 28.7760 acc_train: 0.4937 loss_val: 29.2277 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 01885 loss_train: 6.5775 loss_rec: 6.5775 acc_train: 0.5661 loss_val: 6.5488 acc_val: 0.5717 time: 0.0268s\n",
      "Epoch: 01886 loss_train: 15.5497 loss_rec: 15.5497 acc_train: 0.5560 loss_val: 15.3843 acc_val: 0.5629 time: 0.0268s\n",
      "Epoch: 01887 loss_train: 2.1790 loss_rec: 2.1790 acc_train: 0.6065 loss_val: 2.2085 acc_val: 0.6071 time: 0.0263s\n",
      "Epoch: 01888 loss_train: 29.6809 loss_rec: 29.6809 acc_train: 0.4937 loss_val: 30.1464 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01889 loss_train: 32.4653 loss_rec: 32.4653 acc_train: 0.4937 loss_val: 32.9736 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01890 loss_train: 8.7758 loss_rec: 8.7758 acc_train: 0.4929 loss_val: 8.9279 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 01891 loss_train: 34.8271 loss_rec: 34.8271 acc_train: 0.5499 loss_val: 34.3542 acc_val: 0.5546 time: 0.0258s\n",
      "Test set results: loss= 58.8959 accuracy= 0.4961\n",
      "Epoch: 01892 loss_train: 51.9506 loss_rec: 51.9506 acc_train: 0.5430 loss_val: 51.2679 acc_val: 0.5492 time: 0.0258s\n",
      "Epoch: 01893 loss_train: 44.5425 loss_rec: 44.5425 acc_train: 0.5460 loss_val: 43.9482 acc_val: 0.5492 time: 0.0288s\n",
      "Epoch: 01894 loss_train: 15.2677 loss_rec: 15.2677 acc_train: 0.5566 loss_val: 15.1066 acc_val: 0.5638 time: 0.0258s\n",
      "Epoch: 01895 loss_train: 34.9067 loss_rec: 34.9067 acc_train: 0.4937 loss_val: 35.4521 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01896 loss_train: 55.9501 loss_rec: 55.9501 acc_train: 0.4947 loss_val: 56.8162 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 01897 loss_train: 48.7089 loss_rec: 48.7089 acc_train: 0.4942 loss_val: 49.4648 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01898 loss_train: 16.0325 loss_rec: 16.0325 acc_train: 0.4922 loss_val: 16.2908 acc_val: 0.4971 time: 0.0253s\n",
      "Epoch: 01899 loss_train: 35.5171 loss_rec: 35.5171 acc_train: 0.5495 loss_val: 35.0339 acc_val: 0.5538 time: 0.0268s\n",
      "Epoch: 01900 loss_train: 59.4749 loss_rec: 59.4749 acc_train: 0.5410 loss_val: 58.7023 acc_val: 0.5479 time: 0.0278s\n",
      "Epoch: 01901 loss_train: 58.2580 loss_rec: 58.2580 acc_train: 0.5410 loss_val: 57.4999 acc_val: 0.5479 time: 0.0268s\n",
      "Test set results: loss= 39.0053 accuracy= 0.5078\n",
      "Epoch: 01902 loss_train: 34.4393 loss_rec: 34.4393 acc_train: 0.5504 loss_val: 33.9720 acc_val: 0.5546 time: 0.0263s\n",
      "Epoch: 01903 loss_train: 8.8948 loss_rec: 8.8948 acc_train: 0.4927 loss_val: 9.0487 acc_val: 0.4975 time: 0.0268s\n",
      "Epoch: 01904 loss_train: 25.3277 loss_rec: 25.3277 acc_train: 0.4937 loss_val: 25.7270 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 01905 loss_train: 14.0904 loss_rec: 14.0904 acc_train: 0.4924 loss_val: 14.3214 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 01906 loss_train: 19.3732 loss_rec: 19.3732 acc_train: 0.5533 loss_val: 19.1416 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 01907 loss_train: 27.2380 loss_rec: 27.2380 acc_train: 0.5513 loss_val: 26.8733 acc_val: 0.5521 time: 0.0273s\n",
      "Epoch: 01908 loss_train: 12.2047 loss_rec: 12.2047 acc_train: 0.5550 loss_val: 12.0947 acc_val: 0.5583 time: 0.0258s\n",
      "Epoch: 01909 loss_train: 24.1744 loss_rec: 24.1744 acc_train: 0.4937 loss_val: 24.5562 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01910 loss_train: 32.7074 loss_rec: 32.7074 acc_train: 0.4937 loss_val: 33.2196 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 01911 loss_train: 14.4306 loss_rec: 14.4306 acc_train: 0.4923 loss_val: 14.6667 acc_val: 0.4975 time: 0.0273s\n",
      "Test set results: loss= 28.1406 accuracy= 0.5133\n",
      "Epoch: 01912 loss_train: 24.8541 loss_rec: 24.8541 acc_train: 0.5542 loss_val: 24.5290 acc_val: 0.5588 time: 0.0258s\n",
      "Epoch: 01913 loss_train: 38.0013 loss_rec: 38.0013 acc_train: 0.5479 loss_val: 37.4839 acc_val: 0.5521 time: 0.0283s\n",
      "Epoch: 01914 loss_train: 27.4987 loss_rec: 27.4987 acc_train: 0.5529 loss_val: 27.1303 acc_val: 0.5558 time: 0.0263s\n",
      "Epoch: 01915 loss_train: 3.3668 loss_rec: 3.3668 acc_train: 0.4940 loss_val: 3.4339 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01916 loss_train: 8.5033 loss_rec: 8.5033 acc_train: 0.4934 loss_val: 8.6517 acc_val: 0.4979 time: 0.0278s\n",
      "Epoch: 01917 loss_train: 10.8189 loss_rec: 10.8189 acc_train: 0.5563 loss_val: 10.7301 acc_val: 0.5621 time: 0.0278s\n",
      "Epoch: 01918 loss_train: 6.8389 loss_rec: 6.8389 acc_train: 0.5658 loss_val: 6.8066 acc_val: 0.5717 time: 0.0258s\n",
      "Epoch: 01919 loss_train: 18.5768 loss_rec: 18.5768 acc_train: 0.4921 loss_val: 18.8735 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 01920 loss_train: 16.8574 loss_rec: 16.8574 acc_train: 0.4922 loss_val: 17.1286 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 01921 loss_train: 8.9115 loss_rec: 8.9115 acc_train: 0.5601 loss_val: 8.8526 acc_val: 0.5629 time: 0.0273s\n",
      "Test set results: loss= 11.6524 accuracy= 0.5214\n",
      "Epoch: 01922 loss_train: 10.2964 loss_rec: 10.2964 acc_train: 0.5567 loss_val: 10.2165 acc_val: 0.5633 time: 0.0268s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01923 loss_train: 9.9428 loss_rec: 9.9428 acc_train: 0.4927 loss_val: 10.1131 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 01924 loss_train: 3.9634 loss_rec: 3.9634 acc_train: 0.4940 loss_val: 4.0398 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01925 loss_train: 24.0462 loss_rec: 24.0462 acc_train: 0.5542 loss_val: 23.7348 acc_val: 0.5588 time: 0.0258s\n",
      "Epoch: 01926 loss_train: 27.8038 loss_rec: 27.8038 acc_train: 0.5538 loss_val: 27.4302 acc_val: 0.5567 time: 0.0273s\n",
      "Epoch: 01927 loss_train: 9.2896 loss_rec: 9.2896 acc_train: 0.5585 loss_val: 9.2261 acc_val: 0.5625 time: 0.0273s\n",
      "Epoch: 01928 loss_train: 30.3916 loss_rec: 30.3916 acc_train: 0.4937 loss_val: 30.8686 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 01929 loss_train: 41.8024 loss_rec: 41.8024 acc_train: 0.4944 loss_val: 42.4536 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01930 loss_train: 26.1915 loss_rec: 26.1915 acc_train: 0.4937 loss_val: 26.6045 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01931 loss_train: 11.7092 loss_rec: 11.7092 acc_train: 0.5553 loss_val: 11.6070 acc_val: 0.5596 time: 0.0263s\n",
      "Test set results: loss= 26.1149 accuracy= 0.5132\n",
      "Epoch: 01932 loss_train: 23.0657 loss_rec: 23.0657 acc_train: 0.5540 loss_val: 22.7711 acc_val: 0.5579 time: 0.0273s\n",
      "Epoch: 01933 loss_train: 11.4508 loss_rec: 11.4508 acc_train: 0.5560 loss_val: 11.3524 acc_val: 0.5600 time: 0.0278s\n",
      "Epoch: 01934 loss_train: 21.4726 loss_rec: 21.4726 acc_train: 0.4938 loss_val: 21.8134 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01935 loss_train: 27.0351 loss_rec: 27.0351 acc_train: 0.4937 loss_val: 27.4611 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 01936 loss_train: 6.2814 loss_rec: 6.2814 acc_train: 0.4937 loss_val: 6.3948 acc_val: 0.4983 time: 0.0263s\n",
      "Epoch: 01937 loss_train: 34.2403 loss_rec: 34.2403 acc_train: 0.5504 loss_val: 33.7763 acc_val: 0.5546 time: 0.0273s\n",
      "Epoch: 01938 loss_train: 49.1270 loss_rec: 49.1270 acc_train: 0.5457 loss_val: 48.4779 acc_val: 0.5483 time: 0.0278s\n",
      "Epoch: 01939 loss_train: 40.0779 loss_rec: 40.0779 acc_train: 0.5479 loss_val: 39.5350 acc_val: 0.5529 time: 0.0278s\n",
      "Epoch: 01940 loss_train: 9.9315 loss_rec: 9.9315 acc_train: 0.5572 loss_val: 9.8581 acc_val: 0.5629 time: 0.0253s\n",
      "Epoch: 01941 loss_train: 40.9433 loss_rec: 40.9433 acc_train: 0.4937 loss_val: 41.5816 acc_val: 0.4988 time: 0.0278s\n",
      "Test set results: loss= 56.1369 accuracy= 0.5511\n",
      "Epoch: 01942 loss_train: 62.5688 loss_rec: 62.5688 acc_train: 0.4947 loss_val: 63.5364 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01943 loss_train: 56.2563 loss_rec: 56.2563 acc_train: 0.4942 loss_val: 57.1279 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 01944 loss_train: 24.8177 loss_rec: 24.8177 acc_train: 0.4937 loss_val: 25.2097 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01945 loss_train: 25.9414 loss_rec: 25.9414 acc_train: 0.5527 loss_val: 25.5982 acc_val: 0.5563 time: 0.0278s\n",
      "Epoch: 01946 loss_train: 49.0412 loss_rec: 49.0412 acc_train: 0.5457 loss_val: 48.3926 acc_val: 0.5483 time: 0.0283s\n",
      "Epoch: 01947 loss_train: 47.5267 loss_rec: 47.5267 acc_train: 0.5448 loss_val: 46.8964 acc_val: 0.5479 time: 0.0258s\n",
      "Epoch: 01948 loss_train: 23.9138 loss_rec: 23.9138 acc_train: 0.5542 loss_val: 23.6051 acc_val: 0.5583 time: 0.0273s\n",
      "Epoch: 01949 loss_train: 19.5771 loss_rec: 19.5771 acc_train: 0.4921 loss_val: 19.8894 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 01950 loss_train: 35.7924 loss_rec: 35.7924 acc_train: 0.4937 loss_val: 36.3523 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01951 loss_train: 24.7342 loss_rec: 24.7342 acc_train: 0.4937 loss_val: 25.1253 acc_val: 0.4988 time: 0.0278s\n",
      "Test set results: loss= 10.3402 accuracy= 0.5232\n",
      "Epoch: 01952 loss_train: 9.1399 loss_rec: 9.1399 acc_train: 0.5595 loss_val: 9.0784 acc_val: 0.5629 time: 0.0273s\n",
      "Epoch: 01953 loss_train: 17.3935 loss_rec: 17.3935 acc_train: 0.5537 loss_val: 17.1969 acc_val: 0.5592 time: 0.0263s\n",
      "Epoch: 01954 loss_train: 3.6145 loss_rec: 3.6145 acc_train: 0.5860 loss_val: 3.6281 acc_val: 0.5900 time: 0.0278s\n",
      "Epoch: 01955 loss_train: 30.1999 loss_rec: 30.1999 acc_train: 0.4937 loss_val: 30.6747 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01956 loss_train: 35.8186 loss_rec: 35.8186 acc_train: 0.4937 loss_val: 36.3792 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 01957 loss_train: 15.3145 loss_rec: 15.3145 acc_train: 0.4923 loss_val: 15.5647 acc_val: 0.4975 time: 0.0253s\n",
      "Epoch: 01958 loss_train: 25.5305 loss_rec: 25.5305 acc_train: 0.5535 loss_val: 25.1947 acc_val: 0.5575 time: 0.0278s\n",
      "Epoch: 01959 loss_train: 40.3800 loss_rec: 40.3800 acc_train: 0.5475 loss_val: 39.8334 acc_val: 0.5521 time: 0.0263s\n",
      "Epoch: 01960 loss_train: 31.6915 loss_rec: 31.6915 acc_train: 0.5521 loss_val: 31.2646 acc_val: 0.5542 time: 0.0273s\n",
      "Epoch: 01961 loss_train: 2.6514 loss_rec: 2.6514 acc_train: 0.5979 loss_val: 2.6778 acc_val: 0.5971 time: 0.0278s\n",
      "Test set results: loss= 40.2606 accuracy= 0.5503\n",
      "Epoch: 01962 loss_train: 44.8936 loss_rec: 44.8936 acc_train: 0.4944 loss_val: 45.5932 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01963 loss_train: 62.7065 loss_rec: 62.7065 acc_train: 0.4947 loss_val: 63.6771 acc_val: 0.5000 time: 0.0258s\n",
      "Epoch: 01964 loss_train: 53.2146 loss_rec: 53.2146 acc_train: 0.4942 loss_val: 54.0409 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01965 loss_train: 19.1337 loss_rec: 19.1337 acc_train: 0.4921 loss_val: 19.4402 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 01966 loss_train: 33.1606 loss_rec: 33.1606 acc_train: 0.5523 loss_val: 32.7123 acc_val: 0.5542 time: 0.0263s\n",
      "Epoch: 01967 loss_train: 58.1619 loss_rec: 58.1619 acc_train: 0.5409 loss_val: 57.4044 acc_val: 0.5479 time: 0.0283s\n",
      "Epoch: 01968 loss_train: 58.4841 loss_rec: 58.4841 acc_train: 0.5410 loss_val: 57.7227 acc_val: 0.5479 time: 0.0278s\n",
      "Epoch: 01969 loss_train: 36.6265 loss_rec: 36.6265 acc_train: 0.5489 loss_val: 36.1280 acc_val: 0.5533 time: 0.0253s\n",
      "Epoch: 01970 loss_train: 4.0518 loss_rec: 4.0518 acc_train: 0.4940 loss_val: 4.1301 acc_val: 0.4988 time: 0.0288s\n",
      "Epoch: 01971 loss_train: 19.0397 loss_rec: 19.0397 acc_train: 0.4921 loss_val: 19.3447 acc_val: 0.4971 time: 0.0268s\n",
      "Test set results: loss= 6.3895 accuracy= 0.5492\n",
      "Epoch: 01972 loss_train: 7.1841 loss_rec: 7.1841 acc_train: 0.4937 loss_val: 7.3119 acc_val: 0.4983 time: 0.0288s\n",
      "Epoch: 01973 loss_train: 25.5867 loss_rec: 25.5867 acc_train: 0.5535 loss_val: 25.2498 acc_val: 0.5575 time: 0.0273s\n",
      "Epoch: 01974 loss_train: 33.8673 loss_rec: 33.8673 acc_train: 0.5515 loss_val: 33.4088 acc_val: 0.5558 time: 0.0263s\n",
      "Epoch: 01975 loss_train: 19.5323 loss_rec: 19.5323 acc_train: 0.5528 loss_val: 19.2987 acc_val: 0.5583 time: 0.0263s\n",
      "Epoch: 01976 loss_train: 15.0606 loss_rec: 15.0606 acc_train: 0.4923 loss_val: 15.3074 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 01977 loss_train: 23.2802 loss_rec: 23.2802 acc_train: 0.4937 loss_val: 23.6491 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01978 loss_train: 5.3828 loss_rec: 5.3828 acc_train: 0.4938 loss_val: 5.4816 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 01979 loss_train: 32.2146 loss_rec: 32.2146 acc_train: 0.5522 loss_val: 31.7801 acc_val: 0.5542 time: 0.0258s\n",
      "Epoch: 01980 loss_train: 44.9328 loss_rec: 44.9328 acc_train: 0.5454 loss_val: 44.3333 acc_val: 0.5488 time: 0.0273s\n",
      "Epoch: 01981 loss_train: 34.3966 loss_rec: 34.3966 acc_train: 0.5504 loss_val: 33.9305 acc_val: 0.5546 time: 0.0268s\n",
      "Test set results: loss= 4.1830 accuracy= 0.5550\n",
      "Epoch: 01982 loss_train: 3.7333 loss_rec: 3.7333 acc_train: 0.5832 loss_val: 3.7454 acc_val: 0.5846 time: 0.0268s\n",
      "Epoch: 01983 loss_train: 46.3500 loss_rec: 46.3500 acc_train: 0.4944 loss_val: 47.0715 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 01984 loss_train: 66.9324 loss_rec: 66.9324 acc_train: 0.4952 loss_val: 67.9670 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01985 loss_train: 60.1531 loss_rec: 60.1531 acc_train: 0.4947 loss_val: 61.0848 acc_val: 0.5000 time: 0.0283s\n",
      "Epoch: 01986 loss_train: 28.7752 loss_rec: 28.7752 acc_train: 0.4937 loss_val: 29.2285 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 01987 loss_train: 21.7698 loss_rec: 21.7698 acc_train: 0.5536 loss_val: 21.4979 acc_val: 0.5583 time: 0.0283s\n",
      "Epoch: 01988 loss_train: 44.8228 loss_rec: 44.8228 acc_train: 0.5454 loss_val: 44.2245 acc_val: 0.5483 time: 0.0268s\n",
      "Epoch: 01989 loss_train: 43.7225 loss_rec: 43.7225 acc_train: 0.5453 loss_val: 43.1370 acc_val: 0.5483 time: 0.0268s\n",
      "Epoch: 01990 loss_train: 20.9769 loss_rec: 20.9769 acc_train: 0.5520 loss_val: 20.7193 acc_val: 0.5571 time: 0.0278s\n",
      "Epoch: 01991 loss_train: 21.5118 loss_rec: 21.5118 acc_train: 0.4938 loss_val: 21.8540 acc_val: 0.4988 time: 0.0268s\n",
      "Test set results: loss= 33.2321 accuracy= 0.5497\n",
      "Epoch: 01992 loss_train: 37.0690 loss_rec: 37.0690 acc_train: 0.4937 loss_val: 37.6492 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01993 loss_train: 25.9292 loss_rec: 25.9292 acc_train: 0.4937 loss_val: 26.3392 acc_val: 0.4988 time: 0.0268s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01994 loss_train: 7.7235 loss_rec: 7.7235 acc_train: 0.5616 loss_val: 7.6798 acc_val: 0.5646 time: 0.0278s\n",
      "Epoch: 01995 loss_train: 16.1944 loss_rec: 16.1944 acc_train: 0.5547 loss_val: 16.0192 acc_val: 0.5617 time: 0.0278s\n",
      "Epoch: 01996 loss_train: 3.1469 loss_rec: 3.1469 acc_train: 0.5899 loss_val: 3.1683 acc_val: 0.5938 time: 0.0258s\n",
      "Epoch: 01997 loss_train: 29.1349 loss_rec: 29.1349 acc_train: 0.4937 loss_val: 29.5940 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 01998 loss_train: 33.7505 loss_rec: 33.7505 acc_train: 0.4937 loss_val: 34.2803 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 01999 loss_train: 12.8164 loss_rec: 12.8164 acc_train: 0.4927 loss_val: 13.0318 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 02000 loss_train: 27.6870 loss_rec: 27.6870 acc_train: 0.5528 loss_val: 27.3161 acc_val: 0.5558 time: 0.0283s\n",
      "Epoch: 02001 loss_train: 42.8245 loss_rec: 42.8245 acc_train: 0.5450 loss_val: 42.2495 acc_val: 0.5492 time: 0.0278s\n",
      "Test set results: loss= 39.3534 accuracy= 0.5078\n",
      "Epoch: 02002 loss_train: 34.7463 loss_rec: 34.7463 acc_train: 0.5504 loss_val: 34.2750 acc_val: 0.5546 time: 0.0253s\n",
      "Epoch: 02003 loss_train: 6.2825 loss_rec: 6.2825 acc_train: 0.5669 loss_val: 6.2597 acc_val: 0.5725 time: 0.0263s\n",
      "Epoch: 02004 loss_train: 42.2801 loss_rec: 42.2801 acc_train: 0.4937 loss_val: 42.9402 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 02005 loss_train: 62.1344 loss_rec: 62.1344 acc_train: 0.4947 loss_val: 63.0970 acc_val: 0.5000 time: 0.0278s\n",
      "Epoch: 02006 loss_train: 54.9627 loss_rec: 54.9627 acc_train: 0.4942 loss_val: 55.8162 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 02007 loss_train: 23.4639 loss_rec: 23.4639 acc_train: 0.4938 loss_val: 23.8365 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 02008 loss_train: 26.5288 loss_rec: 26.5288 acc_train: 0.5523 loss_val: 26.1767 acc_val: 0.5558 time: 0.0268s\n",
      "Epoch: 02009 loss_train: 49.6749 loss_rec: 49.6749 acc_train: 0.5457 loss_val: 49.0186 acc_val: 0.5483 time: 0.0273s\n",
      "Epoch: 02010 loss_train: 48.8242 loss_rec: 48.8242 acc_train: 0.5463 loss_val: 48.1780 acc_val: 0.5488 time: 0.0278s\n",
      "Epoch: 02011 loss_train: 26.4161 loss_rec: 26.4161 acc_train: 0.5521 loss_val: 26.0659 acc_val: 0.5558 time: 0.0278s\n",
      "Test set results: loss= 13.4245 accuracy= 0.5483\n",
      "Epoch: 02012 loss_train: 15.0162 loss_rec: 15.0162 acc_train: 0.4924 loss_val: 15.2639 acc_val: 0.4975 time: 0.0283s\n",
      "Epoch: 02013 loss_train: 30.3181 loss_rec: 30.3181 acc_train: 0.4937 loss_val: 30.7956 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 02014 loss_train: 19.1683 loss_rec: 19.1683 acc_train: 0.4922 loss_val: 19.4764 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02015 loss_train: 13.5753 loss_rec: 13.5753 acc_train: 0.5542 loss_val: 13.4459 acc_val: 0.5579 time: 0.0263s\n",
      "Epoch: 02016 loss_train: 21.5151 loss_rec: 21.5151 acc_train: 0.5536 loss_val: 21.2485 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 02017 loss_train: 7.7024 loss_rec: 7.7024 acc_train: 0.5615 loss_val: 7.6591 acc_val: 0.5646 time: 0.0278s\n",
      "Epoch: 02018 loss_train: 26.4463 loss_rec: 26.4463 acc_train: 0.4937 loss_val: 26.8649 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 02019 loss_train: 33.6974 loss_rec: 33.6974 acc_train: 0.4937 loss_val: 34.2267 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 02020 loss_train: 15.3801 loss_rec: 15.3801 acc_train: 0.4924 loss_val: 15.6335 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 02021 loss_train: 22.9400 loss_rec: 22.9400 acc_train: 0.5545 loss_val: 22.6487 acc_val: 0.5592 time: 0.0263s\n",
      "Test set results: loss= 40.9631 accuracy= 0.5055\n",
      "Epoch: 02022 loss_train: 36.1658 loss_rec: 36.1658 acc_train: 0.5489 loss_val: 35.6740 acc_val: 0.5533 time: 0.0273s\n",
      "Epoch: 02023 loss_train: 26.7062 loss_rec: 26.7062 acc_train: 0.5523 loss_val: 26.3509 acc_val: 0.5558 time: 0.0258s\n",
      "Epoch: 02024 loss_train: 2.1385 loss_rec: 2.1385 acc_train: 0.4941 loss_val: 2.1850 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 02025 loss_train: 7.9670 loss_rec: 7.9670 acc_train: 0.4936 loss_val: 8.1075 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 02026 loss_train: 9.6627 loss_rec: 9.6627 acc_train: 0.5579 loss_val: 9.5938 acc_val: 0.5629 time: 0.0273s\n",
      "Epoch: 02027 loss_train: 5.3787 loss_rec: 5.3787 acc_train: 0.5711 loss_val: 5.3696 acc_val: 0.5754 time: 0.0273s\n",
      "Epoch: 02028 loss_train: 19.1791 loss_rec: 19.1791 acc_train: 0.4922 loss_val: 19.4875 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02029 loss_train: 17.6139 loss_rec: 17.6139 acc_train: 0.4922 loss_val: 17.8991 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 02030 loss_train: 7.0948 loss_rec: 7.0948 acc_train: 0.5646 loss_val: 7.0597 acc_val: 0.5700 time: 0.0263s\n",
      "Epoch: 02031 loss_train: 8.5813 loss_rec: 8.5813 acc_train: 0.5603 loss_val: 8.5270 acc_val: 0.5629 time: 0.0278s\n",
      "Test set results: loss= 9.2419 accuracy= 0.5488\n",
      "Epoch: 02032 loss_train: 10.3596 loss_rec: 10.3596 acc_train: 0.4928 loss_val: 10.5378 acc_val: 0.4975 time: 0.0263s\n",
      "Epoch: 02033 loss_train: 4.2300 loss_rec: 4.2300 acc_train: 0.4940 loss_val: 4.3112 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 02034 loss_train: 22.9298 loss_rec: 22.9298 acc_train: 0.5540 loss_val: 22.6386 acc_val: 0.5579 time: 0.0268s\n",
      "Epoch: 02035 loss_train: 26.8951 loss_rec: 26.8951 acc_train: 0.5516 loss_val: 26.5371 acc_val: 0.5538 time: 0.0258s\n",
      "Epoch: 02036 loss_train: 9.5120 loss_rec: 9.5120 acc_train: 0.5578 loss_val: 9.4460 acc_val: 0.5625 time: 0.0283s\n",
      "Epoch: 02037 loss_train: 28.0285 loss_rec: 28.0285 acc_train: 0.4937 loss_val: 28.4713 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 02038 loss_train: 38.6413 loss_rec: 38.6413 acc_train: 0.4937 loss_val: 39.2463 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 02039 loss_train: 23.4517 loss_rec: 23.4517 acc_train: 0.4938 loss_val: 23.8245 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 02040 loss_train: 12.8099 loss_rec: 12.8099 acc_train: 0.5543 loss_val: 12.6920 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 02041 loss_train: 23.8223 loss_rec: 23.8223 acc_train: 0.5542 loss_val: 23.5158 acc_val: 0.5579 time: 0.0273s\n",
      "Test set results: loss= 14.5087 accuracy= 0.5182\n",
      "Epoch: 02042 loss_train: 12.8178 loss_rec: 12.8178 acc_train: 0.5543 loss_val: 12.6997 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 02043 loss_train: 18.3966 loss_rec: 18.3966 acc_train: 0.4922 loss_val: 18.6940 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02044 loss_train: 23.6757 loss_rec: 23.6757 acc_train: 0.4938 loss_val: 24.0521 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 02045 loss_train: 3.7997 loss_rec: 3.7997 acc_train: 0.4940 loss_val: 3.8745 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 02046 loss_train: 34.6733 loss_rec: 34.6733 acc_train: 0.5498 loss_val: 34.2033 acc_val: 0.5542 time: 0.0283s\n",
      "Epoch: 02047 loss_train: 48.8239 loss_rec: 48.8239 acc_train: 0.5463 loss_val: 48.1773 acc_val: 0.5488 time: 0.0278s\n",
      "Epoch: 02048 loss_train: 40.1193 loss_rec: 40.1193 acc_train: 0.5473 loss_val: 39.5751 acc_val: 0.5525 time: 0.0273s\n",
      "Epoch: 02049 loss_train: 11.2024 loss_rec: 11.2024 acc_train: 0.5568 loss_val: 11.1085 acc_val: 0.5625 time: 0.0273s\n",
      "Epoch: 02050 loss_train: 37.4377 loss_rec: 37.4377 acc_train: 0.4937 loss_val: 38.0245 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 02051 loss_train: 58.3004 loss_rec: 58.3004 acc_train: 0.4942 loss_val: 59.2056 acc_val: 0.4988 time: 0.0268s\n",
      "Test set results: loss= 47.0162 accuracy= 0.5503\n",
      "Epoch: 02052 loss_train: 52.4154 loss_rec: 52.4154 acc_train: 0.4944 loss_val: 53.2311 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 02053 loss_train: 22.5221 loss_rec: 22.5221 acc_train: 0.4938 loss_val: 22.8807 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 02054 loss_train: 25.7558 loss_rec: 25.7558 acc_train: 0.5535 loss_val: 25.4167 acc_val: 0.5575 time: 0.0263s\n",
      "Epoch: 02055 loss_train: 47.7254 loss_rec: 47.7254 acc_train: 0.5447 loss_val: 47.0918 acc_val: 0.5479 time: 0.0258s\n",
      "Epoch: 02056 loss_train: 46.1774 loss_rec: 46.1774 acc_train: 0.5455 loss_val: 45.5625 acc_val: 0.5488 time: 0.0278s\n",
      "Epoch: 02057 loss_train: 23.5051 loss_rec: 23.5051 acc_train: 0.5540 loss_val: 23.2042 acc_val: 0.5579 time: 0.0283s\n",
      "Epoch: 02058 loss_train: 18.0632 loss_rec: 18.0632 acc_train: 0.4922 loss_val: 18.3558 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02059 loss_train: 33.6448 loss_rec: 33.6448 acc_train: 0.4937 loss_val: 34.1738 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 02060 loss_train: 23.1472 loss_rec: 23.1472 acc_train: 0.4938 loss_val: 23.5155 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 02061 loss_train: 9.1045 loss_rec: 9.1045 acc_train: 0.5601 loss_val: 9.0438 acc_val: 0.5629 time: 0.0263s\n",
      "Test set results: loss= 19.2055 accuracy= 0.5145\n",
      "Epoch: 02062 loss_train: 16.9656 loss_rec: 16.9656 acc_train: 0.5537 loss_val: 16.7774 acc_val: 0.5592 time: 0.0263s\n",
      "Epoch: 02063 loss_train: 3.7525 loss_rec: 3.7525 acc_train: 0.5832 loss_val: 3.7645 acc_val: 0.5846 time: 0.0278s\n",
      "Epoch: 02064 loss_train: 28.6514 loss_rec: 28.6514 acc_train: 0.4937 loss_val: 29.1043 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 02065 loss_train: 34.1490 loss_rec: 34.1490 acc_train: 0.4937 loss_val: 34.6860 acc_val: 0.4988 time: 0.0273s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02066 loss_train: 14.6145 loss_rec: 14.6145 acc_train: 0.4924 loss_val: 14.8583 acc_val: 0.4975 time: 0.0263s\n",
      "Epoch: 02067 loss_train: 24.2857 loss_rec: 24.2857 acc_train: 0.5542 loss_val: 23.9719 acc_val: 0.5588 time: 0.0258s\n",
      "Epoch: 02068 loss_train: 38.4169 loss_rec: 38.4169 acc_train: 0.5473 loss_val: 37.8940 acc_val: 0.5517 time: 0.0273s\n",
      "Epoch: 02069 loss_train: 30.0503 loss_rec: 30.0503 acc_train: 0.5533 loss_val: 29.6472 acc_val: 0.5542 time: 0.0279s\n",
      "Epoch: 02070 loss_train: 2.3930 loss_rec: 2.3930 acc_train: 0.6030 loss_val: 2.4222 acc_val: 0.6046 time: 0.0268s\n",
      "Epoch: 02071 loss_train: 42.5362 loss_rec: 42.5362 acc_train: 0.4937 loss_val: 43.2015 acc_val: 0.4988 time: 0.0278s\n",
      "Test set results: loss= 52.9240 accuracy= 0.5503\n",
      "Epoch: 02072 loss_train: 58.9926 loss_rec: 58.9926 acc_train: 0.4942 loss_val: 59.9091 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 02073 loss_train: 49.3782 loss_rec: 49.3782 acc_train: 0.4937 loss_val: 50.1482 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 02074 loss_train: 16.3412 loss_rec: 16.3412 acc_train: 0.4923 loss_val: 16.6101 acc_val: 0.4975 time: 0.0268s\n",
      "Epoch: 02075 loss_train: 33.8963 loss_rec: 33.8963 acc_train: 0.5509 loss_val: 33.4380 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 02076 loss_train: 58.1441 loss_rec: 58.1441 acc_train: 0.5409 loss_val: 57.3862 acc_val: 0.5479 time: 0.0268s\n",
      "Epoch: 02077 loss_train: 58.7691 loss_rec: 58.7691 acc_train: 0.5410 loss_val: 58.0038 acc_val: 0.5479 time: 0.0268s\n",
      "Epoch: 02078 loss_train: 38.1121 loss_rec: 38.1121 acc_train: 0.5473 loss_val: 37.5929 acc_val: 0.5517 time: 0.0263s\n",
      "Epoch: 02079 loss_train: 0.9858 loss_rec: 0.9858 acc_train: 0.6099 loss_val: 1.0072 acc_val: 0.6021 time: 0.0283s\n",
      "Epoch: 02080 loss_train: 35.3640 loss_rec: 35.3640 acc_train: 0.4937 loss_val: 35.9198 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 02081 loss_train: 42.6905 loss_rec: 42.6905 acc_train: 0.4937 loss_val: 43.3581 acc_val: 0.4988 time: 0.0268s\n",
      "Test set results: loss= 22.3391 accuracy= 0.5498\n",
      "Epoch: 02082 loss_train: 24.9417 loss_rec: 24.9417 acc_train: 0.4938 loss_val: 25.3381 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 02083 loss_train: 13.1347 loss_rec: 13.1347 acc_train: 0.5543 loss_val: 13.0121 acc_val: 0.5583 time: 0.0283s\n",
      "Epoch: 02084 loss_train: 26.0473 loss_rec: 26.0473 acc_train: 0.5527 loss_val: 25.7036 acc_val: 0.5563 time: 0.0258s\n",
      "Epoch: 02085 loss_train: 17.0790 loss_rec: 17.0790 acc_train: 0.5537 loss_val: 16.8890 acc_val: 0.5592 time: 0.0288s\n",
      "Epoch: 02086 loss_train: 11.5224 loss_rec: 11.5224 acc_train: 0.4927 loss_val: 11.7191 acc_val: 0.4975 time: 0.0268s\n",
      "Epoch: 02087 loss_train: 15.2950 loss_rec: 15.2950 acc_train: 0.4924 loss_val: 15.5489 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 02088 loss_train: 4.4810 loss_rec: 4.4810 acc_train: 0.5771 loss_val: 4.4841 acc_val: 0.5804 time: 0.0268s\n",
      "Epoch: 02089 loss_train: 3.1088 loss_rec: 3.1088 acc_train: 0.5898 loss_val: 3.1311 acc_val: 0.5933 time: 0.0268s\n",
      "Epoch: 02090 loss_train: 16.6700 loss_rec: 16.6700 acc_train: 0.4923 loss_val: 16.9440 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 02091 loss_train: 10.9841 loss_rec: 10.9841 acc_train: 0.4928 loss_val: 11.1724 acc_val: 0.4975 time: 0.0407s\n",
      "Test set results: loss= 18.0247 accuracy= 0.5166\n",
      "Epoch: 02092 loss_train: 15.9228 loss_rec: 15.9228 acc_train: 0.5549 loss_val: 15.7528 acc_val: 0.5625 time: 0.0273s\n",
      "Epoch: 02093 loss_train: 19.7016 loss_rec: 19.7016 acc_train: 0.5528 loss_val: 19.4665 acc_val: 0.5583 time: 0.0338s\n",
      "Epoch: 02094 loss_train: 3.0891 loss_rec: 3.0891 acc_train: 0.5900 loss_val: 3.1115 acc_val: 0.5933 time: 0.0397s\n",
      "Epoch: 02095 loss_train: 31.8252 loss_rec: 31.8252 acc_train: 0.4937 loss_val: 32.3274 acc_val: 0.4988 time: 0.0348s\n",
      "Epoch: 02096 loss_train: 39.7527 loss_rec: 39.7527 acc_train: 0.4937 loss_val: 40.3760 acc_val: 0.4988 time: 0.0318s\n",
      "Epoch: 02097 loss_train: 22.6961 loss_rec: 22.6961 acc_train: 0.4938 loss_val: 23.0588 acc_val: 0.4992 time: 0.0368s\n",
      "Epoch: 02098 loss_train: 14.5329 loss_rec: 14.5329 acc_train: 0.5566 loss_val: 14.3876 acc_val: 0.5638 time: 0.0333s\n",
      "Epoch: 02099 loss_train: 26.8228 loss_rec: 26.8228 acc_train: 0.5517 loss_val: 26.4665 acc_val: 0.5550 time: 0.0363s\n",
      "Epoch: 02100 loss_train: 17.3636 loss_rec: 17.3636 acc_train: 0.5537 loss_val: 17.1686 acc_val: 0.5592 time: 0.0338s\n",
      "Epoch: 02101 loss_train: 11.5765 loss_rec: 11.5765 acc_train: 0.4927 loss_val: 11.7743 acc_val: 0.4975 time: 0.0338s\n",
      "Test set results: loss= 14.1032 accuracy= 0.5483\n",
      "Epoch: 02102 loss_train: 15.7721 loss_rec: 15.7721 acc_train: 0.4924 loss_val: 16.0338 acc_val: 0.4975 time: 0.0318s\n",
      "Epoch: 02103 loss_train: 3.6652 loss_rec: 3.6652 acc_train: 0.5860 loss_val: 3.6789 acc_val: 0.5904 time: 0.0358s\n",
      "Epoch: 02104 loss_train: 2.5894 loss_rec: 2.5894 acc_train: 0.5988 loss_val: 2.6170 acc_val: 0.5992 time: 0.0343s\n",
      "Epoch: 02105 loss_train: 16.0650 loss_rec: 16.0650 acc_train: 0.4924 loss_val: 16.3309 acc_val: 0.4975 time: 0.0343s\n",
      "Epoch: 02106 loss_train: 9.3149 loss_rec: 9.3149 acc_train: 0.4936 loss_val: 9.4770 acc_val: 0.4979 time: 0.0258s\n",
      "Epoch: 02107 loss_train: 18.2180 loss_rec: 18.2180 acc_train: 0.5537 loss_val: 18.0085 acc_val: 0.5592 time: 0.0278s\n",
      "Epoch: 02108 loss_train: 22.7315 loss_rec: 22.7315 acc_train: 0.5545 loss_val: 22.4442 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 02109 loss_train: 6.5456 loss_rec: 6.5456 acc_train: 0.5667 loss_val: 6.5190 acc_val: 0.5721 time: 0.0283s\n",
      "Epoch: 02110 loss_train: 29.2304 loss_rec: 29.2304 acc_train: 0.4937 loss_val: 29.6929 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 02111 loss_train: 38.5738 loss_rec: 38.5738 acc_train: 0.4937 loss_val: 39.1792 acc_val: 0.4988 time: 0.0253s\n",
      "Test set results: loss= 20.5005 accuracy= 0.5498\n",
      "Epoch: 02112 loss_train: 22.8948 loss_rec: 22.8948 acc_train: 0.4938 loss_val: 23.2604 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 02113 loss_train: 13.1682 loss_rec: 13.1682 acc_train: 0.5543 loss_val: 13.0453 acc_val: 0.5583 time: 0.0303s\n",
      "Epoch: 02114 loss_train: 24.5498 loss_rec: 24.5498 acc_train: 0.5542 loss_val: 24.2315 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 02115 loss_train: 14.3970 loss_rec: 14.3970 acc_train: 0.5566 loss_val: 14.2540 acc_val: 0.5638 time: 0.0263s\n",
      "Epoch: 02116 loss_train: 15.3711 loss_rec: 15.3711 acc_train: 0.4925 loss_val: 15.6272 acc_val: 0.4975 time: 0.0288s\n",
      "Epoch: 02117 loss_train: 20.1605 loss_rec: 20.1605 acc_train: 0.4922 loss_val: 20.4859 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 02118 loss_train: 1.0333 loss_rec: 1.0333 acc_train: 0.5820 loss_val: 1.0544 acc_val: 0.5729 time: 0.0273s\n",
      "Epoch: 02119 loss_train: 21.2936 loss_rec: 21.2936 acc_train: 0.5535 loss_val: 21.0308 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 02120 loss_train: 21.2971 loss_rec: 21.2971 acc_train: 0.5535 loss_val: 21.0347 acc_val: 0.5583 time: 0.0263s\n",
      "Epoch: 02121 loss_train: 1.7639 loss_rec: 1.7639 acc_train: 0.6171 loss_val: 1.7969 acc_val: 0.6158 time: 0.0268s\n",
      "Test set results: loss= 29.7042 accuracy= 0.5497\n",
      "Epoch: 02122 loss_train: 33.1422 loss_rec: 33.1422 acc_train: 0.4937 loss_val: 33.6648 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 02123 loss_train: 40.7613 loss_rec: 40.7613 acc_train: 0.4937 loss_val: 41.4002 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 02124 loss_train: 23.5451 loss_rec: 23.5451 acc_train: 0.4938 loss_val: 23.9207 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 02125 loss_train: 13.7532 loss_rec: 13.7532 acc_train: 0.5535 loss_val: 13.6210 acc_val: 0.5575 time: 0.0268s\n",
      "Epoch: 02126 loss_train: 26.2427 loss_rec: 26.2427 acc_train: 0.5515 loss_val: 25.8963 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 02127 loss_train: 17.0636 loss_rec: 17.0636 acc_train: 0.5537 loss_val: 16.8738 acc_val: 0.5592 time: 0.0283s\n",
      "Epoch: 02128 loss_train: 11.4823 loss_rec: 11.4823 acc_train: 0.4928 loss_val: 11.6787 acc_val: 0.4975 time: 0.0258s\n",
      "Epoch: 02129 loss_train: 15.4494 loss_rec: 15.4494 acc_train: 0.4925 loss_val: 15.7068 acc_val: 0.4975 time: 0.0263s\n",
      "Epoch: 02130 loss_train: 3.9665 loss_rec: 3.9665 acc_train: 0.5833 loss_val: 3.9758 acc_val: 0.5854 time: 0.0273s\n",
      "Epoch: 02131 loss_train: 2.8045 loss_rec: 2.8045 acc_train: 0.5955 loss_val: 2.8303 acc_val: 0.5950 time: 0.0278s\n",
      "Test set results: loss= 14.4932 accuracy= 0.5483\n",
      "Epoch: 02132 loss_train: 16.2065 loss_rec: 16.2065 acc_train: 0.4924 loss_val: 16.4753 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 02133 loss_train: 9.9463 loss_rec: 9.9463 acc_train: 0.4936 loss_val: 10.1186 acc_val: 0.4979 time: 0.0268s\n",
      "Epoch: 02134 loss_train: 17.0864 loss_rec: 17.0864 acc_train: 0.5537 loss_val: 16.8963 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 02135 loss_train: 21.2333 loss_rec: 21.2333 acc_train: 0.5520 loss_val: 20.9717 acc_val: 0.5571 time: 0.0273s\n",
      "Epoch: 02136 loss_train: 4.9522 loss_rec: 4.9522 acc_train: 0.5730 loss_val: 4.9495 acc_val: 0.5775 time: 0.0278s\n",
      "Epoch: 02137 loss_train: 30.5944 loss_rec: 30.5944 acc_train: 0.4937 loss_val: 31.0783 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 02138 loss_train: 39.6364 loss_rec: 39.6364 acc_train: 0.4937 loss_val: 40.2586 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 02139 loss_train: 23.8242 loss_rec: 23.8242 acc_train: 0.4938 loss_val: 24.2044 acc_val: 0.4988 time: 0.0263s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02140 loss_train: 12.2926 loss_rec: 12.2926 acc_train: 0.5553 loss_val: 12.1826 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 02141 loss_train: 23.8220 loss_rec: 23.8220 acc_train: 0.5536 loss_val: 23.5162 acc_val: 0.5575 time: 0.0278s\n",
      "Test set results: loss= 15.7620 accuracy= 0.5169\n",
      "Epoch: 02142 loss_train: 13.9242 loss_rec: 13.9242 acc_train: 0.5527 loss_val: 13.7892 acc_val: 0.5575 time: 0.0283s\n",
      "Epoch: 02143 loss_train: 15.4762 loss_rec: 15.4762 acc_train: 0.4925 loss_val: 15.7343 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 02144 loss_train: 20.0406 loss_rec: 20.0406 acc_train: 0.4922 loss_val: 20.3647 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 02145 loss_train: 0.9914 loss_rec: 0.9914 acc_train: 0.6133 loss_val: 1.0126 acc_val: 0.6067 time: 0.0263s\n",
      "Epoch: 02146 loss_train: 18.6042 loss_rec: 18.6042 acc_train: 0.5533 loss_val: 18.3882 acc_val: 0.5579 time: 0.0278s\n",
      "Epoch: 02147 loss_train: 16.1361 loss_rec: 16.1361 acc_train: 0.5550 loss_val: 15.9625 acc_val: 0.5625 time: 0.0278s\n",
      "Epoch: 02148 loss_train: 5.8882 loss_rec: 5.8882 acc_train: 0.4939 loss_val: 5.9955 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 02149 loss_train: 4.0558 loss_rec: 4.0558 acc_train: 0.4940 loss_val: 4.1351 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 02150 loss_train: 18.6770 loss_rec: 18.6770 acc_train: 0.5533 loss_val: 18.4594 acc_val: 0.5579 time: 0.0258s\n",
      "Epoch: 02151 loss_train: 19.4380 loss_rec: 19.4380 acc_train: 0.5528 loss_val: 19.2075 acc_val: 0.5579 time: 0.0273s\n",
      "Test set results: loss= 1.2051 accuracy= 0.6192\n",
      "Epoch: 02152 loss_train: 1.1652 loss_rec: 1.1652 acc_train: 0.6291 loss_val: 1.1948 acc_val: 0.6158 time: 0.0278s\n",
      "Epoch: 02153 loss_train: 27.6978 loss_rec: 27.6978 acc_train: 0.4937 loss_val: 28.1373 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 02154 loss_train: 29.4988 loss_rec: 29.4988 acc_train: 0.4937 loss_val: 29.9659 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 02155 loss_train: 7.2961 loss_rec: 7.2961 acc_train: 0.4938 loss_val: 7.4262 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02156 loss_train: 32.7056 loss_rec: 32.7056 acc_train: 0.5511 loss_val: 32.2643 acc_val: 0.5533 time: 0.0258s\n",
      "Epoch: 02157 loss_train: 48.7423 loss_rec: 48.7423 acc_train: 0.5452 loss_val: 48.0966 acc_val: 0.5488 time: 0.0278s\n",
      "Epoch: 02158 loss_train: 42.4106 loss_rec: 42.4106 acc_train: 0.5451 loss_val: 41.8395 acc_val: 0.5496 time: 0.0278s\n",
      "Epoch: 02159 loss_train: 16.2134 loss_rec: 16.2134 acc_train: 0.5550 loss_val: 16.0389 acc_val: 0.5625 time: 0.0258s\n",
      "Epoch: 02160 loss_train: 28.8125 loss_rec: 28.8125 acc_train: 0.4937 loss_val: 29.2692 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 02161 loss_train: 47.6208 loss_rec: 47.6208 acc_train: 0.4937 loss_val: 48.3650 acc_val: 0.4988 time: 0.0278s\n",
      "Test set results: loss= 36.4965 accuracy= 0.5497\n",
      "Epoch: 02162 loss_train: 40.7044 loss_rec: 40.7044 acc_train: 0.4937 loss_val: 41.3431 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 02163 loss_train: 10.6586 loss_rec: 10.6586 acc_train: 0.4932 loss_val: 10.8421 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 02164 loss_train: 36.0719 loss_rec: 36.0719 acc_train: 0.5483 loss_val: 35.5817 acc_val: 0.5529 time: 0.0278s\n",
      "Epoch: 02165 loss_train: 58.0381 loss_rec: 58.0381 acc_train: 0.5409 loss_val: 57.2812 acc_val: 0.5479 time: 0.0278s\n",
      "Epoch: 02166 loss_train: 57.0944 loss_rec: 57.0944 acc_train: 0.5408 loss_val: 56.3490 acc_val: 0.5475 time: 0.0253s\n",
      "Epoch: 02167 loss_train: 35.5272 loss_rec: 35.5272 acc_train: 0.5487 loss_val: 35.0452 acc_val: 0.5538 time: 0.0283s\n",
      "Epoch: 02168 loss_train: 3.4140 loss_rec: 3.4140 acc_train: 0.4941 loss_val: 3.4833 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 02169 loss_train: 18.5078 loss_rec: 18.5078 acc_train: 0.4922 loss_val: 18.8100 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 02170 loss_train: 8.4109 loss_rec: 8.4109 acc_train: 0.4936 loss_val: 8.5588 acc_val: 0.4983 time: 0.0283s\n",
      "Epoch: 02171 loss_train: 21.4759 loss_rec: 21.4759 acc_train: 0.5529 loss_val: 21.2106 acc_val: 0.5579 time: 0.0278s\n",
      "Test set results: loss= 32.2422 accuracy= 0.5105\n",
      "Epoch: 02172 loss_train: 28.4753 loss_rec: 28.4753 acc_train: 0.5533 loss_val: 28.0930 acc_val: 0.5563 time: 0.0263s\n",
      "Epoch: 02173 loss_train: 14.5741 loss_rec: 14.5741 acc_train: 0.5566 loss_val: 14.4282 acc_val: 0.5638 time: 0.0273s\n",
      "Epoch: 02174 loss_train: 18.4964 loss_rec: 18.4964 acc_train: 0.4922 loss_val: 18.7984 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 02175 loss_train: 26.6049 loss_rec: 26.6049 acc_train: 0.4938 loss_val: 27.0279 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 02176 loss_train: 10.2365 loss_rec: 10.2365 acc_train: 0.4936 loss_val: 10.4133 acc_val: 0.4979 time: 0.0258s\n",
      "Epoch: 02177 loss_train: 24.9316 loss_rec: 24.9316 acc_train: 0.5536 loss_val: 24.6072 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 02178 loss_train: 36.6557 loss_rec: 36.6557 acc_train: 0.5483 loss_val: 36.1570 acc_val: 0.5529 time: 0.0258s\n",
      "Epoch: 02179 loss_train: 26.8475 loss_rec: 26.8475 acc_train: 0.5517 loss_val: 26.4910 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 02180 loss_train: 1.4050 loss_rec: 1.4050 acc_train: 0.4942 loss_val: 1.4343 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 02181 loss_train: 12.6455 loss_rec: 12.6455 acc_train: 0.4927 loss_val: 12.8603 acc_val: 0.4975 time: 0.0273s\n",
      "Test set results: loss= 1.0785 accuracy= 0.6249\n",
      "Epoch: 02182 loss_train: 1.0704 loss_rec: 1.0704 acc_train: 0.6272 loss_val: 1.0979 acc_val: 0.6175 time: 0.0283s\n",
      "Epoch: 02183 loss_train: 4.4679 loss_rec: 4.4679 acc_train: 0.5779 loss_val: 4.4713 acc_val: 0.5808 time: 0.0263s\n",
      "Epoch: 02184 loss_train: 9.9420 loss_rec: 9.9420 acc_train: 0.4936 loss_val: 10.1142 acc_val: 0.4979 time: 0.0278s\n",
      "Epoch: 02185 loss_train: 1.0022 loss_rec: 1.0022 acc_train: 0.6125 loss_val: 1.0234 acc_val: 0.6013 time: 0.0263s\n",
      "Epoch: 02186 loss_train: 10.9361 loss_rec: 10.9361 acc_train: 0.5563 loss_val: 10.8473 acc_val: 0.5621 time: 0.0283s\n",
      "Epoch: 02187 loss_train: 2.6422 loss_rec: 2.6422 acc_train: 0.5987 loss_val: 2.6698 acc_val: 0.5992 time: 0.0278s\n",
      "Epoch: 02188 loss_train: 23.1455 loss_rec: 23.1455 acc_train: 0.4938 loss_val: 23.5162 acc_val: 0.4992 time: 0.0273s\n",
      "Epoch: 02189 loss_train: 23.2619 loss_rec: 23.2619 acc_train: 0.4938 loss_val: 23.6346 acc_val: 0.4992 time: 0.0263s\n",
      "Epoch: 02190 loss_train: 0.9898 loss_rec: 0.9898 acc_train: 0.6304 loss_val: 1.0140 acc_val: 0.6158 time: 0.0278s\n",
      "Epoch: 02191 loss_train: 16.0219 loss_rec: 16.0219 acc_train: 0.5549 loss_val: 15.8513 acc_val: 0.5625 time: 0.0258s\n",
      "Test set results: loss= 12.5856 accuracy= 0.5213\n",
      "Epoch: 02192 loss_train: 11.1200 loss_rec: 11.1200 acc_train: 0.5568 loss_val: 11.0281 acc_val: 0.5625 time: 0.0278s\n",
      "Epoch: 02193 loss_train: 13.2200 loss_rec: 13.2200 acc_train: 0.4927 loss_val: 13.4438 acc_val: 0.4975 time: 0.0263s\n",
      "Epoch: 02194 loss_train: 13.2703 loss_rec: 13.2703 acc_train: 0.4927 loss_val: 13.4949 acc_val: 0.4975 time: 0.0268s\n",
      "Epoch: 02195 loss_train: 8.5740 loss_rec: 8.5740 acc_train: 0.5603 loss_val: 8.5201 acc_val: 0.5625 time: 0.0273s\n",
      "Epoch: 02196 loss_train: 8.6640 loss_rec: 8.6640 acc_train: 0.5603 loss_val: 8.6093 acc_val: 0.5629 time: 0.0278s\n",
      "Epoch: 02197 loss_train: 10.6769 loss_rec: 10.6769 acc_train: 0.4934 loss_val: 10.8607 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 02198 loss_train: 6.0558 loss_rec: 6.0558 acc_train: 0.4939 loss_val: 6.1660 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 02199 loss_train: 18.9452 loss_rec: 18.9452 acc_train: 0.5527 loss_val: 18.7234 acc_val: 0.5575 time: 0.0268s\n",
      "Epoch: 02200 loss_train: 21.8648 loss_rec: 21.8648 acc_train: 0.5530 loss_val: 21.5932 acc_val: 0.5579 time: 0.0268s\n",
      "Epoch: 02201 loss_train: 4.7698 loss_rec: 4.7698 acc_train: 0.5755 loss_val: 4.7696 acc_val: 0.5783 time: 0.0273s\n",
      "Test set results: loss= 27.9083 accuracy= 0.5497\n",
      "Epoch: 02202 loss_train: 31.1431 loss_rec: 31.1431 acc_train: 0.4937 loss_val: 31.6361 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 02203 loss_train: 40.8902 loss_rec: 40.8902 acc_train: 0.4937 loss_val: 41.5321 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 02204 loss_train: 26.0570 loss_rec: 26.0570 acc_train: 0.4938 loss_val: 26.4719 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 02205 loss_train: 9.1385 loss_rec: 9.1385 acc_train: 0.5601 loss_val: 9.0780 acc_val: 0.5629 time: 0.0288s\n",
      "Epoch: 02206 loss_train: 20.2258 loss_rec: 20.2258 acc_train: 0.5523 loss_val: 19.9825 acc_val: 0.5579 time: 0.0268s\n",
      "Epoch: 02207 loss_train: 10.3274 loss_rec: 10.3274 acc_train: 0.5573 loss_val: 10.2487 acc_val: 0.5633 time: 0.0283s\n",
      "Epoch: 02208 loss_train: 18.8014 loss_rec: 18.8014 acc_train: 0.4922 loss_val: 19.1087 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 02209 loss_train: 23.1400 loss_rec: 23.1400 acc_train: 0.4938 loss_val: 23.5112 acc_val: 0.4992 time: 0.0258s\n",
      "Epoch: 02210 loss_train: 3.5590 loss_rec: 3.5590 acc_train: 0.4941 loss_val: 3.6308 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 02211 loss_train: 33.5531 loss_rec: 33.5531 acc_train: 0.5506 loss_val: 33.0996 acc_val: 0.5542 time: 0.0278s\n",
      "Test set results: loss= 53.8375 accuracy= 0.4986\n",
      "Epoch: 02212 loss_train: 47.5028 loss_rec: 47.5028 acc_train: 0.5446 loss_val: 46.8716 acc_val: 0.5479 time: 0.0268s\n",
      "Epoch: 02213 loss_train: 39.6004 loss_rec: 39.6004 acc_train: 0.5478 loss_val: 39.0624 acc_val: 0.5533 time: 0.0278s\n",
      "Epoch: 02214 loss_train: 12.3590 loss_rec: 12.3590 acc_train: 0.5553 loss_val: 12.2486 acc_val: 0.5596 time: 0.0278s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02215 loss_train: 33.5473 loss_rec: 33.5473 acc_train: 0.4937 loss_val: 34.0773 acc_val: 0.4988 time: 0.0253s\n",
      "Epoch: 02216 loss_train: 53.1953 loss_rec: 53.1953 acc_train: 0.4937 loss_val: 54.0256 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 02217 loss_train: 47.3529 loss_rec: 47.3529 acc_train: 0.4937 loss_val: 48.0939 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 02218 loss_train: 18.6193 loss_rec: 18.6193 acc_train: 0.4923 loss_val: 18.9244 acc_val: 0.4975 time: 0.0258s\n",
      "Epoch: 02219 loss_train: 27.3218 loss_rec: 27.3218 acc_train: 0.5511 loss_val: 26.9577 acc_val: 0.5521 time: 0.0278s\n",
      "Epoch: 02220 loss_train: 48.4234 loss_rec: 48.4234 acc_train: 0.5457 loss_val: 47.7813 acc_val: 0.5492 time: 0.0273s\n",
      "Epoch: 02221 loss_train: 47.0468 loss_rec: 47.0468 acc_train: 0.5447 loss_val: 46.4210 acc_val: 0.5492 time: 0.0253s\n",
      "Test set results: loss= 28.8820 accuracy= 0.5125\n",
      "Epoch: 02222 loss_train: 25.5083 loss_rec: 25.5083 acc_train: 0.5536 loss_val: 25.1745 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 02223 loss_train: 13.7575 loss_rec: 13.7575 acc_train: 0.4927 loss_val: 13.9897 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 02224 loss_train: 28.5865 loss_rec: 28.5865 acc_train: 0.4937 loss_val: 29.0408 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 02225 loss_train: 18.5364 loss_rec: 18.5364 acc_train: 0.4923 loss_val: 18.8406 acc_val: 0.4975 time: 0.0268s\n",
      "Epoch: 02226 loss_train: 11.8588 loss_rec: 11.8588 acc_train: 0.5553 loss_val: 11.7557 acc_val: 0.5596 time: 0.0268s\n",
      "Epoch: 02227 loss_train: 19.0889 loss_rec: 19.0889 acc_train: 0.5527 loss_val: 18.8649 acc_val: 0.5575 time: 0.0263s\n",
      "Epoch: 02228 loss_train: 6.0399 loss_rec: 6.0399 acc_train: 0.5693 loss_val: 6.0217 acc_val: 0.5737 time: 0.0278s\n",
      "Epoch: 02229 loss_train: 25.9792 loss_rec: 25.9792 acc_train: 0.4938 loss_val: 26.3935 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 02230 loss_train: 32.6269 loss_rec: 32.6269 acc_train: 0.4937 loss_val: 33.1431 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 02231 loss_train: 15.2644 loss_rec: 15.2644 acc_train: 0.4927 loss_val: 15.5200 acc_val: 0.4975 time: 0.0253s\n",
      "Test set results: loss= 23.5933 accuracy= 0.5121\n",
      "Epoch: 02232 loss_train: 20.8393 loss_rec: 20.8393 acc_train: 0.5514 loss_val: 20.5854 acc_val: 0.5567 time: 0.0273s\n",
      "Epoch: 02233 loss_train: 33.3353 loss_rec: 33.3353 acc_train: 0.5511 loss_val: 32.8853 acc_val: 0.5533 time: 0.0283s\n",
      "Epoch: 02234 loss_train: 24.5814 loss_rec: 24.5814 acc_train: 0.5536 loss_val: 24.2635 acc_val: 0.5583 time: 0.0268s\n",
      "Epoch: 02235 loss_train: 2.3636 loss_rec: 2.3636 acc_train: 0.4942 loss_val: 2.4151 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 02236 loss_train: 7.2381 loss_rec: 7.2381 acc_train: 0.4938 loss_val: 7.3672 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02237 loss_train: 9.7639 loss_rec: 9.7639 acc_train: 0.5579 loss_val: 9.6951 acc_val: 0.5629 time: 0.0253s\n",
      "Epoch: 02238 loss_train: 6.1373 loss_rec: 6.1373 acc_train: 0.5684 loss_val: 6.1177 acc_val: 0.5733 time: 0.0278s\n",
      "Epoch: 02239 loss_train: 16.5211 loss_rec: 16.5211 acc_train: 0.4925 loss_val: 16.7956 acc_val: 0.4975 time: 0.0283s\n",
      "Epoch: 02240 loss_train: 14.8861 loss_rec: 14.8861 acc_train: 0.4927 loss_val: 15.1358 acc_val: 0.4975 time: 0.0253s\n",
      "Epoch: 02241 loss_train: 8.1993 loss_rec: 8.1993 acc_train: 0.5615 loss_val: 8.1502 acc_val: 0.5638 time: 0.0278s\n",
      "Test set results: loss= 10.8039 accuracy= 0.5231\n",
      "Epoch: 02242 loss_train: 9.5485 loss_rec: 9.5485 acc_train: 0.5585 loss_val: 9.4823 acc_val: 0.5625 time: 0.0258s\n",
      "Epoch: 02243 loss_train: 8.2034 loss_rec: 8.2034 acc_train: 0.4938 loss_val: 8.3482 acc_val: 0.4983 time: 0.0263s\n",
      "Epoch: 02244 loss_train: 2.6756 loss_rec: 2.6756 acc_train: 0.4941 loss_val: 2.7326 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 02245 loss_train: 22.0046 loss_rec: 22.0046 acc_train: 0.5530 loss_val: 21.7307 acc_val: 0.5579 time: 0.0283s\n",
      "Epoch: 02246 loss_train: 25.0846 loss_rec: 25.0846 acc_train: 0.5536 loss_val: 24.7578 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 02247 loss_train: 8.2652 loss_rec: 8.2652 acc_train: 0.5607 loss_val: 8.2154 acc_val: 0.5633 time: 0.0273s\n",
      "Epoch: 02248 loss_train: 27.3702 loss_rec: 27.3702 acc_train: 0.4938 loss_val: 27.8058 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 02249 loss_train: 37.6456 loss_rec: 37.6456 acc_train: 0.4937 loss_val: 38.2386 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 02250 loss_train: 23.6487 loss_rec: 23.6487 acc_train: 0.4938 loss_val: 24.0282 acc_val: 0.4992 time: 0.0278s\n",
      "Epoch: 02251 loss_train: 10.2901 loss_rec: 10.2901 acc_train: 0.5573 loss_val: 10.2128 acc_val: 0.5633 time: 0.0278s\n",
      "Test set results: loss= 23.3211 accuracy= 0.5121\n",
      "Epoch: 02252 loss_train: 20.5991 loss_rec: 20.5991 acc_train: 0.5514 loss_val: 20.3497 acc_val: 0.5567 time: 0.0283s\n",
      "Epoch: 02253 loss_train: 10.3015 loss_rec: 10.3015 acc_train: 0.5573 loss_val: 10.2240 acc_val: 0.5633 time: 0.0278s\n",
      "Epoch: 02254 loss_train: 18.9096 loss_rec: 18.9096 acc_train: 0.4924 loss_val: 19.2197 acc_val: 0.4975 time: 0.0258s\n",
      "Epoch: 02255 loss_train: 23.6819 loss_rec: 23.6819 acc_train: 0.4938 loss_val: 24.0621 acc_val: 0.4992 time: 0.0283s\n",
      "Epoch: 02256 loss_train: 4.8638 loss_rec: 4.8638 acc_train: 0.4940 loss_val: 4.9560 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 02257 loss_train: 31.4915 loss_rec: 31.4915 acc_train: 0.5507 loss_val: 31.0685 acc_val: 0.5529 time: 0.0253s\n",
      "Epoch: 02258 loss_train: 44.9749 loss_rec: 44.9749 acc_train: 0.5454 loss_val: 44.3738 acc_val: 0.5483 time: 0.0283s\n",
      "Epoch: 02259 loss_train: 36.9792 loss_rec: 36.9792 acc_train: 0.5483 loss_val: 36.4760 acc_val: 0.5529 time: 0.0278s\n",
      "Epoch: 02260 loss_train: 10.0488 loss_rec: 10.0488 acc_train: 0.5575 loss_val: 9.9753 acc_val: 0.5625 time: 0.0253s\n",
      "Epoch: 02261 loss_train: 35.2858 loss_rec: 35.2858 acc_train: 0.4937 loss_val: 35.8430 acc_val: 0.4988 time: 0.0278s\n",
      "Test set results: loss= 48.9542 accuracy= 0.5496\n",
      "Epoch: 02262 loss_train: 54.5745 loss_rec: 54.5745 acc_train: 0.4937 loss_val: 55.4264 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 02263 loss_train: 48.7508 loss_rec: 48.7508 acc_train: 0.4937 loss_val: 49.5138 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 02264 loss_train: 20.3777 loss_rec: 20.3777 acc_train: 0.4922 loss_val: 20.7093 acc_val: 0.4971 time: 0.0258s\n",
      "Epoch: 02265 loss_train: 25.0428 loss_rec: 25.0428 acc_train: 0.5536 loss_val: 24.7173 acc_val: 0.5583 time: 0.0263s\n",
      "Epoch: 02266 loss_train: 45.8654 loss_rec: 45.8654 acc_train: 0.5445 loss_val: 45.2534 acc_val: 0.5488 time: 0.0273s\n",
      "Epoch: 02267 loss_train: 44.5823 loss_rec: 44.5823 acc_train: 0.5454 loss_val: 43.9856 acc_val: 0.5483 time: 0.0273s\n",
      "Epoch: 02268 loss_train: 23.4674 loss_rec: 23.4674 acc_train: 0.5534 loss_val: 23.1688 acc_val: 0.5575 time: 0.0283s\n",
      "Epoch: 02269 loss_train: 15.2319 loss_rec: 15.2319 acc_train: 0.4927 loss_val: 15.4873 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 02270 loss_train: 29.7525 loss_rec: 29.7525 acc_train: 0.4937 loss_val: 30.2252 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 02271 loss_train: 19.7798 loss_rec: 19.7798 acc_train: 0.4922 loss_val: 20.1028 acc_val: 0.4975 time: 0.0268s\n",
      "Test set results: loss= 11.7026 accuracy= 0.5213\n",
      "Epoch: 02272 loss_train: 10.3405 loss_rec: 10.3405 acc_train: 0.5567 loss_val: 10.2623 acc_val: 0.5629 time: 0.0283s\n",
      "Epoch: 02273 loss_train: 17.6486 loss_rec: 17.6486 acc_train: 0.5526 loss_val: 17.4501 acc_val: 0.5575 time: 0.0278s\n",
      "Epoch: 02274 loss_train: 5.0418 loss_rec: 5.0418 acc_train: 0.5730 loss_val: 5.0385 acc_val: 0.5775 time: 0.0258s\n",
      "Epoch: 02275 loss_train: 26.0462 loss_rec: 26.0462 acc_train: 0.4938 loss_val: 26.4620 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 02276 loss_train: 32.0858 loss_rec: 32.0858 acc_train: 0.4937 loss_val: 32.5943 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 02277 loss_train: 14.5209 loss_rec: 14.5209 acc_train: 0.4927 loss_val: 14.7653 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 02278 loss_train: 21.3417 loss_rec: 21.3417 acc_train: 0.5529 loss_val: 21.0794 acc_val: 0.5579 time: 0.0278s\n",
      "Epoch: 02279 loss_train: 34.0077 loss_rec: 34.0077 acc_train: 0.5502 loss_val: 33.5483 acc_val: 0.5550 time: 0.0278s\n",
      "Epoch: 02280 loss_train: 25.6793 loss_rec: 25.6793 acc_train: 0.5536 loss_val: 25.3431 acc_val: 0.5583 time: 0.0253s\n",
      "Epoch: 02281 loss_train: 1.0149 loss_rec: 1.0149 acc_train: 0.6125 loss_val: 1.0363 acc_val: 0.6013 time: 0.0288s\n",
      "Test set results: loss= 19.4193 accuracy= 0.5479\n",
      "Epoch: 02282 loss_train: 21.6925 loss_rec: 21.6925 acc_train: 0.4922 loss_val: 22.0439 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 02283 loss_train: 17.9954 loss_rec: 17.9954 acc_train: 0.4925 loss_val: 18.2926 acc_val: 0.4975 time: 0.0283s\n",
      "Epoch: 02284 loss_train: 6.8067 loss_rec: 6.8067 acc_train: 0.5663 loss_val: 6.7769 acc_val: 0.5717 time: 0.0278s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02285 loss_train: 9.8692 loss_rec: 9.8692 acc_train: 0.5573 loss_val: 9.7988 acc_val: 0.5625 time: 0.0278s\n",
      "Epoch: 02286 loss_train: 5.8969 loss_rec: 5.8969 acc_train: 0.4940 loss_val: 6.0050 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 02287 loss_train: 1.2947 loss_rec: 1.2947 acc_train: 0.6272 loss_val: 1.3265 acc_val: 0.6158 time: 0.0253s\n",
      "Epoch: 02288 loss_train: 3.4722 loss_rec: 3.4722 acc_train: 0.4941 loss_val: 3.5428 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 02289 loss_train: 12.9897 loss_rec: 12.9897 acc_train: 0.5530 loss_val: 12.8702 acc_val: 0.5575 time: 0.0278s\n",
      "Epoch: 02290 loss_train: 9.1842 loss_rec: 9.1842 acc_train: 0.5595 loss_val: 9.1235 acc_val: 0.5625 time: 0.0283s\n",
      "Epoch: 02291 loss_train: 13.3993 loss_rec: 13.3993 acc_train: 0.4927 loss_val: 13.6264 acc_val: 0.4975 time: 0.0278s\n",
      "Test set results: loss= 11.0822 accuracy= 0.5488\n",
      "Epoch: 02292 loss_train: 12.4126 loss_rec: 12.4126 acc_train: 0.4928 loss_val: 12.6243 acc_val: 0.4975 time: 0.0268s\n",
      "Epoch: 02293 loss_train: 9.5409 loss_rec: 9.5409 acc_train: 0.5579 loss_val: 9.4752 acc_val: 0.5621 time: 0.0278s\n",
      "Epoch: 02294 loss_train: 10.2734 loss_rec: 10.2734 acc_train: 0.5567 loss_val: 10.1965 acc_val: 0.5629 time: 0.0283s\n",
      "Epoch: 02295 loss_train: 7.8354 loss_rec: 7.8354 acc_train: 0.4938 loss_val: 7.9744 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 02296 loss_train: 3.0470 loss_rec: 3.0470 acc_train: 0.4941 loss_val: 3.1107 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 02297 loss_train: 20.9573 loss_rec: 20.9573 acc_train: 0.5514 loss_val: 20.7018 acc_val: 0.5567 time: 0.0268s\n",
      "Epoch: 02298 loss_train: 23.7511 loss_rec: 23.7511 acc_train: 0.5534 loss_val: 23.4477 acc_val: 0.5575 time: 0.0263s\n",
      "Epoch: 02299 loss_train: 7.0180 loss_rec: 7.0180 acc_train: 0.5645 loss_val: 6.9850 acc_val: 0.5708 time: 0.0273s\n",
      "Epoch: 02300 loss_train: 28.1939 loss_rec: 28.1939 acc_train: 0.4938 loss_val: 28.6429 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 02301 loss_train: 38.3065 loss_rec: 38.3065 acc_train: 0.4937 loss_val: 38.9104 acc_val: 0.4988 time: 0.0278s\n",
      "Test set results: loss= 21.9902 accuracy= 0.5498\n",
      "Epoch: 02302 loss_train: 24.5545 loss_rec: 24.5545 acc_train: 0.4938 loss_val: 24.9485 acc_val: 0.4992 time: 0.0258s\n",
      "Epoch: 02303 loss_train: 8.9743 loss_rec: 8.9743 acc_train: 0.5601 loss_val: 8.9161 acc_val: 0.5629 time: 0.0258s\n",
      "Epoch: 02304 loss_train: 19.2667 loss_rec: 19.2667 acc_train: 0.5527 loss_val: 19.0402 acc_val: 0.5575 time: 0.0278s\n",
      "Epoch: 02305 loss_train: 9.3417 loss_rec: 9.3417 acc_train: 0.5589 loss_val: 9.2786 acc_val: 0.5625 time: 0.0278s\n",
      "Epoch: 02306 loss_train: 19.1574 loss_rec: 19.1574 acc_train: 0.4924 loss_val: 19.4719 acc_val: 0.4975 time: 0.0283s\n",
      "Epoch: 02307 loss_train: 23.5580 loss_rec: 23.5580 acc_train: 0.4922 loss_val: 23.9376 acc_val: 0.4971 time: 0.0258s\n",
      "Epoch: 02308 loss_train: 4.7360 loss_rec: 4.7360 acc_train: 0.4940 loss_val: 4.8266 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 02309 loss_train: 31.2569 loss_rec: 31.2569 acc_train: 0.5517 loss_val: 30.8378 acc_val: 0.5533 time: 0.0263s\n",
      "Epoch: 02310 loss_train: 44.2679 loss_rec: 44.2679 acc_train: 0.5454 loss_val: 43.6723 acc_val: 0.5483 time: 0.0273s\n",
      "Epoch: 02311 loss_train: 36.4399 loss_rec: 36.4399 acc_train: 0.5486 loss_val: 35.9428 acc_val: 0.5529 time: 0.0273s\n",
      "Test set results: loss= 11.6498 accuracy= 0.5199\n",
      "Epoch: 02312 loss_train: 10.2949 loss_rec: 10.2949 acc_train: 0.5557 loss_val: 10.2121 acc_val: 0.5617 time: 0.0278s\n",
      "Epoch: 02313 loss_train: 33.5114 loss_rec: 33.5114 acc_train: 0.4937 loss_val: 34.0410 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 02314 loss_train: 52.2811 loss_rec: 52.2811 acc_train: 0.4937 loss_val: 53.0975 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 02315 loss_train: 46.9542 loss_rec: 46.9542 acc_train: 0.4937 loss_val: 47.6893 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 02316 loss_train: 20.0132 loss_rec: 20.0132 acc_train: 0.4922 loss_val: 20.3383 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 02317 loss_train: 23.4144 loss_rec: 23.4144 acc_train: 0.5530 loss_val: 23.1115 acc_val: 0.5579 time: 0.0278s\n",
      "Epoch: 02318 loss_train: 43.1876 loss_rec: 43.1876 acc_train: 0.5445 loss_val: 42.6124 acc_val: 0.5488 time: 0.0278s\n",
      "Epoch: 02319 loss_train: 41.7865 loss_rec: 41.7865 acc_train: 0.5445 loss_val: 41.2279 acc_val: 0.5483 time: 0.0258s\n",
      "Epoch: 02320 loss_train: 21.3825 loss_rec: 21.3825 acc_train: 0.5538 loss_val: 21.1142 acc_val: 0.5588 time: 0.0278s\n",
      "Epoch: 02321 loss_train: 15.9694 loss_rec: 15.9694 acc_train: 0.4925 loss_val: 16.2349 acc_val: 0.4975 time: 0.0268s\n",
      "Test set results: loss= 26.8886 accuracy= 0.5497\n",
      "Epoch: 02322 loss_train: 30.0042 loss_rec: 30.0042 acc_train: 0.4937 loss_val: 30.4804 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 02323 loss_train: 20.5993 loss_rec: 20.5993 acc_train: 0.4922 loss_val: 20.9333 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02324 loss_train: 8.3393 loss_rec: 8.3393 acc_train: 0.5600 loss_val: 8.2864 acc_val: 0.5629 time: 0.0263s\n",
      "Epoch: 02325 loss_train: 15.3976 loss_rec: 15.3976 acc_train: 0.5536 loss_val: 15.2325 acc_val: 0.5600 time: 0.0263s\n",
      "Epoch: 02326 loss_train: 3.5430 loss_rec: 3.5430 acc_train: 0.5835 loss_val: 3.5554 acc_val: 0.5858 time: 0.0268s\n",
      "Epoch: 02327 loss_train: 25.5156 loss_rec: 25.5156 acc_train: 0.4938 loss_val: 25.9232 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 02328 loss_train: 30.4078 loss_rec: 30.4078 acc_train: 0.4937 loss_val: 30.8906 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 02329 loss_train: 12.8374 loss_rec: 12.8374 acc_train: 0.4927 loss_val: 13.0554 acc_val: 0.4975 time: 0.0263s\n",
      "Epoch: 02330 loss_train: 22.0641 loss_rec: 22.0641 acc_train: 0.5534 loss_val: 21.7841 acc_val: 0.5575 time: 0.0273s\n",
      "Epoch: 02331 loss_train: 34.7539 loss_rec: 34.7539 acc_train: 0.5477 loss_val: 34.2818 acc_val: 0.5525 time: 0.0263s\n",
      "Test set results: loss= 30.8863 accuracy= 0.5085\n",
      "Epoch: 02332 loss_train: 27.2776 loss_rec: 27.2776 acc_train: 0.5519 loss_val: 26.9120 acc_val: 0.5533 time: 0.0268s\n",
      "Epoch: 02333 loss_train: 2.4449 loss_rec: 2.4449 acc_train: 0.5982 loss_val: 2.4713 acc_val: 0.5979 time: 0.0273s\n",
      "Epoch: 02334 loss_train: 38.1182 loss_rec: 38.1182 acc_train: 0.4937 loss_val: 38.7189 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 02335 loss_train: 53.1799 loss_rec: 53.1799 acc_train: 0.4937 loss_val: 54.0107 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 02336 loss_train: 44.8003 loss_rec: 44.8003 acc_train: 0.4937 loss_val: 45.5030 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 02337 loss_train: 15.3263 loss_rec: 15.3263 acc_train: 0.4927 loss_val: 15.5827 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 02338 loss_train: 29.5605 loss_rec: 29.5605 acc_train: 0.5502 loss_val: 29.1645 acc_val: 0.5525 time: 0.0258s\n",
      "Epoch: 02339 loss_train: 51.1567 loss_rec: 51.1567 acc_train: 0.5409 loss_val: 50.4866 acc_val: 0.5483 time: 0.0273s\n",
      "Epoch: 02340 loss_train: 51.5314 loss_rec: 51.5314 acc_train: 0.5403 loss_val: 50.8568 acc_val: 0.5471 time: 0.0278s\n",
      "Epoch: 02341 loss_train: 32.8268 loss_rec: 32.8268 acc_train: 0.5493 loss_val: 32.3832 acc_val: 0.5538 time: 0.0268s\n",
      "Test set results: loss= 1.6475 accuracy= 0.5485\n",
      "Epoch: 02342 loss_train: 1.8952 loss_rec: 1.8952 acc_train: 0.4927 loss_val: 1.9369 acc_val: 0.4983 time: 0.0258s\n",
      "Epoch: 02343 loss_train: 16.7873 loss_rec: 16.7873 acc_train: 0.4925 loss_val: 17.0655 acc_val: 0.4975 time: 0.0283s\n",
      "Epoch: 02344 loss_train: 8.5421 loss_rec: 8.5421 acc_train: 0.4932 loss_val: 8.6921 acc_val: 0.4979 time: 0.0283s\n",
      "Epoch: 02345 loss_train: 18.0948 loss_rec: 18.0948 acc_train: 0.5527 loss_val: 17.8832 acc_val: 0.5575 time: 0.0273s\n",
      "Epoch: 02346 loss_train: 23.7565 loss_rec: 23.7565 acc_train: 0.5530 loss_val: 23.4478 acc_val: 0.5579 time: 0.0263s\n",
      "Epoch: 02347 loss_train: 10.3930 loss_rec: 10.3930 acc_train: 0.5563 loss_val: 10.3089 acc_val: 0.5621 time: 0.0263s\n",
      "Epoch: 02348 loss_train: 20.6506 loss_rec: 20.6506 acc_train: 0.4922 loss_val: 20.9856 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02349 loss_train: 28.4511 loss_rec: 28.4511 acc_train: 0.4937 loss_val: 28.9037 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 02350 loss_train: 13.7287 loss_rec: 13.7287 acc_train: 0.4927 loss_val: 13.9601 acc_val: 0.4975 time: 0.0283s\n",
      "Epoch: 02351 loss_train: 18.6630 loss_rec: 18.6630 acc_train: 0.5523 loss_val: 18.4416 acc_val: 0.5579 time: 0.0273s\n",
      "Test set results: loss= 33.0786 accuracy= 0.5077\n",
      "Epoch: 02352 loss_train: 29.2114 loss_rec: 29.2114 acc_train: 0.5517 loss_val: 28.8204 acc_val: 0.5533 time: 0.0258s\n",
      "Epoch: 02353 loss_train: 20.0942 loss_rec: 20.0942 acc_train: 0.5529 loss_val: 19.8482 acc_val: 0.5579 time: 0.0288s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02354 loss_train: 6.1214 loss_rec: 6.1214 acc_train: 0.4933 loss_val: 6.2322 acc_val: 0.4983 time: 0.0283s\n",
      "Epoch: 02355 loss_train: 10.5022 loss_rec: 10.5022 acc_train: 0.4930 loss_val: 10.6830 acc_val: 0.4975 time: 0.0253s\n",
      "Epoch: 02356 loss_train: 5.9036 loss_rec: 5.9036 acc_train: 0.5668 loss_val: 5.8840 acc_val: 0.5721 time: 0.0283s\n",
      "Epoch: 02357 loss_train: 3.2841 loss_rec: 3.2841 acc_train: 0.5871 loss_val: 3.3007 acc_val: 0.5908 time: 0.0278s\n",
      "Epoch: 02358 loss_train: 16.1158 loss_rec: 16.1158 acc_train: 0.4926 loss_val: 16.3839 acc_val: 0.4975 time: 0.0253s\n",
      "Epoch: 02359 loss_train: 12.6533 loss_rec: 12.6533 acc_train: 0.4927 loss_val: 12.8682 acc_val: 0.4975 time: 0.0283s\n",
      "Epoch: 02360 loss_train: 10.2922 loss_rec: 10.2922 acc_train: 0.5557 loss_val: 10.2099 acc_val: 0.5617 time: 0.0268s\n",
      "Epoch: 02361 loss_train: 12.6792 loss_rec: 12.6792 acc_train: 0.5528 loss_val: 12.5608 acc_val: 0.5575 time: 0.0263s\n",
      "Test set results: loss= 2.4198 accuracy= 0.5484\n",
      "Epoch: 02362 loss_train: 2.7618 loss_rec: 2.7618 acc_train: 0.4927 loss_val: 2.8201 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02363 loss_train: 2.2203 loss_rec: 2.2203 acc_train: 0.6038 loss_val: 2.2487 acc_val: 0.6054 time: 0.0268s\n",
      "Epoch: 02364 loss_train: 8.0367 loss_rec: 8.0367 acc_train: 0.4932 loss_val: 8.1785 acc_val: 0.4979 time: 0.0263s\n",
      "Epoch: 02365 loss_train: 3.1389 loss_rec: 3.1389 acc_train: 0.5874 loss_val: 3.1580 acc_val: 0.5933 time: 0.0278s\n",
      "Epoch: 02366 loss_train: 2.6618 loss_rec: 2.6618 acc_train: 0.4927 loss_val: 2.7183 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02367 loss_train: 10.6945 loss_rec: 10.6945 acc_train: 0.5556 loss_val: 10.6059 acc_val: 0.5617 time: 0.0273s\n",
      "Epoch: 02368 loss_train: 5.3304 loss_rec: 5.3304 acc_train: 0.5688 loss_val: 5.3198 acc_val: 0.5733 time: 0.0273s\n",
      "Epoch: 02369 loss_train: 17.6654 loss_rec: 17.6654 acc_train: 0.4925 loss_val: 17.9568 acc_val: 0.4975 time: 0.0263s\n",
      "Epoch: 02370 loss_train: 17.8194 loss_rec: 17.8194 acc_train: 0.4925 loss_val: 18.1130 acc_val: 0.4975 time: 0.0263s\n",
      "Epoch: 02371 loss_train: 2.9327 loss_rec: 2.9327 acc_train: 0.5902 loss_val: 2.9545 acc_val: 0.5925 time: 0.0273s\n",
      "Test set results: loss= 5.5485 accuracy= 0.5378\n",
      "Epoch: 02372 loss_train: 4.9294 loss_rec: 4.9294 acc_train: 0.5719 loss_val: 4.9243 acc_val: 0.5758 time: 0.0258s\n",
      "Epoch: 02373 loss_train: 10.3668 loss_rec: 10.3668 acc_train: 0.4930 loss_val: 10.5457 acc_val: 0.4975 time: 0.0283s\n",
      "Epoch: 02374 loss_train: 3.6262 loss_rec: 3.6262 acc_train: 0.4935 loss_val: 3.6989 acc_val: 0.4983 time: 0.0263s\n",
      "Epoch: 02375 loss_train: 21.1116 loss_rec: 21.1116 acc_train: 0.5538 loss_val: 20.8487 acc_val: 0.5588 time: 0.0268s\n",
      "Epoch: 02376 loss_train: 25.5309 loss_rec: 25.5309 acc_train: 0.5510 loss_val: 25.1929 acc_val: 0.5521 time: 0.0278s\n",
      "Epoch: 02377 loss_train: 11.1736 loss_rec: 11.1736 acc_train: 0.5540 loss_val: 11.0777 acc_val: 0.5588 time: 0.0263s\n",
      "Epoch: 02378 loss_train: 20.6481 loss_rec: 20.6481 acc_train: 0.4923 loss_val: 20.9836 acc_val: 0.4971 time: 0.0258s\n",
      "Epoch: 02379 loss_train: 29.3769 loss_rec: 29.3769 acc_train: 0.4937 loss_val: 29.8442 acc_val: 0.4988 time: 0.0288s\n",
      "Epoch: 02380 loss_train: 15.6754 loss_rec: 15.6754 acc_train: 0.4927 loss_val: 15.9371 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 02381 loss_train: 15.8925 loss_rec: 15.8925 acc_train: 0.5531 loss_val: 15.7193 acc_val: 0.5579 time: 0.0268s\n",
      "Test set results: loss= 29.0917 accuracy= 0.5077\n",
      "Epoch: 02382 loss_train: 25.6936 loss_rec: 25.6936 acc_train: 0.5503 loss_val: 25.3528 acc_val: 0.5517 time: 0.0273s\n",
      "Epoch: 02383 loss_train: 16.1622 loss_rec: 16.1622 acc_train: 0.5526 loss_val: 15.9841 acc_val: 0.5575 time: 0.0278s\n",
      "Epoch: 02384 loss_train: 10.6292 loss_rec: 10.6292 acc_train: 0.4930 loss_val: 10.8122 acc_val: 0.4975 time: 0.0263s\n",
      "Epoch: 02385 loss_train: 15.3659 loss_rec: 15.3659 acc_train: 0.4922 loss_val: 15.6228 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 02386 loss_train: 1.6895 loss_rec: 1.6895 acc_train: 0.6164 loss_val: 1.7209 acc_val: 0.6154 time: 0.0273s\n",
      "Epoch: 02387 loss_train: 3.1071 loss_rec: 3.1071 acc_train: 0.5879 loss_val: 3.1267 acc_val: 0.5933 time: 0.0273s\n",
      "Epoch: 02388 loss_train: 11.4799 loss_rec: 11.4799 acc_train: 0.4925 loss_val: 11.6764 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02389 loss_train: 3.8281 loss_rec: 3.8281 acc_train: 0.4935 loss_val: 3.9039 acc_val: 0.4983 time: 0.0268s\n",
      "Epoch: 02390 loss_train: 21.6513 loss_rec: 21.6513 acc_train: 0.5534 loss_val: 21.3789 acc_val: 0.5575 time: 0.0278s\n",
      "Epoch: 02391 loss_train: 26.7992 loss_rec: 26.7992 acc_train: 0.5527 loss_val: 26.4406 acc_val: 0.5558 time: 0.0278s\n",
      "Test set results: loss= 14.8158 accuracy= 0.5152\n",
      "Epoch: 02392 loss_train: 13.0886 loss_rec: 13.0886 acc_train: 0.5514 loss_val: 12.9638 acc_val: 0.5571 time: 0.0278s\n",
      "Epoch: 02393 loss_train: 17.9330 loss_rec: 17.9330 acc_train: 0.4925 loss_val: 18.2284 acc_val: 0.4975 time: 0.0283s\n",
      "Epoch: 02394 loss_train: 26.1995 loss_rec: 26.1995 acc_train: 0.4938 loss_val: 26.6180 acc_val: 0.4988 time: 0.0258s\n",
      "Epoch: 02395 loss_train: 12.1616 loss_rec: 12.1616 acc_train: 0.4922 loss_val: 12.3689 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 02396 loss_train: 19.3523 loss_rec: 19.3523 acc_train: 0.5523 loss_val: 19.1198 acc_val: 0.5579 time: 0.0283s\n",
      "Epoch: 02397 loss_train: 29.3893 loss_rec: 29.3893 acc_train: 0.5517 loss_val: 28.9958 acc_val: 0.5533 time: 0.0273s\n",
      "Epoch: 02398 loss_train: 20.0186 loss_rec: 20.0186 acc_train: 0.5529 loss_val: 19.7747 acc_val: 0.5579 time: 0.0273s\n",
      "Epoch: 02399 loss_train: 6.2861 loss_rec: 6.2861 acc_train: 0.4933 loss_val: 6.3998 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02400 loss_train: 10.9330 loss_rec: 10.9330 acc_train: 0.4929 loss_val: 11.1208 acc_val: 0.4975 time: 0.0258s\n",
      "Epoch: 02401 loss_train: 5.1427 loss_rec: 5.1427 acc_train: 0.5712 loss_val: 5.1350 acc_val: 0.5750 time: 0.0263s\n",
      "Test set results: loss= 2.9257 accuracy= 0.5716\n",
      "Epoch: 02402 loss_train: 2.6285 loss_rec: 2.6285 acc_train: 0.5967 loss_val: 2.6538 acc_val: 0.5950 time: 0.0273s\n",
      "Epoch: 02403 loss_train: 15.8085 loss_rec: 15.8085 acc_train: 0.4921 loss_val: 16.0722 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02404 loss_train: 11.5167 loss_rec: 11.5167 acc_train: 0.4925 loss_val: 11.7136 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 02405 loss_train: 11.8107 loss_rec: 11.8107 acc_train: 0.5535 loss_val: 11.7054 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 02406 loss_train: 14.7268 loss_rec: 14.7268 acc_train: 0.5543 loss_val: 14.5740 acc_val: 0.5613 time: 0.0263s\n",
      "Epoch: 02407 loss_train: 0.9485 loss_rec: 0.9485 acc_train: 0.6209 loss_val: 0.9705 acc_val: 0.6163 time: 0.0278s\n",
      "Epoch: 02408 loss_train: 16.0274 loss_rec: 16.0274 acc_train: 0.4920 loss_val: 16.2945 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02409 loss_train: 9.1463 loss_rec: 9.1463 acc_train: 0.4930 loss_val: 9.3059 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 02410 loss_train: 16.1111 loss_rec: 16.1111 acc_train: 0.5526 loss_val: 15.9341 acc_val: 0.5575 time: 0.0258s\n",
      "Epoch: 02411 loss_train: 20.8073 loss_rec: 20.8073 acc_train: 0.5537 loss_val: 20.5497 acc_val: 0.5588 time: 0.0278s\n",
      "Test set results: loss= 7.8610 accuracy= 0.5273\n",
      "Epoch: 02412 loss_train: 6.9598 loss_rec: 6.9598 acc_train: 0.5629 loss_val: 6.9243 acc_val: 0.5671 time: 0.0278s\n",
      "Epoch: 02413 loss_train: 24.2312 loss_rec: 24.2312 acc_train: 0.4921 loss_val: 24.6203 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02414 loss_train: 32.1420 loss_rec: 32.1420 acc_train: 0.4937 loss_val: 32.6519 acc_val: 0.4988 time: 0.0278s\n",
      "Epoch: 02415 loss_train: 17.8299 loss_rec: 17.8299 acc_train: 0.4919 loss_val: 18.1243 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02416 loss_train: 14.2608 loss_rec: 14.2608 acc_train: 0.5549 loss_val: 14.1161 acc_val: 0.5621 time: 0.0258s\n",
      "Epoch: 02417 loss_train: 24.5466 loss_rec: 24.5466 acc_train: 0.5510 loss_val: 24.2254 acc_val: 0.5563 time: 0.0278s\n",
      "Epoch: 02418 loss_train: 15.5812 loss_rec: 15.5812 acc_train: 0.5530 loss_val: 15.4134 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 02419 loss_train: 10.5521 loss_rec: 10.5521 acc_train: 0.4930 loss_val: 10.7342 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 02420 loss_train: 14.8038 loss_rec: 14.8038 acc_train: 0.4922 loss_val: 15.0525 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 02421 loss_train: 2.2439 loss_rec: 2.2439 acc_train: 0.6039 loss_val: 2.2723 acc_val: 0.6054 time: 0.0278s\n",
      "Test set results: loss= 2.7883 accuracy= 0.5728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02422 loss_train: 2.5088 loss_rec: 2.5088 acc_train: 0.5979 loss_val: 2.5352 acc_val: 0.5983 time: 0.0273s\n",
      "Epoch: 02423 loss_train: 12.6225 loss_rec: 12.6225 acc_train: 0.4922 loss_val: 12.8373 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02424 loss_train: 5.4219 loss_rec: 5.4219 acc_train: 0.4934 loss_val: 5.5225 acc_val: 0.4983 time: 0.0268s\n",
      "Epoch: 02425 loss_train: 19.7082 loss_rec: 19.7082 acc_train: 0.5514 loss_val: 19.4694 acc_val: 0.5567 time: 0.0283s\n",
      "Epoch: 02426 loss_train: 24.5791 loss_rec: 24.5791 acc_train: 0.5516 loss_val: 24.2572 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 02427 loss_train: 10.8359 loss_rec: 10.8359 acc_train: 0.5556 loss_val: 10.7454 acc_val: 0.5617 time: 0.0258s\n",
      "Epoch: 02428 loss_train: 20.1911 loss_rec: 20.1911 acc_train: 0.4917 loss_val: 20.5200 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 02429 loss_train: 28.4323 loss_rec: 28.4323 acc_train: 0.4937 loss_val: 28.8853 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 02430 loss_train: 14.4981 loss_rec: 14.4981 acc_train: 0.4922 loss_val: 14.7419 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02431 loss_train: 16.9289 loss_rec: 16.9289 acc_train: 0.5526 loss_val: 16.7380 acc_val: 0.5575 time: 0.0278s\n",
      "Test set results: loss= 30.4587 accuracy= 0.5097\n",
      "Epoch: 02432 loss_train: 26.9005 loss_rec: 26.9005 acc_train: 0.5527 loss_val: 26.5405 acc_val: 0.5558 time: 0.0278s\n",
      "Epoch: 02433 loss_train: 17.6933 loss_rec: 17.6933 acc_train: 0.5527 loss_val: 17.4892 acc_val: 0.5575 time: 0.0268s\n",
      "Epoch: 02434 loss_train: 8.4552 loss_rec: 8.4552 acc_train: 0.4932 loss_val: 8.6038 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 02435 loss_train: 12.9760 loss_rec: 12.9760 acc_train: 0.4922 loss_val: 13.1961 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 02436 loss_train: 3.4137 loss_rec: 3.4137 acc_train: 0.5853 loss_val: 3.4288 acc_val: 0.5896 time: 0.0278s\n",
      "Epoch: 02437 loss_train: 2.0682 loss_rec: 2.0682 acc_train: 0.6087 loss_val: 2.0977 acc_val: 0.6054 time: 0.0273s\n",
      "Epoch: 02438 loss_train: 13.9663 loss_rec: 13.9663 acc_train: 0.4922 loss_val: 14.2018 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02439 loss_train: 7.5157 loss_rec: 7.5157 acc_train: 0.4932 loss_val: 7.6491 acc_val: 0.4979 time: 0.0258s\n",
      "Epoch: 02440 loss_train: 17.0919 loss_rec: 17.0919 acc_train: 0.5526 loss_val: 16.8984 acc_val: 0.5575 time: 0.0278s\n",
      "Epoch: 02441 loss_train: 21.4290 loss_rec: 21.4290 acc_train: 0.5533 loss_val: 21.1609 acc_val: 0.5583 time: 0.0268s\n",
      "Test set results: loss= 8.3386 accuracy= 0.5263\n",
      "Epoch: 02442 loss_train: 7.3794 loss_rec: 7.3794 acc_train: 0.5613 loss_val: 7.3383 acc_val: 0.5654 time: 0.0278s\n",
      "Epoch: 02443 loss_train: 23.8661 loss_rec: 23.8661 acc_train: 0.4916 loss_val: 24.2502 acc_val: 0.4967 time: 0.0258s\n",
      "Epoch: 02444 loss_train: 32.0372 loss_rec: 32.0372 acc_train: 0.4937 loss_val: 32.5457 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 02445 loss_train: 18.1042 loss_rec: 18.1042 acc_train: 0.4919 loss_val: 18.4027 acc_val: 0.4971 time: 0.0283s\n",
      "Epoch: 02446 loss_train: 13.5534 loss_rec: 13.5534 acc_train: 0.5559 loss_val: 13.4215 acc_val: 0.5638 time: 0.0273s\n",
      "Epoch: 02447 loss_train: 23.5815 loss_rec: 23.5815 acc_train: 0.5530 loss_val: 23.2768 acc_val: 0.5579 time: 0.0278s\n",
      "Epoch: 02448 loss_train: 14.5530 loss_rec: 14.5530 acc_train: 0.5549 loss_val: 14.4038 acc_val: 0.5621 time: 0.0273s\n",
      "Epoch: 02449 loss_train: 11.6095 loss_rec: 11.6095 acc_train: 0.4927 loss_val: 11.8082 acc_val: 0.4971 time: 0.0258s\n",
      "Epoch: 02450 loss_train: 15.9433 loss_rec: 15.9433 acc_train: 0.4922 loss_val: 16.2095 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 02451 loss_train: 1.3800 loss_rec: 1.3800 acc_train: 0.6268 loss_val: 1.4113 acc_val: 0.6229 time: 0.0283s\n",
      "Test set results: loss= 5.0841 accuracy= 0.5409\n",
      "Epoch: 02452 loss_train: 4.5230 loss_rec: 4.5230 acc_train: 0.5749 loss_val: 4.5236 acc_val: 0.5779 time: 0.0263s\n",
      "Epoch: 02453 loss_train: 8.6908 loss_rec: 8.6908 acc_train: 0.4932 loss_val: 8.8435 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 02454 loss_train: 0.9918 loss_rec: 0.9918 acc_train: 0.6072 loss_val: 1.0122 acc_val: 0.5971 time: 0.0283s\n",
      "Epoch: 02455 loss_train: 10.4657 loss_rec: 10.4657 acc_train: 0.5556 loss_val: 10.3810 acc_val: 0.5617 time: 0.0263s\n",
      "Epoch: 02456 loss_train: 3.5397 loss_rec: 3.5397 acc_train: 0.5854 loss_val: 3.5527 acc_val: 0.5900 time: 0.0273s\n",
      "Epoch: 02457 loss_train: 20.0053 loss_rec: 20.0053 acc_train: 0.4917 loss_val: 20.3320 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02458 loss_train: 20.7390 loss_rec: 20.7390 acc_train: 0.4917 loss_val: 21.0765 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 02459 loss_train: 0.9643 loss_rec: 0.9643 acc_train: 0.6120 loss_val: 0.9849 acc_val: 0.6063 time: 0.0268s\n",
      "Epoch: 02460 loss_train: 18.7559 loss_rec: 18.7559 acc_train: 0.5523 loss_val: 18.5339 acc_val: 0.5579 time: 0.0278s\n",
      "Epoch: 02461 loss_train: 18.5989 loss_rec: 18.5989 acc_train: 0.5523 loss_val: 18.3795 acc_val: 0.5575 time: 0.0263s\n",
      "Test set results: loss= 1.5377 accuracy= 0.6050\n",
      "Epoch: 02462 loss_train: 1.4315 loss_rec: 1.4315 acc_train: 0.6248 loss_val: 1.4630 acc_val: 0.6221 time: 0.0283s\n",
      "Epoch: 02463 loss_train: 28.0363 loss_rec: 28.0363 acc_train: 0.4932 loss_val: 28.4837 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02464 loss_train: 33.3532 loss_rec: 33.3532 acc_train: 0.4937 loss_val: 33.8821 acc_val: 0.4988 time: 0.0273s\n",
      "Epoch: 02465 loss_train: 16.9416 loss_rec: 16.9416 acc_train: 0.4919 loss_val: 17.2229 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02466 loss_train: 16.6291 loss_rec: 16.6291 acc_train: 0.5526 loss_val: 16.4435 acc_val: 0.5575 time: 0.0253s\n",
      "Epoch: 02467 loss_train: 28.4838 loss_rec: 28.4838 acc_train: 0.5527 loss_val: 28.1026 acc_val: 0.5538 time: 0.0273s\n",
      "Epoch: 02468 loss_train: 21.0238 loss_rec: 21.0238 acc_train: 0.5533 loss_val: 20.7626 acc_val: 0.5583 time: 0.0283s\n",
      "Epoch: 02469 loss_train: 3.0416 loss_rec: 3.0416 acc_train: 0.4927 loss_val: 3.1049 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 02470 loss_train: 6.3242 loss_rec: 6.3242 acc_train: 0.4933 loss_val: 6.4386 acc_val: 0.4983 time: 0.0283s\n",
      "Epoch: 02471 loss_train: 10.0116 loss_rec: 10.0116 acc_train: 0.5561 loss_val: 9.9346 acc_val: 0.5625 time: 0.0288s\n",
      "Test set results: loss= 8.3959 accuracy= 0.5255\n",
      "Epoch: 02472 loss_train: 7.4297 loss_rec: 7.4297 acc_train: 0.5611 loss_val: 7.3880 acc_val: 0.5642 time: 0.0258s\n",
      "Epoch: 02473 loss_train: 12.4808 loss_rec: 12.4808 acc_train: 0.4922 loss_val: 12.6934 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02474 loss_train: 10.6374 loss_rec: 10.6374 acc_train: 0.4930 loss_val: 10.8207 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 02475 loss_train: 10.3301 loss_rec: 10.3301 acc_train: 0.5550 loss_val: 10.2477 acc_val: 0.5613 time: 0.0258s\n",
      "Epoch: 02476 loss_train: 11.5043 loss_rec: 11.5043 acc_train: 0.5534 loss_val: 11.4037 acc_val: 0.5588 time: 0.0283s\n",
      "Epoch: 02477 loss_train: 4.7053 loss_rec: 4.7053 acc_train: 0.4935 loss_val: 4.7950 acc_val: 0.4983 time: 0.0268s\n",
      "Epoch: 02478 loss_train: 0.9506 loss_rec: 0.9506 acc_train: 0.6196 loss_val: 0.9727 acc_val: 0.6171 time: 0.0283s\n",
      "Epoch: 02479 loss_train: 2.8499 loss_rec: 2.8499 acc_train: 0.5917 loss_val: 2.8731 acc_val: 0.5925 time: 0.0268s\n",
      "Epoch: 02480 loss_train: 9.7500 loss_rec: 9.7500 acc_train: 0.4930 loss_val: 9.9193 acc_val: 0.4979 time: 0.0268s\n",
      "Epoch: 02481 loss_train: 1.0796 loss_rec: 1.0796 acc_train: 0.5892 loss_val: 1.1010 acc_val: 0.5837 time: 0.0273s\n",
      "Test set results: loss= 16.2776 accuracy= 0.5167\n",
      "Epoch: 02482 loss_train: 14.3794 loss_rec: 14.3794 acc_train: 0.5549 loss_val: 14.2331 acc_val: 0.5621 time: 0.0263s\n",
      "Epoch: 02483 loss_train: 10.7306 loss_rec: 10.7306 acc_train: 0.5556 loss_val: 10.6417 acc_val: 0.5617 time: 0.0263s\n",
      "Epoch: 02484 loss_train: 10.2099 loss_rec: 10.2099 acc_train: 0.4930 loss_val: 10.3866 acc_val: 0.4979 time: 0.0283s\n",
      "Epoch: 02485 loss_train: 9.6830 loss_rec: 9.6830 acc_train: 0.4930 loss_val: 9.8513 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 02486 loss_train: 10.1062 loss_rec: 10.1062 acc_train: 0.5554 loss_val: 10.0277 acc_val: 0.5625 time: 0.0273s\n",
      "Epoch: 02487 loss_train: 10.3349 loss_rec: 10.3349 acc_train: 0.5555 loss_val: 10.2526 acc_val: 0.5617 time: 0.0273s\n",
      "Epoch: 02488 loss_train: 6.8317 loss_rec: 6.8317 acc_train: 0.4932 loss_val: 6.9540 acc_val: 0.4979 time: 0.0258s\n",
      "Epoch: 02489 loss_train: 2.8777 loss_rec: 2.8777 acc_train: 0.4927 loss_val: 2.9382 acc_val: 0.4983 time: 0.0263s\n",
      "Epoch: 02490 loss_train: 18.8331 loss_rec: 18.8331 acc_train: 0.5523 loss_val: 18.6096 acc_val: 0.5579 time: 0.0283s\n",
      "Epoch: 02491 loss_train: 20.9922 loss_rec: 20.9922 acc_train: 0.5533 loss_val: 20.7316 acc_val: 0.5583 time: 0.0273s\n",
      "Test set results: loss= 5.9079 accuracy= 0.5370\n",
      "Epoch: 02492 loss_train: 5.2443 loss_rec: 5.2443 acc_train: 0.5705 loss_val: 5.2353 acc_val: 0.5750 time: 0.0263s\n",
      "Epoch: 02493 loss_train: 27.4698 loss_rec: 27.4698 acc_train: 0.4932 loss_val: 27.9085 acc_val: 0.4983 time: 0.0283s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02494 loss_train: 36.8335 loss_rec: 36.8335 acc_train: 0.4937 loss_val: 37.4156 acc_val: 0.4988 time: 0.0268s\n",
      "Epoch: 02495 loss_train: 24.1150 loss_rec: 24.1150 acc_train: 0.4916 loss_val: 24.5035 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 02496 loss_train: 6.9993 loss_rec: 6.9993 acc_train: 0.5629 loss_val: 6.9634 acc_val: 0.5671 time: 0.0278s\n",
      "Epoch: 02497 loss_train: 16.6326 loss_rec: 16.6326 acc_train: 0.5526 loss_val: 16.4470 acc_val: 0.5575 time: 0.0268s\n",
      "Epoch: 02498 loss_train: 7.6752 loss_rec: 7.6752 acc_train: 0.5609 loss_val: 7.6307 acc_val: 0.5633 time: 0.0268s\n",
      "Epoch: 02499 loss_train: 18.4544 loss_rec: 18.4544 acc_train: 0.4919 loss_val: 18.7586 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02500 loss_train: 22.2561 loss_rec: 22.2561 acc_train: 0.4917 loss_val: 22.6169 acc_val: 0.4967 time: 0.0263s\n",
      "Epoch: 02501 loss_train: 4.6557 loss_rec: 4.6557 acc_train: 0.4927 loss_val: 4.7448 acc_val: 0.4983 time: 0.0273s\n",
      "Test set results: loss= 32.6062 accuracy= 0.5078\n",
      "Epoch: 02502 loss_train: 28.7944 loss_rec: 28.7944 acc_train: 0.5516 loss_val: 28.4091 acc_val: 0.5529 time: 0.0263s\n",
      "Epoch: 02503 loss_train: 41.4442 loss_rec: 41.4442 acc_train: 0.5437 loss_val: 40.8889 acc_val: 0.5479 time: 0.0268s\n",
      "Epoch: 02504 loss_train: 34.5486 loss_rec: 34.5486 acc_train: 0.5483 loss_val: 34.0805 acc_val: 0.5529 time: 0.0263s\n",
      "Epoch: 02505 loss_train: 10.3343 loss_rec: 10.3343 acc_train: 0.5555 loss_val: 10.2518 acc_val: 0.5617 time: 0.0278s\n",
      "Epoch: 02506 loss_train: 30.5422 loss_rec: 30.5422 acc_train: 0.4932 loss_val: 31.0283 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 02507 loss_train: 47.9104 loss_rec: 47.9104 acc_train: 0.4937 loss_val: 48.6620 acc_val: 0.4988 time: 0.0283s\n",
      "Epoch: 02508 loss_train: 42.4739 loss_rec: 42.4739 acc_train: 0.4937 loss_val: 43.1424 acc_val: 0.4988 time: 0.0263s\n",
      "Epoch: 02509 loss_train: 16.5513 loss_rec: 16.5513 acc_train: 0.4920 loss_val: 16.8269 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02510 loss_train: 24.6791 loss_rec: 24.6791 acc_train: 0.5516 loss_val: 24.3562 acc_val: 0.5567 time: 0.0268s\n",
      "Epoch: 02511 loss_train: 43.6879 loss_rec: 43.6879 acc_train: 0.5445 loss_val: 43.1060 acc_val: 0.5488 time: 0.0268s\n",
      "Test set results: loss= 48.2305 accuracy= 0.4999\n",
      "Epoch: 02512 loss_train: 42.5641 loss_rec: 42.5641 acc_train: 0.5445 loss_val: 41.9954 acc_val: 0.5483 time: 0.0273s\n",
      "Epoch: 02513 loss_train: 23.3801 loss_rec: 23.3801 acc_train: 0.5537 loss_val: 23.0790 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 02514 loss_train: 11.5868 loss_rec: 11.5868 acc_train: 0.4929 loss_val: 11.7852 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 02515 loss_train: 24.7865 loss_rec: 24.7865 acc_train: 0.4916 loss_val: 25.1854 acc_val: 0.4967 time: 0.0263s\n",
      "Epoch: 02516 loss_train: 15.7566 loss_rec: 15.7566 acc_train: 0.4922 loss_val: 16.0203 acc_val: 0.4971 time: 0.0258s\n",
      "Epoch: 02517 loss_train: 11.3597 loss_rec: 11.3597 acc_train: 0.5534 loss_val: 11.2615 acc_val: 0.5588 time: 0.0283s\n",
      "Epoch: 02518 loss_train: 17.8534 loss_rec: 17.8534 acc_train: 0.5527 loss_val: 17.6472 acc_val: 0.5575 time: 0.0273s\n",
      "Epoch: 02519 loss_train: 6.1487 loss_rec: 6.1487 acc_train: 0.5666 loss_val: 6.1258 acc_val: 0.5725 time: 0.0278s\n",
      "Epoch: 02520 loss_train: 22.4666 loss_rec: 22.4666 acc_train: 0.4917 loss_val: 22.8308 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 02521 loss_train: 28.5206 loss_rec: 28.5206 acc_train: 0.4932 loss_val: 28.9760 acc_val: 0.4983 time: 0.0263s\n",
      "Test set results: loss= 11.6630 accuracy= 0.5479\n",
      "Epoch: 02522 loss_train: 13.0570 loss_rec: 13.0570 acc_train: 0.4922 loss_val: 13.2791 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02523 loss_train: 19.0707 loss_rec: 19.0707 acc_train: 0.5511 loss_val: 18.8436 acc_val: 0.5571 time: 0.0283s\n",
      "Epoch: 02524 loss_train: 30.1816 loss_rec: 30.1816 acc_train: 0.5503 loss_val: 29.7772 acc_val: 0.5525 time: 0.0268s\n",
      "Epoch: 02525 loss_train: 22.2720 loss_rec: 22.2720 acc_train: 0.5528 loss_val: 21.9900 acc_val: 0.5571 time: 0.0263s\n",
      "Epoch: 02526 loss_train: 1.8928 loss_rec: 1.8928 acc_train: 0.4927 loss_val: 1.9347 acc_val: 0.4983 time: 0.0283s\n",
      "Epoch: 02527 loss_train: 7.4867 loss_rec: 7.4867 acc_train: 0.4932 loss_val: 7.6196 acc_val: 0.4979 time: 0.0263s\n",
      "Epoch: 02528 loss_train: 6.7879 loss_rec: 6.7879 acc_train: 0.5645 loss_val: 6.7551 acc_val: 0.5708 time: 0.0273s\n",
      "Epoch: 02529 loss_train: 3.1067 loss_rec: 3.1067 acc_train: 0.5896 loss_val: 3.1266 acc_val: 0.5933 time: 0.0278s\n",
      "Epoch: 02530 loss_train: 16.5160 loss_rec: 16.5160 acc_train: 0.4922 loss_val: 16.7913 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 02531 loss_train: 14.0440 loss_rec: 14.0440 acc_train: 0.4922 loss_val: 14.2812 acc_val: 0.4971 time: 0.0268s\n",
      "Test set results: loss= 8.4243 accuracy= 0.5263\n",
      "Epoch: 02532 loss_train: 7.4547 loss_rec: 7.4547 acc_train: 0.5614 loss_val: 7.4131 acc_val: 0.5654 time: 0.0268s\n",
      "Epoch: 02533 loss_train: 9.4153 loss_rec: 9.4153 acc_train: 0.5573 loss_val: 9.3485 acc_val: 0.5625 time: 0.0273s\n",
      "Epoch: 02534 loss_train: 5.6387 loss_rec: 5.6387 acc_train: 0.4934 loss_val: 5.7429 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02535 loss_train: 0.9515 loss_rec: 0.9515 acc_train: 0.6182 loss_val: 0.9732 acc_val: 0.6133 time: 0.0268s\n",
      "Epoch: 02536 loss_train: 3.9448 loss_rec: 3.9448 acc_train: 0.5822 loss_val: 3.9525 acc_val: 0.5854 time: 0.0278s\n",
      "Epoch: 02537 loss_train: 8.0257 loss_rec: 8.0257 acc_train: 0.4932 loss_val: 8.1674 acc_val: 0.4979 time: 0.0263s\n",
      "Epoch: 02538 loss_train: 1.2514 loss_rec: 1.2514 acc_train: 0.6269 loss_val: 1.2815 acc_val: 0.6154 time: 0.0258s\n",
      "Epoch: 02539 loss_train: 1.0430 loss_rec: 1.0430 acc_train: 0.5817 loss_val: 1.0638 acc_val: 0.5746 time: 0.0283s\n",
      "Epoch: 02540 loss_train: 4.3952 loss_rec: 4.3952 acc_train: 0.5765 loss_val: 4.3975 acc_val: 0.5800 time: 0.0278s\n",
      "Epoch: 02541 loss_train: 6.8196 loss_rec: 6.8196 acc_train: 0.4933 loss_val: 6.9419 acc_val: 0.4979 time: 0.0273s\n",
      "Test set results: loss= 2.6425 accuracy= 0.5758\n",
      "Epoch: 02542 loss_train: 2.3820 loss_rec: 2.3820 acc_train: 0.5993 loss_val: 2.4096 acc_val: 0.5975 time: 0.0278s\n",
      "Epoch: 02543 loss_train: 3.6946 loss_rec: 3.6946 acc_train: 0.4927 loss_val: 3.7685 acc_val: 0.4983 time: 0.0263s\n",
      "Epoch: 02544 loss_train: 9.3443 loss_rec: 9.3443 acc_train: 0.5573 loss_val: 9.2784 acc_val: 0.5625 time: 0.0278s\n",
      "Epoch: 02545 loss_train: 4.5211 loss_rec: 4.5211 acc_train: 0.5749 loss_val: 4.5219 acc_val: 0.5779 time: 0.0273s\n",
      "Epoch: 02546 loss_train: 17.0028 loss_rec: 17.0028 acc_train: 0.4920 loss_val: 17.2853 acc_val: 0.4971 time: 0.0283s\n",
      "Epoch: 02547 loss_train: 16.5512 loss_rec: 16.5512 acc_train: 0.4922 loss_val: 16.8270 acc_val: 0.4971 time: 0.0258s\n",
      "Epoch: 02548 loss_train: 3.7372 loss_rec: 3.7372 acc_train: 0.5832 loss_val: 3.7476 acc_val: 0.5854 time: 0.0273s\n",
      "Epoch: 02549 loss_train: 5.3936 loss_rec: 5.3936 acc_train: 0.5688 loss_val: 5.3824 acc_val: 0.5733 time: 0.0263s\n",
      "Epoch: 02550 loss_train: 9.6962 loss_rec: 9.6962 acc_train: 0.4930 loss_val: 9.8648 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 02551 loss_train: 3.6480 loss_rec: 3.6480 acc_train: 0.4927 loss_val: 3.7212 acc_val: 0.4983 time: 0.0268s\n",
      "Test set results: loss= 22.4049 accuracy= 0.5125\n",
      "Epoch: 02552 loss_train: 19.7898 loss_rec: 19.7898 acc_train: 0.5523 loss_val: 19.5502 acc_val: 0.5575 time: 0.0283s\n",
      "Epoch: 02553 loss_train: 23.7382 loss_rec: 23.7382 acc_train: 0.5530 loss_val: 23.4309 acc_val: 0.5579 time: 0.0283s\n",
      "Epoch: 02554 loss_train: 9.6810 loss_rec: 9.6810 acc_train: 0.5569 loss_val: 9.6099 acc_val: 0.5621 time: 0.0273s\n",
      "Epoch: 02555 loss_train: 21.1095 loss_rec: 21.1095 acc_train: 0.4917 loss_val: 21.4530 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 02556 loss_train: 29.6164 loss_rec: 29.6164 acc_train: 0.4932 loss_val: 30.0888 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 02557 loss_train: 16.4998 loss_rec: 16.4998 acc_train: 0.4922 loss_val: 16.7750 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02558 loss_train: 13.9034 loss_rec: 13.9034 acc_train: 0.5549 loss_val: 13.7655 acc_val: 0.5621 time: 0.0263s\n",
      "Epoch: 02559 loss_train: 23.3231 loss_rec: 23.3231 acc_train: 0.5537 loss_val: 23.0232 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 02560 loss_train: 14.1635 loss_rec: 14.1635 acc_train: 0.5549 loss_val: 14.0213 acc_val: 0.5621 time: 0.0273s\n",
      "Epoch: 02561 loss_train: 11.7099 loss_rec: 11.7099 acc_train: 0.4929 loss_val: 11.9105 acc_val: 0.4975 time: 0.0263s\n",
      "Test set results: loss= 14.5251 accuracy= 0.5476\n",
      "Epoch: 02562 loss_train: 16.2428 loss_rec: 16.2428 acc_train: 0.4922 loss_val: 16.5141 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02563 loss_train: 0.9961 loss_rec: 0.9961 acc_train: 0.6280 loss_val: 1.0209 acc_val: 0.6138 time: 0.0263s\n",
      "Epoch: 02564 loss_train: 9.1964 loss_rec: 9.1964 acc_train: 0.5573 loss_val: 9.1325 acc_val: 0.5617 time: 0.0258s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02565 loss_train: 1.5690 loss_rec: 1.5690 acc_train: 0.6202 loss_val: 1.6008 acc_val: 0.6196 time: 0.0278s\n",
      "Epoch: 02566 loss_train: 18.9540 loss_rec: 18.9540 acc_train: 0.4919 loss_val: 19.2661 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02567 loss_train: 16.8255 loss_rec: 16.8255 acc_train: 0.4920 loss_val: 17.1057 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 02568 loss_train: 4.7188 loss_rec: 4.7188 acc_train: 0.5734 loss_val: 4.7171 acc_val: 0.5775 time: 0.0278s\n",
      "Epoch: 02569 loss_train: 7.0314 loss_rec: 7.0314 acc_train: 0.5617 loss_val: 6.9950 acc_val: 0.5658 time: 0.0263s\n",
      "Epoch: 02570 loss_train: 7.5034 loss_rec: 7.5034 acc_train: 0.4932 loss_val: 7.6367 acc_val: 0.4979 time: 0.0253s\n",
      "Epoch: 02571 loss_train: 1.4180 loss_rec: 1.4180 acc_train: 0.4927 loss_val: 1.4480 acc_val: 0.4983 time: 0.0283s\n",
      "Test set results: loss= 19.4360 accuracy= 0.5123\n",
      "Epoch: 02572 loss_train: 17.1685 loss_rec: 17.1685 acc_train: 0.5521 loss_val: 16.9739 acc_val: 0.5571 time: 0.0263s\n",
      "Epoch: 02573 loss_train: 16.8438 loss_rec: 16.8438 acc_train: 0.5520 loss_val: 16.6548 acc_val: 0.5571 time: 0.0278s\n",
      "Epoch: 02574 loss_train: 0.9560 loss_rec: 0.9560 acc_train: 0.6108 loss_val: 0.9769 acc_val: 0.6063 time: 0.0273s\n",
      "Epoch: 02575 loss_train: 15.8732 loss_rec: 15.8732 acc_train: 0.4922 loss_val: 16.1387 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 02576 loss_train: 9.3531 loss_rec: 9.3531 acc_train: 0.4932 loss_val: 9.5164 acc_val: 0.4979 time: 0.0278s\n",
      "Epoch: 02577 loss_train: 14.9227 loss_rec: 14.9227 acc_train: 0.5538 loss_val: 14.7670 acc_val: 0.5608 time: 0.0268s\n",
      "Epoch: 02578 loss_train: 19.3498 loss_rec: 19.3498 acc_train: 0.5517 loss_val: 19.1180 acc_val: 0.5575 time: 0.0268s\n",
      "Epoch: 02579 loss_train: 5.9297 loss_rec: 5.9297 acc_train: 0.5668 loss_val: 5.9103 acc_val: 0.5721 time: 0.0273s\n",
      "Epoch: 02580 loss_train: 24.1884 loss_rec: 24.1884 acc_train: 0.4916 loss_val: 24.5790 acc_val: 0.4967 time: 0.0263s\n",
      "Epoch: 02581 loss_train: 31.7274 loss_rec: 31.7274 acc_train: 0.4932 loss_val: 32.2323 acc_val: 0.4983 time: 0.0283s\n",
      "Test set results: loss= 15.9307 accuracy= 0.5475\n",
      "Epoch: 02582 loss_train: 17.8074 loss_rec: 17.8074 acc_train: 0.4919 loss_val: 18.1025 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02583 loss_train: 13.2925 loss_rec: 13.2925 acc_train: 0.5509 loss_val: 13.1648 acc_val: 0.5558 time: 0.0273s\n",
      "Epoch: 02584 loss_train: 23.3641 loss_rec: 23.3641 acc_train: 0.5537 loss_val: 23.0634 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 02585 loss_train: 14.8517 loss_rec: 14.8517 acc_train: 0.5538 loss_val: 14.6975 acc_val: 0.5608 time: 0.0258s\n",
      "Epoch: 02586 loss_train: 10.2754 loss_rec: 10.2754 acc_train: 0.4930 loss_val: 10.4534 acc_val: 0.4979 time: 0.0258s\n",
      "Epoch: 02587 loss_train: 14.2616 loss_rec: 14.2616 acc_train: 0.4922 loss_val: 14.5024 acc_val: 0.4971 time: 0.0283s\n",
      "Epoch: 02588 loss_train: 2.2809 loss_rec: 2.2809 acc_train: 0.6034 loss_val: 2.3092 acc_val: 0.6046 time: 0.0283s\n",
      "Epoch: 02589 loss_train: 2.5497 loss_rec: 2.5497 acc_train: 0.5976 loss_val: 2.5758 acc_val: 0.5983 time: 0.0273s\n",
      "Epoch: 02590 loss_train: 12.0378 loss_rec: 12.0378 acc_train: 0.4927 loss_val: 12.2435 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 02591 loss_train: 5.1395 loss_rec: 5.1395 acc_train: 0.4927 loss_val: 5.2363 acc_val: 0.4983 time: 0.0263s\n",
      "Test set results: loss= 21.5974 accuracy= 0.5115\n",
      "Epoch: 02592 loss_train: 19.0767 loss_rec: 19.0767 acc_train: 0.5517 loss_val: 18.8494 acc_val: 0.5575 time: 0.0278s\n",
      "Epoch: 02593 loss_train: 23.7566 loss_rec: 23.7566 acc_train: 0.5530 loss_val: 23.4493 acc_val: 0.5579 time: 0.0273s\n",
      "Epoch: 02594 loss_train: 10.4297 loss_rec: 10.4297 acc_train: 0.5549 loss_val: 10.3456 acc_val: 0.5617 time: 0.0263s\n",
      "Epoch: 02595 loss_train: 19.5072 loss_rec: 19.5072 acc_train: 0.4918 loss_val: 19.8276 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02596 loss_train: 27.4608 loss_rec: 27.4608 acc_train: 0.4932 loss_val: 27.9003 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 02597 loss_train: 13.9859 loss_rec: 13.9859 acc_train: 0.4922 loss_val: 14.2225 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02598 loss_train: 16.3710 loss_rec: 16.3710 acc_train: 0.5520 loss_val: 16.1903 acc_val: 0.5571 time: 0.0278s\n",
      "Epoch: 02599 loss_train: 26.0115 loss_rec: 26.0115 acc_train: 0.5527 loss_val: 25.6662 acc_val: 0.5563 time: 0.0273s\n",
      "Epoch: 02600 loss_train: 17.1432 loss_rec: 17.1432 acc_train: 0.5521 loss_val: 16.9491 acc_val: 0.5571 time: 0.0268s\n",
      "Epoch: 02601 loss_train: 8.0728 loss_rec: 8.0728 acc_train: 0.4932 loss_val: 8.2153 acc_val: 0.4979 time: 0.0283s\n",
      "Test set results: loss= 11.0681 accuracy= 0.5479\n",
      "Epoch: 02602 loss_train: 12.3956 loss_rec: 12.3956 acc_train: 0.4925 loss_val: 12.6070 acc_val: 0.4971 time: 0.0258s\n",
      "Epoch: 02603 loss_train: 3.4211 loss_rec: 3.4211 acc_train: 0.5862 loss_val: 3.4364 acc_val: 0.5904 time: 0.0268s\n",
      "Epoch: 02604 loss_train: 2.1276 loss_rec: 2.1276 acc_train: 0.6088 loss_val: 2.1571 acc_val: 0.6058 time: 0.0268s\n",
      "Epoch: 02605 loss_train: 13.4877 loss_rec: 13.4877 acc_train: 0.4922 loss_val: 13.7165 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02606 loss_train: 7.3812 loss_rec: 7.3812 acc_train: 0.4932 loss_val: 7.5125 acc_val: 0.4979 time: 0.0268s\n",
      "Epoch: 02607 loss_train: 16.2964 loss_rec: 16.2964 acc_train: 0.5520 loss_val: 16.1166 acc_val: 0.5571 time: 0.0273s\n",
      "Epoch: 02608 loss_train: 20.4300 loss_rec: 20.4300 acc_train: 0.5523 loss_val: 20.1796 acc_val: 0.5575 time: 0.0263s\n",
      "Epoch: 02609 loss_train: 6.7906 loss_rec: 6.7906 acc_train: 0.5645 loss_val: 6.7578 acc_val: 0.5708 time: 0.0278s\n",
      "Epoch: 02610 loss_train: 23.4234 loss_rec: 23.4234 acc_train: 0.4917 loss_val: 23.8023 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 02611 loss_train: 31.2942 loss_rec: 31.2942 acc_train: 0.4932 loss_val: 31.7927 acc_val: 0.4983 time: 0.0263s\n",
      "Test set results: loss= 15.9288 accuracy= 0.5476\n",
      "Epoch: 02612 loss_train: 17.8057 loss_rec: 17.8057 acc_train: 0.4919 loss_val: 18.1009 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02613 loss_train: 12.8166 loss_rec: 12.8166 acc_train: 0.5523 loss_val: 12.6969 acc_val: 0.5563 time: 0.0278s\n",
      "Epoch: 02614 loss_train: 22.5823 loss_rec: 22.5823 acc_train: 0.5537 loss_val: 22.2950 acc_val: 0.5579 time: 0.0288s\n",
      "Epoch: 02615 loss_train: 13.9305 loss_rec: 13.9305 acc_train: 0.5549 loss_val: 13.7924 acc_val: 0.5621 time: 0.0392s\n",
      "Epoch: 02616 loss_train: 11.2413 loss_rec: 11.2413 acc_train: 0.4930 loss_val: 11.4346 acc_val: 0.4975 time: 0.0243s\n",
      "Epoch: 02617 loss_train: 15.3571 loss_rec: 15.3571 acc_train: 0.4922 loss_val: 15.6150 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02618 loss_train: 1.4152 loss_rec: 1.4152 acc_train: 0.6258 loss_val: 1.4466 acc_val: 0.6233 time: 0.0372s\n",
      "Epoch: 02619 loss_train: 4.2968 loss_rec: 4.2968 acc_train: 0.5773 loss_val: 4.3003 acc_val: 0.5804 time: 0.0377s\n",
      "Epoch: 02620 loss_train: 8.5037 loss_rec: 8.5037 acc_train: 0.4932 loss_val: 8.6533 acc_val: 0.4979 time: 0.0353s\n",
      "Epoch: 02621 loss_train: 1.0203 loss_rec: 1.0203 acc_train: 0.5803 loss_val: 1.0409 acc_val: 0.5721 time: 0.0358s\n",
      "Test set results: loss= 12.6325 accuracy= 0.5170\n",
      "Epoch: 02622 loss_train: 11.1606 loss_rec: 11.1606 acc_train: 0.5533 loss_val: 11.0653 acc_val: 0.5588 time: 0.0343s\n",
      "Epoch: 02623 loss_train: 5.3499 loss_rec: 5.3499 acc_train: 0.5688 loss_val: 5.3393 acc_val: 0.5733 time: 0.0353s\n",
      "Epoch: 02624 loss_train: 17.0886 loss_rec: 17.0886 acc_train: 0.4922 loss_val: 17.3730 acc_val: 0.4971 time: 0.0343s\n",
      "Epoch: 02625 loss_train: 17.8583 loss_rec: 17.8583 acc_train: 0.4919 loss_val: 18.1545 acc_val: 0.4971 time: 0.0353s\n",
      "Epoch: 02626 loss_train: 1.7055 loss_rec: 1.7055 acc_train: 0.6166 loss_val: 1.7370 acc_val: 0.6154 time: 0.0353s\n",
      "Epoch: 02627 loss_train: 5.5798 loss_rec: 5.5798 acc_train: 0.5691 loss_val: 5.5657 acc_val: 0.5733 time: 0.0353s\n",
      "Epoch: 02628 loss_train: 6.6763 loss_rec: 6.6763 acc_train: 0.4933 loss_val: 6.7966 acc_val: 0.4983 time: 0.0348s\n",
      "Epoch: 02629 loss_train: 1.5409 loss_rec: 1.5409 acc_train: 0.6213 loss_val: 1.5728 acc_val: 0.6204 time: 0.0293s\n",
      "Epoch: 02630 loss_train: 3.0126 loss_rec: 3.0126 acc_train: 0.4927 loss_val: 3.0758 acc_val: 0.4983 time: 0.0263s\n",
      "Epoch: 02631 loss_train: 11.1078 loss_rec: 11.1078 acc_train: 0.5533 loss_val: 11.0132 acc_val: 0.5588 time: 0.0283s\n",
      "Test set results: loss= 8.1887 accuracy= 0.5271\n",
      "Epoch: 02632 loss_train: 7.2476 loss_rec: 7.2476 acc_train: 0.5618 loss_val: 7.2083 acc_val: 0.5658 time: 0.0273s\n",
      "Epoch: 02633 loss_train: 13.3377 loss_rec: 13.3377 acc_train: 0.4922 loss_val: 13.5642 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 02634 loss_train: 12.7212 loss_rec: 12.7212 acc_train: 0.4923 loss_val: 12.9378 acc_val: 0.4971 time: 0.0288s\n",
      "Epoch: 02635 loss_train: 6.8772 loss_rec: 6.8772 acc_train: 0.5640 loss_val: 6.8430 acc_val: 0.5700 time: 0.0278s\n",
      "Epoch: 02636 loss_train: 7.5927 loss_rec: 7.5927 acc_train: 0.5613 loss_val: 7.5489 acc_val: 0.5642 time: 0.0258s\n",
      "Epoch: 02637 loss_train: 8.4219 loss_rec: 8.4219 acc_train: 0.4932 loss_val: 8.5702 acc_val: 0.4979 time: 0.0293s\n",
      "Epoch: 02638 loss_train: 3.8069 loss_rec: 3.8069 acc_train: 0.4927 loss_val: 3.8829 acc_val: 0.4983 time: 0.0283s\n",
      "Epoch: 02639 loss_train: 18.1963 loss_rec: 18.1963 acc_train: 0.5521 loss_val: 17.9839 acc_val: 0.5571 time: 0.0253s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02640 loss_train: 21.1289 loss_rec: 21.1289 acc_train: 0.5533 loss_val: 20.8664 acc_val: 0.5583 time: 0.0288s\n",
      "Epoch: 02641 loss_train: 6.5204 loss_rec: 6.5204 acc_train: 0.5657 loss_val: 6.4919 acc_val: 0.5713 time: 0.0283s\n",
      "Test set results: loss= 22.0075 accuracy= 0.5471\n",
      "Epoch: 02642 loss_train: 24.5711 loss_rec: 24.5711 acc_train: 0.4917 loss_val: 24.9680 acc_val: 0.4967 time: 0.0268s\n",
      "Epoch: 02643 loss_train: 33.3209 loss_rec: 33.3209 acc_train: 0.4932 loss_val: 33.8506 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02644 loss_train: 20.6802 loss_rec: 20.6802 acc_train: 0.4917 loss_val: 21.0182 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02645 loss_train: 9.4400 loss_rec: 9.4400 acc_train: 0.5567 loss_val: 9.3728 acc_val: 0.5621 time: 0.0273s\n",
      "Epoch: 02646 loss_train: 18.7829 loss_rec: 18.7829 acc_train: 0.5517 loss_val: 18.5608 acc_val: 0.5571 time: 0.0268s\n",
      "Epoch: 02647 loss_train: 9.9330 loss_rec: 9.9330 acc_train: 0.5555 loss_val: 9.8576 acc_val: 0.5625 time: 0.0268s\n",
      "Epoch: 02648 loss_train: 15.4699 loss_rec: 15.4699 acc_train: 0.4922 loss_val: 15.7296 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 02649 loss_train: 19.5168 loss_rec: 19.5168 acc_train: 0.4919 loss_val: 19.8378 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02650 loss_train: 2.7758 loss_rec: 2.7758 acc_train: 0.4927 loss_val: 2.8349 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02651 loss_train: 28.8807 loss_rec: 28.8807 acc_train: 0.5516 loss_val: 28.4940 acc_val: 0.5529 time: 0.0283s\n",
      "Test set results: loss= 45.9569 accuracy= 0.4989\n",
      "Epoch: 02652 loss_train: 40.5636 loss_rec: 40.5636 acc_train: 0.5434 loss_val: 40.0177 acc_val: 0.5479 time: 0.0283s\n",
      "Epoch: 02653 loss_train: 33.3674 loss_rec: 33.3674 acc_train: 0.5478 loss_val: 32.9165 acc_val: 0.5538 time: 0.0258s\n",
      "Epoch: 02654 loss_train: 9.4628 loss_rec: 9.4628 acc_train: 0.5567 loss_val: 9.3952 acc_val: 0.5621 time: 0.0278s\n",
      "Epoch: 02655 loss_train: 30.5428 loss_rec: 30.5428 acc_train: 0.4932 loss_val: 31.0300 acc_val: 0.4983 time: 0.0283s\n",
      "Epoch: 02656 loss_train: 47.6588 loss_rec: 47.6588 acc_train: 0.4931 loss_val: 48.4079 acc_val: 0.4983 time: 0.0258s\n",
      "Epoch: 02657 loss_train: 42.6394 loss_rec: 42.6394 acc_train: 0.4931 loss_val: 43.3118 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02658 loss_train: 17.6968 loss_rec: 17.6968 acc_train: 0.4920 loss_val: 17.9907 acc_val: 0.4971 time: 0.0283s\n",
      "Epoch: 02659 loss_train: 22.2220 loss_rec: 22.2220 acc_train: 0.5528 loss_val: 21.9412 acc_val: 0.5571 time: 0.0258s\n",
      "Epoch: 02660 loss_train: 40.4917 loss_rec: 40.4917 acc_train: 0.5434 loss_val: 39.9469 acc_val: 0.5479 time: 0.0278s\n",
      "Epoch: 02661 loss_train: 39.3101 loss_rec: 39.3101 acc_train: 0.5454 loss_val: 38.7788 acc_val: 0.5504 time: 0.0273s\n",
      "Test set results: loss= 23.4494 accuracy= 0.5124\n",
      "Epoch: 02662 loss_train: 20.7114 loss_rec: 20.7114 acc_train: 0.5532 loss_val: 20.4564 acc_val: 0.5583 time: 0.0283s\n",
      "Epoch: 02663 loss_train: 13.3534 loss_rec: 13.3534 acc_train: 0.4922 loss_val: 13.5803 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 02664 loss_train: 26.1654 loss_rec: 26.1654 acc_train: 0.4916 loss_val: 26.5865 acc_val: 0.4967 time: 0.0283s\n",
      "Epoch: 02665 loss_train: 17.3743 loss_rec: 17.3743 acc_train: 0.4921 loss_val: 17.6634 acc_val: 0.4971 time: 0.0258s\n",
      "Epoch: 02666 loss_train: 9.1049 loss_rec: 9.1049 acc_train: 0.5573 loss_val: 9.0424 acc_val: 0.5617 time: 0.0278s\n",
      "Epoch: 02667 loss_train: 15.6107 loss_rec: 15.6107 acc_train: 0.5530 loss_val: 15.4432 acc_val: 0.5596 time: 0.0283s\n",
      "Epoch: 02668 loss_train: 4.6078 loss_rec: 4.6078 acc_train: 0.5749 loss_val: 4.6074 acc_val: 0.5779 time: 0.0273s\n",
      "Epoch: 02669 loss_train: 22.5662 loss_rec: 22.5662 acc_train: 0.4917 loss_val: 22.9325 acc_val: 0.4967 time: 0.0263s\n",
      "Epoch: 02670 loss_train: 27.7322 loss_rec: 27.7322 acc_train: 0.4915 loss_val: 28.1767 acc_val: 0.4967 time: 0.0268s\n",
      "Epoch: 02671 loss_train: 12.1177 loss_rec: 12.1177 acc_train: 0.4929 loss_val: 12.3248 acc_val: 0.4975 time: 0.0273s\n",
      "Test set results: loss= 22.0670 accuracy= 0.5108\n",
      "Epoch: 02672 loss_train: 19.4910 loss_rec: 19.4910 acc_train: 0.5503 loss_val: 19.2569 acc_val: 0.5558 time: 0.0273s\n",
      "Epoch: 02673 loss_train: 30.7128 loss_rec: 30.7128 acc_train: 0.5503 loss_val: 30.3005 acc_val: 0.5529 time: 0.0268s\n",
      "Epoch: 02674 loss_train: 23.4530 loss_rec: 23.4530 acc_train: 0.5532 loss_val: 23.1511 acc_val: 0.5579 time: 0.0278s\n",
      "Epoch: 02675 loss_train: 1.0785 loss_rec: 1.0785 acc_train: 0.6240 loss_val: 1.1056 acc_val: 0.6150 time: 0.0258s\n",
      "Epoch: 02676 loss_train: 29.2927 loss_rec: 29.2927 acc_train: 0.4932 loss_val: 29.7610 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02677 loss_train: 36.0653 loss_rec: 36.0653 acc_train: 0.4932 loss_val: 36.6375 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02678 loss_train: 21.9348 loss_rec: 21.9348 acc_train: 0.4917 loss_val: 22.2916 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02679 loss_train: 9.2535 loss_rec: 9.2535 acc_train: 0.5566 loss_val: 9.1888 acc_val: 0.5617 time: 0.0263s\n",
      "Epoch: 02680 loss_train: 19.7478 loss_rec: 19.7478 acc_train: 0.5523 loss_val: 19.5090 acc_val: 0.5575 time: 0.0278s\n",
      "Epoch: 02681 loss_train: 12.1343 loss_rec: 12.1343 acc_train: 0.5527 loss_val: 12.0247 acc_val: 0.5563 time: 0.0258s\n",
      "Test set results: loss= 10.4688 accuracy= 0.5483\n",
      "Epoch: 02682 loss_train: 11.7293 loss_rec: 11.7293 acc_train: 0.4930 loss_val: 11.9304 acc_val: 0.4975 time: 0.0263s\n",
      "Epoch: 02683 loss_train: 14.8840 loss_rec: 14.8840 acc_train: 0.4922 loss_val: 15.1348 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02684 loss_train: 2.0222 loss_rec: 2.0222 acc_train: 0.6094 loss_val: 2.0524 acc_val: 0.6075 time: 0.0278s\n",
      "Epoch: 02685 loss_train: 3.2678 loss_rec: 3.2678 acc_train: 0.5871 loss_val: 3.2855 acc_val: 0.5921 time: 0.0278s\n",
      "Epoch: 02686 loss_train: 10.5766 loss_rec: 10.5766 acc_train: 0.4930 loss_val: 10.7595 acc_val: 0.4979 time: 0.0263s\n",
      "Epoch: 02687 loss_train: 3.7094 loss_rec: 3.7094 acc_train: 0.4927 loss_val: 3.7840 acc_val: 0.4983 time: 0.0263s\n",
      "Epoch: 02688 loss_train: 19.8223 loss_rec: 19.8223 acc_train: 0.5523 loss_val: 19.5825 acc_val: 0.5575 time: 0.0273s\n",
      "Epoch: 02689 loss_train: 24.4091 loss_rec: 24.4091 acc_train: 0.5516 loss_val: 24.0908 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 02690 loss_train: 11.4457 loss_rec: 11.4457 acc_train: 0.5529 loss_val: 11.3462 acc_val: 0.5575 time: 0.0273s\n",
      "Epoch: 02691 loss_train: 17.6223 loss_rec: 17.6223 acc_train: 0.4921 loss_val: 17.9154 acc_val: 0.4971 time: 0.0263s\n",
      "Test set results: loss= 22.7762 accuracy= 0.5471\n",
      "Epoch: 02692 loss_train: 25.4273 loss_rec: 25.4273 acc_train: 0.4917 loss_val: 25.8375 acc_val: 0.4967 time: 0.0283s\n",
      "Epoch: 02693 loss_train: 12.3597 loss_rec: 12.3597 acc_train: 0.4929 loss_val: 12.5707 acc_val: 0.4975 time: 0.0283s\n",
      "Epoch: 02694 loss_train: 17.0305 loss_rec: 17.0305 acc_train: 0.5520 loss_val: 16.8388 acc_val: 0.5571 time: 0.0273s\n",
      "Epoch: 02695 loss_train: 26.3563 loss_rec: 26.3563 acc_train: 0.5523 loss_val: 26.0056 acc_val: 0.5558 time: 0.0278s\n",
      "Epoch: 02696 loss_train: 17.6530 loss_rec: 17.6530 acc_train: 0.5521 loss_val: 17.4505 acc_val: 0.5571 time: 0.0263s\n",
      "Epoch: 02697 loss_train: 6.8852 loss_rec: 6.8852 acc_train: 0.4925 loss_val: 7.0089 acc_val: 0.4983 time: 0.0258s\n",
      "Epoch: 02698 loss_train: 11.1753 loss_rec: 11.1753 acc_train: 0.4930 loss_val: 11.3676 acc_val: 0.4979 time: 0.0278s\n",
      "Epoch: 02699 loss_train: 4.0045 loss_rec: 4.0045 acc_train: 0.5809 loss_val: 4.0115 acc_val: 0.5846 time: 0.0273s\n",
      "Epoch: 02700 loss_train: 2.2846 loss_rec: 2.2846 acc_train: 0.6033 loss_val: 2.3129 acc_val: 0.6046 time: 0.0278s\n",
      "Epoch: 02701 loss_train: 13.7060 loss_rec: 13.7060 acc_train: 0.4922 loss_val: 13.9385 acc_val: 0.4971 time: 0.0263s\n",
      "Test set results: loss= 7.5984 accuracy= 0.5486\n",
      "Epoch: 02702 loss_train: 8.5343 loss_rec: 8.5343 acc_train: 0.4932 loss_val: 8.6844 acc_val: 0.4979 time: 0.0258s\n",
      "Epoch: 02703 loss_train: 13.9613 loss_rec: 13.9613 acc_train: 0.5543 loss_val: 13.8226 acc_val: 0.5617 time: 0.0273s\n",
      "Epoch: 02704 loss_train: 17.4240 loss_rec: 17.4240 acc_train: 0.5521 loss_val: 17.2255 acc_val: 0.5571 time: 0.0273s\n",
      "Epoch: 02705 loss_train: 3.8680 loss_rec: 3.8680 acc_train: 0.5826 loss_val: 3.8766 acc_val: 0.5854 time: 0.0273s\n",
      "Epoch: 02706 loss_train: 25.3314 loss_rec: 25.3314 acc_train: 0.4917 loss_val: 25.7402 acc_val: 0.4967 time: 0.0263s\n",
      "Epoch: 02707 loss_train: 32.4156 loss_rec: 32.4156 acc_train: 0.4932 loss_val: 32.9323 acc_val: 0.4983 time: 0.0273s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02708 loss_train: 18.7574 loss_rec: 18.7574 acc_train: 0.4919 loss_val: 19.0677 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 02709 loss_train: 11.6137 loss_rec: 11.6137 acc_train: 0.5523 loss_val: 11.5117 acc_val: 0.5571 time: 0.0278s\n",
      "Epoch: 02710 loss_train: 21.5664 loss_rec: 21.5664 acc_train: 0.5522 loss_val: 21.2966 acc_val: 0.5567 time: 0.0273s\n",
      "Epoch: 02711 loss_train: 13.5561 loss_rec: 13.5561 acc_train: 0.5543 loss_val: 13.4245 acc_val: 0.5617 time: 0.0263s\n",
      "Test set results: loss= 9.3778 accuracy= 0.5483\n",
      "Epoch: 02712 loss_train: 10.5150 loss_rec: 10.5150 acc_train: 0.4930 loss_val: 10.6970 acc_val: 0.4979 time: 0.0278s\n",
      "Epoch: 02713 loss_train: 14.1465 loss_rec: 14.1465 acc_train: 0.4922 loss_val: 14.3860 acc_val: 0.4971 time: 0.0258s\n",
      "Epoch: 02714 loss_train: 2.1279 loss_rec: 2.1279 acc_train: 0.6087 loss_val: 2.1574 acc_val: 0.6058 time: 0.0268s\n",
      "Epoch: 02715 loss_train: 2.8345 loss_rec: 2.8345 acc_train: 0.5942 loss_val: 2.8581 acc_val: 0.5950 time: 0.0283s\n",
      "Epoch: 02716 loss_train: 11.1080 loss_rec: 11.1080 acc_train: 0.4930 loss_val: 11.2993 acc_val: 0.4979 time: 0.0283s\n",
      "Epoch: 02717 loss_train: 4.3492 loss_rec: 4.3492 acc_train: 0.4927 loss_val: 4.4340 acc_val: 0.4983 time: 0.0268s\n",
      "Epoch: 02718 loss_train: 19.0302 loss_rec: 19.0302 acc_train: 0.5511 loss_val: 18.8037 acc_val: 0.5571 time: 0.0263s\n",
      "Epoch: 02719 loss_train: 23.5913 loss_rec: 23.5913 acc_train: 0.5524 loss_val: 23.2868 acc_val: 0.5575 time: 0.0268s\n",
      "Epoch: 02720 loss_train: 10.7500 loss_rec: 10.7500 acc_train: 0.5550 loss_val: 10.6612 acc_val: 0.5617 time: 0.0263s\n",
      "Epoch: 02721 loss_train: 18.1227 loss_rec: 18.1227 acc_train: 0.4920 loss_val: 18.4235 acc_val: 0.4971 time: 0.0273s\n",
      "Test set results: loss= 23.1376 accuracy= 0.5471\n",
      "Epoch: 02722 loss_train: 25.8296 loss_rec: 25.8296 acc_train: 0.4917 loss_val: 26.2460 acc_val: 0.4967 time: 0.0263s\n",
      "Epoch: 02723 loss_train: 12.8120 loss_rec: 12.8120 acc_train: 0.4925 loss_val: 13.0302 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 02724 loss_train: 16.4388 loss_rec: 16.4388 acc_train: 0.5520 loss_val: 16.2572 acc_val: 0.5571 time: 0.0278s\n",
      "Epoch: 02725 loss_train: 25.7422 loss_rec: 25.7422 acc_train: 0.5527 loss_val: 25.4016 acc_val: 0.5563 time: 0.0273s\n",
      "Epoch: 02726 loss_train: 17.1361 loss_rec: 17.1361 acc_train: 0.5520 loss_val: 16.9424 acc_val: 0.5571 time: 0.0273s\n",
      "Epoch: 02727 loss_train: 7.2012 loss_rec: 7.2012 acc_train: 0.4925 loss_val: 7.3298 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02728 loss_train: 11.4232 loss_rec: 11.4232 acc_train: 0.4930 loss_val: 11.6195 acc_val: 0.4979 time: 0.0263s\n",
      "Epoch: 02729 loss_train: 3.7397 loss_rec: 3.7397 acc_train: 0.5833 loss_val: 3.7502 acc_val: 0.5854 time: 0.0258s\n",
      "Epoch: 02730 loss_train: 2.2388 loss_rec: 2.2388 acc_train: 0.6043 loss_val: 2.2675 acc_val: 0.6071 time: 0.0283s\n",
      "Epoch: 02731 loss_train: 13.3276 loss_rec: 13.3276 acc_train: 0.4923 loss_val: 13.5542 acc_val: 0.4971 time: 0.0278s\n",
      "Test set results: loss= 7.0393 accuracy= 0.5486\n",
      "Epoch: 02732 loss_train: 7.9119 loss_rec: 7.9119 acc_train: 0.4932 loss_val: 8.0520 acc_val: 0.4979 time: 0.0283s\n",
      "Epoch: 02733 loss_train: 14.6043 loss_rec: 14.6043 acc_train: 0.5527 loss_val: 14.4546 acc_val: 0.5600 time: 0.0273s\n",
      "Epoch: 02734 loss_train: 18.2364 loss_rec: 18.2364 acc_train: 0.5515 loss_val: 18.0242 acc_val: 0.5567 time: 0.0263s\n",
      "Epoch: 02735 loss_train: 4.8509 loss_rec: 4.8509 acc_train: 0.5724 loss_val: 4.8476 acc_val: 0.5771 time: 0.0278s\n",
      "Epoch: 02736 loss_train: 24.3927 loss_rec: 24.3927 acc_train: 0.4917 loss_val: 24.7871 acc_val: 0.4967 time: 0.0268s\n",
      "Epoch: 02737 loss_train: 31.8223 loss_rec: 31.8223 acc_train: 0.4932 loss_val: 32.3301 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 02738 loss_train: 18.5736 loss_rec: 18.5736 acc_train: 0.4919 loss_val: 18.8813 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02739 loss_train: 11.2763 loss_rec: 11.2763 acc_train: 0.5529 loss_val: 11.1794 acc_val: 0.5575 time: 0.0258s\n",
      "Epoch: 02740 loss_train: 20.9124 loss_rec: 20.9124 acc_train: 0.5526 loss_val: 20.6540 acc_val: 0.5579 time: 0.0278s\n",
      "Epoch: 02741 loss_train: 12.7458 loss_rec: 12.7458 acc_train: 0.5512 loss_val: 12.6271 acc_val: 0.5554 time: 0.0273s\n",
      "Test set results: loss= 10.1266 accuracy= 0.5483\n",
      "Epoch: 02742 loss_train: 11.3488 loss_rec: 11.3488 acc_train: 0.4930 loss_val: 11.5440 acc_val: 0.4979 time: 0.0283s\n",
      "Epoch: 02743 loss_train: 15.0672 loss_rec: 15.0672 acc_train: 0.4922 loss_val: 15.3212 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02744 loss_train: 1.4007 loss_rec: 1.4007 acc_train: 0.6259 loss_val: 1.4321 acc_val: 0.6217 time: 0.0258s\n",
      "Epoch: 02745 loss_train: 4.4428 loss_rec: 4.4428 acc_train: 0.5763 loss_val: 4.4445 acc_val: 0.5796 time: 0.0268s\n",
      "Epoch: 02746 loss_train: 7.6625 loss_rec: 7.6625 acc_train: 0.4932 loss_val: 7.7984 acc_val: 0.4979 time: 0.0278s\n",
      "Epoch: 02747 loss_train: 0.9826 loss_rec: 0.9826 acc_train: 0.6278 loss_val: 1.0067 acc_val: 0.6125 time: 0.0278s\n",
      "Epoch: 02748 loss_train: 3.1470 loss_rec: 3.1470 acc_train: 0.5888 loss_val: 3.1667 acc_val: 0.5938 time: 0.0278s\n",
      "Epoch: 02749 loss_train: 8.6513 loss_rec: 8.6513 acc_train: 0.4932 loss_val: 8.8033 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 02750 loss_train: 0.9585 loss_rec: 0.9585 acc_train: 0.6108 loss_val: 0.9795 acc_val: 0.6063 time: 0.0263s\n",
      "Epoch: 02751 loss_train: 7.7082 loss_rec: 7.7082 acc_train: 0.5603 loss_val: 7.6631 acc_val: 0.5629 time: 0.0258s\n",
      "Test set results: loss= 0.8937 accuracy= 0.6267\n",
      "Epoch: 02752 loss_train: 0.9526 loss_rec: 0.9526 acc_train: 0.6156 loss_val: 0.9742 acc_val: 0.6113 time: 0.0288s\n",
      "Epoch: 02753 loss_train: 8.2332 loss_rec: 8.2332 acc_train: 0.4932 loss_val: 8.3784 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 02754 loss_train: 3.3712 loss_rec: 3.3712 acc_train: 0.5872 loss_val: 3.3873 acc_val: 0.5908 time: 0.0273s\n",
      "Epoch: 02755 loss_train: 1.0931 loss_rec: 1.0931 acc_train: 0.5898 loss_val: 1.1147 acc_val: 0.5850 time: 0.0283s\n",
      "Epoch: 02756 loss_train: 2.9681 loss_rec: 2.9681 acc_train: 0.5912 loss_val: 2.9900 acc_val: 0.5933 time: 0.0258s\n",
      "Epoch: 02757 loss_train: 7.9622 loss_rec: 7.9622 acc_train: 0.4932 loss_val: 8.1029 acc_val: 0.4979 time: 0.0283s\n",
      "Epoch: 02758 loss_train: 1.3837 loss_rec: 1.3837 acc_train: 0.6264 loss_val: 1.4150 acc_val: 0.6221 time: 0.0273s\n",
      "Epoch: 02759 loss_train: 1.2146 loss_rec: 1.2146 acc_train: 0.5629 loss_val: 1.2391 acc_val: 0.5538 time: 0.0268s\n",
      "Epoch: 02760 loss_train: 7.2420 loss_rec: 7.2420 acc_train: 0.5618 loss_val: 7.2028 acc_val: 0.5658 time: 0.0268s\n",
      "Epoch: 02761 loss_train: 0.9818 loss_rec: 0.9818 acc_train: 0.6107 loss_val: 1.0022 acc_val: 0.6000 time: 0.0273s\n",
      "Test set results: loss= 3.8676 accuracy= 0.5486\n",
      "Epoch: 02762 loss_train: 4.3782 loss_rec: 4.3782 acc_train: 0.4927 loss_val: 4.4634 acc_val: 0.4983 time: 0.0253s\n",
      "Epoch: 02763 loss_train: 10.0016 loss_rec: 10.0016 acc_train: 0.5555 loss_val: 9.9250 acc_val: 0.5625 time: 0.0278s\n",
      "Epoch: 02764 loss_train: 6.9079 loss_rec: 6.9079 acc_train: 0.5638 loss_val: 6.8735 acc_val: 0.5688 time: 0.0283s\n",
      "Epoch: 02765 loss_train: 12.4000 loss_rec: 12.4000 acc_train: 0.4927 loss_val: 12.6116 acc_val: 0.4975 time: 0.0268s\n",
      "Epoch: 02766 loss_train: 11.1991 loss_rec: 11.1991 acc_train: 0.4930 loss_val: 11.3919 acc_val: 0.4979 time: 0.0268s\n",
      "Epoch: 02767 loss_train: 8.1262 loss_rec: 8.1262 acc_train: 0.5598 loss_val: 8.0758 acc_val: 0.5625 time: 0.0278s\n",
      "Epoch: 02768 loss_train: 9.0772 loss_rec: 9.0772 acc_train: 0.5577 loss_val: 9.0149 acc_val: 0.5621 time: 0.0258s\n",
      "Epoch: 02769 loss_train: 6.1607 loss_rec: 6.1607 acc_train: 0.4926 loss_val: 6.2734 acc_val: 0.4983 time: 0.0283s\n",
      "Epoch: 02770 loss_train: 1.6985 loss_rec: 1.6985 acc_train: 0.4927 loss_val: 1.7359 acc_val: 0.4983 time: 0.0283s\n",
      "Epoch: 02771 loss_train: 16.8773 loss_rec: 16.8773 acc_train: 0.5520 loss_val: 16.6877 acc_val: 0.5571 time: 0.0273s\n",
      "Test set results: loss= 19.6186 accuracy= 0.5123\n",
      "Epoch: 02772 loss_train: 17.3293 loss_rec: 17.3293 acc_train: 0.5521 loss_val: 17.1322 acc_val: 0.5571 time: 0.0258s\n",
      "Epoch: 02773 loss_train: 1.6868 loss_rec: 1.6868 acc_train: 0.6168 loss_val: 1.7184 acc_val: 0.6179 time: 0.0288s\n",
      "Epoch: 02774 loss_train: 26.6368 loss_rec: 26.6368 acc_train: 0.4917 loss_val: 27.0657 acc_val: 0.4967 time: 0.0288s\n",
      "Epoch: 02775 loss_train: 32.4635 loss_rec: 32.4635 acc_train: 0.4932 loss_val: 32.9812 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 02776 loss_train: 17.8027 loss_rec: 17.8027 acc_train: 0.4922 loss_val: 18.0988 acc_val: 0.4971 time: 0.0268s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02777 loss_train: 13.1281 loss_rec: 13.1281 acc_train: 0.5498 loss_val: 13.0031 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 02778 loss_train: 23.7609 loss_rec: 23.7609 acc_train: 0.5517 loss_val: 23.4534 acc_val: 0.5563 time: 0.0258s\n",
      "Epoch: 02779 loss_train: 16.4667 loss_rec: 16.4667 acc_train: 0.5514 loss_val: 16.2846 acc_val: 0.5567 time: 0.0283s\n",
      "Epoch: 02780 loss_train: 6.6081 loss_rec: 6.6081 acc_train: 0.4926 loss_val: 6.7278 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 02781 loss_train: 9.6971 loss_rec: 9.6971 acc_train: 0.4932 loss_val: 9.8661 acc_val: 0.4979 time: 0.0273s\n",
      "Test set results: loss= 6.7496 accuracy= 0.5320\n",
      "Epoch: 02782 loss_train: 5.9822 loss_rec: 5.9822 acc_train: 0.5663 loss_val: 5.9619 acc_val: 0.5717 time: 0.0273s\n",
      "Epoch: 02783 loss_train: 4.1642 loss_rec: 4.1642 acc_train: 0.5777 loss_val: 4.1690 acc_val: 0.5804 time: 0.0263s\n",
      "Epoch: 02784 loss_train: 13.3457 loss_rec: 13.3457 acc_train: 0.4923 loss_val: 13.5726 acc_val: 0.4971 time: 0.0283s\n",
      "Epoch: 02785 loss_train: 10.2247 loss_rec: 10.2247 acc_train: 0.4931 loss_val: 10.4021 acc_val: 0.4979 time: 0.0263s\n",
      "Epoch: 02786 loss_train: 10.5295 loss_rec: 10.5295 acc_train: 0.5533 loss_val: 10.4440 acc_val: 0.5596 time: 0.0283s\n",
      "Epoch: 02787 loss_train: 12.6388 loss_rec: 12.6388 acc_train: 0.5512 loss_val: 12.5216 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 02788 loss_train: 1.5353 loss_rec: 1.5353 acc_train: 0.4928 loss_val: 1.5685 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 02789 loss_train: 0.9638 loss_rec: 0.9638 acc_train: 0.6103 loss_val: 0.9844 acc_val: 0.6058 time: 0.0263s\n",
      "Epoch: 02790 loss_train: 2.7261 loss_rec: 2.7261 acc_train: 0.5942 loss_val: 2.7506 acc_val: 0.5946 time: 0.0283s\n",
      "Epoch: 02791 loss_train: 8.5992 loss_rec: 8.5992 acc_train: 0.4932 loss_val: 8.7503 acc_val: 0.4979 time: 0.0263s\n",
      "Test set results: loss= 0.9829 accuracy= 0.6265\n",
      "Epoch: 02792 loss_train: 0.9946 loss_rec: 0.9946 acc_train: 0.6280 loss_val: 1.0191 acc_val: 0.6138 time: 0.0273s\n",
      "Epoch: 02793 loss_train: 3.3878 loss_rec: 3.3878 acc_train: 0.5871 loss_val: 3.4032 acc_val: 0.5904 time: 0.0278s\n",
      "Epoch: 02794 loss_train: 8.3535 loss_rec: 8.3535 acc_train: 0.4932 loss_val: 8.5006 acc_val: 0.4979 time: 0.0268s\n",
      "Epoch: 02795 loss_train: 0.9528 loss_rec: 0.9528 acc_train: 0.6242 loss_val: 0.9749 acc_val: 0.6163 time: 0.0253s\n",
      "Epoch: 02796 loss_train: 5.3725 loss_rec: 5.3725 acc_train: 0.5688 loss_val: 5.3615 acc_val: 0.5733 time: 0.0283s\n",
      "Epoch: 02797 loss_train: 4.8291 loss_rec: 4.8291 acc_train: 0.4927 loss_val: 4.9214 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02798 loss_train: 3.7746 loss_rec: 3.7746 acc_train: 0.5839 loss_val: 3.7844 acc_val: 0.5863 time: 0.0273s\n",
      "Epoch: 02799 loss_train: 3.1732 loss_rec: 3.1732 acc_train: 0.4927 loss_val: 3.2391 acc_val: 0.4983 time: 0.0263s\n",
      "Epoch: 02800 loss_train: 7.8386 loss_rec: 7.8386 acc_train: 0.5603 loss_val: 7.7916 acc_val: 0.5629 time: 0.0263s\n",
      "Epoch: 02801 loss_train: 2.4175 loss_rec: 2.4175 acc_train: 0.5997 loss_val: 2.4447 acc_val: 0.5975 time: 0.0278s\n",
      "Test set results: loss= 15.6242 accuracy= 0.5476\n",
      "Epoch: 02802 loss_train: 17.4678 loss_rec: 17.4678 acc_train: 0.4922 loss_val: 17.7585 acc_val: 0.4971 time: 0.0248s\n",
      "Epoch: 02803 loss_train: 16.0285 loss_rec: 16.0285 acc_train: 0.4922 loss_val: 16.2971 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 02804 loss_train: 4.0839 loss_rec: 4.0839 acc_train: 0.5797 loss_val: 4.0897 acc_val: 0.5817 time: 0.0258s\n",
      "Epoch: 02805 loss_train: 6.1743 loss_rec: 6.1743 acc_train: 0.5666 loss_val: 6.1507 acc_val: 0.5725 time: 0.0278s\n",
      "Epoch: 02806 loss_train: 7.7455 loss_rec: 7.7455 acc_train: 0.4924 loss_val: 7.8827 acc_val: 0.4979 time: 0.0288s\n",
      "Epoch: 02807 loss_train: 1.8011 loss_rec: 1.8011 acc_train: 0.4927 loss_val: 1.8410 acc_val: 0.4983 time: 0.0268s\n",
      "Epoch: 02808 loss_train: 18.4390 loss_rec: 18.4390 acc_train: 0.5516 loss_val: 18.2223 acc_val: 0.5571 time: 0.0263s\n",
      "Epoch: 02809 loss_train: 20.4250 loss_rec: 20.4250 acc_train: 0.5526 loss_val: 20.1747 acc_val: 0.5579 time: 0.0278s\n",
      "Epoch: 02810 loss_train: 5.5610 loss_rec: 5.5610 acc_train: 0.5691 loss_val: 5.5469 acc_val: 0.5733 time: 0.0258s\n",
      "Epoch: 02811 loss_train: 25.1497 loss_rec: 25.1497 acc_train: 0.4917 loss_val: 25.5558 acc_val: 0.4967 time: 0.0278s\n",
      "Test set results: loss= 30.5643 accuracy= 0.5489\n",
      "Epoch: 02812 loss_train: 34.0978 loss_rec: 34.0978 acc_train: 0.4932 loss_val: 34.6406 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02813 loss_train: 22.2790 loss_rec: 22.2790 acc_train: 0.4917 loss_val: 22.6416 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02814 loss_train: 6.7416 loss_rec: 6.7416 acc_train: 0.5639 loss_val: 6.7093 acc_val: 0.5704 time: 0.0268s\n",
      "Epoch: 02815 loss_train: 15.7123 loss_rec: 15.7123 acc_train: 0.5519 loss_val: 15.5427 acc_val: 0.5571 time: 0.0263s\n",
      "Epoch: 02816 loss_train: 7.2582 loss_rec: 7.2582 acc_train: 0.5607 loss_val: 7.2186 acc_val: 0.5650 time: 0.0273s\n",
      "Epoch: 02817 loss_train: 17.2115 loss_rec: 17.2115 acc_train: 0.4922 loss_val: 17.4988 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 02818 loss_train: 20.7777 loss_rec: 20.7777 acc_train: 0.4918 loss_val: 21.1186 acc_val: 0.4971 time: 0.0283s\n",
      "Epoch: 02819 loss_train: 4.2718 loss_rec: 4.2718 acc_train: 0.4921 loss_val: 4.3555 acc_val: 0.4979 time: 0.0283s\n",
      "Epoch: 02820 loss_train: 27.0401 loss_rec: 27.0401 acc_train: 0.5507 loss_val: 26.6783 acc_val: 0.5550 time: 0.0273s\n",
      "Epoch: 02821 loss_train: 38.9335 loss_rec: 38.9335 acc_train: 0.5439 loss_val: 38.4070 acc_val: 0.5508 time: 0.0263s\n",
      "Test set results: loss= 36.8109 accuracy= 0.5043\n",
      "Epoch: 02822 loss_train: 32.5020 loss_rec: 32.5020 acc_train: 0.5461 loss_val: 32.0637 acc_val: 0.5529 time: 0.0273s\n",
      "Epoch: 02823 loss_train: 9.8420 loss_rec: 9.8420 acc_train: 0.5538 loss_val: 9.7677 acc_val: 0.5604 time: 0.0273s\n",
      "Epoch: 02824 loss_train: 28.3473 loss_rec: 28.3473 acc_train: 0.4915 loss_val: 28.8025 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 02825 loss_train: 44.5630 loss_rec: 44.5630 acc_train: 0.4931 loss_val: 45.2662 acc_val: 0.4983 time: 0.0268s\n",
      "Epoch: 02826 loss_train: 39.3467 loss_rec: 39.3467 acc_train: 0.4932 loss_val: 39.9702 acc_val: 0.4983 time: 0.0258s\n",
      "Epoch: 02827 loss_train: 14.9215 loss_rec: 14.9215 acc_train: 0.4922 loss_val: 15.1734 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02828 loss_train: 23.7443 loss_rec: 23.7443 acc_train: 0.5510 loss_val: 23.4372 acc_val: 0.5558 time: 0.0278s\n",
      "Epoch: 02829 loss_train: 41.6508 loss_rec: 41.6508 acc_train: 0.5414 loss_val: 41.0927 acc_val: 0.5467 time: 0.0273s\n",
      "Epoch: 02830 loss_train: 40.6913 loss_rec: 40.6913 acc_train: 0.5418 loss_val: 40.1442 acc_val: 0.5471 time: 0.0278s\n",
      "Epoch: 02831 loss_train: 22.7879 loss_rec: 22.7879 acc_train: 0.5525 loss_val: 22.4971 acc_val: 0.5575 time: 0.0263s\n",
      "Test set results: loss= 8.7340 accuracy= 0.5483\n",
      "Epoch: 02832 loss_train: 9.7989 loss_rec: 9.7989 acc_train: 0.4932 loss_val: 9.9695 acc_val: 0.4979 time: 0.0288s\n",
      "Epoch: 02833 loss_train: 22.1316 loss_rec: 22.1316 acc_train: 0.4917 loss_val: 22.4923 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 02834 loss_train: 13.6020 loss_rec: 13.6020 acc_train: 0.4923 loss_val: 13.8330 acc_val: 0.4971 time: 0.0283s\n",
      "Epoch: 02835 loss_train: 11.7440 loss_rec: 11.7440 acc_train: 0.5517 loss_val: 11.6401 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 02836 loss_train: 17.8367 loss_rec: 17.8367 acc_train: 0.5515 loss_val: 17.6307 acc_val: 0.5567 time: 0.0274s\n",
      "Epoch: 02837 loss_train: 6.8417 loss_rec: 6.8417 acc_train: 0.5634 loss_val: 6.8081 acc_val: 0.5696 time: 0.0258s\n",
      "Epoch: 02838 loss_train: 19.9389 loss_rec: 19.9389 acc_train: 0.4919 loss_val: 20.2675 acc_val: 0.4971 time: 0.0283s\n",
      "Epoch: 02839 loss_train: 25.7300 loss_rec: 25.7300 acc_train: 0.4917 loss_val: 26.1452 acc_val: 0.4967 time: 0.0263s\n",
      "Epoch: 02840 loss_train: 11.3417 loss_rec: 11.3417 acc_train: 0.4930 loss_val: 11.5369 acc_val: 0.4979 time: 0.0278s\n",
      "Epoch: 02841 loss_train: 18.5788 loss_rec: 18.5788 acc_train: 0.5511 loss_val: 18.3601 acc_val: 0.5567 time: 0.0278s\n",
      "Test set results: loss= 32.7180 accuracy= 0.5059\n",
      "Epoch: 02842 loss_train: 28.8928 loss_rec: 28.8928 acc_train: 0.5483 loss_val: 28.5058 acc_val: 0.5517 time: 0.0263s\n",
      "Epoch: 02843 loss_train: 21.4232 loss_rec: 21.4232 acc_train: 0.5508 loss_val: 21.1556 acc_val: 0.5563 time: 0.0258s\n",
      "Epoch: 02844 loss_train: 1.3390 loss_rec: 1.3390 acc_train: 0.5713 loss_val: 1.3669 acc_val: 0.5621 time: 0.0278s\n",
      "Epoch: 02845 loss_train: 10.4827 loss_rec: 10.4827 acc_train: 0.4931 loss_val: 10.6643 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 02846 loss_train: 1.0281 loss_rec: 1.0281 acc_train: 0.6248 loss_val: 1.0538 acc_val: 0.6154 time: 0.0273s\n",
      "Epoch: 02847 loss_train: 4.0876 loss_rec: 4.0876 acc_train: 0.5791 loss_val: 4.0933 acc_val: 0.5813 time: 0.0263s\n",
      "Epoch: 02848 loss_train: 7.2220 loss_rec: 7.2220 acc_train: 0.4925 loss_val: 7.3511 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02849 loss_train: 1.3088 loss_rec: 1.3088 acc_train: 0.6308 loss_val: 1.3394 acc_val: 0.6246 time: 0.0258s\n",
      "Epoch: 02850 loss_train: 1.2834 loss_rec: 1.2834 acc_train: 0.5653 loss_val: 1.3098 acc_val: 0.5579 time: 0.0278s\n",
      "Epoch: 02851 loss_train: 7.9912 loss_rec: 7.9912 acc_train: 0.5587 loss_val: 7.9423 acc_val: 0.5621 time: 0.0278s\n",
      "Test set results: loss= 1.6315 accuracy= 0.6032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02852 loss_train: 1.5101 loss_rec: 1.5101 acc_train: 0.6231 loss_val: 1.5417 acc_val: 0.6208 time: 0.0278s\n",
      "Epoch: 02853 loss_train: 16.8631 loss_rec: 16.8631 acc_train: 0.4922 loss_val: 17.1448 acc_val: 0.4971 time: 0.0258s\n",
      "Epoch: 02854 loss_train: 13.9793 loss_rec: 13.9793 acc_train: 0.4923 loss_val: 14.2163 acc_val: 0.4971 time: 0.0283s\n",
      "Epoch: 02855 loss_train: 6.7367 loss_rec: 6.7367 acc_train: 0.5639 loss_val: 6.7048 acc_val: 0.5704 time: 0.0278s\n",
      "Epoch: 02856 loss_train: 9.0855 loss_rec: 9.0855 acc_train: 0.5565 loss_val: 9.0230 acc_val: 0.5613 time: 0.0258s\n",
      "Epoch: 02857 loss_train: 4.4289 loss_rec: 4.4289 acc_train: 0.4927 loss_val: 4.5150 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02858 loss_train: 1.3832 loss_rec: 1.3832 acc_train: 0.6248 loss_val: 1.4143 acc_val: 0.6200 time: 0.0278s\n",
      "Epoch: 02859 loss_train: 3.9461 loss_rec: 3.9461 acc_train: 0.4927 loss_val: 4.0246 acc_val: 0.4983 time: 0.0258s\n",
      "Epoch: 02860 loss_train: 9.0337 loss_rec: 9.0337 acc_train: 0.5565 loss_val: 8.9719 acc_val: 0.5613 time: 0.0283s\n",
      "Epoch: 02861 loss_train: 5.1601 loss_rec: 5.1601 acc_train: 0.5706 loss_val: 5.1519 acc_val: 0.5746 time: 0.0278s\n",
      "Test set results: loss= 12.9194 accuracy= 0.5479\n",
      "Epoch: 02862 loss_train: 14.4578 loss_rec: 14.4578 acc_train: 0.4922 loss_val: 14.7024 acc_val: 0.4971 time: 0.0283s\n",
      "Epoch: 02863 loss_train: 13.6724 loss_rec: 13.6724 acc_train: 0.4923 loss_val: 13.9045 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02864 loss_train: 5.3146 loss_rec: 5.3146 acc_train: 0.5689 loss_val: 5.3043 acc_val: 0.5737 time: 0.0263s\n",
      "Epoch: 02865 loss_train: 6.4154 loss_rec: 6.4154 acc_train: 0.5650 loss_val: 6.3881 acc_val: 0.5708 time: 0.0258s\n",
      "Epoch: 02866 loss_train: 8.2909 loss_rec: 8.2909 acc_train: 0.4924 loss_val: 8.4370 acc_val: 0.4979 time: 0.0283s\n",
      "Epoch: 02867 loss_train: 3.3218 loss_rec: 3.3218 acc_train: 0.4922 loss_val: 3.3903 acc_val: 0.4979 time: 0.0278s\n",
      "Epoch: 02868 loss_train: 17.9428 loss_rec: 17.9428 acc_train: 0.5515 loss_val: 17.7344 acc_val: 0.5567 time: 0.0273s\n",
      "Epoch: 02869 loss_train: 21.0954 loss_rec: 21.0954 acc_train: 0.5504 loss_val: 20.8334 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 02870 loss_train: 7.4693 loss_rec: 7.4693 acc_train: 0.5594 loss_val: 7.4270 acc_val: 0.5629 time: 0.0263s\n",
      "Epoch: 02871 loss_train: 21.7958 loss_rec: 21.7958 acc_train: 0.4918 loss_val: 22.1516 acc_val: 0.4971 time: 0.0258s\n",
      "Test set results: loss= 26.8656 accuracy= 0.5471\n",
      "Epoch: 02872 loss_train: 29.9794 loss_rec: 29.9794 acc_train: 0.4915 loss_val: 30.4594 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 02873 loss_train: 17.8265 loss_rec: 17.8265 acc_train: 0.4922 loss_val: 18.1232 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 02874 loss_train: 10.7169 loss_rec: 10.7169 acc_train: 0.5533 loss_val: 10.6281 acc_val: 0.5596 time: 0.0283s\n",
      "Epoch: 02875 loss_train: 19.5724 loss_rec: 19.5724 acc_train: 0.5504 loss_val: 19.3366 acc_val: 0.5567 time: 0.0273s\n",
      "Epoch: 02876 loss_train: 11.0661 loss_rec: 11.0661 acc_train: 0.5531 loss_val: 10.9721 acc_val: 0.5583 time: 0.0268s\n",
      "Epoch: 02877 loss_train: 13.1213 loss_rec: 13.1213 acc_train: 0.4927 loss_val: 13.3446 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02878 loss_train: 17.1543 loss_rec: 17.1543 acc_train: 0.4916 loss_val: 17.4408 acc_val: 0.4967 time: 0.0268s\n",
      "Epoch: 02879 loss_train: 1.5301 loss_rec: 1.5301 acc_train: 0.4922 loss_val: 1.5634 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 02880 loss_train: 25.0154 loss_rec: 25.0154 acc_train: 0.5493 loss_val: 24.6866 acc_val: 0.5542 time: 0.0283s\n",
      "Epoch: 02881 loss_train: 32.9567 loss_rec: 32.9567 acc_train: 0.5448 loss_val: 32.5120 acc_val: 0.5521 time: 0.0258s\n",
      "Test set results: loss= 26.4180 accuracy= 0.5103\n",
      "Epoch: 02882 loss_train: 23.3314 loss_rec: 23.3314 acc_train: 0.5507 loss_val: 23.0314 acc_val: 0.5567 time: 0.0273s\n",
      "Epoch: 02883 loss_train: 1.2745 loss_rec: 1.2745 acc_train: 0.5647 loss_val: 1.3008 acc_val: 0.5575 time: 0.0288s\n",
      "Epoch: 02884 loss_train: 13.1828 loss_rec: 13.1828 acc_train: 0.4922 loss_val: 13.4071 acc_val: 0.4967 time: 0.0278s\n",
      "Epoch: 02885 loss_train: 4.7799 loss_rec: 4.7799 acc_train: 0.4921 loss_val: 4.8717 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 02886 loss_train: 19.5019 loss_rec: 19.5019 acc_train: 0.5493 loss_val: 19.2674 acc_val: 0.5558 time: 0.0263s\n",
      "Epoch: 02887 loss_train: 25.2971 loss_rec: 25.2971 acc_train: 0.5503 loss_val: 24.9638 acc_val: 0.5550 time: 0.0258s\n",
      "Epoch: 02888 loss_train: 13.9953 loss_rec: 13.9953 acc_train: 0.5537 loss_val: 13.8561 acc_val: 0.5613 time: 0.0288s\n",
      "Epoch: 02889 loss_train: 12.7397 loss_rec: 12.7397 acc_train: 0.4927 loss_val: 12.9569 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 02890 loss_train: 19.3519 loss_rec: 19.3519 acc_train: 0.4919 loss_val: 19.6717 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 02891 loss_train: 5.9786 loss_rec: 5.9786 acc_train: 0.4921 loss_val: 6.0887 acc_val: 0.4979 time: 0.0268s\n",
      "Test set results: loss= 25.5069 accuracy= 0.5103\n",
      "Epoch: 02892 loss_train: 22.5274 loss_rec: 22.5274 acc_train: 0.5507 loss_val: 22.2410 acc_val: 0.5563 time: 0.0258s\n",
      "Epoch: 02893 loss_train: 32.0822 loss_rec: 32.0822 acc_train: 0.5464 loss_val: 31.6498 acc_val: 0.5533 time: 0.0268s\n",
      "Epoch: 02894 loss_train: 24.0267 loss_rec: 24.0267 acc_train: 0.5486 loss_val: 23.7149 acc_val: 0.5546 time: 0.0288s\n",
      "Epoch: 02895 loss_train: 1.3522 loss_rec: 1.3522 acc_train: 0.6252 loss_val: 1.3832 acc_val: 0.6204 time: 0.0263s\n",
      "Epoch: 02896 loss_train: 31.9525 loss_rec: 31.9525 acc_train: 0.4932 loss_val: 32.4630 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 02897 loss_train: 42.5132 loss_rec: 42.5132 acc_train: 0.4931 loss_val: 43.1854 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02898 loss_train: 32.5953 loss_rec: 32.5953 acc_train: 0.4932 loss_val: 33.1158 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 02899 loss_train: 4.3922 loss_rec: 4.3922 acc_train: 0.4921 loss_val: 4.4778 acc_val: 0.4979 time: 0.0268s\n",
      "Epoch: 02900 loss_train: 36.2921 loss_rec: 36.2921 acc_train: 0.5450 loss_val: 35.7986 acc_val: 0.5529 time: 0.0273s\n",
      "Epoch: 02901 loss_train: 57.0088 loss_rec: 57.0088 acc_train: 0.5375 loss_val: 56.2678 acc_val: 0.5458 time: 0.0263s\n",
      "Test set results: loss= 66.6443 accuracy= 0.4900\n",
      "Epoch: 02902 loss_train: 58.7664 loss_rec: 58.7664 acc_train: 0.5378 loss_val: 58.0047 acc_val: 0.5458 time: 0.0258s\n",
      "Epoch: 02903 loss_train: 43.4336 loss_rec: 43.4336 acc_train: 0.5415 loss_val: 42.8544 acc_val: 0.5467 time: 0.0268s\n",
      "Epoch: 02904 loss_train: 13.0793 loss_rec: 13.0793 acc_train: 0.5548 loss_val: 12.9549 acc_val: 0.5621 time: 0.0263s\n",
      "Epoch: 02905 loss_train: 31.9440 loss_rec: 31.9440 acc_train: 0.4932 loss_val: 32.4546 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02906 loss_train: 55.0591 loss_rec: 55.0591 acc_train: 0.4931 loss_val: 55.9232 acc_val: 0.4983 time: 0.0273s\n",
      "Epoch: 02907 loss_train: 56.5306 loss_rec: 56.5306 acc_train: 0.4931 loss_val: 57.4174 acc_val: 0.4983 time: 0.0278s\n",
      "Epoch: 02908 loss_train: 38.5779 loss_rec: 38.5779 acc_train: 0.4926 loss_val: 39.1904 acc_val: 0.4979 time: 0.0263s\n",
      "Epoch: 02909 loss_train: 3.2587 loss_rec: 3.2587 acc_train: 0.4922 loss_val: 3.3264 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 02910 loss_train: 43.0594 loss_rec: 43.0594 acc_train: 0.5417 loss_val: 42.4848 acc_val: 0.5479 time: 0.0263s\n",
      "Epoch: 02911 loss_train: 68.9713 loss_rec: 68.9713 acc_train: 0.5341 loss_val: 68.1345 acc_val: 0.5404 time: 0.0283s\n",
      "Test set results: loss= 85.5455 accuracy= 0.4837\n",
      "Epoch: 02912 loss_train: 75.4015 loss_rec: 75.4015 acc_train: 0.5339 loss_val: 74.4509 acc_val: 0.5400 time: 0.0283s\n",
      "Epoch: 02913 loss_train: 64.3774 loss_rec: 64.3774 acc_train: 0.5384 loss_val: 63.5505 acc_val: 0.5446 time: 0.0293s\n",
      "Epoch: 02914 loss_train: 37.7358 loss_rec: 37.7358 acc_train: 0.5439 loss_val: 37.2228 acc_val: 0.5517 time: 0.0363s\n",
      "Epoch: 02915 loss_train: 1.7300 loss_rec: 1.7300 acc_train: 0.4922 loss_val: 1.7677 acc_val: 0.4979 time: 0.0363s\n",
      "Epoch: 02916 loss_train: 24.3731 loss_rec: 24.3731 acc_train: 0.4916 loss_val: 24.7642 acc_val: 0.4967 time: 0.0308s\n",
      "Epoch: 02917 loss_train: 25.7812 loss_rec: 25.7812 acc_train: 0.4915 loss_val: 26.1929 acc_val: 0.4967 time: 0.0263s\n",
      "Epoch: 02918 loss_train: 8.0114 loss_rec: 8.0114 acc_train: 0.4912 loss_val: 8.1519 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 02919 loss_train: 23.9883 loss_rec: 23.9883 acc_train: 0.5486 loss_val: 23.6760 acc_val: 0.5546 time: 0.0263s\n",
      "Epoch: 02920 loss_train: 36.8719 loss_rec: 36.8719 acc_train: 0.5439 loss_val: 36.3697 acc_val: 0.5517 time: 0.0268s\n",
      "Epoch: 02921 loss_train: 32.0581 loss_rec: 32.0581 acc_train: 0.5464 loss_val: 31.6252 acc_val: 0.5533 time: 0.0288s\n",
      "Test set results: loss= 12.9909 accuracy= 0.5151\n",
      "Epoch: 02922 loss_train: 11.4766 loss_rec: 11.4766 acc_train: 0.5517 loss_val: 11.3763 acc_val: 0.5567 time: 0.0328s\n",
      "Epoch: 02923 loss_train: 23.9768 loss_rec: 23.9768 acc_train: 0.4915 loss_val: 24.3582 acc_val: 0.4967 time: 0.0293s\n",
      "Epoch: 02924 loss_train: 38.6247 loss_rec: 38.6247 acc_train: 0.4931 loss_val: 39.2282 acc_val: 0.4983 time: 0.0258s\n",
      "Epoch: 02925 loss_train: 32.7858 loss_rec: 32.7858 acc_train: 0.4931 loss_val: 33.2998 acc_val: 0.4983 time: 0.0268s\n",
      "Epoch: 02926 loss_train: 8.5989 loss_rec: 8.5989 acc_train: 0.4910 loss_val: 8.7462 acc_val: 0.4971 time: 0.0273s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02927 loss_train: 28.6562 loss_rec: 28.6562 acc_train: 0.5488 loss_val: 28.2713 acc_val: 0.5521 time: 0.0373s\n",
      "Epoch: 02928 loss_train: 46.3803 loss_rec: 46.3803 acc_train: 0.5423 loss_val: 45.7657 acc_val: 0.5479 time: 0.0338s\n",
      "Epoch: 02929 loss_train: 45.8532 loss_rec: 45.8532 acc_train: 0.5433 loss_val: 45.2447 acc_val: 0.5483 time: 0.0353s\n",
      "Epoch: 02930 loss_train: 28.9564 loss_rec: 28.9564 acc_train: 0.5483 loss_val: 28.5680 acc_val: 0.5513 time: 0.0328s\n",
      "Epoch: 02931 loss_train: 1.5949 loss_rec: 1.5949 acc_train: 0.4917 loss_val: 1.6288 acc_val: 0.4971 time: 0.0343s\n",
      "Test set results: loss= 14.8578 accuracy= 0.5468\n",
      "Epoch: 02932 loss_train: 16.6177 loss_rec: 16.6177 acc_train: 0.4912 loss_val: 16.8884 acc_val: 0.4963 time: 0.0293s\n",
      "Epoch: 02933 loss_train: 11.4505 loss_rec: 11.4505 acc_train: 0.4915 loss_val: 11.6414 acc_val: 0.4963 time: 0.0348s\n",
      "Epoch: 02934 loss_train: 10.2382 loss_rec: 10.2382 acc_train: 0.5536 loss_val: 10.1576 acc_val: 0.5604 time: 0.0333s\n",
      "Epoch: 02935 loss_train: 13.9287 loss_rec: 13.9287 acc_train: 0.5525 loss_val: 13.7907 acc_val: 0.5600 time: 0.0348s\n",
      "Epoch: 02936 loss_train: 2.0287 loss_rec: 2.0287 acc_train: 0.6074 loss_val: 2.0580 acc_val: 0.6050 time: 0.0353s\n",
      "Epoch: 02937 loss_train: 22.8523 loss_rec: 22.8523 acc_train: 0.4913 loss_val: 23.2149 acc_val: 0.4963 time: 0.0348s\n",
      "Epoch: 02938 loss_train: 26.7797 loss_rec: 26.7797 acc_train: 0.4912 loss_val: 27.2005 acc_val: 0.4963 time: 0.0343s\n",
      "Epoch: 02939 loss_train: 11.5175 loss_rec: 11.5175 acc_train: 0.4915 loss_val: 11.7094 acc_val: 0.4963 time: 0.0333s\n",
      "Epoch: 02940 loss_train: 18.4217 loss_rec: 18.4217 acc_train: 0.5490 loss_val: 18.2048 acc_val: 0.5554 time: 0.0343s\n",
      "Epoch: 02941 loss_train: 29.4467 loss_rec: 29.4467 acc_train: 0.5467 loss_val: 29.0520 acc_val: 0.5508 time: 0.0343s\n",
      "Test set results: loss= 26.3490 accuracy= 0.5102\n",
      "Epoch: 02942 loss_train: 23.2717 loss_rec: 23.2717 acc_train: 0.5506 loss_val: 22.9718 acc_val: 0.5563 time: 0.0333s\n",
      "Epoch: 02943 loss_train: 2.3914 loss_rec: 2.3914 acc_train: 0.5987 loss_val: 2.4183 acc_val: 0.5967 time: 0.0358s\n",
      "Epoch: 02944 loss_train: 31.8524 loss_rec: 31.8524 acc_train: 0.4929 loss_val: 32.3510 acc_val: 0.4979 time: 0.0348s\n",
      "Epoch: 02945 loss_train: 44.3913 loss_rec: 44.3913 acc_train: 0.4929 loss_val: 45.0816 acc_val: 0.4979 time: 0.0338s\n",
      "Epoch: 02946 loss_train: 36.8799 loss_rec: 36.8799 acc_train: 0.4929 loss_val: 37.4554 acc_val: 0.4979 time: 0.0328s\n",
      "Epoch: 02947 loss_train: 11.4033 loss_rec: 11.4033 acc_train: 0.4915 loss_val: 11.5934 acc_val: 0.4963 time: 0.0323s\n",
      "Epoch: 02948 loss_train: 26.9300 loss_rec: 26.9300 acc_train: 0.5486 loss_val: 26.5690 acc_val: 0.5538 time: 0.0353s\n",
      "Epoch: 02949 loss_train: 45.6562 loss_rec: 45.6562 acc_train: 0.5433 loss_val: 45.0502 acc_val: 0.5483 time: 0.0333s\n",
      "Epoch: 02950 loss_train: 46.2459 loss_rec: 46.2459 acc_train: 0.5427 loss_val: 45.6328 acc_val: 0.5479 time: 0.0343s\n",
      "Epoch: 02951 loss_train: 30.5159 loss_rec: 30.5159 acc_train: 0.5464 loss_val: 30.1057 acc_val: 0.5521 time: 0.0318s\n",
      "Test set results: loss= 1.5912 accuracy= 0.6039\n",
      "Epoch: 02952 loss_train: 1.4768 loss_rec: 1.4768 acc_train: 0.6240 loss_val: 1.5078 acc_val: 0.6213 time: 0.0323s\n",
      "Epoch: 02953 loss_train: 38.3609 loss_rec: 38.3609 acc_train: 0.4929 loss_val: 38.9594 acc_val: 0.4979 time: 0.0313s\n",
      "Epoch: 02954 loss_train: 55.6093 loss_rec: 55.6093 acc_train: 0.4928 loss_val: 56.4713 acc_val: 0.4979 time: 0.0323s\n",
      "Epoch: 02955 loss_train: 52.4276 loss_rec: 52.4276 acc_train: 0.4928 loss_val: 53.2411 acc_val: 0.4979 time: 0.0318s\n",
      "Epoch: 02956 loss_train: 30.9000 loss_rec: 30.9000 acc_train: 0.4912 loss_val: 31.3846 acc_val: 0.4958 time: 0.0303s\n",
      "Epoch: 02957 loss_train: 5.7904 loss_rec: 5.7904 acc_train: 0.5675 loss_val: 5.7728 acc_val: 0.5725 time: 0.0303s\n",
      "Epoch: 02958 loss_train: 22.0942 loss_rec: 22.0942 acc_train: 0.5506 loss_val: 21.8141 acc_val: 0.5563 time: 0.0323s\n",
      "Epoch: 02959 loss_train: 21.1549 loss_rec: 21.1549 acc_train: 0.5497 loss_val: 20.8909 acc_val: 0.5554 time: 0.0323s\n",
      "Epoch: 02960 loss_train: 4.8109 loss_rec: 4.8109 acc_train: 0.5718 loss_val: 4.8077 acc_val: 0.5767 time: 0.0343s\n",
      "Epoch: 02961 loss_train: 26.1966 loss_rec: 26.1966 acc_train: 0.4909 loss_val: 26.6098 acc_val: 0.4963 time: 0.0338s\n",
      "Test set results: loss= 32.7353 accuracy= 0.5489\n",
      "Epoch: 02962 loss_train: 36.5204 loss_rec: 36.5204 acc_train: 0.4931 loss_val: 37.0913 acc_val: 0.4983 time: 0.0348s\n",
      "Epoch: 02963 loss_train: 27.2456 loss_rec: 27.2456 acc_train: 0.4908 loss_val: 27.6746 acc_val: 0.4963 time: 0.0343s\n",
      "Epoch: 02964 loss_train: 0.9965 loss_rec: 0.9965 acc_train: 0.6068 loss_val: 1.0158 acc_val: 0.5967 time: 0.0343s\n",
      "Epoch: 02965 loss_train: 25.3891 loss_rec: 25.3891 acc_train: 0.5487 loss_val: 25.0531 acc_val: 0.5546 time: 0.0318s\n",
      "Epoch: 02966 loss_train: 33.6495 loss_rec: 33.6495 acc_train: 0.5448 loss_val: 33.1932 acc_val: 0.5521 time: 0.0343s\n",
      "Epoch: 02967 loss_train: 25.1724 loss_rec: 25.1724 acc_train: 0.5494 loss_val: 24.8401 acc_val: 0.5550 time: 0.0363s\n",
      "Epoch: 02968 loss_train: 2.4402 loss_rec: 2.4402 acc_train: 0.5992 loss_val: 2.4664 acc_val: 0.5975 time: 0.0348s\n",
      "Epoch: 02969 loss_train: 33.4643 loss_rec: 33.4643 acc_train: 0.4931 loss_val: 33.9892 acc_val: 0.4983 time: 0.0343s\n",
      "Epoch: 02970 loss_train: 47.7776 loss_rec: 47.7776 acc_train: 0.4931 loss_val: 48.5212 acc_val: 0.4983 time: 0.0328s\n",
      "Epoch: 02971 loss_train: 42.1687 loss_rec: 42.1687 acc_train: 0.4931 loss_val: 42.8267 acc_val: 0.4983 time: 0.0348s\n",
      "Test set results: loss= 16.6826 accuracy= 0.5469\n",
      "Epoch: 02972 loss_train: 18.6484 loss_rec: 18.6484 acc_train: 0.4912 loss_val: 18.9508 acc_val: 0.4967 time: 0.0348s\n",
      "Epoch: 02973 loss_train: 18.3494 loss_rec: 18.3494 acc_train: 0.5492 loss_val: 18.1332 acc_val: 0.5558 time: 0.0318s\n",
      "Epoch: 02974 loss_train: 35.5697 loss_rec: 35.5697 acc_train: 0.5450 loss_val: 35.0857 acc_val: 0.5529 time: 0.0268s\n",
      "Epoch: 02975 loss_train: 35.2361 loss_rec: 35.2361 acc_train: 0.5449 loss_val: 34.7570 acc_val: 0.5529 time: 0.0323s\n",
      "Epoch: 02976 loss_train: 19.1244 loss_rec: 19.1244 acc_train: 0.5493 loss_val: 18.8950 acc_val: 0.5558 time: 0.0288s\n",
      "Epoch: 02977 loss_train: 10.8890 loss_rec: 10.8890 acc_train: 0.4917 loss_val: 11.0722 acc_val: 0.4967 time: 0.0298s\n",
      "Epoch: 02978 loss_train: 21.8719 loss_rec: 21.8719 acc_train: 0.4911 loss_val: 22.2221 acc_val: 0.4963 time: 0.0268s\n",
      "Epoch: 02979 loss_train: 13.4292 loss_rec: 13.4292 acc_train: 0.4916 loss_val: 13.6530 acc_val: 0.4967 time: 0.0283s\n",
      "Epoch: 02980 loss_train: 10.6527 loss_rec: 10.6527 acc_train: 0.5516 loss_val: 10.5648 acc_val: 0.5588 time: 0.0258s\n",
      "Epoch: 02981 loss_train: 16.7378 loss_rec: 16.7378 acc_train: 0.5493 loss_val: 16.5495 acc_val: 0.5563 time: 0.0273s\n",
      "Test set results: loss= 7.7457 accuracy= 0.5266\n",
      "Epoch: 02982 loss_train: 6.8576 loss_rec: 6.8576 acc_train: 0.5609 loss_val: 6.8232 acc_val: 0.5667 time: 0.0353s\n",
      "Epoch: 02983 loss_train: 17.6951 loss_rec: 17.6951 acc_train: 0.4913 loss_val: 17.9842 acc_val: 0.4967 time: 0.0412s\n",
      "Epoch: 02984 loss_train: 22.7756 loss_rec: 22.7756 acc_train: 0.4911 loss_val: 23.1399 acc_val: 0.4963 time: 0.0353s\n",
      "Epoch: 02985 loss_train: 9.0658 loss_rec: 9.0658 acc_train: 0.4911 loss_val: 9.2215 acc_val: 0.4971 time: 0.0338s\n",
      "Epoch: 02986 loss_train: 18.9534 loss_rec: 18.9534 acc_train: 0.5502 loss_val: 18.7269 acc_val: 0.5571 time: 0.0343s\n",
      "Epoch: 02987 loss_train: 28.7992 loss_rec: 28.7992 acc_train: 0.5484 loss_val: 28.4126 acc_val: 0.5517 time: 0.0333s\n",
      "Epoch: 02988 loss_train: 22.0350 loss_rec: 22.0350 acc_train: 0.5506 loss_val: 21.7555 acc_val: 0.5563 time: 0.0263s\n",
      "Epoch: 02989 loss_train: 1.4249 loss_rec: 1.4249 acc_train: 0.6229 loss_val: 1.4555 acc_val: 0.6204 time: 0.0253s\n",
      "Epoch: 02990 loss_train: 29.7398 loss_rec: 29.7398 acc_train: 0.4908 loss_val: 30.2093 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 02991 loss_train: 39.5435 loss_rec: 39.5435 acc_train: 0.4931 loss_val: 40.1632 acc_val: 0.4983 time: 0.0303s\n",
      "Test set results: loss= 26.9598 accuracy= 0.5464\n",
      "Epoch: 02992 loss_train: 30.0882 loss_rec: 30.0882 acc_train: 0.4908 loss_val: 30.5632 acc_val: 0.4958 time: 0.0353s\n",
      "Epoch: 02993 loss_train: 3.3961 loss_rec: 3.3961 acc_train: 0.4920 loss_val: 3.4642 acc_val: 0.4975 time: 0.0363s\n",
      "Epoch: 02994 loss_train: 34.8462 loss_rec: 34.8462 acc_train: 0.5448 loss_val: 34.3729 acc_val: 0.5529 time: 0.0387s\n",
      "Epoch: 02995 loss_train: 54.3532 loss_rec: 54.3532 acc_train: 0.5380 loss_val: 53.6438 acc_val: 0.5463 time: 0.0382s\n",
      "Epoch: 02996 loss_train: 56.0197 loss_rec: 56.0197 acc_train: 0.5371 loss_val: 55.2903 acc_val: 0.5454 time: 0.0377s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02997 loss_train: 41.6063 loss_rec: 41.6063 acc_train: 0.5415 loss_val: 41.0484 acc_val: 0.5467 time: 0.0353s\n",
      "Epoch: 02998 loss_train: 13.0635 loss_rec: 13.0635 acc_train: 0.5524 loss_val: 12.9390 acc_val: 0.5608 time: 0.0288s\n",
      "Epoch: 02999 loss_train: 29.2216 loss_rec: 29.2216 acc_train: 0.4908 loss_val: 29.6838 acc_val: 0.4958 time: 0.0328s\n",
      "Epoch: 03000 loss_train: 50.9637 loss_rec: 50.9637 acc_train: 0.4931 loss_val: 51.7585 acc_val: 0.4983 time: 0.0313s\n",
      "Epoch: 03001 loss_train: 52.3291 loss_rec: 52.3291 acc_train: 0.4931 loss_val: 53.1450 acc_val: 0.4983 time: 0.0323s\n",
      "Test set results: loss= 31.7152 accuracy= 0.5482\n",
      "Epoch: 03002 loss_train: 35.3827 loss_rec: 35.3827 acc_train: 0.4925 loss_val: 35.9398 acc_val: 0.4979 time: 0.0288s\n",
      "Epoch: 03003 loss_train: 2.1464 loss_rec: 2.1464 acc_train: 0.4910 loss_val: 2.1926 acc_val: 0.4971 time: 0.0298s\n",
      "Epoch: 03004 loss_train: 40.4514 loss_rec: 40.4514 acc_train: 0.5418 loss_val: 39.9067 acc_val: 0.5471 time: 0.0283s\n",
      "Epoch: 03005 loss_train: 64.0287 loss_rec: 64.0287 acc_train: 0.5384 loss_val: 63.2064 acc_val: 0.5446 time: 0.0278s\n",
      "Epoch: 03006 loss_train: 69.4254 loss_rec: 69.4254 acc_train: 0.5350 loss_val: 68.5422 acc_val: 0.5421 time: 0.0258s\n",
      "Epoch: 03007 loss_train: 58.4731 loss_rec: 58.4731 acc_train: 0.5379 loss_val: 57.7147 acc_val: 0.5458 time: 0.0273s\n",
      "Epoch: 03008 loss_train: 32.8833 loss_rec: 32.8833 acc_train: 0.5448 loss_val: 32.4386 acc_val: 0.5521 time: 0.0258s\n",
      "Epoch: 03009 loss_train: 4.8135 loss_rec: 4.8135 acc_train: 0.4918 loss_val: 4.9040 acc_val: 0.4975 time: 0.0273s\n",
      "Epoch: 03010 loss_train: 24.0906 loss_rec: 24.0906 acc_train: 0.4910 loss_val: 24.4766 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03011 loss_train: 23.4873 loss_rec: 23.4873 acc_train: 0.4911 loss_val: 23.8641 acc_val: 0.4963 time: 0.0273s\n",
      "Test set results: loss= 4.4299 accuracy= 0.5476\n",
      "Epoch: 03012 loss_train: 5.0041 loss_rec: 5.0041 acc_train: 0.4918 loss_val: 5.0976 acc_val: 0.4975 time: 0.0278s\n",
      "Epoch: 03013 loss_train: 26.2926 loss_rec: 26.2926 acc_train: 0.5493 loss_val: 25.9412 acc_val: 0.5546 time: 0.0263s\n",
      "Epoch: 03014 loss_train: 39.7279 loss_rec: 39.7279 acc_train: 0.5417 loss_val: 39.1915 acc_val: 0.5471 time: 0.0268s\n",
      "Epoch: 03015 loss_train: 36.2698 loss_rec: 36.2698 acc_train: 0.5442 loss_val: 35.7760 acc_val: 0.5529 time: 0.0268s\n",
      "Epoch: 03016 loss_train: 17.7714 loss_rec: 17.7714 acc_train: 0.5491 loss_val: 17.5650 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03017 loss_train: 14.1907 loss_rec: 14.1907 acc_train: 0.4917 loss_val: 14.4282 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 03018 loss_train: 27.3209 loss_rec: 27.3209 acc_train: 0.4909 loss_val: 27.7562 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 03019 loss_train: 21.2586 loss_rec: 21.2586 acc_train: 0.4912 loss_val: 21.6025 acc_val: 0.4967 time: 0.0283s\n",
      "Epoch: 03020 loss_train: 1.7773 loss_rec: 1.7773 acc_train: 0.6108 loss_val: 1.8079 acc_val: 0.6088 time: 0.0263s\n",
      "Epoch: 03021 loss_train: 9.6911 loss_rec: 9.6911 acc_train: 0.5529 loss_val: 9.6189 acc_val: 0.5596 time: 0.0273s\n",
      "Test set results: loss= 3.3904 accuracy= 0.5604\n",
      "Epoch: 03022 loss_train: 3.0337 loss_rec: 3.0337 acc_train: 0.5878 loss_val: 3.0540 acc_val: 0.5921 time: 0.0273s\n",
      "Epoch: 03023 loss_train: 17.1838 loss_rec: 17.1838 acc_train: 0.4914 loss_val: 17.4675 acc_val: 0.4967 time: 0.0263s\n",
      "Epoch: 03024 loss_train: 18.2309 loss_rec: 18.2309 acc_train: 0.4914 loss_val: 18.5304 acc_val: 0.4967 time: 0.0258s\n",
      "Epoch: 03025 loss_train: 1.5483 loss_rec: 1.5483 acc_train: 0.4911 loss_val: 1.5813 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 03026 loss_train: 24.8135 loss_rec: 24.8135 acc_train: 0.5494 loss_val: 24.4866 acc_val: 0.5550 time: 0.0278s\n",
      "Epoch: 03027 loss_train: 34.0110 loss_rec: 34.0110 acc_train: 0.5444 loss_val: 33.5496 acc_val: 0.5513 time: 0.0278s\n",
      "Epoch: 03028 loss_train: 26.9454 loss_rec: 26.9454 acc_train: 0.5489 loss_val: 26.5838 acc_val: 0.5542 time: 0.0268s\n",
      "Epoch: 03029 loss_train: 5.6698 loss_rec: 5.6698 acc_train: 0.5650 loss_val: 5.6535 acc_val: 0.5713 time: 0.0263s\n",
      "Epoch: 03030 loss_train: 29.5309 loss_rec: 29.5309 acc_train: 0.4908 loss_val: 30.0002 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 03031 loss_train: 44.5612 loss_rec: 44.5612 acc_train: 0.4925 loss_val: 45.2604 acc_val: 0.4979 time: 0.0278s\n",
      "Test set results: loss= 36.0995 accuracy= 0.5482\n",
      "Epoch: 03032 loss_train: 40.2624 loss_rec: 40.2624 acc_train: 0.4925 loss_val: 40.8962 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 03033 loss_train: 18.6510 loss_rec: 18.6510 acc_train: 0.4914 loss_val: 18.9573 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 03034 loss_train: 16.1549 loss_rec: 16.1549 acc_train: 0.5493 loss_val: 15.9764 acc_val: 0.5563 time: 0.0273s\n",
      "Epoch: 03035 loss_train: 31.9679 loss_rec: 31.9679 acc_train: 0.5460 loss_val: 31.5364 acc_val: 0.5533 time: 0.0263s\n",
      "Epoch: 03036 loss_train: 30.9596 loss_rec: 30.9596 acc_train: 0.5467 loss_val: 30.5425 acc_val: 0.5525 time: 0.0268s\n",
      "Epoch: 03037 loss_train: 14.9081 loss_rec: 14.9081 acc_train: 0.5503 loss_val: 14.7516 acc_val: 0.5588 time: 0.0263s\n",
      "Epoch: 03038 loss_train: 14.7813 loss_rec: 14.7813 acc_train: 0.4916 loss_val: 15.0288 acc_val: 0.4967 time: 0.0278s\n",
      "Epoch: 03039 loss_train: 25.8388 loss_rec: 25.8388 acc_train: 0.4911 loss_val: 26.2531 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03040 loss_train: 18.1329 loss_rec: 18.1329 acc_train: 0.4914 loss_val: 18.4318 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 03041 loss_train: 5.2628 loss_rec: 5.2628 acc_train: 0.5666 loss_val: 5.2524 acc_val: 0.5725 time: 0.0263s\n",
      "Test set results: loss= 12.8845 accuracy= 0.5136\n",
      "Epoch: 03042 loss_train: 11.3824 loss_rec: 11.3824 acc_train: 0.5493 loss_val: 11.2830 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03043 loss_train: 2.7219 loss_rec: 2.7219 acc_train: 0.5932 loss_val: 2.7457 acc_val: 0.5950 time: 0.0278s\n",
      "Epoch: 03044 loss_train: 19.0831 loss_rec: 19.0831 acc_train: 0.4914 loss_val: 19.3965 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 03045 loss_train: 21.6657 loss_rec: 21.6657 acc_train: 0.4912 loss_val: 22.0173 acc_val: 0.4967 time: 0.0258s\n",
      "Epoch: 03046 loss_train: 6.3966 loss_rec: 6.3966 acc_train: 0.4907 loss_val: 6.5118 acc_val: 0.4971 time: 0.0348s\n",
      "Epoch: 03047 loss_train: 22.0657 loss_rec: 22.0657 acc_train: 0.5498 loss_val: 21.7855 acc_val: 0.5563 time: 0.0358s\n",
      "Epoch: 03048 loss_train: 33.0891 loss_rec: 33.0891 acc_train: 0.5448 loss_val: 32.6412 acc_val: 0.5521 time: 0.0348s\n",
      "Epoch: 03049 loss_train: 27.8512 loss_rec: 27.8512 acc_train: 0.5488 loss_val: 27.4768 acc_val: 0.5521 time: 0.0358s\n",
      "Epoch: 03050 loss_train: 8.2466 loss_rec: 8.2466 acc_train: 0.5547 loss_val: 8.1940 acc_val: 0.5588 time: 0.0363s\n",
      "Epoch: 03051 loss_train: 25.0900 loss_rec: 25.0900 acc_train: 0.4912 loss_val: 25.4933 acc_val: 0.4963 time: 0.0348s\n",
      "Test set results: loss= 34.8986 accuracy= 0.5482\n",
      "Epoch: 03052 loss_train: 38.9246 loss_rec: 38.9246 acc_train: 0.4925 loss_val: 39.5395 acc_val: 0.4979 time: 0.0323s\n",
      "Epoch: 03053 loss_train: 33.7828 loss_rec: 33.7828 acc_train: 0.4925 loss_val: 34.3192 acc_val: 0.4979 time: 0.0248s\n",
      "Epoch: 03054 loss_train: 11.5896 loss_rec: 11.5896 acc_train: 0.4908 loss_val: 11.7875 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03055 loss_train: 22.9556 loss_rec: 22.9556 acc_train: 0.5498 loss_val: 22.6600 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 03056 loss_train: 39.2191 loss_rec: 39.2191 acc_train: 0.5425 loss_val: 38.6889 acc_val: 0.5479 time: 0.0278s\n",
      "Epoch: 03057 loss_train: 38.6885 loss_rec: 38.6885 acc_train: 0.5425 loss_val: 38.1645 acc_val: 0.5479 time: 0.0258s\n",
      "Epoch: 03058 loss_train: 23.0887 loss_rec: 23.0887 acc_train: 0.5491 loss_val: 22.7909 acc_val: 0.5554 time: 0.0293s\n",
      "Epoch: 03059 loss_train: 5.2826 loss_rec: 5.2826 acc_train: 0.4910 loss_val: 5.3813 acc_val: 0.4971 time: 0.0363s\n",
      "Epoch: 03060 loss_train: 15.9862 loss_rec: 15.9862 acc_train: 0.4916 loss_val: 16.2533 acc_val: 0.4967 time: 0.0348s\n",
      "Epoch: 03061 loss_train: 8.2116 loss_rec: 8.2116 acc_train: 0.4907 loss_val: 8.3558 acc_val: 0.4967 time: 0.0392s\n",
      "Test set results: loss= 15.9270 accuracy= 0.5123\n",
      "Epoch: 03062 loss_train: 14.0690 loss_rec: 14.0690 acc_train: 0.5503 loss_val: 13.9270 acc_val: 0.5588 time: 0.0348s\n",
      "Epoch: 03063 loss_train: 19.5179 loss_rec: 19.5179 acc_train: 0.5501 loss_val: 19.2812 acc_val: 0.5567 time: 0.0343s\n",
      "Epoch: 03064 loss_train: 9.6327 loss_rec: 9.6327 acc_train: 0.5523 loss_val: 9.5611 acc_val: 0.5592 time: 0.0343s\n",
      "Epoch: 03065 loss_train: 14.0638 loss_rec: 14.0638 acc_train: 0.4917 loss_val: 14.3010 acc_val: 0.4967 time: 0.0333s\n",
      "Epoch: 03066 loss_train: 19.5353 loss_rec: 19.5353 acc_train: 0.4913 loss_val: 19.8567 acc_val: 0.4967 time: 0.0293s\n",
      "Epoch: 03067 loss_train: 7.0727 loss_rec: 7.0727 acc_train: 0.4907 loss_val: 7.1987 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 03068 loss_train: 18.9726 loss_rec: 18.9726 acc_train: 0.5493 loss_val: 18.7454 acc_val: 0.5558 time: 0.0263s\n",
      "Epoch: 03069 loss_train: 27.8983 loss_rec: 27.8983 acc_train: 0.5488 loss_val: 27.5234 acc_val: 0.5521 time: 0.0278s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03070 loss_train: 21.0343 loss_rec: 21.0343 acc_train: 0.5497 loss_val: 20.7715 acc_val: 0.5554 time: 0.0298s\n",
      "Epoch: 03071 loss_train: 1.2245 loss_rec: 1.2245 acc_train: 0.6325 loss_val: 1.2538 acc_val: 0.6242 time: 0.0278s\n",
      "Test set results: loss= 24.5822 accuracy= 0.5464\n",
      "Epoch: 03072 loss_train: 27.4388 loss_rec: 27.4388 acc_train: 0.4911 loss_val: 27.8792 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03073 loss_train: 35.7007 loss_rec: 35.7007 acc_train: 0.4925 loss_val: 36.2677 acc_val: 0.4979 time: 0.0293s\n",
      "Epoch: 03074 loss_train: 25.6917 loss_rec: 25.6917 acc_train: 0.4912 loss_val: 26.1053 acc_val: 0.4963 time: 0.0298s\n",
      "Epoch: 03075 loss_train: 1.0667 loss_rec: 1.0667 acc_train: 0.6228 loss_val: 1.0932 acc_val: 0.6146 time: 0.0283s\n",
      "Epoch: 03076 loss_train: 16.2215 loss_rec: 16.2215 acc_train: 0.5493 loss_val: 16.0420 acc_val: 0.5563 time: 0.0283s\n",
      "Epoch: 03077 loss_train: 16.3140 loss_rec: 16.3140 acc_train: 0.5493 loss_val: 16.1329 acc_val: 0.5563 time: 0.0278s\n",
      "Epoch: 03078 loss_train: 2.2767 loss_rec: 2.2767 acc_train: 0.6005 loss_val: 2.3042 acc_val: 0.6033 time: 0.0268s\n",
      "Epoch: 03079 loss_train: 24.1204 loss_rec: 24.1204 acc_train: 0.4912 loss_val: 24.5101 acc_val: 0.4963 time: 0.0288s\n",
      "Epoch: 03080 loss_train: 30.9681 loss_rec: 30.9681 acc_train: 0.4909 loss_val: 31.4629 acc_val: 0.4958 time: 0.0284s\n",
      "Epoch: 03081 loss_train: 19.7567 loss_rec: 19.7567 acc_train: 0.4913 loss_val: 20.0820 acc_val: 0.4967 time: 0.0282s\n",
      "Test set results: loss= 7.1465 accuracy= 0.5268\n",
      "Epoch: 03082 loss_train: 6.3303 loss_rec: 6.3303 acc_train: 0.5607 loss_val: 6.3036 acc_val: 0.5688 time: 0.0278s\n",
      "Epoch: 03083 loss_train: 14.8527 loss_rec: 14.8527 acc_train: 0.5503 loss_val: 14.6973 acc_val: 0.5588 time: 0.0268s\n",
      "Epoch: 03084 loss_train: 8.0511 loss_rec: 8.0511 acc_train: 0.5553 loss_val: 8.0008 acc_val: 0.5592 time: 0.0268s\n",
      "Epoch: 03085 loss_train: 12.5728 loss_rec: 12.5728 acc_train: 0.4909 loss_val: 12.7871 acc_val: 0.4963 time: 0.0298s\n",
      "Epoch: 03086 loss_train: 15.2488 loss_rec: 15.2488 acc_train: 0.4905 loss_val: 15.5053 acc_val: 0.4958 time: 0.0437s\n",
      "Epoch: 03087 loss_train: 0.9875 loss_rec: 0.9875 acc_train: 0.6052 loss_val: 1.0075 acc_val: 0.5963 time: 0.0363s\n",
      "Epoch: 03088 loss_train: 15.0669 loss_rec: 15.0669 acc_train: 0.5497 loss_val: 14.9075 acc_val: 0.5567 time: 0.0328s\n",
      "Epoch: 03089 loss_train: 15.0672 loss_rec: 15.0672 acc_train: 0.5497 loss_val: 14.9079 acc_val: 0.5567 time: 0.0358s\n",
      "Epoch: 03090 loss_train: 1.3856 loss_rec: 1.3856 acc_train: 0.6235 loss_val: 1.4164 acc_val: 0.6204 time: 0.0353s\n",
      "Epoch: 03091 loss_train: 22.3005 loss_rec: 22.3005 acc_train: 0.4912 loss_val: 22.6641 acc_val: 0.4967 time: 0.0308s\n",
      "Test set results: loss= 23.5959 accuracy= 0.5464\n",
      "Epoch: 03092 loss_train: 26.3415 loss_rec: 26.3415 acc_train: 0.4912 loss_val: 26.7658 acc_val: 0.4963 time: 0.0323s\n",
      "Epoch: 03093 loss_train: 12.6873 loss_rec: 12.6873 acc_train: 0.4910 loss_val: 12.9034 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03094 loss_train: 14.6490 loss_rec: 14.6490 acc_train: 0.5503 loss_val: 14.4970 acc_val: 0.5588 time: 0.0283s\n",
      "Epoch: 03095 loss_train: 24.4882 loss_rec: 24.4882 acc_train: 0.5490 loss_val: 24.1672 acc_val: 0.5558 time: 0.0303s\n",
      "Epoch: 03096 loss_train: 18.6355 loss_rec: 18.6355 acc_train: 0.5502 loss_val: 18.4142 acc_val: 0.5571 time: 0.0288s\n",
      "Epoch: 03097 loss_train: 1.0023 loss_rec: 1.0023 acc_train: 0.6084 loss_val: 1.0223 acc_val: 0.5967 time: 0.0253s\n",
      "Epoch: 03098 loss_train: 14.8941 loss_rec: 14.8941 acc_train: 0.4906 loss_val: 15.1455 acc_val: 0.4958 time: 0.0268s\n",
      "Epoch: 03099 loss_train: 10.6791 loss_rec: 10.6791 acc_train: 0.4906 loss_val: 10.8636 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 03100 loss_train: 8.6900 loss_rec: 8.6900 acc_train: 0.5542 loss_val: 8.6320 acc_val: 0.5588 time: 0.0268s\n",
      "Epoch: 03101 loss_train: 11.7929 loss_rec: 11.7929 acc_train: 0.5495 loss_val: 11.6873 acc_val: 0.5542 time: 0.0273s\n",
      "Test set results: loss= 1.3037 accuracy= 0.6192\n",
      "Epoch: 03102 loss_train: 1.2351 loss_rec: 1.2351 acc_train: 0.6327 loss_val: 1.2644 acc_val: 0.6250 time: 0.0253s\n",
      "Epoch: 03103 loss_train: 18.3962 loss_rec: 18.3962 acc_train: 0.4905 loss_val: 18.7018 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 03104 loss_train: 18.7059 loss_rec: 18.7059 acc_train: 0.4904 loss_val: 19.0163 acc_val: 0.4958 time: 0.0303s\n",
      "Epoch: 03105 loss_train: 1.8792 loss_rec: 1.8792 acc_train: 0.4904 loss_val: 1.9210 acc_val: 0.4971 time: 0.0288s\n",
      "Epoch: 03106 loss_train: 25.6300 loss_rec: 25.6300 acc_train: 0.5489 loss_val: 25.2902 acc_val: 0.5546 time: 0.0293s\n",
      "Epoch: 03107 loss_train: 36.3629 loss_rec: 36.3629 acc_train: 0.5439 loss_val: 35.8688 acc_val: 0.5517 time: 0.0288s\n",
      "Epoch: 03108 loss_train: 31.1262 loss_rec: 31.1262 acc_train: 0.5467 loss_val: 30.7074 acc_val: 0.5525 time: 0.0288s\n",
      "Epoch: 03109 loss_train: 11.6901 loss_rec: 11.6901 acc_train: 0.5498 loss_val: 11.5856 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03110 loss_train: 21.0023 loss_rec: 21.0023 acc_train: 0.4902 loss_val: 21.3473 acc_val: 0.4958 time: 0.0283s\n",
      "Epoch: 03111 loss_train: 34.9718 loss_rec: 34.9718 acc_train: 0.4926 loss_val: 35.5296 acc_val: 0.4979 time: 0.0293s\n",
      "Test set results: loss= 27.1730 accuracy= 0.5464\n",
      "Epoch: 03112 loss_train: 30.3214 loss_rec: 30.3214 acc_train: 0.4910 loss_val: 30.8074 acc_val: 0.4963 time: 0.0288s\n",
      "Epoch: 03113 loss_train: 8.9557 loss_rec: 8.9557 acc_train: 0.4907 loss_val: 9.1125 acc_val: 0.4967 time: 0.0283s\n",
      "Epoch: 03114 loss_train: 24.3687 loss_rec: 24.3687 acc_train: 0.5490 loss_val: 24.0499 acc_val: 0.5558 time: 0.0278s\n",
      "Epoch: 03115 loss_train: 40.0097 loss_rec: 40.0097 acc_train: 0.5418 loss_val: 39.4715 acc_val: 0.5471 time: 0.0278s\n",
      "Epoch: 03116 loss_train: 39.2195 loss_rec: 39.2195 acc_train: 0.5421 loss_val: 38.6905 acc_val: 0.5471 time: 0.0273s\n",
      "Epoch: 03117 loss_train: 23.6591 loss_rec: 23.6591 acc_train: 0.5477 loss_val: 23.3524 acc_val: 0.5546 time: 0.0293s\n",
      "Epoch: 03118 loss_train: 4.3156 loss_rec: 4.3156 acc_train: 0.4903 loss_val: 4.4000 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 03119 loss_train: 15.0418 loss_rec: 15.0418 acc_train: 0.4905 loss_val: 15.2959 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 03120 loss_train: 7.6561 loss_rec: 7.6561 acc_train: 0.4902 loss_val: 7.7920 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 03121 loss_train: 13.9366 loss_rec: 13.9366 acc_train: 0.5503 loss_val: 13.7970 acc_val: 0.5588 time: 0.0273s\n",
      "Test set results: loss= 21.6109 accuracy= 0.5104\n",
      "Epoch: 03122 loss_train: 19.0876 loss_rec: 19.0876 acc_train: 0.5493 loss_val: 18.8590 acc_val: 0.5558 time: 0.0258s\n",
      "Epoch: 03123 loss_train: 9.2586 loss_rec: 9.2586 acc_train: 0.5520 loss_val: 9.1929 acc_val: 0.5588 time: 0.0283s\n",
      "Epoch: 03124 loss_train: 14.0932 loss_rec: 14.0932 acc_train: 0.4907 loss_val: 14.3321 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 03125 loss_train: 19.5489 loss_rec: 19.5489 acc_train: 0.4903 loss_val: 19.8725 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03126 loss_train: 7.4363 loss_rec: 7.4363 acc_train: 0.4902 loss_val: 7.5690 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 03127 loss_train: 18.0233 loss_rec: 18.0233 acc_train: 0.5492 loss_val: 17.8131 acc_val: 0.5558 time: 0.0382s\n",
      "Epoch: 03128 loss_train: 26.6948 loss_rec: 26.6948 acc_train: 0.5488 loss_val: 26.3379 acc_val: 0.5542 time: 0.0397s\n",
      "Epoch: 03129 loss_train: 19.9296 loss_rec: 19.9296 acc_train: 0.5492 loss_val: 19.6866 acc_val: 0.5567 time: 0.0363s\n",
      "Epoch: 03130 loss_train: 0.9622 loss_rec: 0.9622 acc_train: 0.6246 loss_val: 0.9853 acc_val: 0.6083 time: 0.0372s\n",
      "Epoch: 03131 loss_train: 22.4205 loss_rec: 22.4205 acc_train: 0.4902 loss_val: 22.7868 acc_val: 0.4958 time: 0.0313s\n",
      "Test set results: loss= 23.1163 accuracy= 0.5463\n",
      "Epoch: 03132 loss_train: 25.8079 loss_rec: 25.8079 acc_train: 0.4901 loss_val: 26.2247 acc_val: 0.4954 time: 0.0268s\n",
      "Epoch: 03133 loss_train: 11.8241 loss_rec: 11.8241 acc_train: 0.4905 loss_val: 12.0271 acc_val: 0.4967 time: 0.0258s\n",
      "Epoch: 03134 loss_train: 15.4877 loss_rec: 15.4877 acc_train: 0.5495 loss_val: 15.3210 acc_val: 0.5558 time: 0.0283s\n",
      "Epoch: 03135 loss_train: 25.5689 loss_rec: 25.5689 acc_train: 0.5489 loss_val: 25.2302 acc_val: 0.5546 time: 0.0273s\n",
      "Epoch: 03136 loss_train: 20.1374 loss_rec: 20.1374 acc_train: 0.5487 loss_val: 19.8908 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03137 loss_train: 1.6380 loss_rec: 1.6380 acc_train: 0.6145 loss_val: 1.6689 acc_val: 0.6154 time: 0.0268s\n",
      "Epoch: 03138 loss_train: 27.5949 loss_rec: 27.5949 acc_train: 0.4900 loss_val: 28.0393 acc_val: 0.4954 time: 0.0268s\n",
      "Epoch: 03139 loss_train: 37.0627 loss_rec: 37.0627 acc_train: 0.4926 loss_val: 37.6530 acc_val: 0.4979 time: 0.0263s\n",
      "Epoch: 03140 loss_train: 28.5507 loss_rec: 28.5507 acc_train: 0.4900 loss_val: 29.0098 acc_val: 0.4954 time: 0.0258s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03141 loss_train: 3.9471 loss_rec: 3.9471 acc_train: 0.4904 loss_val: 4.0258 acc_val: 0.4971 time: 0.0278s\n",
      "Test set results: loss= 35.6835 accuracy= 0.5051\n",
      "Epoch: 03142 loss_train: 31.5070 loss_rec: 31.5070 acc_train: 0.5464 loss_val: 31.0832 acc_val: 0.5533 time: 0.0263s\n",
      "Epoch: 03143 loss_train: 49.5213 loss_rec: 49.5213 acc_train: 0.5390 loss_val: 48.8711 acc_val: 0.5479 time: 0.0328s\n",
      "Epoch: 03144 loss_train: 50.9416 loss_rec: 50.9416 acc_train: 0.5383 loss_val: 50.2745 acc_val: 0.5463 time: 0.0402s\n",
      "Epoch: 03145 loss_train: 37.4121 loss_rec: 37.4121 acc_train: 0.5439 loss_val: 36.9053 acc_val: 0.5504 time: 0.0358s\n",
      "Epoch: 03146 loss_train: 10.8000 loss_rec: 10.8000 acc_train: 0.5509 loss_val: 10.7091 acc_val: 0.5583 time: 0.0392s\n",
      "Epoch: 03147 loss_train: 28.6851 loss_rec: 28.6851 acc_train: 0.4900 loss_val: 29.1462 acc_val: 0.4954 time: 0.0392s\n",
      "Epoch: 03148 loss_train: 48.9304 loss_rec: 48.9304 acc_train: 0.4925 loss_val: 49.7024 acc_val: 0.4979 time: 0.0313s\n",
      "Epoch: 03149 loss_train: 50.1866 loss_rec: 50.1866 acc_train: 0.4925 loss_val: 50.9778 acc_val: 0.4979 time: 0.0248s\n",
      "Epoch: 03150 loss_train: 34.3591 loss_rec: 34.3591 acc_train: 0.4926 loss_val: 34.9082 acc_val: 0.4979 time: 0.0278s\n",
      "Epoch: 03151 loss_train: 3.3086 loss_rec: 3.3086 acc_train: 0.4904 loss_val: 3.3770 acc_val: 0.4971 time: 0.0268s\n",
      "Test set results: loss= 42.2738 accuracy= 0.5016\n",
      "Epoch: 03152 loss_train: 37.3179 loss_rec: 37.3179 acc_train: 0.5439 loss_val: 36.8122 acc_val: 0.5504 time: 0.0273s\n",
      "Epoch: 03153 loss_train: 60.1470 loss_rec: 60.1470 acc_train: 0.5382 loss_val: 59.3707 acc_val: 0.5450 time: 0.0263s\n",
      "Epoch: 03154 loss_train: 65.9557 loss_rec: 65.9557 acc_train: 0.5359 loss_val: 65.1137 acc_val: 0.5433 time: 0.0263s\n",
      "Epoch: 03155 loss_train: 56.4129 loss_rec: 56.4129 acc_train: 0.5377 loss_val: 55.6803 acc_val: 0.5458 time: 0.0293s\n",
      "Epoch: 03156 loss_train: 33.1887 loss_rec: 33.1887 acc_train: 0.5444 loss_val: 32.7404 acc_val: 0.5513 time: 0.0273s\n",
      "Epoch: 03157 loss_train: 1.3328 loss_rec: 1.3328 acc_train: 0.5689 loss_val: 1.3607 acc_val: 0.5608 time: 0.0273s\n",
      "Epoch: 03158 loss_train: 23.7113 loss_rec: 23.7113 acc_train: 0.4901 loss_val: 24.0966 acc_val: 0.4958 time: 0.0308s\n",
      "Epoch: 03159 loss_train: 27.3202 loss_rec: 27.3202 acc_train: 0.4900 loss_val: 27.7603 acc_val: 0.4954 time: 0.0387s\n",
      "Epoch: 03160 loss_train: 13.8064 loss_rec: 13.8064 acc_train: 0.4903 loss_val: 14.0407 acc_val: 0.4958 time: 0.0387s\n",
      "Epoch: 03161 loss_train: 13.0009 loss_rec: 13.0009 acc_train: 0.5524 loss_val: 12.8767 acc_val: 0.5608 time: 0.0392s\n",
      "Test set results: loss= 25.8129 accuracy= 0.5079\n",
      "Epoch: 03162 loss_train: 22.7968 loss_rec: 22.7968 acc_train: 0.5484 loss_val: 22.5045 acc_val: 0.5550 time: 0.0387s\n",
      "Epoch: 03163 loss_train: 17.4011 loss_rec: 17.4011 acc_train: 0.5491 loss_val: 17.2010 acc_val: 0.5554 time: 0.0243s\n",
      "Epoch: 03164 loss_train: 1.1569 loss_rec: 1.1569 acc_train: 0.5594 loss_val: 1.1799 acc_val: 0.5504 time: 0.0268s\n",
      "Epoch: 03165 loss_train: 10.3992 loss_rec: 10.3992 acc_train: 0.4901 loss_val: 10.5794 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 03166 loss_train: 2.4549 loss_rec: 2.4549 acc_train: 0.4904 loss_val: 2.5084 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 03167 loss_train: 18.2747 loss_rec: 18.2747 acc_train: 0.5487 loss_val: 18.0594 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03168 loss_train: 23.3393 loss_rec: 23.3393 acc_train: 0.5477 loss_val: 23.0375 acc_val: 0.5546 time: 0.0273s\n",
      "Epoch: 03169 loss_train: 13.7349 loss_rec: 13.7349 acc_train: 0.5507 loss_val: 13.5983 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 03170 loss_train: 8.8563 loss_rec: 8.8563 acc_train: 0.4901 loss_val: 9.0114 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 03171 loss_train: 14.4784 loss_rec: 14.4784 acc_train: 0.4900 loss_val: 14.7234 acc_val: 0.4958 time: 0.0263s\n",
      "Test set results: loss= 2.6356 accuracy= 0.5473\n",
      "Epoch: 03172 loss_train: 3.0018 loss_rec: 3.0018 acc_train: 0.4904 loss_val: 3.0650 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 03173 loss_train: 20.9911 loss_rec: 20.9911 acc_train: 0.5497 loss_val: 20.7292 acc_val: 0.5563 time: 0.0278s\n",
      "Epoch: 03174 loss_train: 29.0187 loss_rec: 29.0187 acc_train: 0.5463 loss_val: 28.6305 acc_val: 0.5521 time: 0.0263s\n",
      "Epoch: 03175 loss_train: 21.9764 loss_rec: 21.9764 acc_train: 0.5498 loss_val: 21.6975 acc_val: 0.5563 time: 0.0263s\n",
      "Epoch: 03176 loss_train: 2.1836 loss_rec: 2.1836 acc_train: 0.6014 loss_val: 2.2112 acc_val: 0.6054 time: 0.0293s\n",
      "Epoch: 03177 loss_train: 29.1820 loss_rec: 29.1820 acc_train: 0.4900 loss_val: 29.6506 acc_val: 0.4954 time: 0.0273s\n",
      "Epoch: 03178 loss_train: 41.2353 loss_rec: 41.2353 acc_train: 0.4926 loss_val: 41.8893 acc_val: 0.4979 time: 0.0273s\n",
      "Epoch: 03179 loss_train: 35.4469 loss_rec: 35.4469 acc_train: 0.4915 loss_val: 36.0124 acc_val: 0.4971 time: 0.0258s\n",
      "Epoch: 03180 loss_train: 13.6391 loss_rec: 13.6391 acc_train: 0.4904 loss_val: 13.8706 acc_val: 0.4958 time: 0.0358s\n",
      "Epoch: 03181 loss_train: 19.8716 loss_rec: 19.8716 acc_train: 0.5493 loss_val: 19.6290 acc_val: 0.5567 time: 0.0397s\n",
      "Test set results: loss= 40.5929 accuracy= 0.5018\n",
      "Epoch: 03182 loss_train: 35.8364 loss_rec: 35.8364 acc_train: 0.5443 loss_val: 35.3493 acc_val: 0.5529 time: 0.0407s\n",
      "Epoch: 03183 loss_train: 35.9488 loss_rec: 35.9488 acc_train: 0.5443 loss_val: 35.4604 acc_val: 0.5529 time: 0.0338s\n",
      "Epoch: 03184 loss_train: 21.7871 loss_rec: 21.7871 acc_train: 0.5498 loss_val: 21.5119 acc_val: 0.5563 time: 0.0353s\n",
      "Epoch: 03185 loss_train: 4.4403 loss_rec: 4.4403 acc_train: 0.4903 loss_val: 4.5268 acc_val: 0.4971 time: 0.0258s\n",
      "Epoch: 03186 loss_train: 14.0805 loss_rec: 14.0805 acc_train: 0.4902 loss_val: 14.3191 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 03187 loss_train: 6.3319 loss_rec: 6.3319 acc_train: 0.4903 loss_val: 6.4478 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 03188 loss_train: 14.8806 loss_rec: 14.8806 acc_train: 0.5494 loss_val: 14.7239 acc_val: 0.5558 time: 0.0273s\n",
      "Epoch: 03189 loss_train: 20.2871 loss_rec: 20.2871 acc_train: 0.5488 loss_val: 20.0372 acc_val: 0.5554 time: 0.0343s\n",
      "Epoch: 03190 loss_train: 11.1891 loss_rec: 11.1891 acc_train: 0.5493 loss_val: 11.0918 acc_val: 0.5554 time: 0.0372s\n",
      "Epoch: 03191 loss_train: 10.8509 loss_rec: 10.8509 acc_train: 0.4899 loss_val: 11.0384 acc_val: 0.4967 time: 0.0368s\n",
      "Test set results: loss= 14.2710 accuracy= 0.5463\n",
      "Epoch: 03192 loss_train: 15.9622 loss_rec: 15.9622 acc_train: 0.4898 loss_val: 16.2307 acc_val: 0.4958 time: 0.0368s\n",
      "Epoch: 03193 loss_train: 4.1596 loss_rec: 4.1596 acc_train: 0.4904 loss_val: 4.2416 acc_val: 0.4971 time: 0.0353s\n",
      "Epoch: 03194 loss_train: 20.1923 loss_rec: 20.1923 acc_train: 0.5487 loss_val: 19.9444 acc_val: 0.5554 time: 0.0392s\n",
      "Epoch: 03195 loss_train: 28.6055 loss_rec: 28.6055 acc_train: 0.5484 loss_val: 28.2227 acc_val: 0.5517 time: 0.0298s\n",
      "Epoch: 03196 loss_train: 22.1000 loss_rec: 22.1000 acc_train: 0.5498 loss_val: 21.8196 acc_val: 0.5567 time: 0.0263s\n",
      "Epoch: 03197 loss_train: 2.7743 loss_rec: 2.7743 acc_train: 0.5906 loss_val: 2.7971 acc_val: 0.5929 time: 0.0273s\n",
      "Epoch: 03198 loss_train: 28.6075 loss_rec: 28.6075 acc_train: 0.4900 loss_val: 29.0673 acc_val: 0.4954 time: 0.0377s\n",
      "Epoch: 03199 loss_train: 41.0090 loss_rec: 41.0090 acc_train: 0.4915 loss_val: 41.6597 acc_val: 0.4971 time: 0.0377s\n",
      "Epoch: 03200 loss_train: 35.6878 loss_rec: 35.6878 acc_train: 0.4915 loss_val: 36.2571 acc_val: 0.4971 time: 0.0377s\n",
      "Epoch: 03201 loss_train: 14.4836 loss_rec: 14.4836 acc_train: 0.4900 loss_val: 14.7288 acc_val: 0.4958 time: 0.0368s\n",
      "Test set results: loss= 20.8733 accuracy= 0.5100\n",
      "Epoch: 03202 loss_train: 18.4361 loss_rec: 18.4361 acc_train: 0.5493 loss_val: 18.2183 acc_val: 0.5571 time: 0.0377s\n",
      "Epoch: 03203 loss_train: 33.9450 loss_rec: 33.9450 acc_train: 0.5447 loss_val: 33.4852 acc_val: 0.5529 time: 0.0328s\n",
      "Epoch: 03204 loss_train: 33.8169 loss_rec: 33.8169 acc_train: 0.5447 loss_val: 33.3591 acc_val: 0.5529 time: 0.0248s\n",
      "Epoch: 03205 loss_train: 19.6097 loss_rec: 19.6097 acc_train: 0.5492 loss_val: 19.3716 acc_val: 0.5567 time: 0.0283s\n",
      "Epoch: 03206 loss_train: 6.6867 loss_rec: 6.6867 acc_train: 0.4903 loss_val: 6.8080 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 03207 loss_train: 16.3658 loss_rec: 16.3658 acc_train: 0.4898 loss_val: 16.6406 acc_val: 0.4958 time: 0.0358s\n",
      "Epoch: 03208 loss_train: 8.8002 loss_rec: 8.8002 acc_train: 0.4901 loss_val: 8.9544 acc_val: 0.4967 time: 0.0382s\n",
      "Epoch: 03209 loss_train: 12.3203 loss_rec: 12.3203 acc_train: 0.5478 loss_val: 12.2063 acc_val: 0.5542 time: 0.0382s\n",
      "Epoch: 03210 loss_train: 17.7148 loss_rec: 17.7148 acc_train: 0.5491 loss_val: 17.5092 acc_val: 0.5554 time: 0.0363s\n",
      "Epoch: 03211 loss_train: 8.7797 loss_rec: 8.7797 acc_train: 0.5536 loss_val: 8.7199 acc_val: 0.5588 time: 0.0392s\n",
      "Test set results: loss= 11.6149 accuracy= 0.5470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03212 loss_train: 13.0055 loss_rec: 13.0055 acc_train: 0.4907 loss_val: 13.2270 acc_val: 0.4963 time: 0.0363s\n",
      "Epoch: 03213 loss_train: 17.7935 loss_rec: 17.7935 acc_train: 0.4898 loss_val: 18.0904 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03214 loss_train: 5.8633 loss_rec: 5.8633 acc_train: 0.4903 loss_val: 5.9719 acc_val: 0.4971 time: 0.0253s\n",
      "Epoch: 03215 loss_train: 18.5968 loss_rec: 18.5968 acc_train: 0.5493 loss_val: 18.3759 acc_val: 0.5571 time: 0.0273s\n",
      "Epoch: 03216 loss_train: 27.1356 loss_rec: 27.1356 acc_train: 0.5487 loss_val: 26.7719 acc_val: 0.5521 time: 0.0298s\n",
      "Epoch: 03217 loss_train: 20.9136 loss_rec: 20.9136 acc_train: 0.5497 loss_val: 20.6531 acc_val: 0.5563 time: 0.0412s\n",
      "Epoch: 03218 loss_train: 2.1259 loss_rec: 2.1259 acc_train: 0.6028 loss_val: 2.1539 acc_val: 0.6042 time: 0.0353s\n",
      "Epoch: 03219 loss_train: 27.9278 loss_rec: 27.9278 acc_train: 0.4900 loss_val: 28.3772 acc_val: 0.4954 time: 0.0358s\n",
      "Epoch: 03220 loss_train: 39.1264 loss_rec: 39.1264 acc_train: 0.4915 loss_val: 39.7483 acc_val: 0.4971 time: 0.0377s\n",
      "Epoch: 03221 loss_train: 32.8661 loss_rec: 32.8661 acc_train: 0.4898 loss_val: 33.3922 acc_val: 0.4950 time: 0.0377s\n",
      "Test set results: loss= 9.8150 accuracy= 0.5463\n",
      "Epoch: 03222 loss_train: 11.0020 loss_rec: 11.0020 acc_train: 0.4899 loss_val: 11.1918 acc_val: 0.4967 time: 0.0268s\n",
      "Epoch: 03223 loss_train: 22.0537 loss_rec: 22.0537 acc_train: 0.5498 loss_val: 21.7738 acc_val: 0.5567 time: 0.0273s\n",
      "Epoch: 03224 loss_train: 38.0704 loss_rec: 38.0704 acc_train: 0.5433 loss_val: 37.5554 acc_val: 0.5496 time: 0.0263s\n",
      "Epoch: 03225 loss_train: 38.4860 loss_rec: 38.4860 acc_train: 0.5425 loss_val: 37.9660 acc_val: 0.5479 time: 0.0278s\n",
      "Epoch: 03226 loss_train: 24.8270 loss_rec: 24.8270 acc_train: 0.5487 loss_val: 24.5000 acc_val: 0.5546 time: 0.0289s\n",
      "Epoch: 03227 loss_train: 0.9683 loss_rec: 0.9683 acc_train: 0.6071 loss_val: 0.9879 acc_val: 0.5979 time: 0.0268s\n",
      "Epoch: 03228 loss_train: 22.2917 loss_rec: 22.2917 acc_train: 0.4895 loss_val: 22.6563 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 03229 loss_train: 25.8695 loss_rec: 25.8695 acc_train: 0.4901 loss_val: 26.2873 acc_val: 0.4954 time: 0.0278s\n",
      "Epoch: 03230 loss_train: 12.9135 loss_rec: 12.9135 acc_train: 0.4907 loss_val: 13.1336 acc_val: 0.4963 time: 0.0343s\n",
      "Epoch: 03231 loss_train: 12.8452 loss_rec: 12.8452 acc_train: 0.5524 loss_val: 12.7230 acc_val: 0.5608 time: 0.0397s\n",
      "Test set results: loss= 25.1846 accuracy= 0.5098\n",
      "Epoch: 03232 loss_train: 22.2419 loss_rec: 22.2419 acc_train: 0.5498 loss_val: 21.9586 acc_val: 0.5567 time: 0.0368s\n",
      "Epoch: 03233 loss_train: 16.9619 loss_rec: 16.9619 acc_train: 0.5491 loss_val: 16.7692 acc_val: 0.5554 time: 0.0372s\n",
      "Epoch: 03234 loss_train: 1.0906 loss_rec: 1.0906 acc_train: 0.5577 loss_val: 1.1119 acc_val: 0.5496 time: 0.0377s\n",
      "Epoch: 03235 loss_train: 11.2624 loss_rec: 11.2624 acc_train: 0.4899 loss_val: 11.4565 acc_val: 0.4967 time: 0.0368s\n",
      "Epoch: 03236 loss_train: 4.7511 loss_rec: 4.7511 acc_train: 0.4897 loss_val: 4.8424 acc_val: 0.4967 time: 0.0258s\n",
      "Epoch: 03237 loss_train: 14.9907 loss_rec: 14.9907 acc_train: 0.5494 loss_val: 14.8319 acc_val: 0.5558 time: 0.0258s\n",
      "Epoch: 03238 loss_train: 19.4980 loss_rec: 19.4980 acc_train: 0.5492 loss_val: 19.2617 acc_val: 0.5567 time: 0.0263s\n",
      "Epoch: 03239 loss_train: 9.8932 loss_rec: 9.8932 acc_train: 0.5520 loss_val: 9.8165 acc_val: 0.5596 time: 0.0303s\n",
      "Epoch: 03240 loss_train: 12.3759 loss_rec: 12.3759 acc_train: 0.4898 loss_val: 12.5876 acc_val: 0.4967 time: 0.0407s\n",
      "Epoch: 03241 loss_train: 17.9102 loss_rec: 17.9102 acc_train: 0.4898 loss_val: 18.2087 acc_val: 0.4958 time: 0.0373s\n",
      "Test set results: loss= 6.0612 accuracy= 0.5471\n",
      "Epoch: 03242 loss_train: 6.8217 loss_rec: 6.8217 acc_train: 0.4903 loss_val: 6.9450 acc_val: 0.4971 time: 0.0387s\n",
      "Epoch: 03243 loss_train: 16.8331 loss_rec: 16.8331 acc_train: 0.5491 loss_val: 16.6425 acc_val: 0.5554 time: 0.0407s\n",
      "Epoch: 03244 loss_train: 24.7393 loss_rec: 24.7393 acc_train: 0.5487 loss_val: 24.4134 acc_val: 0.5546 time: 0.0373s\n",
      "Epoch: 03245 loss_train: 18.1746 loss_rec: 18.1746 acc_train: 0.5477 loss_val: 17.9610 acc_val: 0.5554 time: 0.0263s\n",
      "Epoch: 03246 loss_train: 1.0482 loss_rec: 1.0482 acc_train: 0.5797 loss_val: 1.0687 acc_val: 0.5758 time: 0.0248s\n",
      "Epoch: 03247 loss_train: 13.3325 loss_rec: 13.3325 acc_train: 0.4906 loss_val: 13.5592 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03248 loss_train: 8.8003 loss_rec: 8.8003 acc_train: 0.4901 loss_val: 8.9544 acc_val: 0.4967 time: 0.0278s\n",
      "Epoch: 03249 loss_train: 9.6209 loss_rec: 9.6209 acc_train: 0.5519 loss_val: 9.5487 acc_val: 0.5596 time: 0.0397s\n",
      "Epoch: 03250 loss_train: 12.8745 loss_rec: 12.8745 acc_train: 0.5524 loss_val: 12.7519 acc_val: 0.5608 time: 0.0387s\n",
      "Epoch: 03251 loss_train: 2.7628 loss_rec: 2.7628 acc_train: 0.5898 loss_val: 2.7853 acc_val: 0.5925 time: 0.0368s\n",
      "Test set results: loss= 17.2831 accuracy= 0.5462\n",
      "Epoch: 03252 loss_train: 19.3144 loss_rec: 19.3144 acc_train: 0.4897 loss_val: 19.6344 acc_val: 0.4958 time: 0.0378s\n",
      "Epoch: 03253 loss_train: 23.7108 loss_rec: 23.7108 acc_train: 0.4894 loss_val: 24.0961 acc_val: 0.4958 time: 0.0372s\n",
      "Epoch: 03254 loss_train: 11.6542 loss_rec: 11.6542 acc_train: 0.4899 loss_val: 11.8544 acc_val: 0.4967 time: 0.0363s\n",
      "Epoch: 03255 loss_train: 13.1484 loss_rec: 13.1484 acc_train: 0.5519 loss_val: 13.0212 acc_val: 0.5604 time: 0.0248s\n",
      "Epoch: 03256 loss_train: 21.8758 loss_rec: 21.8758 acc_train: 0.5498 loss_val: 21.5988 acc_val: 0.5563 time: 0.0273s\n",
      "Epoch: 03257 loss_train: 16.0872 loss_rec: 16.0872 acc_train: 0.5490 loss_val: 15.9094 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03258 loss_train: 2.1025 loss_rec: 2.1025 acc_train: 0.4892 loss_val: 2.1491 acc_val: 0.4963 time: 0.0288s\n",
      "Epoch: 03259 loss_train: 5.5823 loss_rec: 5.5823 acc_train: 0.4897 loss_val: 5.6865 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 03260 loss_train: 5.9430 loss_rec: 5.9430 acc_train: 0.5613 loss_val: 5.9213 acc_val: 0.5688 time: 0.0263s\n",
      "Epoch: 03261 loss_train: 3.7410 loss_rec: 3.7410 acc_train: 0.5802 loss_val: 3.7495 acc_val: 0.5837 time: 0.0263s\n",
      "Test set results: loss= 9.7218 accuracy= 0.5463\n",
      "Epoch: 03262 loss_train: 10.8982 loss_rec: 10.8982 acc_train: 0.4899 loss_val: 11.0863 acc_val: 0.4967 time: 0.0258s\n",
      "Epoch: 03263 loss_train: 8.9216 loss_rec: 8.9216 acc_train: 0.4901 loss_val: 9.0779 acc_val: 0.4967 time: 0.0278s\n",
      "Epoch: 03264 loss_train: 7.3530 loss_rec: 7.3530 acc_train: 0.5559 loss_val: 7.3106 acc_val: 0.5604 time: 0.0273s\n",
      "Epoch: 03265 loss_train: 8.9096 loss_rec: 8.9096 acc_train: 0.5537 loss_val: 8.8482 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 03266 loss_train: 2.3617 loss_rec: 2.3617 acc_train: 0.4892 loss_val: 2.4134 acc_val: 0.4963 time: 0.0268s\n",
      "Epoch: 03267 loss_train: 1.3829 loss_rec: 1.3829 acc_train: 0.6216 loss_val: 1.4130 acc_val: 0.6200 time: 0.0273s\n",
      "Epoch: 03268 loss_train: 4.0537 loss_rec: 4.0537 acc_train: 0.4897 loss_val: 4.1341 acc_val: 0.4967 time: 0.0258s\n",
      "Epoch: 03269 loss_train: 5.7574 loss_rec: 5.7574 acc_train: 0.5618 loss_val: 5.7385 acc_val: 0.5679 time: 0.0273s\n",
      "Epoch: 03270 loss_train: 2.3985 loss_rec: 2.3985 acc_train: 0.5963 loss_val: 2.4243 acc_val: 0.5958 time: 0.0278s\n",
      "Epoch: 03271 loss_train: 12.5929 loss_rec: 12.5929 acc_train: 0.4898 loss_val: 12.8080 acc_val: 0.4967 time: 0.0258s\n",
      "Test set results: loss= 9.4864 accuracy= 0.5463\n",
      "Epoch: 03272 loss_train: 10.6363 loss_rec: 10.6363 acc_train: 0.4901 loss_val: 10.8202 acc_val: 0.4967 time: 0.0293s\n",
      "Epoch: 03273 loss_train: 5.8438 loss_rec: 5.8438 acc_train: 0.5610 loss_val: 5.8236 acc_val: 0.5679 time: 0.0273s\n",
      "Epoch: 03274 loss_train: 7.5640 loss_rec: 7.5640 acc_train: 0.5556 loss_val: 7.5191 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 03275 loss_train: 3.4903 loss_rec: 3.4903 acc_train: 0.4897 loss_val: 3.5616 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 03276 loss_train: 1.2413 loss_rec: 1.2413 acc_train: 0.6311 loss_val: 1.2702 acc_val: 0.6263 time: 0.0263s\n",
      "Epoch: 03277 loss_train: 2.5021 loss_rec: 2.5021 acc_train: 0.4892 loss_val: 2.5565 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03278 loss_train: 8.1889 loss_rec: 8.1889 acc_train: 0.5547 loss_val: 8.1363 acc_val: 0.5592 time: 0.0278s\n",
      "Epoch: 03279 loss_train: 5.2526 loss_rec: 5.2526 acc_train: 0.5640 loss_val: 5.2416 acc_val: 0.5708 time: 0.0273s\n",
      "Epoch: 03280 loss_train: 10.4586 loss_rec: 10.4586 acc_train: 0.4901 loss_val: 10.6397 acc_val: 0.4967 time: 0.0278s\n",
      "Epoch: 03281 loss_train: 9.6987 loss_rec: 9.6987 acc_train: 0.4901 loss_val: 9.8675 acc_val: 0.4967 time: 0.0288s\n",
      "Test set results: loss= 6.4158 accuracy= 0.5287\n",
      "Epoch: 03282 loss_train: 5.6872 loss_rec: 5.6872 acc_train: 0.5627 loss_val: 5.6694 acc_val: 0.5692 time: 0.0258s\n",
      "Epoch: 03283 loss_train: 6.5511 loss_rec: 6.5511 acc_train: 0.5592 loss_val: 6.5199 acc_val: 0.5675 time: 0.0278s\n",
      "Epoch: 03284 loss_train: 5.3486 loss_rec: 5.3486 acc_train: 0.4897 loss_val: 5.4493 acc_val: 0.4967 time: 0.0278s\n",
      "Epoch: 03285 loss_train: 1.5005 loss_rec: 1.5005 acc_train: 0.4892 loss_val: 1.5329 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03286 loss_train: 12.6990 loss_rec: 12.6990 acc_train: 0.5523 loss_val: 12.5789 acc_val: 0.5608 time: 0.0273s\n",
      "Epoch: 03287 loss_train: 12.5354 loss_rec: 12.5354 acc_train: 0.5523 loss_val: 12.4180 acc_val: 0.5608 time: 0.0268s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03288 loss_train: 0.9558 loss_rec: 0.9558 acc_train: 0.6071 loss_val: 0.9755 acc_val: 0.5979 time: 0.0273s\n",
      "Epoch: 03289 loss_train: 10.8541 loss_rec: 10.8541 acc_train: 0.4899 loss_val: 11.0416 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 03290 loss_train: 4.4917 loss_rec: 4.4917 acc_train: 0.4897 loss_val: 4.5789 acc_val: 0.4967 time: 0.0258s\n",
      "Epoch: 03291 loss_train: 14.9753 loss_rec: 14.9753 acc_train: 0.5494 loss_val: 14.8166 acc_val: 0.5558 time: 0.0273s\n",
      "Test set results: loss= 21.9209 accuracy= 0.5098\n",
      "Epoch: 03292 loss_train: 19.3609 loss_rec: 19.3609 acc_train: 0.5492 loss_val: 19.1267 acc_val: 0.5567 time: 0.0258s\n",
      "Epoch: 03293 loss_train: 9.7735 loss_rec: 9.7735 acc_train: 0.5519 loss_val: 9.6985 acc_val: 0.5596 time: 0.0268s\n",
      "Epoch: 03294 loss_train: 12.3641 loss_rec: 12.3641 acc_train: 0.4898 loss_val: 12.5755 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 03295 loss_train: 17.9030 loss_rec: 17.9030 acc_train: 0.4898 loss_val: 18.2015 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03296 loss_train: 6.9539 loss_rec: 6.9539 acc_train: 0.4897 loss_val: 7.0793 acc_val: 0.4967 time: 0.0303s\n",
      "Epoch: 03297 loss_train: 16.5040 loss_rec: 16.5040 acc_train: 0.5481 loss_val: 16.3189 acc_val: 0.5554 time: 0.0283s\n",
      "Epoch: 03298 loss_train: 24.3362 loss_rec: 24.3362 acc_train: 0.5494 loss_val: 24.0174 acc_val: 0.5550 time: 0.0278s\n",
      "Epoch: 03299 loss_train: 17.7706 loss_rec: 17.7706 acc_train: 0.5483 loss_val: 17.5636 acc_val: 0.5558 time: 0.0323s\n",
      "Epoch: 03300 loss_train: 1.1917 loss_rec: 1.1917 acc_train: 0.5593 loss_val: 1.2155 acc_val: 0.5517 time: 0.0407s\n",
      "Epoch: 03301 loss_train: 10.6554 loss_rec: 10.6554 acc_train: 0.4893 loss_val: 10.8397 acc_val: 0.4963 time: 0.0382s\n",
      "Test set results: loss= 3.1593 accuracy= 0.5458\n",
      "Epoch: 03302 loss_train: 3.5858 loss_rec: 3.5858 acc_train: 0.4892 loss_val: 3.6586 acc_val: 0.4963 time: 0.0353s\n",
      "Epoch: 03303 loss_train: 16.3208 loss_rec: 16.3208 acc_train: 0.5481 loss_val: 16.1388 acc_val: 0.5554 time: 0.0372s\n",
      "Epoch: 03304 loss_train: 21.1874 loss_rec: 21.1874 acc_train: 0.5497 loss_val: 20.9220 acc_val: 0.5563 time: 0.0298s\n",
      "Epoch: 03305 loss_train: 12.0169 loss_rec: 12.0169 acc_train: 0.5486 loss_val: 11.9071 acc_val: 0.5542 time: 0.0402s\n",
      "Epoch: 03306 loss_train: 9.5528 loss_rec: 9.5528 acc_train: 0.4894 loss_val: 9.7194 acc_val: 0.4963 time: 0.0243s\n",
      "Epoch: 03307 loss_train: 14.8279 loss_rec: 14.8279 acc_train: 0.4900 loss_val: 15.0787 acc_val: 0.4958 time: 0.0253s\n",
      "Epoch: 03308 loss_train: 3.7146 loss_rec: 3.7146 acc_train: 0.4892 loss_val: 3.7895 acc_val: 0.4963 time: 0.0288s\n",
      "Epoch: 03309 loss_train: 19.5422 loss_rec: 19.5422 acc_train: 0.5492 loss_val: 19.3049 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 03310 loss_train: 27.4368 loss_rec: 27.4368 acc_train: 0.5488 loss_val: 27.0688 acc_val: 0.5521 time: 0.0298s\n",
      "Epoch: 03311 loss_train: 20.9265 loss_rec: 20.9265 acc_train: 0.5497 loss_val: 20.6657 acc_val: 0.5563 time: 0.0259s\n",
      "Test set results: loss= 2.3939 accuracy= 0.5788\n",
      "Epoch: 03312 loss_train: 2.1628 loss_rec: 2.1628 acc_train: 0.6019 loss_val: 2.1904 acc_val: 0.6054 time: 0.0268s\n",
      "Epoch: 03313 loss_train: 27.6828 loss_rec: 27.6828 acc_train: 0.4893 loss_val: 28.1286 acc_val: 0.4954 time: 0.0278s\n",
      "Epoch: 03314 loss_train: 39.0423 loss_rec: 39.0423 acc_train: 0.4915 loss_val: 39.6631 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 03315 loss_train: 33.2890 loss_rec: 33.2890 acc_train: 0.4898 loss_val: 33.8217 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03316 loss_train: 12.1939 loss_rec: 12.1939 acc_train: 0.4892 loss_val: 12.4028 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03317 loss_train: 19.9735 loss_rec: 19.9735 acc_train: 0.5487 loss_val: 19.7289 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03318 loss_train: 35.4205 loss_rec: 35.4205 acc_train: 0.5450 loss_val: 34.9396 acc_val: 0.5529 time: 0.0268s\n",
      "Epoch: 03319 loss_train: 35.6387 loss_rec: 35.6387 acc_train: 0.5443 loss_val: 35.1549 acc_val: 0.5529 time: 0.0278s\n",
      "Epoch: 03320 loss_train: 22.1276 loss_rec: 22.1276 acc_train: 0.5498 loss_val: 21.8462 acc_val: 0.5567 time: 0.0258s\n",
      "Epoch: 03321 loss_train: 2.8412 loss_rec: 2.8412 acc_train: 0.4892 loss_val: 2.9016 acc_val: 0.4963 time: 0.0273s\n",
      "Test set results: loss= 10.9375 accuracy= 0.5456\n",
      "Epoch: 03322 loss_train: 12.2512 loss_rec: 12.2512 acc_train: 0.4892 loss_val: 12.4610 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03323 loss_train: 4.9923 loss_rec: 4.9923 acc_train: 0.4897 loss_val: 5.0875 acc_val: 0.4967 time: 0.0263s\n",
      "Epoch: 03324 loss_train: 15.1019 loss_rec: 15.1019 acc_train: 0.5495 loss_val: 14.9410 acc_val: 0.5558 time: 0.0278s\n",
      "Epoch: 03325 loss_train: 20.1574 loss_rec: 20.1574 acc_train: 0.5488 loss_val: 19.9096 acc_val: 0.5554 time: 0.0298s\n",
      "Epoch: 03326 loss_train: 11.2819 loss_rec: 11.2819 acc_train: 0.5491 loss_val: 11.1828 acc_val: 0.5558 time: 0.0268s\n",
      "Epoch: 03327 loss_train: 9.9720 loss_rec: 9.9720 acc_train: 0.4894 loss_val: 10.1451 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03328 loss_train: 14.9767 loss_rec: 14.9767 acc_train: 0.4899 loss_val: 15.2298 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 03329 loss_train: 3.7629 loss_rec: 3.7629 acc_train: 0.4892 loss_val: 3.8386 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 03330 loss_train: 19.4588 loss_rec: 19.4588 acc_train: 0.5492 loss_val: 19.2228 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 03331 loss_train: 27.4346 loss_rec: 27.4346 acc_train: 0.5488 loss_val: 27.0668 acc_val: 0.5521 time: 0.0278s\n",
      "Test set results: loss= 23.9025 accuracy= 0.5096\n",
      "Epoch: 03332 loss_train: 21.1101 loss_rec: 21.1101 acc_train: 0.5497 loss_val: 20.8460 acc_val: 0.5563 time: 0.0273s\n",
      "Epoch: 03333 loss_train: 2.5426 loss_rec: 2.5426 acc_train: 0.5937 loss_val: 2.5671 acc_val: 0.5946 time: 0.0308s\n",
      "Epoch: 03334 loss_train: 27.4282 loss_rec: 27.4282 acc_train: 0.4893 loss_val: 27.8699 acc_val: 0.4954 time: 0.0258s\n",
      "Epoch: 03335 loss_train: 39.1408 loss_rec: 39.1408 acc_train: 0.4915 loss_val: 39.7630 acc_val: 0.4971 time: 0.0273s\n",
      "Epoch: 03336 loss_train: 33.8798 loss_rec: 33.8798 acc_train: 0.4891 loss_val: 34.4214 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03337 loss_train: 13.3486 loss_rec: 13.3486 acc_train: 0.4891 loss_val: 13.5756 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 03338 loss_train: 18.3073 loss_rec: 18.3073 acc_train: 0.5493 loss_val: 18.0912 acc_val: 0.5571 time: 0.0278s\n",
      "Epoch: 03339 loss_train: 33.3243 loss_rec: 33.3243 acc_train: 0.5447 loss_val: 32.8736 acc_val: 0.5529 time: 0.0273s\n",
      "Epoch: 03340 loss_train: 33.2954 loss_rec: 33.2954 acc_train: 0.5447 loss_val: 32.8452 acc_val: 0.5529 time: 0.0258s\n",
      "Epoch: 03341 loss_train: 19.7307 loss_rec: 19.7307 acc_train: 0.5487 loss_val: 19.4901 acc_val: 0.5554 time: 0.0278s\n",
      "Test set results: loss= 4.7212 accuracy= 0.5465\n",
      "Epoch: 03342 loss_train: 5.3277 loss_rec: 5.3277 acc_train: 0.4897 loss_val: 5.4281 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 03343 loss_train: 14.5641 loss_rec: 14.5641 acc_train: 0.4900 loss_val: 14.8106 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03344 loss_train: 7.2459 loss_rec: 7.2459 acc_train: 0.4895 loss_val: 7.3759 acc_val: 0.4967 time: 0.0278s\n",
      "Epoch: 03345 loss_train: 12.9409 loss_rec: 12.9409 acc_train: 0.5524 loss_val: 12.8170 acc_val: 0.5608 time: 0.0253s\n",
      "Epoch: 03346 loss_train: 18.1130 loss_rec: 18.1130 acc_train: 0.5493 loss_val: 17.9001 acc_val: 0.5567 time: 0.0263s\n",
      "Epoch: 03347 loss_train: 9.5137 loss_rec: 9.5137 acc_train: 0.5527 loss_val: 9.4430 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 03348 loss_train: 11.3624 loss_rec: 11.3624 acc_train: 0.4892 loss_val: 11.5582 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03349 loss_train: 16.0818 loss_rec: 16.0818 acc_train: 0.4898 loss_val: 16.3522 acc_val: 0.4958 time: 0.0298s\n",
      "Epoch: 03350 loss_train: 4.6988 loss_rec: 4.6988 acc_train: 0.4891 loss_val: 4.7894 acc_val: 0.4963 time: 0.0253s\n",
      "Epoch: 03351 loss_train: 18.6088 loss_rec: 18.6088 acc_train: 0.5484 loss_val: 18.3875 acc_val: 0.5558 time: 0.0258s\n",
      "Test set results: loss= 30.2708 accuracy= 0.5073\n",
      "Epoch: 03352 loss_train: 26.7328 loss_rec: 26.7328 acc_train: 0.5489 loss_val: 26.3748 acc_val: 0.5542 time: 0.0293s\n",
      "Epoch: 03353 loss_train: 20.6882 loss_rec: 20.6882 acc_train: 0.5497 loss_val: 20.4313 acc_val: 0.5563 time: 0.0273s\n",
      "Epoch: 03354 loss_train: 2.5104 loss_rec: 2.5104 acc_train: 0.5936 loss_val: 2.5351 acc_val: 0.5946 time: 0.0263s\n",
      "Epoch: 03355 loss_train: 26.9326 loss_rec: 26.9326 acc_train: 0.4894 loss_val: 27.3669 acc_val: 0.4954 time: 0.0278s\n",
      "Epoch: 03356 loss_train: 38.3360 loss_rec: 38.3360 acc_train: 0.4915 loss_val: 38.9460 acc_val: 0.4971 time: 0.0283s\n",
      "Epoch: 03357 loss_train: 32.9235 loss_rec: 32.9235 acc_train: 0.4892 loss_val: 33.4506 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03358 loss_train: 12.4423 loss_rec: 12.4423 acc_train: 0.4892 loss_val: 12.6550 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03359 loss_train: 18.9877 loss_rec: 18.9877 acc_train: 0.5492 loss_val: 18.7600 acc_val: 0.5567 time: 0.0283s\n",
      "Epoch: 03360 loss_train: 33.9794 loss_rec: 33.9794 acc_train: 0.5448 loss_val: 33.5194 acc_val: 0.5529 time: 0.0258s\n",
      "Epoch: 03361 loss_train: 34.0604 loss_rec: 34.0604 acc_train: 0.5448 loss_val: 33.5993 acc_val: 0.5529 time: 0.0273s\n",
      "Test set results: loss= 23.4414 accuracy= 0.5096\n",
      "Epoch: 03362 loss_train: 20.7030 loss_rec: 20.7030 acc_train: 0.5497 loss_val: 20.4458 acc_val: 0.5563 time: 0.0263s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03363 loss_train: 3.9341 loss_rec: 3.9341 acc_train: 0.4892 loss_val: 4.0126 acc_val: 0.4963 time: 0.0293s\n",
      "Epoch: 03364 loss_train: 13.0451 loss_rec: 13.0451 acc_train: 0.4892 loss_val: 13.2674 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 03365 loss_train: 5.7856 loss_rec: 5.7856 acc_train: 0.4897 loss_val: 5.8931 acc_val: 0.4967 time: 0.0278s\n",
      "Epoch: 03366 loss_train: 14.1020 loss_rec: 14.1020 acc_train: 0.5503 loss_val: 13.9584 acc_val: 0.5588 time: 0.0258s\n",
      "Epoch: 03367 loss_train: 19.1772 loss_rec: 19.1772 acc_train: 0.5492 loss_val: 18.9462 acc_val: 0.5567 time: 0.0273s\n",
      "Epoch: 03368 loss_train: 10.5805 loss_rec: 10.5805 acc_train: 0.5509 loss_val: 10.4920 acc_val: 0.5583 time: 0.0258s\n",
      "Epoch: 03369 loss_train: 10.1359 loss_rec: 10.1359 acc_train: 0.4894 loss_val: 10.3119 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03370 loss_train: 14.9441 loss_rec: 14.9441 acc_train: 0.4893 loss_val: 15.1968 acc_val: 0.4954 time: 0.0273s\n",
      "Epoch: 03371 loss_train: 3.8457 loss_rec: 3.8457 acc_train: 0.4892 loss_val: 3.9227 acc_val: 0.4963 time: 0.0273s\n",
      "Test set results: loss= 21.5461 accuracy= 0.5098\n",
      "Epoch: 03372 loss_train: 19.0299 loss_rec: 19.0299 acc_train: 0.5492 loss_val: 18.8013 acc_val: 0.5567 time: 0.0258s\n",
      "Epoch: 03373 loss_train: 26.9490 loss_rec: 26.9490 acc_train: 0.5490 loss_val: 26.5883 acc_val: 0.5542 time: 0.0268s\n",
      "Epoch: 03374 loss_train: 20.8120 loss_rec: 20.8120 acc_train: 0.5497 loss_val: 20.5529 acc_val: 0.5563 time: 0.0268s\n",
      "Epoch: 03375 loss_train: 2.6642 loss_rec: 2.6642 acc_train: 0.5917 loss_val: 2.6876 acc_val: 0.5942 time: 0.0278s\n",
      "Epoch: 03376 loss_train: 26.7662 loss_rec: 26.7662 acc_train: 0.4894 loss_val: 27.1979 acc_val: 0.4954 time: 0.0268s\n",
      "Epoch: 03377 loss_train: 38.3193 loss_rec: 38.3193 acc_train: 0.4908 loss_val: 38.9293 acc_val: 0.4971 time: 0.0263s\n",
      "Epoch: 03378 loss_train: 33.2230 loss_rec: 33.2230 acc_train: 0.4892 loss_val: 33.7548 acc_val: 0.4950 time: 0.0258s\n",
      "Epoch: 03379 loss_train: 13.1575 loss_rec: 13.1575 acc_train: 0.4892 loss_val: 13.3816 acc_val: 0.4958 time: 0.0293s\n",
      "Epoch: 03380 loss_train: 17.8390 loss_rec: 17.8390 acc_train: 0.5483 loss_val: 17.6310 acc_val: 0.5558 time: 0.0268s\n",
      "Epoch: 03381 loss_train: 32.5245 loss_rec: 32.5245 acc_train: 0.5448 loss_val: 32.0859 acc_val: 0.5521 time: 0.0273s\n",
      "Test set results: loss= 36.7475 accuracy= 0.5028\n",
      "Epoch: 03382 loss_train: 32.4446 loss_rec: 32.4446 acc_train: 0.5448 loss_val: 32.0069 acc_val: 0.5521 time: 0.0263s\n",
      "Epoch: 03383 loss_train: 19.1220 loss_rec: 19.1220 acc_train: 0.5492 loss_val: 18.8919 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 03384 loss_train: 5.5036 loss_rec: 5.5036 acc_train: 0.4891 loss_val: 5.6067 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03385 loss_train: 14.5901 loss_rec: 14.5901 acc_train: 0.4893 loss_val: 14.8370 acc_val: 0.4954 time: 0.0258s\n",
      "Epoch: 03386 loss_train: 7.4290 loss_rec: 7.4290 acc_train: 0.4895 loss_val: 7.5617 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 03387 loss_train: 12.3819 loss_rec: 12.3819 acc_train: 0.5523 loss_val: 12.2668 acc_val: 0.5608 time: 0.0293s\n",
      "Epoch: 03388 loss_train: 17.4527 loss_rec: 17.4527 acc_train: 0.5482 loss_val: 17.2511 acc_val: 0.5554 time: 0.0258s\n",
      "Epoch: 03389 loss_train: 9.0544 loss_rec: 9.0544 acc_train: 0.5519 loss_val: 8.9908 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 03390 loss_train: 11.3865 loss_rec: 11.3865 acc_train: 0.4892 loss_val: 11.5826 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03391 loss_train: 15.9374 loss_rec: 15.9374 acc_train: 0.4892 loss_val: 16.2056 acc_val: 0.4954 time: 0.0258s\n",
      "Test set results: loss= 4.1886 accuracy= 0.5458\n",
      "Epoch: 03392 loss_train: 4.7337 loss_rec: 4.7337 acc_train: 0.4891 loss_val: 4.8249 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03393 loss_train: 18.1796 loss_rec: 18.1796 acc_train: 0.5493 loss_val: 17.9657 acc_val: 0.5567 time: 0.0263s\n",
      "Epoch: 03394 loss_train: 26.1983 loss_rec: 26.1983 acc_train: 0.5488 loss_val: 25.8489 acc_val: 0.5542 time: 0.0263s\n",
      "Epoch: 03395 loss_train: 20.3152 loss_rec: 20.3152 acc_train: 0.5488 loss_val: 20.0648 acc_val: 0.5554 time: 0.0293s\n",
      "Epoch: 03396 loss_train: 2.5286 loss_rec: 2.5286 acc_train: 0.5936 loss_val: 2.5532 acc_val: 0.5946 time: 0.0268s\n",
      "Epoch: 03397 loss_train: 26.3181 loss_rec: 26.3181 acc_train: 0.4894 loss_val: 26.7429 acc_val: 0.4954 time: 0.0268s\n",
      "Epoch: 03398 loss_train: 37.4967 loss_rec: 37.4967 acc_train: 0.4908 loss_val: 38.0940 acc_val: 0.4971 time: 0.0278s\n",
      "Epoch: 03399 loss_train: 32.1664 loss_rec: 32.1664 acc_train: 0.4892 loss_val: 32.6815 acc_val: 0.4950 time: 0.0263s\n",
      "Epoch: 03400 loss_train: 12.0700 loss_rec: 12.0700 acc_train: 0.4892 loss_val: 12.2769 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03401 loss_train: 18.7381 loss_rec: 18.7381 acc_train: 0.5492 loss_val: 18.5147 acc_val: 0.5567 time: 0.0273s\n",
      "Test set results: loss= 37.8816 accuracy= 0.5022\n",
      "Epoch: 03402 loss_train: 33.4448 loss_rec: 33.4448 acc_train: 0.5447 loss_val: 32.9928 acc_val: 0.5529 time: 0.0268s\n",
      "Epoch: 03403 loss_train: 33.5173 loss_rec: 33.5173 acc_train: 0.5447 loss_val: 33.0641 acc_val: 0.5529 time: 0.0278s\n",
      "Epoch: 03404 loss_train: 20.4329 loss_rec: 20.4329 acc_train: 0.5488 loss_val: 20.1804 acc_val: 0.5554 time: 0.0263s\n",
      "Epoch: 03405 loss_train: 3.7154 loss_rec: 3.7154 acc_train: 0.4892 loss_val: 3.7904 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 03406 loss_train: 12.6465 loss_rec: 12.6465 acc_train: 0.4892 loss_val: 12.8626 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03407 loss_train: 5.5442 loss_rec: 5.5442 acc_train: 0.4891 loss_val: 5.6480 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03408 loss_train: 13.9245 loss_rec: 13.9245 acc_train: 0.5493 loss_val: 13.7840 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 03409 loss_train: 18.9002 loss_rec: 18.9002 acc_train: 0.5492 loss_val: 18.6742 acc_val: 0.5567 time: 0.0263s\n",
      "Epoch: 03410 loss_train: 10.4582 loss_rec: 10.4582 acc_train: 0.5509 loss_val: 10.3716 acc_val: 0.5583 time: 0.0263s\n",
      "Epoch: 03411 loss_train: 9.8493 loss_rec: 9.8493 acc_train: 0.4894 loss_val: 10.0207 acc_val: 0.4963 time: 0.0293s\n",
      "Test set results: loss= 13.0101 accuracy= 0.5458\n",
      "Epoch: 03412 loss_train: 14.5585 loss_rec: 14.5585 acc_train: 0.4893 loss_val: 14.8051 acc_val: 0.4954 time: 0.0263s\n",
      "Epoch: 03413 loss_train: 3.6667 loss_rec: 3.6667 acc_train: 0.4892 loss_val: 3.7410 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03414 loss_train: 18.7146 loss_rec: 18.7146 acc_train: 0.5492 loss_val: 18.4913 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 03415 loss_train: 26.4577 loss_rec: 26.4577 acc_train: 0.5489 loss_val: 26.1041 acc_val: 0.5542 time: 0.0263s\n",
      "Epoch: 03416 loss_train: 20.4429 loss_rec: 20.4429 acc_train: 0.5488 loss_val: 20.1901 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03417 loss_train: 2.6486 loss_rec: 2.6486 acc_train: 0.5928 loss_val: 2.6721 acc_val: 0.5950 time: 0.0263s\n",
      "Epoch: 03418 loss_train: 26.2035 loss_rec: 26.2035 acc_train: 0.4894 loss_val: 26.6266 acc_val: 0.4954 time: 0.0283s\n",
      "Epoch: 03419 loss_train: 37.5385 loss_rec: 37.5385 acc_train: 0.4908 loss_val: 38.1366 acc_val: 0.4971 time: 0.0268s\n",
      "Epoch: 03420 loss_train: 32.5216 loss_rec: 32.5216 acc_train: 0.4892 loss_val: 33.0425 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03421 loss_train: 12.8412 loss_rec: 12.8412 acc_train: 0.4892 loss_val: 13.0604 acc_val: 0.4963 time: 0.0258s\n",
      "Test set results: loss= 19.8497 accuracy= 0.5098\n",
      "Epoch: 03422 loss_train: 17.5320 loss_rec: 17.5320 acc_train: 0.5482 loss_val: 17.3291 acc_val: 0.5554 time: 0.0283s\n",
      "Epoch: 03423 loss_train: 31.9231 loss_rec: 31.9231 acc_train: 0.5448 loss_val: 31.4931 acc_val: 0.5521 time: 0.0283s\n",
      "Epoch: 03424 loss_train: 31.8624 loss_rec: 31.8624 acc_train: 0.5448 loss_val: 31.4333 acc_val: 0.5521 time: 0.0278s\n",
      "Epoch: 03425 loss_train: 18.7903 loss_rec: 18.7903 acc_train: 0.5492 loss_val: 18.5657 acc_val: 0.5567 time: 0.0288s\n",
      "Epoch: 03426 loss_train: 5.3324 loss_rec: 5.3324 acc_train: 0.4891 loss_val: 5.4330 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03427 loss_train: 14.2487 loss_rec: 14.2487 acc_train: 0.4887 loss_val: 14.4901 acc_val: 0.4954 time: 0.0258s\n",
      "Epoch: 03428 loss_train: 7.2348 loss_rec: 7.2348 acc_train: 0.4897 loss_val: 7.3646 acc_val: 0.4967 time: 0.0273s\n",
      "Epoch: 03429 loss_train: 12.1638 loss_rec: 12.1638 acc_train: 0.5523 loss_val: 12.0516 acc_val: 0.5608 time: 0.0273s\n",
      "Epoch: 03430 loss_train: 17.1508 loss_rec: 17.1508 acc_train: 0.5482 loss_val: 16.9540 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03431 loss_train: 8.9112 loss_rec: 8.9112 acc_train: 0.5527 loss_val: 8.8490 acc_val: 0.5583 time: 0.0263s\n",
      "Test set results: loss= 9.9008 accuracy= 0.5456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03432 loss_train: 11.0971 loss_rec: 11.0971 acc_train: 0.4892 loss_val: 11.2886 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03433 loss_train: 15.5422 loss_rec: 15.5422 acc_train: 0.4892 loss_val: 15.8043 acc_val: 0.4954 time: 0.0273s\n",
      "Epoch: 03434 loss_train: 4.5416 loss_rec: 4.5416 acc_train: 0.4891 loss_val: 4.6297 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 03435 loss_train: 17.9088 loss_rec: 17.9088 acc_train: 0.5497 loss_val: 17.6992 acc_val: 0.5571 time: 0.0273s\n",
      "Epoch: 03436 loss_train: 25.7769 loss_rec: 25.7769 acc_train: 0.5493 loss_val: 25.4340 acc_val: 0.5546 time: 0.0273s\n",
      "Epoch: 03437 loss_train: 20.0231 loss_rec: 20.0231 acc_train: 0.5488 loss_val: 19.7774 acc_val: 0.5554 time: 0.0258s\n",
      "Epoch: 03438 loss_train: 2.5784 loss_rec: 2.5784 acc_train: 0.5942 loss_val: 2.6024 acc_val: 0.5946 time: 0.0278s\n",
      "Epoch: 03439 loss_train: 25.7524 loss_rec: 25.7524 acc_train: 0.4894 loss_val: 26.1685 acc_val: 0.4954 time: 0.0278s\n",
      "Epoch: 03440 loss_train: 36.7593 loss_rec: 36.7593 acc_train: 0.4908 loss_val: 37.3454 acc_val: 0.4971 time: 0.0258s\n",
      "Epoch: 03441 loss_train: 31.5902 loss_rec: 31.5902 acc_train: 0.4892 loss_val: 32.0962 acc_val: 0.4950 time: 0.0288s\n",
      "Test set results: loss= 10.6398 accuracy= 0.5456\n",
      "Epoch: 03442 loss_train: 11.9196 loss_rec: 11.9196 acc_train: 0.4892 loss_val: 12.1242 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03443 loss_train: 18.2508 loss_rec: 18.2508 acc_train: 0.5493 loss_val: 18.0356 acc_val: 0.5571 time: 0.0268s\n",
      "Epoch: 03444 loss_train: 32.6374 loss_rec: 32.6374 acc_train: 0.5447 loss_val: 32.1969 acc_val: 0.5529 time: 0.0273s\n",
      "Epoch: 03445 loss_train: 32.6849 loss_rec: 32.6849 acc_train: 0.5447 loss_val: 32.2437 acc_val: 0.5529 time: 0.0273s\n",
      "Epoch: 03446 loss_train: 19.8379 loss_rec: 19.8379 acc_train: 0.5487 loss_val: 19.5955 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03447 loss_train: 3.8781 loss_rec: 3.8781 acc_train: 0.4892 loss_val: 3.9558 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03448 loss_train: 12.6604 loss_rec: 12.6604 acc_train: 0.4892 loss_val: 12.8766 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03449 loss_train: 5.6952 loss_rec: 5.6952 acc_train: 0.4891 loss_val: 5.8013 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03450 loss_train: 13.4109 loss_rec: 13.4109 acc_train: 0.5498 loss_val: 13.2791 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 03451 loss_train: 18.2974 loss_rec: 18.2974 acc_train: 0.5493 loss_val: 18.0813 acc_val: 0.5571 time: 0.0273s\n",
      "Test set results: loss= 11.3822 accuracy= 0.5161\n",
      "Epoch: 03452 loss_train: 10.0551 loss_rec: 10.0551 acc_train: 0.5513 loss_val: 9.9752 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 03453 loss_train: 9.8428 loss_rec: 9.8428 acc_train: 0.4894 loss_val: 10.0142 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03454 loss_train: 14.4076 loss_rec: 14.4076 acc_train: 0.4887 loss_val: 14.6517 acc_val: 0.4954 time: 0.0263s\n",
      "Epoch: 03455 loss_train: 3.6895 loss_rec: 3.6895 acc_train: 0.4892 loss_val: 3.7640 acc_val: 0.4963 time: 0.0298s\n",
      "Epoch: 03456 loss_train: 18.3127 loss_rec: 18.3127 acc_train: 0.5493 loss_val: 18.0959 acc_val: 0.5571 time: 0.0273s\n",
      "Epoch: 03457 loss_train: 25.9375 loss_rec: 25.9375 acc_train: 0.5488 loss_val: 25.5917 acc_val: 0.5542 time: 0.0263s\n",
      "Epoch: 03458 loss_train: 20.0851 loss_rec: 20.0851 acc_train: 0.5488 loss_val: 19.8380 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03459 loss_train: 2.6457 loss_rec: 2.6457 acc_train: 0.5917 loss_val: 2.6687 acc_val: 0.5942 time: 0.0273s\n",
      "Epoch: 03460 loss_train: 25.6235 loss_rec: 25.6235 acc_train: 0.4894 loss_val: 26.0376 acc_val: 0.4954 time: 0.0263s\n",
      "Epoch: 03461 loss_train: 36.7462 loss_rec: 36.7462 acc_train: 0.4908 loss_val: 37.3323 acc_val: 0.4971 time: 0.0278s\n",
      "Test set results: loss= 28.5087 accuracy= 0.5457\n",
      "Epoch: 03462 loss_train: 31.8070 loss_rec: 31.8070 acc_train: 0.4892 loss_val: 32.3164 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03463 loss_train: 12.4903 loss_rec: 12.4903 acc_train: 0.4892 loss_val: 12.7039 acc_val: 0.4963 time: 0.0293s\n",
      "Epoch: 03464 loss_train: 17.3113 loss_rec: 17.3113 acc_train: 0.5482 loss_val: 17.1118 acc_val: 0.5554 time: 0.0268s\n",
      "Epoch: 03465 loss_train: 31.4292 loss_rec: 31.4292 acc_train: 0.5448 loss_val: 31.0060 acc_val: 0.5521 time: 0.0258s\n",
      "Epoch: 03466 loss_train: 31.3737 loss_rec: 31.3737 acc_train: 0.5448 loss_val: 30.9513 acc_val: 0.5521 time: 0.0258s\n",
      "Epoch: 03467 loss_train: 18.5594 loss_rec: 18.5594 acc_train: 0.5492 loss_val: 18.3382 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 03468 loss_train: 5.0949 loss_rec: 5.0949 acc_train: 0.4891 loss_val: 5.1917 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03469 loss_train: 13.8280 loss_rec: 13.8280 acc_train: 0.4889 loss_val: 14.0628 acc_val: 0.4954 time: 0.0278s\n",
      "Epoch: 03470 loss_train: 6.9556 loss_rec: 6.9556 acc_train: 0.4891 loss_val: 7.0811 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03471 loss_train: 12.0587 loss_rec: 12.0587 acc_train: 0.5513 loss_val: 11.9477 acc_val: 0.5608 time: 0.0263s\n",
      "Test set results: loss= 19.1972 accuracy= 0.5098\n",
      "Epoch: 03472 loss_train: 16.9557 loss_rec: 16.9557 acc_train: 0.5482 loss_val: 16.7619 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03473 loss_train: 8.8550 loss_rec: 8.8550 acc_train: 0.5527 loss_val: 8.7931 acc_val: 0.5583 time: 0.0253s\n",
      "Epoch: 03474 loss_train: 10.7489 loss_rec: 10.7489 acc_train: 0.4892 loss_val: 10.9348 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03475 loss_train: 15.1311 loss_rec: 15.1311 acc_train: 0.4884 loss_val: 15.3867 acc_val: 0.4954 time: 0.0273s\n",
      "Epoch: 03476 loss_train: 4.3440 loss_rec: 4.3440 acc_train: 0.4891 loss_val: 4.4290 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 03477 loss_train: 17.6508 loss_rec: 17.6508 acc_train: 0.5497 loss_val: 17.4449 acc_val: 0.5571 time: 0.0273s\n",
      "Epoch: 03478 loss_train: 25.3657 loss_rec: 25.3657 acc_train: 0.5493 loss_val: 25.0289 acc_val: 0.5546 time: 0.0278s\n",
      "Epoch: 03479 loss_train: 19.7125 loss_rec: 19.7125 acc_train: 0.5487 loss_val: 19.4714 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03480 loss_train: 2.5797 loss_rec: 2.5797 acc_train: 0.5936 loss_val: 2.6031 acc_val: 0.5942 time: 0.0268s\n",
      "Epoch: 03481 loss_train: 25.2318 loss_rec: 25.2318 acc_train: 0.4887 loss_val: 25.6398 acc_val: 0.4950 time: 0.0278s\n",
      "Test set results: loss= 32.3354 accuracy= 0.5456\n",
      "Epoch: 03482 loss_train: 36.0668 loss_rec: 36.0668 acc_train: 0.4891 loss_val: 36.6424 acc_val: 0.4950 time: 0.0263s\n",
      "Epoch: 03483 loss_train: 30.9980 loss_rec: 30.9980 acc_train: 0.4892 loss_val: 31.4947 acc_val: 0.4950 time: 0.0283s\n",
      "Epoch: 03484 loss_train: 11.7342 loss_rec: 11.7342 acc_train: 0.4892 loss_val: 11.9358 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03485 loss_train: 17.8444 loss_rec: 17.8444 acc_train: 0.5497 loss_val: 17.6355 acc_val: 0.5571 time: 0.0273s\n",
      "Epoch: 03486 loss_train: 31.9403 loss_rec: 31.9403 acc_train: 0.5448 loss_val: 31.5098 acc_val: 0.5521 time: 0.0283s\n",
      "Epoch: 03487 loss_train: 31.9752 loss_rec: 31.9752 acc_train: 0.5448 loss_val: 31.5443 acc_val: 0.5521 time: 0.0258s\n",
      "Epoch: 03488 loss_train: 19.3458 loss_rec: 19.3458 acc_train: 0.5487 loss_val: 19.1110 acc_val: 0.5554 time: 0.0263s\n",
      "Epoch: 03489 loss_train: 3.9360 loss_rec: 3.9360 acc_train: 0.4892 loss_val: 4.0145 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03490 loss_train: 12.5546 loss_rec: 12.5546 acc_train: 0.4892 loss_val: 12.7692 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03491 loss_train: 5.7235 loss_rec: 5.7235 acc_train: 0.4891 loss_val: 5.8300 acc_val: 0.4963 time: 0.0273s\n",
      "Test set results: loss= 14.7478 accuracy= 0.5139\n",
      "Epoch: 03492 loss_train: 13.0261 loss_rec: 13.0261 acc_train: 0.5511 loss_val: 12.9001 acc_val: 0.5604 time: 0.0273s\n",
      "Epoch: 03493 loss_train: 17.8336 loss_rec: 17.8336 acc_train: 0.5497 loss_val: 17.6247 acc_val: 0.5571 time: 0.0278s\n",
      "Epoch: 03494 loss_train: 9.7548 loss_rec: 9.7548 acc_train: 0.5520 loss_val: 9.6792 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 03495 loss_train: 9.7324 loss_rec: 9.7324 acc_train: 0.4888 loss_val: 9.9019 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03496 loss_train: 14.1855 loss_rec: 14.1855 acc_train: 0.4887 loss_val: 14.4260 acc_val: 0.4954 time: 0.0258s\n",
      "Epoch: 03497 loss_train: 3.6334 loss_rec: 3.6334 acc_train: 0.4892 loss_val: 3.7071 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03498 loss_train: 17.9530 loss_rec: 17.9530 acc_train: 0.5493 loss_val: 17.7420 acc_val: 0.5567 time: 0.0263s\n",
      "Epoch: 03499 loss_train: 25.4512 loss_rec: 25.4512 acc_train: 0.5493 loss_val: 25.1130 acc_val: 0.5546 time: 0.0278s\n",
      "Epoch: 03500 loss_train: 19.7137 loss_rec: 19.7137 acc_train: 0.5488 loss_val: 19.4723 acc_val: 0.5554 time: 0.0273s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03501 loss_train: 2.6428 loss_rec: 2.6428 acc_train: 0.5911 loss_val: 2.6657 acc_val: 0.5938 time: 0.0268s\n",
      "Test set results: loss= 22.4626 accuracy= 0.5451\n",
      "Epoch: 03502 loss_train: 25.0793 loss_rec: 25.0793 acc_train: 0.4887 loss_val: 25.4852 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03503 loss_train: 35.9815 loss_rec: 35.9815 acc_train: 0.4891 loss_val: 36.5559 acc_val: 0.4950 time: 0.0253s\n",
      "Epoch: 03504 loss_train: 31.1378 loss_rec: 31.1378 acc_train: 0.4892 loss_val: 31.6367 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03505 loss_train: 12.1688 loss_rec: 12.1688 acc_train: 0.4892 loss_val: 12.3774 acc_val: 0.4963 time: 0.0283s\n",
      "Epoch: 03506 loss_train: 17.0459 loss_rec: 17.0459 acc_train: 0.5482 loss_val: 16.8503 acc_val: 0.5554 time: 0.0263s\n",
      "Epoch: 03507 loss_train: 30.9160 loss_rec: 30.9160 acc_train: 0.5448 loss_val: 30.5004 acc_val: 0.5521 time: 0.0278s\n",
      "Epoch: 03508 loss_train: 30.8575 loss_rec: 30.8575 acc_train: 0.5453 loss_val: 30.4427 acc_val: 0.5525 time: 0.0273s\n",
      "Epoch: 03509 loss_train: 18.2890 loss_rec: 18.2890 acc_train: 0.5492 loss_val: 18.0721 acc_val: 0.5567 time: 0.0273s\n",
      "Epoch: 03510 loss_train: 4.8849 loss_rec: 4.8849 acc_train: 0.4891 loss_val: 4.9785 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03511 loss_train: 13.4537 loss_rec: 13.4537 acc_train: 0.4891 loss_val: 13.6826 acc_val: 0.4958 time: 0.0268s\n",
      "Test set results: loss= 5.9717 accuracy= 0.5456\n",
      "Epoch: 03512 loss_train: 6.7204 loss_rec: 6.7204 acc_train: 0.4891 loss_val: 6.8424 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03513 loss_train: 11.9223 loss_rec: 11.9223 acc_train: 0.5477 loss_val: 11.8131 acc_val: 0.5542 time: 0.0278s\n",
      "Epoch: 03514 loss_train: 16.7163 loss_rec: 16.7163 acc_train: 0.5482 loss_val: 16.5262 acc_val: 0.5554 time: 0.0268s\n",
      "Epoch: 03515 loss_train: 8.7877 loss_rec: 8.7877 acc_train: 0.5537 loss_val: 8.7269 acc_val: 0.5588 time: 0.0263s\n",
      "Epoch: 03516 loss_train: 10.4478 loss_rec: 10.4478 acc_train: 0.4888 loss_val: 10.6288 acc_val: 0.4958 time: 0.0298s\n",
      "Epoch: 03517 loss_train: 14.7205 loss_rec: 14.7205 acc_train: 0.4885 loss_val: 14.9696 acc_val: 0.4954 time: 0.0268s\n",
      "Epoch: 03518 loss_train: 4.1573 loss_rec: 4.1573 acc_train: 0.4892 loss_val: 4.2393 acc_val: 0.4963 time: 0.0268s\n",
      "Epoch: 03519 loss_train: 17.4161 loss_rec: 17.4161 acc_train: 0.5483 loss_val: 17.2141 acc_val: 0.5558 time: 0.0273s\n",
      "Epoch: 03520 loss_train: 24.9752 loss_rec: 24.9752 acc_train: 0.5492 loss_val: 24.6448 acc_val: 0.5546 time: 0.0263s\n",
      "Epoch: 03521 loss_train: 19.4138 loss_rec: 19.4138 acc_train: 0.5487 loss_val: 19.1775 acc_val: 0.5554 time: 0.0263s\n",
      "Test set results: loss= 2.9228 accuracy= 0.5663\n",
      "Epoch: 03522 loss_train: 2.6214 loss_rec: 2.6214 acc_train: 0.5918 loss_val: 2.6445 acc_val: 0.5942 time: 0.0273s\n",
      "Epoch: 03523 loss_train: 24.7236 loss_rec: 24.7236 acc_train: 0.4887 loss_val: 25.1241 acc_val: 0.4954 time: 0.0263s\n",
      "Epoch: 03524 loss_train: 35.4109 loss_rec: 35.4109 acc_train: 0.4891 loss_val: 35.9766 acc_val: 0.4950 time: 0.0288s\n",
      "Epoch: 03525 loss_train: 30.4993 loss_rec: 30.4993 acc_train: 0.4892 loss_val: 30.9883 acc_val: 0.4954 time: 0.0258s\n",
      "Epoch: 03526 loss_train: 11.6219 loss_rec: 11.6219 acc_train: 0.4892 loss_val: 11.8217 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03527 loss_train: 17.3811 loss_rec: 17.3811 acc_train: 0.5483 loss_val: 17.1798 acc_val: 0.5558 time: 0.0273s\n",
      "Epoch: 03528 loss_train: 31.1854 loss_rec: 31.1854 acc_train: 0.5448 loss_val: 30.7658 acc_val: 0.5521 time: 0.0273s\n",
      "Epoch: 03529 loss_train: 31.1957 loss_rec: 31.1957 acc_train: 0.5448 loss_val: 30.7761 acc_val: 0.5521 time: 0.0263s\n",
      "Epoch: 03530 loss_train: 18.7835 loss_rec: 18.7835 acc_train: 0.5492 loss_val: 18.5581 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 03531 loss_train: 4.0823 loss_rec: 4.0823 acc_train: 0.4892 loss_val: 4.1632 acc_val: 0.4963 time: 0.0263s\n",
      "Test set results: loss= 11.2050 accuracy= 0.5456\n",
      "Epoch: 03532 loss_train: 12.5479 loss_rec: 12.5479 acc_train: 0.4892 loss_val: 12.7624 acc_val: 0.4963 time: 0.0293s\n",
      "Epoch: 03533 loss_train: 5.8542 loss_rec: 5.8542 acc_train: 0.4891 loss_val: 5.9628 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03534 loss_train: 12.5468 loss_rec: 12.5468 acc_train: 0.5514 loss_val: 12.4281 acc_val: 0.5608 time: 0.0273s\n",
      "Epoch: 03535 loss_train: 17.2933 loss_rec: 17.2933 acc_train: 0.5482 loss_val: 17.0934 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03536 loss_train: 9.3916 loss_rec: 9.3916 acc_train: 0.5528 loss_val: 9.3216 acc_val: 0.5596 time: 0.0258s\n",
      "Epoch: 03537 loss_train: 9.6867 loss_rec: 9.6867 acc_train: 0.4888 loss_val: 9.8553 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 03538 loss_train: 14.0007 loss_rec: 14.0007 acc_train: 0.4888 loss_val: 14.2382 acc_val: 0.4954 time: 0.0273s\n",
      "Epoch: 03539 loss_train: 3.6042 loss_rec: 3.6042 acc_train: 0.4892 loss_val: 3.6774 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03540 loss_train: 17.6173 loss_rec: 17.6173 acc_train: 0.5497 loss_val: 17.4118 acc_val: 0.5571 time: 0.0278s\n",
      "Epoch: 03541 loss_train: 25.0110 loss_rec: 25.0110 acc_train: 0.5493 loss_val: 24.6799 acc_val: 0.5546 time: 0.0273s\n",
      "Test set results: loss= 21.9813 accuracy= 0.5091\n",
      "Epoch: 03542 loss_train: 19.4136 loss_rec: 19.4136 acc_train: 0.5487 loss_val: 19.1772 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03543 loss_train: 2.6802 loss_rec: 2.6802 acc_train: 0.5917 loss_val: 2.7025 acc_val: 0.5942 time: 0.0273s\n",
      "Epoch: 03544 loss_train: 24.5234 loss_rec: 24.5234 acc_train: 0.4887 loss_val: 24.9210 acc_val: 0.4954 time: 0.0278s\n",
      "Epoch: 03545 loss_train: 35.2486 loss_rec: 35.2486 acc_train: 0.4891 loss_val: 35.8119 acc_val: 0.4950 time: 0.0258s\n",
      "Epoch: 03546 loss_train: 30.5060 loss_rec: 30.5060 acc_train: 0.4886 loss_val: 30.9952 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 03547 loss_train: 11.9099 loss_rec: 11.9099 acc_train: 0.4892 loss_val: 12.1144 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03548 loss_train: 16.7506 loss_rec: 16.7506 acc_train: 0.5482 loss_val: 16.5600 acc_val: 0.5554 time: 0.0288s\n",
      "Epoch: 03549 loss_train: 30.3440 loss_rec: 30.3440 acc_train: 0.5460 loss_val: 29.9367 acc_val: 0.5533 time: 0.0273s\n",
      "Epoch: 03550 loss_train: 30.2751 loss_rec: 30.2751 acc_train: 0.5460 loss_val: 29.8687 acc_val: 0.5533 time: 0.0273s\n",
      "Epoch: 03551 loss_train: 17.9362 loss_rec: 17.9362 acc_train: 0.5493 loss_val: 17.7253 acc_val: 0.5571 time: 0.0258s\n",
      "Test set results: loss= 4.2665 accuracy= 0.5458\n",
      "Epoch: 03552 loss_train: 4.8190 loss_rec: 4.8190 acc_train: 0.4891 loss_val: 4.9116 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03553 loss_train: 13.2486 loss_rec: 13.2486 acc_train: 0.4892 loss_val: 13.4741 acc_val: 0.4958 time: 0.0268s\n",
      "Epoch: 03554 loss_train: 6.6264 loss_rec: 6.6264 acc_train: 0.4891 loss_val: 6.7469 acc_val: 0.4963 time: 0.0268s\n",
      "Epoch: 03555 loss_train: 11.6624 loss_rec: 11.6624 acc_train: 0.5477 loss_val: 11.5570 acc_val: 0.5542 time: 0.0283s\n",
      "Epoch: 03556 loss_train: 16.3615 loss_rec: 16.3615 acc_train: 0.5482 loss_val: 16.1772 acc_val: 0.5554 time: 0.0288s\n",
      "Epoch: 03557 loss_train: 8.5820 loss_rec: 8.5820 acc_train: 0.5536 loss_val: 8.5237 acc_val: 0.5588 time: 0.0268s\n",
      "Epoch: 03558 loss_train: 10.2683 loss_rec: 10.2683 acc_train: 0.4888 loss_val: 10.4465 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 03559 loss_train: 14.4520 loss_rec: 14.4520 acc_train: 0.4885 loss_val: 14.6969 acc_val: 0.4954 time: 0.0263s\n",
      "Epoch: 03560 loss_train: 4.0620 loss_rec: 4.0620 acc_train: 0.4892 loss_val: 4.1427 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03561 loss_train: 17.0911 loss_rec: 17.0911 acc_train: 0.5482 loss_val: 16.8942 acc_val: 0.5554 time: 0.0278s\n",
      "Test set results: loss= 27.7622 accuracy= 0.5080\n",
      "Epoch: 03562 loss_train: 24.5174 loss_rec: 24.5174 acc_train: 0.5489 loss_val: 24.1943 acc_val: 0.5546 time: 0.0263s\n",
      "Epoch: 03563 loss_train: 19.0786 loss_rec: 19.0786 acc_train: 0.5487 loss_val: 18.8479 acc_val: 0.5554 time: 0.0283s\n",
      "Epoch: 03564 loss_train: 2.6048 loss_rec: 2.6048 acc_train: 0.5922 loss_val: 2.6278 acc_val: 0.5946 time: 0.0278s\n",
      "Epoch: 03565 loss_train: 24.2006 loss_rec: 24.2006 acc_train: 0.4887 loss_val: 24.5934 acc_val: 0.4954 time: 0.0273s\n",
      "Epoch: 03566 loss_train: 34.6614 loss_rec: 34.6614 acc_train: 0.4891 loss_val: 35.2158 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03567 loss_train: 29.8341 loss_rec: 29.8341 acc_train: 0.4887 loss_val: 30.3129 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 03568 loss_train: 11.2920 loss_rec: 11.2920 acc_train: 0.4887 loss_val: 11.4867 acc_val: 0.4958 time: 0.0253s\n",
      "Epoch: 03569 loss_train: 17.1463 loss_rec: 17.1463 acc_train: 0.5482 loss_val: 16.9487 acc_val: 0.5554 time: 0.0268s\n",
      "Epoch: 03570 loss_train: 30.6970 loss_rec: 30.6970 acc_train: 0.5448 loss_val: 30.2843 acc_val: 0.5521 time: 0.0278s\n",
      "Epoch: 03571 loss_train: 30.7076 loss_rec: 30.7076 acc_train: 0.5448 loss_val: 30.2948 acc_val: 0.5521 time: 0.0278s\n",
      "Test set results: loss= 21.0045 accuracy= 0.5098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03572 loss_train: 18.5511 loss_rec: 18.5511 acc_train: 0.5492 loss_val: 18.3294 acc_val: 0.5567 time: 0.0293s\n",
      "Epoch: 03573 loss_train: 3.8680 loss_rec: 3.8680 acc_train: 0.4892 loss_val: 3.9455 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03574 loss_train: 12.1891 loss_rec: 12.1891 acc_train: 0.4886 loss_val: 12.3980 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 03575 loss_train: 5.6288 loss_rec: 5.6288 acc_train: 0.4891 loss_val: 5.7339 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03576 loss_train: 12.4130 loss_rec: 12.4130 acc_train: 0.5513 loss_val: 12.2962 acc_val: 0.5608 time: 0.0263s\n",
      "Epoch: 03577 loss_train: 17.0625 loss_rec: 17.0625 acc_train: 0.5482 loss_val: 16.8661 acc_val: 0.5554 time: 0.0268s\n",
      "Epoch: 03578 loss_train: 9.3180 loss_rec: 9.3180 acc_train: 0.5518 loss_val: 9.2490 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 03579 loss_train: 9.3847 loss_rec: 9.3847 acc_train: 0.4888 loss_val: 9.5485 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03580 loss_train: 13.6293 loss_rec: 13.6293 acc_train: 0.4889 loss_val: 13.8608 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 03581 loss_train: 3.4202 loss_rec: 3.4202 acc_train: 0.4892 loss_val: 3.4905 acc_val: 0.4963 time: 0.0293s\n",
      "Test set results: loss= 19.6668 accuracy= 0.5109\n",
      "Epoch: 03582 loss_train: 17.3701 loss_rec: 17.3701 acc_train: 0.5497 loss_val: 17.1685 acc_val: 0.5571 time: 0.0273s\n",
      "Epoch: 03583 loss_train: 24.6160 loss_rec: 24.6160 acc_train: 0.5489 loss_val: 24.2913 acc_val: 0.5546 time: 0.0283s\n",
      "Epoch: 03584 loss_train: 19.1039 loss_rec: 19.1039 acc_train: 0.5487 loss_val: 18.8724 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03585 loss_train: 2.6761 loss_rec: 2.6761 acc_train: 0.5911 loss_val: 2.6983 acc_val: 0.5942 time: 0.0258s\n",
      "Epoch: 03586 loss_train: 24.0272 loss_rec: 24.0272 acc_train: 0.4887 loss_val: 24.4176 acc_val: 0.4954 time: 0.0263s\n",
      "Epoch: 03587 loss_train: 34.5549 loss_rec: 34.5549 acc_train: 0.4884 loss_val: 35.1077 acc_val: 0.4946 time: 0.0283s\n",
      "Epoch: 03588 loss_train: 29.9110 loss_rec: 29.9110 acc_train: 0.4887 loss_val: 30.3911 acc_val: 0.4950 time: 0.0293s\n",
      "Epoch: 03589 loss_train: 11.6752 loss_rec: 11.6752 acc_train: 0.4887 loss_val: 11.8760 acc_val: 0.4958 time: 0.0268s\n",
      "Epoch: 03590 loss_train: 16.4228 loss_rec: 16.4228 acc_train: 0.5482 loss_val: 16.2373 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03591 loss_train: 29.7578 loss_rec: 29.7578 acc_train: 0.5463 loss_val: 29.3589 acc_val: 0.5533 time: 0.0253s\n",
      "Test set results: loss= 33.6280 accuracy= 0.5051\n",
      "Epoch: 03592 loss_train: 29.6925 loss_rec: 29.6925 acc_train: 0.5463 loss_val: 29.2946 acc_val: 0.5533 time: 0.0283s\n",
      "Epoch: 03593 loss_train: 17.5695 loss_rec: 17.5695 acc_train: 0.5497 loss_val: 17.3643 acc_val: 0.5571 time: 0.0268s\n",
      "Epoch: 03594 loss_train: 4.7642 loss_rec: 4.7642 acc_train: 0.4891 loss_val: 4.8559 acc_val: 0.4963 time: 0.0268s\n",
      "Epoch: 03595 loss_train: 13.0211 loss_rec: 13.0211 acc_train: 0.4886 loss_val: 13.2431 acc_val: 0.4954 time: 0.0293s\n",
      "Epoch: 03596 loss_train: 6.5301 loss_rec: 6.5301 acc_train: 0.4891 loss_val: 6.6492 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03597 loss_train: 11.4073 loss_rec: 11.4073 acc_train: 0.5477 loss_val: 11.3053 acc_val: 0.5542 time: 0.0278s\n",
      "Epoch: 03598 loss_train: 16.0187 loss_rec: 16.0187 acc_train: 0.5481 loss_val: 15.8399 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03599 loss_train: 8.3957 loss_rec: 8.3957 acc_train: 0.5533 loss_val: 8.3396 acc_val: 0.5588 time: 0.0258s\n",
      "Epoch: 03600 loss_train: 10.0949 loss_rec: 10.0949 acc_train: 0.4888 loss_val: 10.2703 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03601 loss_train: 14.1941 loss_rec: 14.1941 acc_train: 0.4887 loss_val: 14.4347 acc_val: 0.4954 time: 0.0278s\n",
      "Test set results: loss= 3.5096 accuracy= 0.5458\n",
      "Epoch: 03602 loss_train: 3.9744 loss_rec: 3.9744 acc_train: 0.4892 loss_val: 4.0536 acc_val: 0.4963 time: 0.0268s\n",
      "Epoch: 03603 loss_train: 16.7898 loss_rec: 16.7898 acc_train: 0.5482 loss_val: 16.5978 acc_val: 0.5554 time: 0.0283s\n",
      "Epoch: 03604 loss_train: 24.0832 loss_rec: 24.0832 acc_train: 0.5489 loss_val: 23.7669 acc_val: 0.5546 time: 0.0253s\n",
      "Epoch: 03605 loss_train: 18.7565 loss_rec: 18.7565 acc_train: 0.5487 loss_val: 18.5308 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03606 loss_train: 2.5977 loss_rec: 2.5977 acc_train: 0.5916 loss_val: 2.6205 acc_val: 0.5946 time: 0.0278s\n",
      "Epoch: 03607 loss_train: 23.7024 loss_rec: 23.7024 acc_train: 0.4887 loss_val: 24.0880 acc_val: 0.4954 time: 0.0258s\n",
      "Epoch: 03608 loss_train: 33.9688 loss_rec: 33.9688 acc_train: 0.4884 loss_val: 34.5125 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 03609 loss_train: 29.2274 loss_rec: 29.2274 acc_train: 0.4887 loss_val: 29.6970 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 03610 loss_train: 11.0277 loss_rec: 11.0277 acc_train: 0.4887 loss_val: 11.2180 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 03611 loss_train: 16.8761 loss_rec: 16.8761 acc_train: 0.5482 loss_val: 16.6827 acc_val: 0.5554 time: 0.0293s\n",
      "Test set results: loss= 34.1857 accuracy= 0.5042\n",
      "Epoch: 03612 loss_train: 30.1844 loss_rec: 30.1844 acc_train: 0.5460 loss_val: 29.7791 acc_val: 0.5533 time: 0.0263s\n",
      "Epoch: 03613 loss_train: 30.2059 loss_rec: 30.2059 acc_train: 0.5460 loss_val: 29.8005 acc_val: 0.5533 time: 0.0258s\n",
      "Epoch: 03614 loss_train: 18.2621 loss_rec: 18.2621 acc_train: 0.5492 loss_val: 18.0448 acc_val: 0.5567 time: 0.0283s\n",
      "Epoch: 03615 loss_train: 3.7208 loss_rec: 3.7208 acc_train: 0.4892 loss_val: 3.7960 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03616 loss_train: 11.8878 loss_rec: 11.8878 acc_train: 0.4887 loss_val: 12.0920 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03617 loss_train: 5.4601 loss_rec: 5.4601 acc_train: 0.4891 loss_val: 5.5626 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03618 loss_train: 12.2186 loss_rec: 12.2186 acc_train: 0.5513 loss_val: 12.1045 acc_val: 0.5608 time: 0.0273s\n",
      "Epoch: 03619 loss_train: 16.7852 loss_rec: 16.7852 acc_train: 0.5482 loss_val: 16.5933 acc_val: 0.5554 time: 0.0258s\n",
      "Epoch: 03620 loss_train: 9.1714 loss_rec: 9.1714 acc_train: 0.5518 loss_val: 9.1043 acc_val: 0.5592 time: 0.0278s\n",
      "Epoch: 03621 loss_train: 9.1578 loss_rec: 9.1578 acc_train: 0.4888 loss_val: 9.3180 acc_val: 0.4958 time: 0.0273s\n",
      "Test set results: loss= 11.8882 accuracy= 0.5449\n",
      "Epoch: 03622 loss_train: 13.3080 loss_rec: 13.3080 acc_train: 0.4885 loss_val: 13.5345 acc_val: 0.4954 time: 0.0268s\n",
      "Epoch: 03623 loss_train: 3.2844 loss_rec: 3.2844 acc_train: 0.4892 loss_val: 3.3524 acc_val: 0.4963 time: 0.0269s\n",
      "Epoch: 03624 loss_train: 17.0886 loss_rec: 17.0886 acc_train: 0.5482 loss_val: 16.8914 acc_val: 0.5554 time: 0.0277s\n",
      "Epoch: 03625 loss_train: 24.1861 loss_rec: 24.1861 acc_train: 0.5489 loss_val: 23.8681 acc_val: 0.5546 time: 0.0293s\n",
      "Epoch: 03626 loss_train: 18.7712 loss_rec: 18.7712 acc_train: 0.5487 loss_val: 18.5453 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03627 loss_train: 2.6427 loss_rec: 2.6427 acc_train: 0.5904 loss_val: 2.6650 acc_val: 0.5929 time: 0.0263s\n",
      "Epoch: 03628 loss_train: 23.5584 loss_rec: 23.5584 acc_train: 0.4887 loss_val: 23.9419 acc_val: 0.4954 time: 0.0278s\n",
      "Epoch: 03629 loss_train: 33.8792 loss_rec: 33.8792 acc_train: 0.4886 loss_val: 34.4216 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 03630 loss_train: 29.3128 loss_rec: 29.3128 acc_train: 0.4887 loss_val: 29.7837 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 03631 loss_train: 11.3781 loss_rec: 11.3781 acc_train: 0.4887 loss_val: 11.5741 acc_val: 0.4958 time: 0.0273s\n",
      "Test set results: loss= 18.3395 accuracy= 0.5097\n",
      "Epoch: 03632 loss_train: 16.1978 loss_rec: 16.1978 acc_train: 0.5482 loss_val: 16.0160 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03633 loss_train: 29.2927 loss_rec: 29.2927 acc_train: 0.5467 loss_val: 28.9005 acc_val: 0.5525 time: 0.0293s\n",
      "Epoch: 03634 loss_train: 29.2382 loss_rec: 29.2382 acc_train: 0.5467 loss_val: 28.8469 acc_val: 0.5525 time: 0.0258s\n",
      "Epoch: 03635 loss_train: 17.3503 loss_rec: 17.3503 acc_train: 0.5497 loss_val: 17.1487 acc_val: 0.5571 time: 0.0263s\n",
      "Epoch: 03636 loss_train: 4.5427 loss_rec: 4.5427 acc_train: 0.4891 loss_val: 4.6309 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03637 loss_train: 12.6621 loss_rec: 12.6621 acc_train: 0.4886 loss_val: 12.8784 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03638 loss_train: 6.2927 loss_rec: 6.2927 acc_train: 0.4891 loss_val: 6.4080 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03639 loss_train: 11.3100 loss_rec: 11.3100 acc_train: 0.5477 loss_val: 11.2093 acc_val: 0.5542 time: 0.0273s\n",
      "Epoch: 03640 loss_train: 15.8346 loss_rec: 15.8346 acc_train: 0.5481 loss_val: 15.6587 acc_val: 0.5554 time: 0.0258s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03641 loss_train: 8.3502 loss_rec: 8.3502 acc_train: 0.5533 loss_val: 8.2944 acc_val: 0.5588 time: 0.0268s\n",
      "Test set results: loss= 8.7201 accuracy= 0.5452\n",
      "Epoch: 03642 loss_train: 9.7815 loss_rec: 9.7815 acc_train: 0.4888 loss_val: 9.9518 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03643 loss_train: 13.8005 loss_rec: 13.8005 acc_train: 0.4883 loss_val: 14.0348 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03644 loss_train: 3.7676 loss_rec: 3.7676 acc_train: 0.4892 loss_val: 3.8434 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 03645 loss_train: 16.5845 loss_rec: 16.5845 acc_train: 0.5482 loss_val: 16.3957 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03646 loss_train: 23.7418 loss_rec: 23.7418 acc_train: 0.5487 loss_val: 23.4310 acc_val: 0.5546 time: 0.0263s\n",
      "Epoch: 03647 loss_train: 18.4945 loss_rec: 18.4945 acc_train: 0.5492 loss_val: 18.2731 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 03648 loss_train: 2.6246 loss_rec: 2.6246 acc_train: 0.5902 loss_val: 2.6471 acc_val: 0.5929 time: 0.0273s\n",
      "Epoch: 03649 loss_train: 23.2271 loss_rec: 23.2271 acc_train: 0.4887 loss_val: 23.6057 acc_val: 0.4954 time: 0.0288s\n",
      "Epoch: 03650 loss_train: 33.3525 loss_rec: 33.3525 acc_train: 0.4886 loss_val: 33.8866 acc_val: 0.4946 time: 0.0253s\n",
      "Epoch: 03651 loss_train: 28.7367 loss_rec: 28.7367 acc_train: 0.4887 loss_val: 29.1987 acc_val: 0.4950 time: 0.0263s\n",
      "Test set results: loss= 9.7275 accuracy= 0.5450\n",
      "Epoch: 03652 loss_train: 10.9026 loss_rec: 10.9026 acc_train: 0.4887 loss_val: 11.0910 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 03653 loss_train: 16.4606 loss_rec: 16.4606 acc_train: 0.5482 loss_val: 16.2741 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03654 loss_train: 29.4962 loss_rec: 29.4962 acc_train: 0.5463 loss_val: 29.1009 acc_val: 0.5533 time: 0.0273s\n",
      "Epoch: 03655 loss_train: 29.4936 loss_rec: 29.4936 acc_train: 0.5463 loss_val: 29.0984 acc_val: 0.5533 time: 0.0273s\n",
      "Epoch: 03656 loss_train: 17.7483 loss_rec: 17.7483 acc_train: 0.5500 loss_val: 17.5395 acc_val: 0.5579 time: 0.0278s\n",
      "Epoch: 03657 loss_train: 3.8437 loss_rec: 3.8437 acc_train: 0.4892 loss_val: 3.9208 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03658 loss_train: 11.8869 loss_rec: 11.8869 acc_train: 0.4887 loss_val: 12.0910 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 03659 loss_train: 5.5595 loss_rec: 5.5595 acc_train: 0.4891 loss_val: 5.6636 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03660 loss_train: 11.7992 loss_rec: 11.7992 acc_train: 0.5513 loss_val: 11.6911 acc_val: 0.5608 time: 0.0278s\n",
      "Epoch: 03661 loss_train: 16.2887 loss_rec: 16.2887 acc_train: 0.5482 loss_val: 16.1051 acc_val: 0.5554 time: 0.0273s\n",
      "Test set results: loss= 10.0027 accuracy= 0.5167\n",
      "Epoch: 03662 loss_train: 8.8376 loss_rec: 8.8376 acc_train: 0.5515 loss_val: 8.7754 acc_val: 0.5592 time: 0.0263s\n",
      "Epoch: 03663 loss_train: 9.1346 loss_rec: 9.1346 acc_train: 0.4888 loss_val: 9.2944 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03664 loss_train: 13.1672 loss_rec: 13.1672 acc_train: 0.4886 loss_val: 13.3915 acc_val: 0.4954 time: 0.0293s\n",
      "Epoch: 03665 loss_train: 3.2912 loss_rec: 3.2912 acc_train: 0.4892 loss_val: 3.3594 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03666 loss_train: 16.7408 loss_rec: 16.7408 acc_train: 0.5482 loss_val: 16.5495 acc_val: 0.5554 time: 0.0268s\n",
      "Epoch: 03667 loss_train: 23.7348 loss_rec: 23.7348 acc_train: 0.5487 loss_val: 23.4243 acc_val: 0.5546 time: 0.0263s\n",
      "Epoch: 03668 loss_train: 18.4456 loss_rec: 18.4456 acc_train: 0.5492 loss_val: 18.2249 acc_val: 0.5567 time: 0.0263s\n",
      "Epoch: 03669 loss_train: 2.6390 loss_rec: 2.6390 acc_train: 0.5904 loss_val: 2.6612 acc_val: 0.5929 time: 0.0283s\n",
      "Epoch: 03670 loss_train: 23.0556 loss_rec: 23.0556 acc_train: 0.4888 loss_val: 23.4318 acc_val: 0.4954 time: 0.0278s\n",
      "Epoch: 03671 loss_train: 33.1710 loss_rec: 33.1710 acc_train: 0.4886 loss_val: 33.7022 acc_val: 0.4946 time: 0.0273s\n",
      "Test set results: loss= 25.6897 accuracy= 0.5450\n",
      "Epoch: 03672 loss_train: 28.6690 loss_rec: 28.6690 acc_train: 0.4887 loss_val: 29.1301 acc_val: 0.4950 time: 0.0293s\n",
      "Epoch: 03673 loss_train: 11.0675 loss_rec: 11.0675 acc_train: 0.4887 loss_val: 11.2586 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 03674 loss_train: 15.9961 loss_rec: 15.9961 acc_train: 0.5481 loss_val: 15.8172 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03675 loss_train: 28.8712 loss_rec: 28.8712 acc_train: 0.5467 loss_val: 28.4851 acc_val: 0.5525 time: 0.0268s\n",
      "Epoch: 03676 loss_train: 28.8181 loss_rec: 28.8181 acc_train: 0.5467 loss_val: 28.4326 acc_val: 0.5525 time: 0.0273s\n",
      "Epoch: 03677 loss_train: 17.1600 loss_rec: 17.1600 acc_train: 0.5497 loss_val: 16.9613 acc_val: 0.5571 time: 0.0278s\n",
      "Epoch: 03678 loss_train: 4.3064 loss_rec: 4.3064 acc_train: 0.4892 loss_val: 4.3909 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 03679 loss_train: 12.2731 loss_rec: 12.2731 acc_train: 0.4886 loss_val: 12.4834 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03680 loss_train: 6.0078 loss_rec: 6.0078 acc_train: 0.4891 loss_val: 6.1188 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03681 loss_train: 11.2355 loss_rec: 11.2355 acc_train: 0.5480 loss_val: 11.1355 acc_val: 0.5554 time: 0.0278s\n",
      "Test set results: loss= 17.7680 accuracy= 0.5097\n",
      "Epoch: 03682 loss_train: 15.6931 loss_rec: 15.6931 acc_train: 0.5481 loss_val: 15.5195 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03683 loss_train: 8.3424 loss_rec: 8.3424 acc_train: 0.5533 loss_val: 8.2865 acc_val: 0.5588 time: 0.0263s\n",
      "Epoch: 03684 loss_train: 9.4480 loss_rec: 9.4480 acc_train: 0.4888 loss_val: 9.6128 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 03685 loss_train: 13.3930 loss_rec: 13.3930 acc_train: 0.4885 loss_val: 13.6208 acc_val: 0.4954 time: 0.0278s\n",
      "Epoch: 03686 loss_train: 3.5478 loss_rec: 3.5478 acc_train: 0.4892 loss_val: 3.6201 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03687 loss_train: 16.3943 loss_rec: 16.3943 acc_train: 0.5482 loss_val: 16.2084 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03688 loss_train: 23.3954 loss_rec: 23.3954 acc_train: 0.5501 loss_val: 23.0901 acc_val: 0.5567 time: 0.0288s\n",
      "Epoch: 03689 loss_train: 18.2271 loss_rec: 18.2271 acc_train: 0.5492 loss_val: 18.0100 acc_val: 0.5567 time: 0.0258s\n",
      "Epoch: 03690 loss_train: 2.6339 loss_rec: 2.6339 acc_train: 0.5904 loss_val: 2.6560 acc_val: 0.5929 time: 0.0263s\n",
      "Epoch: 03691 loss_train: 22.7663 loss_rec: 22.7663 acc_train: 0.4888 loss_val: 23.1381 acc_val: 0.4954 time: 0.0273s\n",
      "Test set results: loss= 29.3525 accuracy= 0.5450\n",
      "Epoch: 03692 loss_train: 32.7446 loss_rec: 32.7446 acc_train: 0.4886 loss_val: 33.2689 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 03693 loss_train: 28.2411 loss_rec: 28.2411 acc_train: 0.4887 loss_val: 28.6955 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 03694 loss_train: 10.7454 loss_rec: 10.7454 acc_train: 0.4887 loss_val: 10.9312 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03695 loss_train: 16.0927 loss_rec: 16.0927 acc_train: 0.5482 loss_val: 15.9122 acc_val: 0.5554 time: 0.0258s\n",
      "Epoch: 03696 loss_train: 28.8732 loss_rec: 28.8732 acc_train: 0.5467 loss_val: 28.4868 acc_val: 0.5525 time: 0.0293s\n",
      "Epoch: 03697 loss_train: 28.8525 loss_rec: 28.8525 acc_train: 0.5467 loss_val: 28.4665 acc_val: 0.5525 time: 0.0273s\n",
      "Epoch: 03698 loss_train: 17.3134 loss_rec: 17.3134 acc_train: 0.5497 loss_val: 17.1117 acc_val: 0.5571 time: 0.0263s\n",
      "Epoch: 03699 loss_train: 3.8959 loss_rec: 3.8959 acc_train: 0.4892 loss_val: 3.9738 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03700 loss_train: 11.7855 loss_rec: 11.7855 acc_train: 0.4887 loss_val: 11.9880 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03701 loss_train: 5.5898 loss_rec: 5.5898 acc_train: 0.4891 loss_val: 5.6944 acc_val: 0.4963 time: 0.0258s\n",
      "Test set results: loss= 12.9765 accuracy= 0.5126\n",
      "Epoch: 03702 loss_train: 11.4615 loss_rec: 11.4615 acc_train: 0.5477 loss_val: 11.3582 acc_val: 0.5542 time: 0.0263s\n",
      "Epoch: 03703 loss_train: 15.8816 loss_rec: 15.8816 acc_train: 0.5481 loss_val: 15.7049 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03704 loss_train: 8.5640 loss_rec: 8.5640 acc_train: 0.5527 loss_val: 8.5053 acc_val: 0.5588 time: 0.0293s\n",
      "Epoch: 03705 loss_train: 9.0523 loss_rec: 9.0523 acc_train: 0.4888 loss_val: 9.2107 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03706 loss_train: 12.9906 loss_rec: 12.9906 acc_train: 0.4886 loss_val: 13.2121 acc_val: 0.4954 time: 0.0258s\n",
      "Epoch: 03707 loss_train: 3.2670 loss_rec: 3.2670 acc_train: 0.4892 loss_val: 3.3348 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 03708 loss_train: 16.4159 loss_rec: 16.4159 acc_train: 0.5482 loss_val: 16.2297 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03709 loss_train: 23.2876 loss_rec: 23.2876 acc_train: 0.5501 loss_val: 22.9841 acc_val: 0.5567 time: 0.0273s\n",
      "Epoch: 03710 loss_train: 18.1157 loss_rec: 18.1157 acc_train: 0.5492 loss_val: 17.9004 acc_val: 0.5567 time: 0.0263s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03711 loss_train: 2.6249 loss_rec: 2.6249 acc_train: 0.5904 loss_val: 2.6471 acc_val: 0.5929 time: 0.0283s\n",
      "Test set results: loss= 20.2257 accuracy= 0.5454\n",
      "Epoch: 03712 loss_train: 22.5880 loss_rec: 22.5880 acc_train: 0.4888 loss_val: 22.9574 acc_val: 0.4954 time: 0.0268s\n",
      "Epoch: 03713 loss_train: 32.5139 loss_rec: 32.5139 acc_train: 0.4886 loss_val: 33.0346 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 03714 loss_train: 28.0647 loss_rec: 28.0647 acc_train: 0.4887 loss_val: 28.5166 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03715 loss_train: 10.7536 loss_rec: 10.7536 acc_train: 0.4888 loss_val: 10.9397 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 03716 loss_train: 15.8189 loss_rec: 15.8189 acc_train: 0.5481 loss_val: 15.6430 acc_val: 0.5554 time: 0.0263s\n",
      "Epoch: 03717 loss_train: 28.4625 loss_rec: 28.4625 acc_train: 0.5467 loss_val: 28.0822 acc_val: 0.5525 time: 0.0273s\n",
      "Epoch: 03718 loss_train: 28.4383 loss_rec: 28.4383 acc_train: 0.5467 loss_val: 28.0584 acc_val: 0.5525 time: 0.0278s\n",
      "Epoch: 03719 loss_train: 16.9962 loss_rec: 16.9962 acc_train: 0.5497 loss_val: 16.8001 acc_val: 0.5571 time: 0.0288s\n",
      "Epoch: 03720 loss_train: 4.0421 loss_rec: 4.0421 acc_train: 0.4892 loss_val: 4.1224 acc_val: 0.4963 time: 0.0268s\n",
      "Epoch: 03721 loss_train: 11.8591 loss_rec: 11.8591 acc_train: 0.4887 loss_val: 12.0629 acc_val: 0.4958 time: 0.0263s\n",
      "Test set results: loss= 5.0686 accuracy= 0.5457\n",
      "Epoch: 03722 loss_train: 5.7122 loss_rec: 5.7122 acc_train: 0.4891 loss_val: 5.8186 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03723 loss_train: 11.1962 loss_rec: 11.1962 acc_train: 0.5480 loss_val: 11.0966 acc_val: 0.5554 time: 0.0283s\n",
      "Epoch: 03724 loss_train: 15.5745 loss_rec: 15.5745 acc_train: 0.5481 loss_val: 15.4027 acc_val: 0.5554 time: 0.0263s\n",
      "Epoch: 03725 loss_train: 8.3408 loss_rec: 8.3408 acc_train: 0.5533 loss_val: 8.2846 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 03726 loss_train: 9.0930 loss_rec: 9.0930 acc_train: 0.4888 loss_val: 9.2521 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03727 loss_train: 12.9603 loss_rec: 12.9603 acc_train: 0.4886 loss_val: 13.1814 acc_val: 0.4954 time: 0.0278s\n",
      "Epoch: 03728 loss_train: 3.3073 loss_rec: 3.3073 acc_train: 0.4892 loss_val: 3.3757 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03729 loss_train: 16.2092 loss_rec: 16.2092 acc_train: 0.5482 loss_val: 16.0262 acc_val: 0.5554 time: 0.0268s\n",
      "Epoch: 03730 loss_train: 23.0538 loss_rec: 23.0538 acc_train: 0.5499 loss_val: 22.7540 acc_val: 0.5567 time: 0.0263s\n",
      "Epoch: 03731 loss_train: 17.9577 loss_rec: 17.9577 acc_train: 0.5492 loss_val: 17.7450 acc_val: 0.5567 time: 0.0273s\n",
      "Test set results: loss= 2.9166 accuracy= 0.5643\n",
      "Epoch: 03732 loss_train: 2.6142 loss_rec: 2.6142 acc_train: 0.5904 loss_val: 2.6363 acc_val: 0.5929 time: 0.0263s\n",
      "Epoch: 03733 loss_train: 22.3301 loss_rec: 22.3301 acc_train: 0.4888 loss_val: 22.6957 acc_val: 0.4954 time: 0.0263s\n",
      "Epoch: 03734 loss_train: 32.1298 loss_rec: 32.1298 acc_train: 0.4886 loss_val: 32.6444 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 03735 loss_train: 27.7213 loss_rec: 27.7213 acc_train: 0.4887 loss_val: 28.1678 acc_val: 0.4950 time: 0.0293s\n",
      "Epoch: 03736 loss_train: 10.5452 loss_rec: 10.5452 acc_train: 0.4888 loss_val: 10.7279 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03737 loss_train: 15.7822 loss_rec: 15.7822 acc_train: 0.5481 loss_val: 15.6065 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03738 loss_train: 28.3346 loss_rec: 28.3346 acc_train: 0.5467 loss_val: 27.9560 acc_val: 0.5525 time: 0.0258s\n",
      "Epoch: 03739 loss_train: 28.3174 loss_rec: 28.3174 acc_train: 0.5467 loss_val: 27.9391 acc_val: 0.5525 time: 0.0263s\n",
      "Epoch: 03740 loss_train: 16.9864 loss_rec: 16.9864 acc_train: 0.5497 loss_val: 16.7901 acc_val: 0.5571 time: 0.0273s\n",
      "Epoch: 03741 loss_train: 3.8342 loss_rec: 3.8342 acc_train: 0.4892 loss_val: 3.9112 acc_val: 0.4963 time: 0.0273s\n",
      "Test set results: loss= 10.3257 accuracy= 0.5450\n",
      "Epoch: 03742 loss_train: 11.5676 loss_rec: 11.5676 acc_train: 0.4887 loss_val: 11.7667 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03743 loss_train: 5.4921 loss_rec: 5.4921 acc_train: 0.4891 loss_val: 5.5951 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03744 loss_train: 11.2416 loss_rec: 11.2416 acc_train: 0.5477 loss_val: 11.1410 acc_val: 0.5542 time: 0.0273s\n",
      "Epoch: 03745 loss_train: 15.5834 loss_rec: 15.5834 acc_train: 0.5481 loss_val: 15.4113 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03746 loss_train: 8.4065 loss_rec: 8.4065 acc_train: 0.5527 loss_val: 8.3493 acc_val: 0.5588 time: 0.0278s\n",
      "Epoch: 03747 loss_train: 8.8565 loss_rec: 8.8565 acc_train: 0.4888 loss_val: 9.0118 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 03748 loss_train: 12.7208 loss_rec: 12.7208 acc_train: 0.4886 loss_val: 12.9380 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03749 loss_train: 3.1495 loss_rec: 3.1495 acc_train: 0.4892 loss_val: 3.2153 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 03750 loss_train: 16.1384 loss_rec: 16.1384 acc_train: 0.5482 loss_val: 15.9565 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03751 loss_train: 22.8967 loss_rec: 22.8967 acc_train: 0.5498 loss_val: 22.5996 acc_val: 0.5567 time: 0.0278s\n",
      "Test set results: loss= 20.1689 accuracy= 0.5097\n",
      "Epoch: 03752 loss_train: 17.8129 loss_rec: 17.8129 acc_train: 0.5492 loss_val: 17.6025 acc_val: 0.5567 time: 0.0273s\n",
      "Epoch: 03753 loss_train: 2.5873 loss_rec: 2.5873 acc_train: 0.5901 loss_val: 2.6095 acc_val: 0.5925 time: 0.0273s\n",
      "Epoch: 03754 loss_train: 22.1431 loss_rec: 22.1431 acc_train: 0.4889 loss_val: 22.5059 acc_val: 0.4954 time: 0.0263s\n",
      "Epoch: 03755 loss_train: 31.8656 loss_rec: 31.8656 acc_train: 0.4886 loss_val: 32.3760 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 03756 loss_train: 27.4939 loss_rec: 27.4939 acc_train: 0.4887 loss_val: 27.9368 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03757 loss_train: 10.4737 loss_rec: 10.4737 acc_train: 0.4888 loss_val: 10.6552 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03758 loss_train: 15.6329 loss_rec: 15.6329 acc_train: 0.5481 loss_val: 15.4600 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03759 loss_train: 28.0558 loss_rec: 28.0558 acc_train: 0.5467 loss_val: 27.6811 acc_val: 0.5525 time: 0.0288s\n",
      "Epoch: 03760 loss_train: 28.0371 loss_rec: 28.0371 acc_train: 0.5466 loss_val: 27.6625 acc_val: 0.5525 time: 0.0258s\n",
      "Epoch: 03761 loss_train: 16.8352 loss_rec: 16.8352 acc_train: 0.5497 loss_val: 16.6414 acc_val: 0.5567 time: 0.0263s\n",
      "Test set results: loss= 3.3415 accuracy= 0.5458\n",
      "Epoch: 03762 loss_train: 3.7858 loss_rec: 3.7858 acc_train: 0.4892 loss_val: 3.8620 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03763 loss_train: 11.4565 loss_rec: 11.4565 acc_train: 0.4887 loss_val: 11.6538 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03764 loss_train: 5.4271 loss_rec: 5.4271 acc_train: 0.4891 loss_val: 5.5291 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03765 loss_train: 11.1615 loss_rec: 11.1615 acc_train: 0.5480 loss_val: 11.0621 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03766 loss_train: 15.4648 loss_rec: 15.4648 acc_train: 0.5481 loss_val: 15.2945 acc_val: 0.5554 time: 0.0263s\n",
      "Epoch: 03767 loss_train: 8.3696 loss_rec: 8.3696 acc_train: 0.5527 loss_val: 8.3131 acc_val: 0.5588 time: 0.0293s\n",
      "Epoch: 03768 loss_train: 8.7459 loss_rec: 8.7459 acc_train: 0.4888 loss_val: 8.8994 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03769 loss_train: 12.5505 loss_rec: 12.5505 acc_train: 0.4886 loss_val: 12.7652 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03770 loss_train: 3.0835 loss_rec: 3.0835 acc_train: 0.4892 loss_val: 3.1481 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 03771 loss_train: 16.0429 loss_rec: 16.0429 acc_train: 0.5482 loss_val: 15.8628 acc_val: 0.5554 time: 0.0258s\n",
      "Test set results: loss= 25.7185 accuracy= 0.5086\n",
      "Epoch: 03772 loss_train: 22.7122 loss_rec: 22.7122 acc_train: 0.5498 loss_val: 22.4179 acc_val: 0.5567 time: 0.0263s\n",
      "Epoch: 03773 loss_train: 17.6700 loss_rec: 17.6700 acc_train: 0.5492 loss_val: 17.4619 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 03774 loss_train: 2.5821 loss_rec: 2.5821 acc_train: 0.5901 loss_val: 2.6042 acc_val: 0.5925 time: 0.0278s\n",
      "Epoch: 03775 loss_train: 21.9221 loss_rec: 21.9221 acc_train: 0.4889 loss_val: 22.2816 acc_val: 0.4954 time: 0.0293s\n",
      "Epoch: 03776 loss_train: 31.5776 loss_rec: 31.5776 acc_train: 0.4886 loss_val: 32.0834 acc_val: 0.4946 time: 0.0253s\n",
      "Epoch: 03777 loss_train: 27.2406 loss_rec: 27.2406 acc_train: 0.4887 loss_val: 27.6797 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03778 loss_train: 10.3663 loss_rec: 10.3663 acc_train: 0.4888 loss_val: 10.5461 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 03779 loss_train: 15.4929 loss_rec: 15.4929 acc_train: 0.5481 loss_val: 15.3222 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03780 loss_train: 27.8023 loss_rec: 27.8023 acc_train: 0.5472 loss_val: 27.4309 acc_val: 0.5525 time: 0.0273s\n",
      "Epoch: 03781 loss_train: 27.7941 loss_rec: 27.7941 acc_train: 0.5472 loss_val: 27.4227 acc_val: 0.5525 time: 0.0273s\n",
      "Test set results: loss= 18.9134 accuracy= 0.5109\n",
      "Epoch: 03782 loss_train: 16.7041 loss_rec: 16.7041 acc_train: 0.5497 loss_val: 16.5126 acc_val: 0.5567 time: 0.0263s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03783 loss_train: 3.7102 loss_rec: 3.7102 acc_train: 0.4892 loss_val: 3.7853 acc_val: 0.4963 time: 0.0293s\n",
      "Epoch: 03784 loss_train: 11.3144 loss_rec: 11.3144 acc_train: 0.4887 loss_val: 11.5094 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03785 loss_train: 5.3393 loss_rec: 5.3393 acc_train: 0.4891 loss_val: 5.4400 acc_val: 0.4963 time: 0.0268s\n",
      "Epoch: 03786 loss_train: 11.0913 loss_rec: 11.0913 acc_train: 0.5480 loss_val: 10.9926 acc_val: 0.5554 time: 0.0263s\n",
      "Epoch: 03787 loss_train: 15.3595 loss_rec: 15.3595 acc_train: 0.5481 loss_val: 15.1907 acc_val: 0.5554 time: 0.0268s\n",
      "Epoch: 03788 loss_train: 8.3346 loss_rec: 8.3346 acc_train: 0.5527 loss_val: 8.2782 acc_val: 0.5588 time: 0.0258s\n",
      "Epoch: 03789 loss_train: 8.6090 loss_rec: 8.6090 acc_train: 0.4888 loss_val: 8.7603 acc_val: 0.4958 time: 0.0283s\n",
      "Epoch: 03790 loss_train: 12.3917 loss_rec: 12.3917 acc_train: 0.4886 loss_val: 12.6038 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03791 loss_train: 2.9921 loss_rec: 2.9921 acc_train: 0.4892 loss_val: 3.0552 acc_val: 0.4963 time: 0.0278s\n",
      "Test set results: loss= 18.0292 accuracy= 0.5098\n",
      "Epoch: 03792 loss_train: 15.9235 loss_rec: 15.9235 acc_train: 0.5482 loss_val: 15.7449 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03793 loss_train: 22.5332 loss_rec: 22.5332 acc_train: 0.5498 loss_val: 22.2420 acc_val: 0.5567 time: 0.0263s\n",
      "Epoch: 03794 loss_train: 17.5032 loss_rec: 17.5032 acc_train: 0.5500 loss_val: 17.2977 acc_val: 0.5575 time: 0.0278s\n",
      "Epoch: 03795 loss_train: 2.5507 loss_rec: 2.5507 acc_train: 0.5910 loss_val: 2.5731 acc_val: 0.5933 time: 0.0263s\n",
      "Epoch: 03796 loss_train: 21.7364 loss_rec: 21.7364 acc_train: 0.4889 loss_val: 22.0930 acc_val: 0.4954 time: 0.0278s\n",
      "Epoch: 03797 loss_train: 31.2801 loss_rec: 31.2801 acc_train: 0.4886 loss_val: 31.7812 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 03798 loss_train: 26.9788 loss_rec: 26.9788 acc_train: 0.4887 loss_val: 27.4137 acc_val: 0.4950 time: 0.0258s\n",
      "Epoch: 03799 loss_train: 10.2388 loss_rec: 10.2388 acc_train: 0.4888 loss_val: 10.4165 acc_val: 0.4958 time: 0.0288s\n",
      "Epoch: 03800 loss_train: 15.3875 loss_rec: 15.3875 acc_train: 0.5481 loss_val: 15.2182 acc_val: 0.5554 time: 0.0270s\n",
      "Epoch: 03801 loss_train: 27.6044 loss_rec: 27.6044 acc_train: 0.5472 loss_val: 27.2355 acc_val: 0.5525 time: 0.0258s\n",
      "Test set results: loss= 31.2698 accuracy= 0.5059\n",
      "Epoch: 03802 loss_train: 27.6119 loss_rec: 27.6119 acc_train: 0.5472 loss_val: 27.2431 acc_val: 0.5525 time: 0.0263s\n",
      "Epoch: 03803 loss_train: 16.6247 loss_rec: 16.6247 acc_train: 0.5497 loss_val: 16.4342 acc_val: 0.5567 time: 0.0258s\n",
      "Epoch: 03804 loss_train: 3.5943 loss_rec: 3.5943 acc_train: 0.4892 loss_val: 3.6674 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03805 loss_train: 11.1123 loss_rec: 11.1123 acc_train: 0.4887 loss_val: 11.3040 acc_val: 0.4958 time: 0.0268s\n",
      "Epoch: 03806 loss_train: 5.1782 loss_rec: 5.1782 acc_train: 0.4891 loss_val: 5.2763 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03807 loss_train: 11.0956 loss_rec: 11.0956 acc_train: 0.5480 loss_val: 10.9969 acc_val: 0.5554 time: 0.0293s\n",
      "Epoch: 03808 loss_train: 15.3319 loss_rec: 15.3319 acc_train: 0.5481 loss_val: 15.1635 acc_val: 0.5554 time: 0.0268s\n",
      "Epoch: 03809 loss_train: 8.3566 loss_rec: 8.3566 acc_train: 0.5527 loss_val: 8.2998 acc_val: 0.5588 time: 0.0258s\n",
      "Epoch: 03810 loss_train: 8.4201 loss_rec: 8.4201 acc_train: 0.4888 loss_val: 8.5683 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03811 loss_train: 12.1747 loss_rec: 12.1747 acc_train: 0.4886 loss_val: 12.3833 acc_val: 0.4958 time: 0.0263s\n",
      "Test set results: loss= 2.5107 accuracy= 0.5458\n",
      "Epoch: 03812 loss_train: 2.8584 loss_rec: 2.8584 acc_train: 0.4892 loss_val: 2.9192 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03813 loss_train: 15.8267 loss_rec: 15.8267 acc_train: 0.5482 loss_val: 15.6497 acc_val: 0.5554 time: 0.0279s\n",
      "Epoch: 03814 loss_train: 22.3457 loss_rec: 22.3457 acc_train: 0.5498 loss_val: 22.0575 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 03815 loss_train: 17.3267 loss_rec: 17.3267 acc_train: 0.5505 loss_val: 17.1240 acc_val: 0.5579 time: 0.0258s\n",
      "Epoch: 03816 loss_train: 2.4827 loss_rec: 2.4827 acc_train: 0.5923 loss_val: 2.5054 acc_val: 0.5946 time: 0.0278s\n",
      "Epoch: 03817 loss_train: 21.5630 loss_rec: 21.5630 acc_train: 0.4883 loss_val: 21.9169 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03818 loss_train: 30.9850 loss_rec: 30.9850 acc_train: 0.4886 loss_val: 31.4813 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 03819 loss_train: 26.6701 loss_rec: 26.6701 acc_train: 0.4887 loss_val: 27.1003 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 03820 loss_train: 10.0652 loss_rec: 10.0652 acc_train: 0.4888 loss_val: 10.2400 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 03821 loss_train: 15.3567 loss_rec: 15.3567 acc_train: 0.5481 loss_val: 15.1878 acc_val: 0.5554 time: 0.0263s\n",
      "Test set results: loss= 31.1357 accuracy= 0.5059\n",
      "Epoch: 03822 loss_train: 27.4936 loss_rec: 27.4936 acc_train: 0.5472 loss_val: 27.1263 acc_val: 0.5525 time: 0.0273s\n",
      "Epoch: 03823 loss_train: 27.5172 loss_rec: 27.5172 acc_train: 0.5472 loss_val: 27.1495 acc_val: 0.5525 time: 0.0263s\n",
      "Epoch: 03824 loss_train: 16.6511 loss_rec: 16.6511 acc_train: 0.5497 loss_val: 16.4600 acc_val: 0.5567 time: 0.0273s\n",
      "Epoch: 03825 loss_train: 3.3499 loss_rec: 3.3499 acc_train: 0.4892 loss_val: 3.4190 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03826 loss_train: 10.8158 loss_rec: 10.8158 acc_train: 0.4887 loss_val: 11.0028 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03827 loss_train: 4.9489 loss_rec: 4.9489 acc_train: 0.4891 loss_val: 5.0434 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03828 loss_train: 11.1594 loss_rec: 11.1594 acc_train: 0.5477 loss_val: 11.0598 acc_val: 0.5542 time: 0.0263s\n",
      "Epoch: 03829 loss_train: 15.3372 loss_rec: 15.3372 acc_train: 0.5481 loss_val: 15.1687 acc_val: 0.5554 time: 0.0288s\n",
      "Epoch: 03830 loss_train: 8.4224 loss_rec: 8.4224 acc_train: 0.5527 loss_val: 8.3649 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 03831 loss_train: 8.2244 loss_rec: 8.2244 acc_train: 0.4888 loss_val: 8.3694 acc_val: 0.4958 time: 0.0258s\n",
      "Test set results: loss= 10.6749 accuracy= 0.5450\n",
      "Epoch: 03832 loss_train: 11.9556 loss_rec: 11.9556 acc_train: 0.4887 loss_val: 12.1607 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03833 loss_train: 2.7395 loss_rec: 2.7395 acc_train: 0.4892 loss_val: 2.7983 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03834 loss_train: 15.7072 loss_rec: 15.7072 acc_train: 0.5481 loss_val: 15.5321 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03835 loss_train: 22.1113 loss_rec: 22.1113 acc_train: 0.5506 loss_val: 21.8269 acc_val: 0.5571 time: 0.0278s\n",
      "Epoch: 03836 loss_train: 17.0909 loss_rec: 17.0909 acc_train: 0.5505 loss_val: 16.8922 acc_val: 0.5579 time: 0.0278s\n",
      "Epoch: 03837 loss_train: 2.3628 loss_rec: 2.3628 acc_train: 0.5924 loss_val: 2.3864 acc_val: 0.5958 time: 0.0258s\n",
      "Epoch: 03838 loss_train: 21.4147 loss_rec: 21.4147 acc_train: 0.4883 loss_val: 21.7664 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 03839 loss_train: 30.6542 loss_rec: 30.6542 acc_train: 0.4886 loss_val: 31.1456 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03840 loss_train: 26.3115 loss_rec: 26.3115 acc_train: 0.4887 loss_val: 26.7362 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03841 loss_train: 9.7790 loss_rec: 9.7790 acc_train: 0.4888 loss_val: 9.9493 acc_val: 0.4958 time: 0.0278s\n",
      "Test set results: loss= 17.5063 accuracy= 0.5097\n",
      "Epoch: 03842 loss_train: 15.4615 loss_rec: 15.4615 acc_train: 0.5481 loss_val: 15.2907 acc_val: 0.5554 time: 0.0263s\n",
      "Epoch: 03843 loss_train: 27.5438 loss_rec: 27.5438 acc_train: 0.5472 loss_val: 27.1758 acc_val: 0.5525 time: 0.0293s\n",
      "Epoch: 03844 loss_train: 27.6080 loss_rec: 27.6080 acc_train: 0.5472 loss_val: 27.2390 acc_val: 0.5525 time: 0.0278s\n",
      "Epoch: 03845 loss_train: 16.8764 loss_rec: 16.8764 acc_train: 0.5497 loss_val: 16.6813 acc_val: 0.5571 time: 0.0258s\n",
      "Epoch: 03846 loss_train: 2.8755 loss_rec: 2.8755 acc_train: 0.4892 loss_val: 2.9367 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03847 loss_train: 10.3542 loss_rec: 10.3542 acc_train: 0.4888 loss_val: 10.5338 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03848 loss_train: 4.6162 loss_rec: 4.6162 acc_train: 0.4891 loss_val: 4.7056 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 03849 loss_train: 11.2402 loss_rec: 11.2402 acc_train: 0.5521 loss_val: 11.1391 acc_val: 0.5608 time: 0.0273s\n",
      "Epoch: 03850 loss_train: 15.3290 loss_rec: 15.3290 acc_train: 0.5481 loss_val: 15.1604 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03851 loss_train: 8.4038 loss_rec: 8.4038 acc_train: 0.5527 loss_val: 8.3462 acc_val: 0.5588 time: 0.0273s\n",
      "Test set results: loss= 7.2466 accuracy= 0.5454\n",
      "Epoch: 03852 loss_train: 8.1384 loss_rec: 8.1384 acc_train: 0.4888 loss_val: 8.2821 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03853 loss_train: 11.8770 loss_rec: 11.8770 acc_train: 0.4887 loss_val: 12.0810 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 03854 loss_train: 2.7942 loss_rec: 2.7942 acc_train: 0.4892 loss_val: 2.8539 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03855 loss_train: 15.4888 loss_rec: 15.4888 acc_train: 0.5481 loss_val: 15.3175 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03856 loss_train: 21.8145 loss_rec: 21.8145 acc_train: 0.5506 loss_val: 21.5350 acc_val: 0.5567 time: 0.0274s\n",
      "Epoch: 03857 loss_train: 16.8293 loss_rec: 16.8293 acc_train: 0.5497 loss_val: 16.6349 acc_val: 0.5571 time: 0.0272s\n",
      "Epoch: 03858 loss_train: 2.2552 loss_rec: 2.2552 acc_train: 0.5943 loss_val: 2.2796 acc_val: 0.5958 time: 0.0258s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03859 loss_train: 21.2133 loss_rec: 21.2133 acc_train: 0.4883 loss_val: 21.5621 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03860 loss_train: 30.2724 loss_rec: 30.2724 acc_train: 0.4886 loss_val: 30.7578 acc_val: 0.4950 time: 0.0268s\n",
      "Epoch: 03861 loss_train: 25.8500 loss_rec: 25.8500 acc_train: 0.4887 loss_val: 26.2676 acc_val: 0.4950 time: 0.0273s\n",
      "Test set results: loss= 8.3444 accuracy= 0.5453\n",
      "Epoch: 03862 loss_train: 9.3613 loss_rec: 9.3613 acc_train: 0.4888 loss_val: 9.5248 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 03863 loss_train: 15.6969 loss_rec: 15.6969 acc_train: 0.5482 loss_val: 15.5218 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03864 loss_train: 27.7425 loss_rec: 27.7425 acc_train: 0.5473 loss_val: 27.3717 acc_val: 0.5525 time: 0.0263s\n",
      "Epoch: 03865 loss_train: 27.8747 loss_rec: 27.8747 acc_train: 0.5467 loss_val: 27.5022 acc_val: 0.5525 time: 0.0278s\n",
      "Epoch: 03866 loss_train: 17.2696 loss_rec: 17.2696 acc_train: 0.5505 loss_val: 17.0676 acc_val: 0.5579 time: 0.0278s\n",
      "Epoch: 03867 loss_train: 2.2454 loss_rec: 2.2454 acc_train: 0.4892 loss_val: 2.2951 acc_val: 0.4963 time: 0.0293s\n",
      "Epoch: 03868 loss_train: 10.0292 loss_rec: 10.0292 acc_train: 0.4888 loss_val: 10.2035 acc_val: 0.4958 time: 0.0253s\n",
      "Epoch: 03869 loss_train: 4.6956 loss_rec: 4.6956 acc_train: 0.4891 loss_val: 4.7861 acc_val: 0.4963 time: 0.0283s\n",
      "Epoch: 03870 loss_train: 10.7559 loss_rec: 10.7559 acc_train: 0.5488 loss_val: 10.6618 acc_val: 0.5554 time: 0.0263s\n",
      "Epoch: 03871 loss_train: 14.5444 loss_rec: 14.5444 acc_train: 0.5486 loss_val: 14.3891 acc_val: 0.5558 time: 0.0278s\n",
      "Test set results: loss= 8.4714 accuracy= 0.5191\n",
      "Epoch: 03872 loss_train: 7.4903 loss_rec: 7.4903 acc_train: 0.5545 loss_val: 7.4438 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 03873 loss_train: 9.1499 loss_rec: 9.1499 acc_train: 0.4888 loss_val: 9.3098 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03874 loss_train: 13.0077 loss_rec: 13.0077 acc_train: 0.4886 loss_val: 13.2293 acc_val: 0.4954 time: 0.0288s\n",
      "Epoch: 03875 loss_train: 4.0908 loss_rec: 4.0908 acc_train: 0.4892 loss_val: 4.1718 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03876 loss_train: 14.2138 loss_rec: 14.2138 acc_train: 0.5488 loss_val: 14.0643 acc_val: 0.5567 time: 0.0263s\n",
      "Epoch: 03877 loss_train: 20.5699 loss_rec: 20.5699 acc_train: 0.5484 loss_val: 20.3113 acc_val: 0.5550 time: 0.0278s\n",
      "Epoch: 03878 loss_train: 15.7242 loss_rec: 15.7242 acc_train: 0.5482 loss_val: 15.5485 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03879 loss_train: 1.5880 loss_rec: 1.5880 acc_train: 0.6108 loss_val: 1.6164 acc_val: 0.6142 time: 0.0273s\n",
      "Epoch: 03880 loss_train: 20.5237 loss_rec: 20.5237 acc_train: 0.4883 loss_val: 20.8622 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03881 loss_train: 28.1880 loss_rec: 28.1880 acc_train: 0.4887 loss_val: 28.6415 acc_val: 0.4950 time: 0.0263s\n",
      "Test set results: loss= 20.2819 accuracy= 0.5447\n",
      "Epoch: 03882 loss_train: 22.6493 loss_rec: 22.6493 acc_train: 0.4882 loss_val: 23.0192 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03883 loss_train: 5.2684 loss_rec: 5.2684 acc_train: 0.4891 loss_val: 5.3679 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03884 loss_train: 20.1280 loss_rec: 20.1280 acc_train: 0.5497 loss_val: 19.8770 acc_val: 0.5563 time: 0.0273s\n",
      "Epoch: 03885 loss_train: 32.8729 loss_rec: 32.8729 acc_train: 0.5457 loss_val: 32.4282 acc_val: 0.5521 time: 0.0263s\n",
      "Epoch: 03886 loss_train: 33.6544 loss_rec: 33.6544 acc_train: 0.5450 loss_val: 33.1983 acc_val: 0.5521 time: 0.0278s\n",
      "Epoch: 03887 loss_train: 23.6624 loss_rec: 23.6624 acc_train: 0.5489 loss_val: 23.3515 acc_val: 0.5546 time: 0.0278s\n",
      "Epoch: 03888 loss_train: 4.4359 loss_rec: 4.4359 acc_train: 0.5679 loss_val: 4.4330 acc_val: 0.5742 time: 0.0273s\n",
      "Epoch: 03889 loss_train: 24.0960 loss_rec: 24.0960 acc_train: 0.4887 loss_val: 24.4871 acc_val: 0.4954 time: 0.0273s\n",
      "Epoch: 03890 loss_train: 38.3515 loss_rec: 38.3515 acc_train: 0.4884 loss_val: 38.9623 acc_val: 0.4946 time: 0.0258s\n",
      "Epoch: 03891 loss_train: 38.7288 loss_rec: 38.7288 acc_train: 0.4884 loss_val: 39.3455 acc_val: 0.4946 time: 0.0278s\n",
      "Test set results: loss= 23.9014 accuracy= 0.5450\n",
      "Epoch: 03892 loss_train: 26.6775 loss_rec: 26.6775 acc_train: 0.4887 loss_val: 27.1078 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 03893 loss_train: 3.5412 loss_rec: 3.5412 acc_train: 0.4892 loss_val: 3.6134 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 03894 loss_train: 26.5233 loss_rec: 26.5233 acc_train: 0.5485 loss_val: 26.1727 acc_val: 0.5529 time: 0.0278s\n",
      "Epoch: 03895 loss_train: 43.3707 loss_rec: 43.3707 acc_train: 0.5400 loss_val: 42.7973 acc_val: 0.5483 time: 0.0263s\n",
      "Epoch: 03896 loss_train: 47.9241 loss_rec: 47.9241 acc_train: 0.5372 loss_val: 47.2964 acc_val: 0.5446 time: 0.0293s\n",
      "Epoch: 03897 loss_train: 41.2619 loss_rec: 41.2619 acc_train: 0.5423 loss_val: 40.7136 acc_val: 0.5492 time: 0.0273s\n",
      "Epoch: 03898 loss_train: 24.5843 loss_rec: 24.5843 acc_train: 0.5493 loss_val: 24.2583 acc_val: 0.5546 time: 0.0273s\n",
      "Epoch: 03899 loss_train: 0.9065 loss_rec: 0.9065 acc_train: 0.6196 loss_val: 0.9265 acc_val: 0.6117 time: 0.0258s\n",
      "Epoch: 03900 loss_train: 25.1586 loss_rec: 25.1586 acc_train: 0.4887 loss_val: 25.5657 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 03901 loss_train: 35.5738 loss_rec: 35.5738 acc_train: 0.4884 loss_val: 36.1424 acc_val: 0.4946 time: 0.0263s\n",
      "Test set results: loss= 29.2166 accuracy= 0.5450\n",
      "Epoch: 03902 loss_train: 32.5921 loss_rec: 32.5921 acc_train: 0.4886 loss_val: 33.1140 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 03903 loss_train: 17.5814 loss_rec: 17.5814 acc_train: 0.4877 loss_val: 17.8752 acc_val: 0.4950 time: 0.0258s\n",
      "Epoch: 03904 loss_train: 6.7921 loss_rec: 6.7921 acc_train: 0.5560 loss_val: 6.7541 acc_val: 0.5621 time: 0.0288s\n",
      "Epoch: 03905 loss_train: 17.9995 loss_rec: 17.9995 acc_train: 0.5487 loss_val: 17.7847 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03906 loss_train: 17.7653 loss_rec: 17.7653 acc_train: 0.5487 loss_val: 17.5547 acc_val: 0.5554 time: 0.0263s\n",
      "Epoch: 03907 loss_train: 7.2271 loss_rec: 7.2271 acc_train: 0.5543 loss_val: 7.1837 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 03908 loss_train: 12.6499 loss_rec: 12.6499 acc_train: 0.4886 loss_val: 12.8661 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03909 loss_train: 19.5806 loss_rec: 19.5806 acc_train: 0.4885 loss_val: 19.9051 acc_val: 0.4950 time: 0.0263s\n",
      "Epoch: 03910 loss_train: 13.5835 loss_rec: 13.5835 acc_train: 0.4885 loss_val: 13.8143 acc_val: 0.4954 time: 0.0278s\n",
      "Epoch: 03911 loss_train: 3.2244 loss_rec: 3.2244 acc_train: 0.5787 loss_val: 3.2372 acc_val: 0.5863 time: 0.0278s\n",
      "Test set results: loss= 9.5010 accuracy= 0.5175\n",
      "Epoch: 03912 loss_train: 8.3952 loss_rec: 8.3952 acc_train: 0.5533 loss_val: 8.3374 acc_val: 0.5592 time: 0.0263s\n",
      "Epoch: 03913 loss_train: 3.4821 loss_rec: 3.4821 acc_train: 0.5777 loss_val: 3.4910 acc_val: 0.5825 time: 0.0283s\n",
      "Epoch: 03914 loss_train: 10.7749 loss_rec: 10.7749 acc_train: 0.4888 loss_val: 10.9613 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03915 loss_train: 12.1833 loss_rec: 12.1833 acc_train: 0.4886 loss_val: 12.3922 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03916 loss_train: 1.4620 loss_rec: 1.4620 acc_train: 0.4892 loss_val: 1.4936 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 03917 loss_train: 15.7501 loss_rec: 15.7501 acc_train: 0.5482 loss_val: 15.5736 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03918 loss_train: 21.3788 loss_rec: 21.3788 acc_train: 0.5503 loss_val: 21.1058 acc_val: 0.5567 time: 0.0268s\n",
      "Epoch: 03919 loss_train: 16.0921 loss_rec: 16.0921 acc_train: 0.5482 loss_val: 15.9098 acc_val: 0.5554 time: 0.0293s\n",
      "Epoch: 03920 loss_train: 1.7026 loss_rec: 1.7026 acc_train: 0.6066 loss_val: 1.7305 acc_val: 0.6071 time: 0.0273s\n",
      "Epoch: 03921 loss_train: 20.7182 loss_rec: 20.7182 acc_train: 0.4883 loss_val: 21.0597 acc_val: 0.4950 time: 0.0268s\n",
      "Test set results: loss= 25.9535 accuracy= 0.5450\n",
      "Epoch: 03922 loss_train: 28.9609 loss_rec: 28.9609 acc_train: 0.4887 loss_val: 29.4262 acc_val: 0.4950 time: 0.0263s\n",
      "Epoch: 03923 loss_train: 24.1697 loss_rec: 24.1697 acc_train: 0.4887 loss_val: 24.5620 acc_val: 0.4954 time: 0.0283s\n",
      "Epoch: 03924 loss_train: 7.7091 loss_rec: 7.7091 acc_train: 0.4889 loss_val: 7.8462 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03925 loss_train: 16.8857 loss_rec: 16.8857 acc_train: 0.5505 loss_val: 16.6898 acc_val: 0.5579 time: 0.0273s\n",
      "Epoch: 03926 loss_train: 28.9263 loss_rec: 28.9263 acc_train: 0.5459 loss_val: 28.5390 acc_val: 0.5533 time: 0.0288s\n",
      "Epoch: 03927 loss_train: 29.3224 loss_rec: 29.3224 acc_train: 0.5461 loss_val: 28.9294 acc_val: 0.5533 time: 0.0263s\n",
      "Epoch: 03928 loss_train: 19.2437 loss_rec: 19.2437 acc_train: 0.5496 loss_val: 19.0074 acc_val: 0.5563 time: 0.0258s\n",
      "Epoch: 03929 loss_train: 1.0648 loss_rec: 1.0648 acc_train: 0.6218 loss_val: 1.0900 acc_val: 0.6129 time: 0.0278s\n",
      "Epoch: 03930 loss_train: 22.7272 loss_rec: 22.7272 acc_train: 0.4882 loss_val: 23.0985 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 03931 loss_train: 31.7817 loss_rec: 31.7817 acc_train: 0.4886 loss_val: 32.2908 acc_val: 0.4946 time: 0.0273s\n",
      "Test set results: loss= 24.8922 accuracy= 0.5450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03932 loss_train: 27.7801 loss_rec: 27.7801 acc_train: 0.4887 loss_val: 28.2274 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 03933 loss_train: 12.0628 loss_rec: 12.0628 acc_train: 0.4887 loss_val: 12.2698 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03934 loss_train: 12.1997 loss_rec: 12.1997 acc_train: 0.5509 loss_val: 12.0839 acc_val: 0.5600 time: 0.0273s\n",
      "Epoch: 03935 loss_train: 23.7092 loss_rec: 23.7092 acc_train: 0.5491 loss_val: 23.3971 acc_val: 0.5546 time: 0.0263s\n",
      "Epoch: 03936 loss_train: 23.7870 loss_rec: 23.7870 acc_train: 0.5491 loss_val: 23.4737 acc_val: 0.5546 time: 0.0278s\n",
      "Epoch: 03937 loss_train: 13.5717 loss_rec: 13.5717 acc_train: 0.5488 loss_val: 13.4328 acc_val: 0.5567 time: 0.0273s\n",
      "Epoch: 03938 loss_train: 5.4531 loss_rec: 5.4531 acc_train: 0.4891 loss_val: 5.5555 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03939 loss_train: 12.3808 loss_rec: 12.3808 acc_train: 0.4886 loss_val: 12.5927 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 03940 loss_train: 6.6411 loss_rec: 6.6411 acc_train: 0.4891 loss_val: 6.7619 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03941 loss_train: 8.9329 loss_rec: 8.9329 acc_train: 0.5518 loss_val: 8.8675 acc_val: 0.5596 time: 0.0273s\n",
      "Test set results: loss= 14.8527 accuracy= 0.5118\n",
      "Epoch: 03942 loss_train: 13.1180 loss_rec: 13.1180 acc_train: 0.5493 loss_val: 12.9870 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 03943 loss_train: 6.8381 loss_rec: 6.8381 acc_train: 0.5553 loss_val: 6.7996 acc_val: 0.5617 time: 0.0258s\n",
      "Epoch: 03944 loss_train: 8.7161 loss_rec: 8.7161 acc_train: 0.4888 loss_val: 8.8691 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 03945 loss_train: 11.9523 loss_rec: 11.9523 acc_train: 0.4887 loss_val: 12.1575 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03946 loss_train: 2.9261 loss_rec: 2.9261 acc_train: 0.4892 loss_val: 2.9881 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 03947 loss_train: 14.8695 loss_rec: 14.8695 acc_train: 0.5481 loss_val: 14.7081 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03948 loss_train: 21.2044 loss_rec: 21.2044 acc_train: 0.5503 loss_val: 20.9345 acc_val: 0.5567 time: 0.0288s\n",
      "Epoch: 03949 loss_train: 16.6995 loss_rec: 16.6995 acc_train: 0.5497 loss_val: 16.5068 acc_val: 0.5571 time: 0.0259s\n",
      "Epoch: 03950 loss_train: 2.8736 loss_rec: 2.8736 acc_train: 0.5837 loss_val: 2.8917 acc_val: 0.5900 time: 0.0262s\n",
      "Epoch: 03951 loss_train: 19.8013 loss_rec: 19.8013 acc_train: 0.4876 loss_val: 20.1290 acc_val: 0.4950 time: 0.0273s\n",
      "Test set results: loss= 25.8500 accuracy= 0.5450\n",
      "Epoch: 03952 loss_train: 28.8457 loss_rec: 28.8457 acc_train: 0.4887 loss_val: 29.3092 acc_val: 0.4950 time: 0.0263s\n",
      "Epoch: 03953 loss_train: 24.9735 loss_rec: 24.9735 acc_train: 0.4882 loss_val: 25.3776 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 03954 loss_train: 9.5021 loss_rec: 9.5021 acc_train: 0.4888 loss_val: 9.6678 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03955 loss_train: 14.2331 loss_rec: 14.2331 acc_train: 0.5485 loss_val: 14.0827 acc_val: 0.5558 time: 0.0273s\n",
      "Epoch: 03956 loss_train: 25.5177 loss_rec: 25.5177 acc_train: 0.5486 loss_val: 25.1768 acc_val: 0.5550 time: 0.0273s\n",
      "Epoch: 03957 loss_train: 25.4551 loss_rec: 25.4551 acc_train: 0.5486 loss_val: 25.1150 acc_val: 0.5550 time: 0.0263s\n",
      "Epoch: 03958 loss_train: 15.1961 loss_rec: 15.1961 acc_train: 0.5481 loss_val: 15.0288 acc_val: 0.5554 time: 0.0268s\n",
      "Epoch: 03959 loss_train: 3.6358 loss_rec: 3.6358 acc_train: 0.4892 loss_val: 3.7095 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 03960 loss_train: 10.6732 loss_rec: 10.6732 acc_train: 0.4888 loss_val: 10.8579 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03961 loss_train: 5.1541 loss_rec: 5.1541 acc_train: 0.4891 loss_val: 5.2518 acc_val: 0.4963 time: 0.0258s\n",
      "Test set results: loss= 11.3011 accuracy= 0.5150\n",
      "Epoch: 03962 loss_train: 9.9821 loss_rec: 9.9821 acc_train: 0.5500 loss_val: 9.8992 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 03963 loss_train: 13.9349 loss_rec: 13.9349 acc_train: 0.5488 loss_val: 13.7896 acc_val: 0.5567 time: 0.0298s\n",
      "Epoch: 03964 loss_train: 7.4811 loss_rec: 7.4811 acc_train: 0.5545 loss_val: 7.4344 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 03965 loss_train: 8.0819 loss_rec: 8.0819 acc_train: 0.4888 loss_val: 8.2247 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03966 loss_train: 11.5185 loss_rec: 11.5185 acc_train: 0.4887 loss_val: 11.7167 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 03967 loss_train: 2.7760 loss_rec: 2.7760 acc_train: 0.4892 loss_val: 2.8354 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 03968 loss_train: 14.6136 loss_rec: 14.6136 acc_train: 0.5481 loss_val: 14.4565 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 03969 loss_train: 20.6899 loss_rec: 20.6899 acc_train: 0.5503 loss_val: 20.4287 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 03970 loss_train: 16.0485 loss_rec: 16.0485 acc_train: 0.5497 loss_val: 15.8668 acc_val: 0.5567 time: 0.0273s\n",
      "Epoch: 03971 loss_train: 2.3017 loss_rec: 2.3017 acc_train: 0.5929 loss_val: 2.3255 acc_val: 0.5954 time: 0.0273s\n",
      "Test set results: loss= 17.8373 accuracy= 0.5441\n",
      "Epoch: 03972 loss_train: 19.9275 loss_rec: 19.9275 acc_train: 0.4876 loss_val: 20.2571 acc_val: 0.4950 time: 0.0253s\n",
      "Epoch: 03973 loss_train: 28.5417 loss_rec: 28.5417 acc_train: 0.4887 loss_val: 29.0006 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 03974 loss_train: 24.3667 loss_rec: 24.3667 acc_train: 0.4882 loss_val: 24.7618 acc_val: 0.4950 time: 0.0258s\n",
      "Epoch: 03975 loss_train: 8.7451 loss_rec: 8.7451 acc_train: 0.4888 loss_val: 8.8985 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 03976 loss_train: 14.9782 loss_rec: 14.9782 acc_train: 0.5481 loss_val: 14.8149 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 03977 loss_train: 26.3833 loss_rec: 26.3833 acc_train: 0.5477 loss_val: 26.0312 acc_val: 0.5525 time: 0.0263s\n",
      "Epoch: 03978 loss_train: 26.5124 loss_rec: 26.5124 acc_train: 0.5477 loss_val: 26.1587 acc_val: 0.5529 time: 0.0273s\n",
      "Epoch: 03979 loss_train: 16.4838 loss_rec: 16.4838 acc_train: 0.5497 loss_val: 16.2944 acc_val: 0.5571 time: 0.0293s\n",
      "Epoch: 03980 loss_train: 1.9729 loss_rec: 1.9729 acc_train: 0.4883 loss_val: 2.0169 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03981 loss_train: 9.6901 loss_rec: 9.6901 acc_train: 0.4888 loss_val: 9.8589 acc_val: 0.4958 time: 0.0268s\n",
      "Test set results: loss= 4.3671 accuracy= 0.5458\n",
      "Epoch: 03982 loss_train: 4.9280 loss_rec: 4.9280 acc_train: 0.4891 loss_val: 5.0222 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 03983 loss_train: 9.4787 loss_rec: 9.4787 acc_train: 0.5504 loss_val: 9.4039 acc_val: 0.5596 time: 0.0263s\n",
      "Epoch: 03984 loss_train: 12.8945 loss_rec: 12.8945 acc_train: 0.5493 loss_val: 12.7672 acc_val: 0.5588 time: 0.0278s\n",
      "Epoch: 03985 loss_train: 6.1122 loss_rec: 6.1122 acc_train: 0.5587 loss_val: 6.0838 acc_val: 0.5667 time: 0.0278s\n",
      "Epoch: 03986 loss_train: 9.7868 loss_rec: 9.7868 acc_train: 0.4888 loss_val: 9.9572 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 03987 loss_train: 13.4623 loss_rec: 13.4623 acc_train: 0.4885 loss_val: 13.6912 acc_val: 0.4954 time: 0.0288s\n",
      "Epoch: 03988 loss_train: 5.0097 loss_rec: 5.0097 acc_train: 0.4891 loss_val: 5.1052 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 03989 loss_train: 12.4195 loss_rec: 12.4195 acc_train: 0.5500 loss_val: 12.2999 acc_val: 0.5592 time: 0.0258s\n",
      "Epoch: 03990 loss_train: 18.5034 loss_rec: 18.5034 acc_train: 0.5496 loss_val: 18.2794 acc_val: 0.5563 time: 0.0278s\n",
      "Epoch: 03991 loss_train: 13.9569 loss_rec: 13.9569 acc_train: 0.5485 loss_val: 13.8111 acc_val: 0.5558 time: 0.0278s\n",
      "Test set results: loss= 1.0842 accuracy= 0.6128\n",
      "Epoch: 03992 loss_train: 1.0485 loss_rec: 1.0485 acc_train: 0.6207 loss_val: 1.0735 acc_val: 0.6117 time: 0.0278s\n",
      "Epoch: 03993 loss_train: 17.3897 loss_rec: 17.3897 acc_train: 0.4877 loss_val: 17.6805 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 03994 loss_train: 21.9294 loss_rec: 21.9294 acc_train: 0.4883 loss_val: 22.2890 acc_val: 0.4950 time: 0.0258s\n",
      "Epoch: 03995 loss_train: 14.2360 loss_rec: 14.2360 acc_train: 0.4882 loss_val: 14.4773 acc_val: 0.4950 time: 0.0288s\n",
      "Epoch: 03996 loss_train: 3.5908 loss_rec: 3.5908 acc_train: 0.5757 loss_val: 3.5979 acc_val: 0.5808 time: 0.0273s\n",
      "Epoch: 03997 loss_train: 9.8176 loss_rec: 9.8176 acc_train: 0.5500 loss_val: 9.7370 acc_val: 0.5583 time: 0.0263s\n",
      "Epoch: 03998 loss_train: 5.9416 loss_rec: 5.9416 acc_train: 0.5595 loss_val: 5.9156 acc_val: 0.5679 time: 0.0273s\n",
      "Epoch: 03999 loss_train: 7.0723 loss_rec: 7.0723 acc_train: 0.4891 loss_val: 7.1997 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 04000 loss_train: 8.1860 loss_rec: 8.1860 acc_train: 0.4888 loss_val: 8.3305 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 04001 loss_train: 2.0849 loss_rec: 2.0849 acc_train: 0.5981 loss_val: 2.1101 acc_val: 0.6029 time: 0.0273s\n",
      "Test set results: loss= 3.3588 accuracy= 0.5551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04002 loss_train: 2.9997 loss_rec: 2.9997 acc_train: 0.5822 loss_val: 3.0156 acc_val: 0.5900 time: 0.0278s\n",
      "Epoch: 04003 loss_train: 4.6101 loss_rec: 4.6101 acc_train: 0.4891 loss_val: 4.6994 acc_val: 0.4963 time: 0.0283s\n",
      "Epoch: 04004 loss_train: 0.9527 loss_rec: 0.9527 acc_train: 0.6035 loss_val: 0.9713 acc_val: 0.5938 time: 0.0258s\n",
      "Epoch: 04005 loss_train: 5.5194 loss_rec: 5.5194 acc_train: 0.5603 loss_val: 5.5001 acc_val: 0.5679 time: 0.0273s\n",
      "Epoch: 04006 loss_train: 2.2827 loss_rec: 2.2827 acc_train: 0.5938 loss_val: 2.3064 acc_val: 0.5958 time: 0.0258s\n",
      "Epoch: 04007 loss_train: 9.4431 loss_rec: 9.4431 acc_train: 0.4888 loss_val: 9.6079 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 04008 loss_train: 8.7472 loss_rec: 8.7472 acc_train: 0.4888 loss_val: 8.9008 acc_val: 0.4958 time: 0.0268s\n",
      "Epoch: 04009 loss_train: 2.8960 loss_rec: 2.8960 acc_train: 0.5840 loss_val: 2.9133 acc_val: 0.5900 time: 0.0273s\n",
      "Epoch: 04010 loss_train: 4.3733 loss_rec: 4.3733 acc_train: 0.5680 loss_val: 4.3704 acc_val: 0.5742 time: 0.0263s\n",
      "Epoch: 04011 loss_train: 3.0654 loss_rec: 3.0654 acc_train: 0.4882 loss_val: 3.1298 acc_val: 0.4963 time: 0.0293s\n",
      "Test set results: loss= 1.1836 accuracy= 0.6175\n",
      "Epoch: 04012 loss_train: 1.1258 loss_rec: 1.1258 acc_train: 0.6296 loss_val: 1.1519 acc_val: 0.6208 time: 0.0273s\n",
      "Epoch: 04013 loss_train: 1.0448 loss_rec: 1.0448 acc_train: 0.5547 loss_val: 1.0651 acc_val: 0.5475 time: 0.0278s\n",
      "Epoch: 04014 loss_train: 2.6611 loss_rec: 2.6611 acc_train: 0.5861 loss_val: 2.6812 acc_val: 0.5904 time: 0.0278s\n",
      "Epoch: 04015 loss_train: 2.3397 loss_rec: 2.3397 acc_train: 0.4883 loss_val: 2.3911 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 04016 loss_train: 3.1390 loss_rec: 3.1390 acc_train: 0.5788 loss_val: 3.1525 acc_val: 0.5863 time: 0.0263s\n",
      "Epoch: 04017 loss_train: 0.8953 loss_rec: 0.8953 acc_train: 0.6100 loss_val: 0.9145 acc_val: 0.6071 time: 0.0278s\n",
      "Epoch: 04018 loss_train: 3.3162 loss_rec: 3.3162 acc_train: 0.4882 loss_val: 3.3847 acc_val: 0.4963 time: 0.0288s\n",
      "Epoch: 04019 loss_train: 4.3722 loss_rec: 4.3722 acc_train: 0.5680 loss_val: 4.3693 acc_val: 0.5742 time: 0.0273s\n",
      "Epoch: 04020 loss_train: 2.5404 loss_rec: 2.5404 acc_train: 0.5885 loss_val: 2.5616 acc_val: 0.5921 time: 0.0273s\n",
      "Epoch: 04021 loss_train: 7.8861 loss_rec: 7.8861 acc_train: 0.4889 loss_val: 8.0258 acc_val: 0.4963 time: 0.0263s\n",
      "Test set results: loss= 5.4311 accuracy= 0.5457\n",
      "Epoch: 04022 loss_train: 6.1136 loss_rec: 6.1136 acc_train: 0.4891 loss_val: 6.2262 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 04023 loss_train: 5.9574 loss_rec: 5.9574 acc_train: 0.5593 loss_val: 5.9309 acc_val: 0.5671 time: 0.0263s\n",
      "Epoch: 04024 loss_train: 7.3969 loss_rec: 7.3969 acc_train: 0.5545 loss_val: 7.3508 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 04025 loss_train: 0.9119 loss_rec: 0.9119 acc_train: 0.6038 loss_val: 0.9304 acc_val: 0.5963 time: 0.0293s\n",
      "Epoch: 04026 loss_train: 6.4416 loss_rec: 6.4416 acc_train: 0.4891 loss_val: 6.5592 acc_val: 0.4963 time: 0.0268s\n",
      "Epoch: 04027 loss_train: 0.9329 loss_rec: 0.9329 acc_train: 0.5998 loss_val: 0.9513 acc_val: 0.5925 time: 0.0258s\n",
      "Epoch: 04028 loss_train: 6.6263 loss_rec: 6.6263 acc_train: 0.5560 loss_val: 6.5900 acc_val: 0.5621 time: 0.0278s\n",
      "Epoch: 04029 loss_train: 4.0409 loss_rec: 4.0409 acc_train: 0.5707 loss_val: 4.0421 acc_val: 0.5763 time: 0.0263s\n",
      "Epoch: 04030 loss_train: 7.6351 loss_rec: 7.6351 acc_train: 0.4889 loss_val: 7.7709 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 04031 loss_train: 7.2851 loss_rec: 7.2851 acc_train: 0.4890 loss_val: 7.4155 acc_val: 0.4963 time: 0.0273s\n",
      "Test set results: loss= 4.3044 accuracy= 0.5401\n",
      "Epoch: 04032 loss_train: 3.8305 loss_rec: 3.8305 acc_train: 0.5715 loss_val: 3.8340 acc_val: 0.5767 time: 0.0278s\n",
      "Epoch: 04033 loss_train: 4.6459 loss_rec: 4.6459 acc_train: 0.5658 loss_val: 4.6389 acc_val: 0.5721 time: 0.0278s\n",
      "Epoch: 04034 loss_train: 3.6022 loss_rec: 3.6022 acc_train: 0.4882 loss_val: 3.6751 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 04035 loss_train: 0.9161 loss_rec: 0.9161 acc_train: 0.6044 loss_val: 0.9338 acc_val: 0.5963 time: 0.0278s\n",
      "Epoch: 04036 loss_train: 4.3004 loss_rec: 4.3004 acc_train: 0.5663 loss_val: 4.2962 acc_val: 0.5733 time: 0.0273s\n",
      "Epoch: 04037 loss_train: 0.9051 loss_rec: 0.9051 acc_train: 0.6220 loss_val: 0.9254 acc_val: 0.6096 time: 0.0258s\n",
      "Epoch: 04038 loss_train: 6.1951 loss_rec: 6.1951 acc_train: 0.4891 loss_val: 6.3083 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 04039 loss_train: 0.9347 loss_rec: 0.9347 acc_train: 0.6028 loss_val: 0.9525 acc_val: 0.5938 time: 0.0278s\n",
      "Epoch: 04040 loss_train: 6.8002 loss_rec: 6.8002 acc_train: 0.5554 loss_val: 6.7595 acc_val: 0.5604 time: 0.0278s\n",
      "Epoch: 04041 loss_train: 4.3927 loss_rec: 4.3927 acc_train: 0.5664 loss_val: 4.3873 acc_val: 0.5733 time: 0.0288s\n",
      "Test set results: loss= 6.3479 accuracy= 0.5455\n",
      "Epoch: 04042 loss_train: 7.1337 loss_rec: 7.1337 acc_train: 0.4890 loss_val: 7.2614 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 04043 loss_train: 6.7554 loss_rec: 6.7554 acc_train: 0.4891 loss_val: 6.8773 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 04044 loss_train: 4.2657 loss_rec: 4.2657 acc_train: 0.5671 loss_val: 4.2620 acc_val: 0.5733 time: 0.0278s\n",
      "Epoch: 04045 loss_train: 4.9449 loss_rec: 4.9449 acc_train: 0.5629 loss_val: 4.9315 acc_val: 0.5696 time: 0.0278s\n",
      "Epoch: 04046 loss_train: 3.4644 loss_rec: 3.4644 acc_train: 0.4882 loss_val: 3.5347 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 04047 loss_train: 0.9331 loss_rec: 0.9331 acc_train: 0.6028 loss_val: 0.9509 acc_val: 0.5938 time: 0.0273s\n",
      "Epoch: 04048 loss_train: 4.5929 loss_rec: 4.5929 acc_train: 0.5648 loss_val: 4.5845 acc_val: 0.5708 time: 0.0288s\n",
      "Epoch: 04049 loss_train: 1.1024 loss_rec: 1.1024 acc_train: 0.6291 loss_val: 1.1270 acc_val: 0.6208 time: 0.0258s\n",
      "Epoch: 04050 loss_train: 8.4651 loss_rec: 8.4651 acc_train: 0.4888 loss_val: 8.6134 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04051 loss_train: 5.2465 loss_rec: 5.2465 acc_train: 0.4882 loss_val: 5.3450 acc_val: 0.4963 time: 0.0283s\n",
      "Test set results: loss= 8.8781 accuracy= 0.5175\n",
      "Epoch: 04052 loss_train: 7.8464 loss_rec: 7.8464 acc_train: 0.5532 loss_val: 7.7927 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 04053 loss_train: 10.2029 loss_rec: 10.2029 acc_train: 0.5530 loss_val: 10.1134 acc_val: 0.5621 time: 0.0278s\n",
      "Epoch: 04054 loss_train: 2.8887 loss_rec: 2.8887 acc_train: 0.5825 loss_val: 2.9034 acc_val: 0.5900 time: 0.0258s\n",
      "Epoch: 04055 loss_train: 13.2519 loss_rec: 13.2519 acc_train: 0.4883 loss_val: 13.4767 acc_val: 0.4954 time: 0.0268s\n",
      "Epoch: 04056 loss_train: 16.7089 loss_rec: 16.7089 acc_train: 0.4877 loss_val: 16.9884 acc_val: 0.4950 time: 0.0288s\n",
      "Epoch: 04057 loss_train: 8.1475 loss_rec: 8.1475 acc_train: 0.4888 loss_val: 8.2908 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 04058 loss_train: 9.5848 loss_rec: 9.5848 acc_train: 0.5504 loss_val: 9.5046 acc_val: 0.5583 time: 0.0283s\n",
      "Epoch: 04059 loss_train: 15.8213 loss_rec: 15.8213 acc_train: 0.5505 loss_val: 15.6391 acc_val: 0.5579 time: 0.0258s\n",
      "Epoch: 04060 loss_train: 11.5358 loss_rec: 11.5358 acc_train: 0.5503 loss_val: 11.4264 acc_val: 0.5596 time: 0.0263s\n",
      "Epoch: 04061 loss_train: 1.6443 loss_rec: 1.6443 acc_train: 0.4866 loss_val: 1.6802 acc_val: 0.4946 time: 0.0278s\n",
      "Test set results: loss= 4.4190 accuracy= 0.5454\n",
      "Epoch: 04062 loss_train: 4.9833 loss_rec: 4.9833 acc_train: 0.4882 loss_val: 5.0777 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 04063 loss_train: 2.8755 loss_rec: 2.8755 acc_train: 0.5822 loss_val: 2.8903 acc_val: 0.5900 time: 0.0263s\n",
      "Epoch: 04064 loss_train: 1.6585 loss_rec: 1.6585 acc_train: 0.6069 loss_val: 1.6842 acc_val: 0.6075 time: 0.0293s\n",
      "Epoch: 04065 loss_train: 7.2356 loss_rec: 7.2356 acc_train: 0.4890 loss_val: 7.3648 acc_val: 0.4963 time: 0.0253s\n",
      "Epoch: 04066 loss_train: 3.8572 loss_rec: 3.8572 acc_train: 0.4882 loss_val: 3.9338 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 04067 loss_train: 9.2239 loss_rec: 9.2239 acc_train: 0.5502 loss_val: 9.1496 acc_val: 0.5600 time: 0.0273s\n",
      "Epoch: 04068 loss_train: 11.5799 loss_rec: 11.5799 acc_train: 0.5503 loss_val: 11.4699 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 04069 loss_train: 4.1160 loss_rec: 4.1160 acc_train: 0.5679 loss_val: 4.1139 acc_val: 0.5742 time: 0.0263s\n",
      "Epoch: 04070 loss_train: 12.3903 loss_rec: 12.3903 acc_train: 0.4886 loss_val: 12.6017 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 04071 loss_train: 16.4452 loss_rec: 16.4452 acc_train: 0.4877 loss_val: 16.7206 acc_val: 0.4950 time: 0.0278s\n",
      "Test set results: loss= 7.5206 accuracy= 0.5454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04072 loss_train: 8.4403 loss_rec: 8.4403 acc_train: 0.4888 loss_val: 8.5883 acc_val: 0.4958 time: 0.0288s\n",
      "Epoch: 04073 loss_train: 8.8405 loss_rec: 8.8405 acc_train: 0.5511 loss_val: 8.7725 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 04074 loss_train: 14.7164 loss_rec: 14.7164 acc_train: 0.5482 loss_val: 14.5532 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 04075 loss_train: 10.1387 loss_rec: 10.1387 acc_train: 0.5529 loss_val: 10.0500 acc_val: 0.5621 time: 0.0258s\n",
      "Epoch: 04076 loss_train: 3.3073 loss_rec: 3.3073 acc_train: 0.4882 loss_val: 3.3750 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 04077 loss_train: 5.3715 loss_rec: 5.3715 acc_train: 0.4882 loss_val: 5.4720 acc_val: 0.4963 time: 0.0268s\n",
      "Epoch: 04078 loss_train: 3.5310 loss_rec: 3.5310 acc_train: 0.5747 loss_val: 3.5360 acc_val: 0.5808 time: 0.0273s\n",
      "Epoch: 04079 loss_train: 2.7884 loss_rec: 2.7884 acc_train: 0.5842 loss_val: 2.8044 acc_val: 0.5900 time: 0.0273s\n",
      "Epoch: 04080 loss_train: 6.6178 loss_rec: 6.6178 acc_train: 0.4891 loss_val: 6.7376 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 04081 loss_train: 4.0819 loss_rec: 4.0819 acc_train: 0.4882 loss_val: 4.1623 acc_val: 0.4963 time: 0.0273s\n",
      "Test set results: loss= 9.3890 accuracy= 0.5167\n",
      "Epoch: 04082 loss_train: 8.2958 loss_rec: 8.2958 acc_train: 0.5517 loss_val: 8.2360 acc_val: 0.5592 time: 0.0278s\n",
      "Epoch: 04083 loss_train: 10.1009 loss_rec: 10.1009 acc_train: 0.5529 loss_val: 10.0129 acc_val: 0.5621 time: 0.0278s\n",
      "Epoch: 04084 loss_train: 2.4116 loss_rec: 2.4116 acc_train: 0.5892 loss_val: 2.4318 acc_val: 0.5929 time: 0.0273s\n",
      "Epoch: 04085 loss_train: 13.8371 loss_rec: 13.8371 acc_train: 0.4882 loss_val: 14.0714 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04086 loss_train: 17.3432 loss_rec: 17.3432 acc_train: 0.4877 loss_val: 17.6325 acc_val: 0.4950 time: 0.0263s\n",
      "Epoch: 04087 loss_train: 8.8446 loss_rec: 8.8446 acc_train: 0.4888 loss_val: 8.9992 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 04088 loss_train: 8.8353 loss_rec: 8.8353 acc_train: 0.5511 loss_val: 8.7674 acc_val: 0.5596 time: 0.0293s\n",
      "Epoch: 04089 loss_train: 15.0595 loss_rec: 15.0595 acc_train: 0.5497 loss_val: 14.8904 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 04090 loss_train: 10.8353 loss_rec: 10.8353 acc_train: 0.5521 loss_val: 10.7365 acc_val: 0.5608 time: 0.0273s\n",
      "Epoch: 04091 loss_train: 2.2293 loss_rec: 2.2293 acc_train: 0.4866 loss_val: 2.2781 acc_val: 0.4946 time: 0.0263s\n",
      "Test set results: loss= 3.9635 accuracy= 0.5454\n",
      "Epoch: 04092 loss_train: 4.4752 loss_rec: 4.4752 acc_train: 0.4882 loss_val: 4.5617 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 04093 loss_train: 4.1110 loss_rec: 4.1110 acc_train: 0.5679 loss_val: 4.1090 acc_val: 0.5742 time: 0.0278s\n",
      "Epoch: 04094 loss_train: 3.0078 loss_rec: 3.0078 acc_train: 0.5788 loss_val: 3.0207 acc_val: 0.5863 time: 0.0263s\n",
      "Epoch: 04095 loss_train: 6.8562 loss_rec: 6.8562 acc_train: 0.4891 loss_val: 6.9797 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 04096 loss_train: 4.8055 loss_rec: 4.8055 acc_train: 0.4882 loss_val: 4.8972 acc_val: 0.4963 time: 0.0288s\n",
      "Epoch: 04097 loss_train: 7.2478 loss_rec: 7.2478 acc_train: 0.5545 loss_val: 7.2015 acc_val: 0.5596 time: 0.0258s\n",
      "Epoch: 04098 loss_train: 8.7700 loss_rec: 8.7700 acc_train: 0.5520 loss_val: 8.7032 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 04099 loss_train: 1.3217 loss_rec: 1.3217 acc_train: 0.6191 loss_val: 1.3480 acc_val: 0.6188 time: 0.0278s\n",
      "Epoch: 04100 loss_train: 13.2016 loss_rec: 13.2016 acc_train: 0.4885 loss_val: 13.4259 acc_val: 0.4954 time: 0.0263s\n",
      "Epoch: 04101 loss_train: 14.7000 loss_rec: 14.7000 acc_train: 0.4879 loss_val: 14.9484 acc_val: 0.4950 time: 0.0273s\n",
      "Test set results: loss= 3.9786 accuracy= 0.5454\n",
      "Epoch: 04102 loss_train: 4.4920 loss_rec: 4.4920 acc_train: 0.4882 loss_val: 4.5788 acc_val: 0.4963 time: 0.0253s\n",
      "Epoch: 04103 loss_train: 14.2442 loss_rec: 14.2442 acc_train: 0.5481 loss_val: 14.0892 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 04104 loss_train: 21.6156 loss_rec: 21.6156 acc_train: 0.5494 loss_val: 21.3347 acc_val: 0.5563 time: 0.0288s\n",
      "Epoch: 04105 loss_train: 18.3556 loss_rec: 18.3556 acc_train: 0.5497 loss_val: 18.1301 acc_val: 0.5563 time: 0.0263s\n",
      "Epoch: 04106 loss_train: 5.7021 loss_rec: 5.7021 acc_train: 0.5593 loss_val: 5.6767 acc_val: 0.5667 time: 0.0268s\n",
      "Epoch: 04107 loss_train: 15.7970 loss_rec: 15.7970 acc_train: 0.4877 loss_val: 16.0626 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 04108 loss_train: 24.6206 loss_rec: 24.6206 acc_train: 0.4882 loss_val: 25.0190 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 04109 loss_train: 20.9423 loss_rec: 20.9423 acc_train: 0.4875 loss_val: 21.2865 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04110 loss_train: 6.0711 loss_rec: 6.0711 acc_train: 0.4882 loss_val: 6.1826 acc_val: 0.4963 time: 0.0268s\n",
      "Epoch: 04111 loss_train: 16.6076 loss_rec: 16.6076 acc_train: 0.5487 loss_val: 16.4120 acc_val: 0.5554 time: 0.0278s\n",
      "Test set results: loss= 31.1251 accuracy= 0.5042\n",
      "Epoch: 04112 loss_train: 27.4830 loss_rec: 27.4830 acc_train: 0.5460 loss_val: 27.1150 acc_val: 0.5533 time: 0.0278s\n",
      "Epoch: 04113 loss_train: 27.3206 loss_rec: 27.3206 acc_train: 0.5459 loss_val: 26.9551 acc_val: 0.5533 time: 0.0253s\n",
      "Epoch: 04114 loss_train: 17.2242 loss_rec: 17.2242 acc_train: 0.5494 loss_val: 17.0184 acc_val: 0.5563 time: 0.0263s\n",
      "Epoch: 04115 loss_train: 1.2032 loss_rec: 1.2032 acc_train: 0.5650 loss_val: 1.2274 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 04116 loss_train: 11.9135 loss_rec: 11.9135 acc_train: 0.4887 loss_val: 12.1176 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04117 loss_train: 10.2622 loss_rec: 10.2622 acc_train: 0.4888 loss_val: 10.4399 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 04118 loss_train: 2.2367 loss_rec: 2.2367 acc_train: 0.5923 loss_val: 2.2588 acc_val: 0.5958 time: 0.0273s\n",
      "Epoch: 04119 loss_train: 4.7901 loss_rec: 4.7901 acc_train: 0.5628 loss_val: 4.7792 acc_val: 0.5696 time: 0.0278s\n",
      "Epoch: 04120 loss_train: 1.5188 loss_rec: 1.5188 acc_train: 0.4866 loss_val: 1.5517 acc_val: 0.4946 time: 0.0258s\n",
      "Epoch: 04121 loss_train: 1.4063 loss_rec: 1.4063 acc_train: 0.6138 loss_val: 1.4329 acc_val: 0.6154 time: 0.0278s\n",
      "Test set results: loss= 2.3998 accuracy= 0.5440\n",
      "Epoch: 04122 loss_train: 2.7310 loss_rec: 2.7310 acc_train: 0.4871 loss_val: 2.7890 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 04123 loss_train: 3.8786 loss_rec: 3.8786 acc_train: 0.5711 loss_val: 3.8798 acc_val: 0.5767 time: 0.0273s\n",
      "Epoch: 04124 loss_train: 1.5742 loss_rec: 1.5742 acc_train: 0.6102 loss_val: 1.6006 acc_val: 0.6100 time: 0.0273s\n",
      "Epoch: 04125 loss_train: 8.2542 loss_rec: 8.2542 acc_train: 0.4888 loss_val: 8.3993 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 04126 loss_train: 5.8099 loss_rec: 5.8099 acc_train: 0.4882 loss_val: 5.9172 acc_val: 0.4963 time: 0.0288s\n",
      "Epoch: 04127 loss_train: 6.5978 loss_rec: 6.5978 acc_train: 0.5553 loss_val: 6.5598 acc_val: 0.5617 time: 0.0263s\n",
      "Epoch: 04128 loss_train: 8.4582 loss_rec: 8.4582 acc_train: 0.5518 loss_val: 8.3965 acc_val: 0.5592 time: 0.0268s\n",
      "Epoch: 04129 loss_train: 1.3779 loss_rec: 1.3779 acc_train: 0.6154 loss_val: 1.4045 acc_val: 0.6167 time: 0.0273s\n",
      "Epoch: 04130 loss_train: 12.8684 loss_rec: 12.8684 acc_train: 0.4886 loss_val: 13.0873 acc_val: 0.4954 time: 0.0263s\n",
      "Epoch: 04131 loss_train: 14.2707 loss_rec: 14.2707 acc_train: 0.4879 loss_val: 14.5120 acc_val: 0.4950 time: 0.0278s\n",
      "Test set results: loss= 3.5801 accuracy= 0.5454\n",
      "Epoch: 04132 loss_train: 4.0477 loss_rec: 4.0477 acc_train: 0.4882 loss_val: 4.1274 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 04133 loss_train: 14.5551 loss_rec: 14.5551 acc_train: 0.5481 loss_val: 14.3947 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 04134 loss_train: 21.9156 loss_rec: 21.9156 acc_train: 0.5496 loss_val: 21.6295 acc_val: 0.5563 time: 0.0288s\n",
      "Epoch: 04135 loss_train: 18.7418 loss_rec: 18.7418 acc_train: 0.5490 loss_val: 18.5098 acc_val: 0.5558 time: 0.0258s\n",
      "Epoch: 04136 loss_train: 6.1955 loss_rec: 6.1955 acc_train: 0.5573 loss_val: 6.1629 acc_val: 0.5642 time: 0.0258s\n",
      "Epoch: 04137 loss_train: 15.0720 loss_rec: 15.0720 acc_train: 0.4877 loss_val: 15.3262 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 04138 loss_train: 23.8549 loss_rec: 23.8549 acc_train: 0.4882 loss_val: 24.2415 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04139 loss_train: 20.2267 loss_rec: 20.2267 acc_train: 0.4875 loss_val: 20.5599 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04140 loss_train: 5.4858 loss_rec: 5.4858 acc_train: 0.4882 loss_val: 5.5880 acc_val: 0.4963 time: 0.0268s\n",
      "Epoch: 04141 loss_train: 16.9685 loss_rec: 16.9685 acc_train: 0.5487 loss_val: 16.7669 acc_val: 0.5554 time: 0.0258s\n",
      "Test set results: loss= 31.4050 accuracy= 0.5037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04142 loss_train: 27.7300 loss_rec: 27.7300 acc_train: 0.5457 loss_val: 27.3582 acc_val: 0.5542 time: 0.0293s\n",
      "Epoch: 04143 loss_train: 27.5525 loss_rec: 27.5525 acc_train: 0.5464 loss_val: 27.1834 acc_val: 0.5550 time: 0.0273s\n",
      "Epoch: 04144 loss_train: 17.5197 loss_rec: 17.5197 acc_train: 0.5495 loss_val: 17.3083 acc_val: 0.5563 time: 0.0263s\n",
      "Epoch: 04145 loss_train: 0.9646 loss_rec: 0.9646 acc_train: 0.5757 loss_val: 0.9828 acc_val: 0.5721 time: 0.0263s\n",
      "Epoch: 04146 loss_train: 14.7118 loss_rec: 14.7118 acc_train: 0.4879 loss_val: 14.9603 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04147 loss_train: 16.0689 loss_rec: 16.0689 acc_train: 0.4877 loss_val: 16.3386 acc_val: 0.4950 time: 0.0258s\n",
      "Epoch: 04148 loss_train: 5.8535 loss_rec: 5.8535 acc_train: 0.4882 loss_val: 5.9615 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 04149 loss_train: 12.8354 loss_rec: 12.8354 acc_train: 0.5488 loss_val: 12.7045 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 04150 loss_train: 20.2179 loss_rec: 20.2179 acc_train: 0.5506 loss_val: 19.9605 acc_val: 0.5567 time: 0.0258s\n",
      "Epoch: 04151 loss_train: 17.1206 loss_rec: 17.1206 acc_train: 0.5494 loss_val: 16.9161 acc_val: 0.5563 time: 0.0293s\n",
      "Test set results: loss= 5.4004 accuracy= 0.5297\n",
      "Epoch: 04152 loss_train: 4.7908 loss_rec: 4.7908 acc_train: 0.5628 loss_val: 4.7793 acc_val: 0.5696 time: 0.0258s\n",
      "Epoch: 04153 loss_train: 16.2681 loss_rec: 16.2681 acc_train: 0.4877 loss_val: 16.5409 acc_val: 0.4950 time: 0.0263s\n",
      "Epoch: 04154 loss_train: 24.7493 loss_rec: 24.7493 acc_train: 0.4882 loss_val: 25.1494 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04155 loss_train: 20.9206 loss_rec: 20.9206 acc_train: 0.4875 loss_val: 21.2644 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04156 loss_train: 6.0824 loss_rec: 6.0824 acc_train: 0.4882 loss_val: 6.1940 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 04157 loss_train: 16.4114 loss_rec: 16.4114 acc_train: 0.5487 loss_val: 16.2188 acc_val: 0.5554 time: 0.0258s\n",
      "Epoch: 04158 loss_train: 27.2575 loss_rec: 27.2575 acc_train: 0.5451 loss_val: 26.8926 acc_val: 0.5525 time: 0.0293s\n",
      "Epoch: 04159 loss_train: 27.2025 loss_rec: 27.2025 acc_train: 0.5451 loss_val: 26.8381 acc_val: 0.5525 time: 0.0263s\n",
      "Epoch: 04160 loss_train: 17.3846 loss_rec: 17.3846 acc_train: 0.5495 loss_val: 17.1754 acc_val: 0.5563 time: 0.0283s\n",
      "Epoch: 04161 loss_train: 0.9265 loss_rec: 0.9265 acc_train: 0.6022 loss_val: 0.9441 acc_val: 0.5929 time: 0.0273s\n",
      "Test set results: loss= 13.7013 accuracy= 0.5443\n",
      "Epoch: 04162 loss_train: 15.3212 loss_rec: 15.3212 acc_train: 0.4877 loss_val: 15.5794 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04163 loss_train: 17.4148 loss_rec: 17.4148 acc_train: 0.4877 loss_val: 17.7053 acc_val: 0.4950 time: 0.0268s\n",
      "Epoch: 04164 loss_train: 7.9337 loss_rec: 7.9337 acc_train: 0.4879 loss_val: 8.0737 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04165 loss_train: 10.2451 loss_rec: 10.2451 acc_train: 0.5524 loss_val: 10.1546 acc_val: 0.5621 time: 0.0273s\n",
      "Epoch: 04166 loss_train: 17.1555 loss_rec: 17.1555 acc_train: 0.5494 loss_val: 16.9503 acc_val: 0.5563 time: 0.0288s\n",
      "Epoch: 04167 loss_train: 13.7201 loss_rec: 13.7201 acc_train: 0.5481 loss_val: 13.5735 acc_val: 0.5554 time: 0.0263s\n",
      "Epoch: 04168 loss_train: 1.6824 loss_rec: 1.6824 acc_train: 0.6048 loss_val: 1.7078 acc_val: 0.6054 time: 0.0268s\n",
      "Epoch: 04169 loss_train: 17.9625 loss_rec: 17.9625 acc_train: 0.4877 loss_val: 18.2615 acc_val: 0.4950 time: 0.0263s\n",
      "Epoch: 04170 loss_train: 24.5468 loss_rec: 24.5468 acc_train: 0.4882 loss_val: 24.9440 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04171 loss_train: 19.1146 loss_rec: 19.1146 acc_train: 0.4877 loss_val: 19.4312 acc_val: 0.4950 time: 0.0278s\n",
      "Test set results: loss= 2.5738 accuracy= 0.5432\n",
      "Epoch: 04172 loss_train: 2.9247 loss_rec: 2.9247 acc_train: 0.4865 loss_val: 2.9861 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04173 loss_train: 20.2449 loss_rec: 20.2449 acc_train: 0.5506 loss_val: 19.9867 acc_val: 0.5567 time: 0.0293s\n",
      "Epoch: 04174 loss_train: 32.0090 loss_rec: 32.0090 acc_train: 0.5441 loss_val: 31.5752 acc_val: 0.5513 time: 0.0258s\n",
      "Epoch: 04175 loss_train: 32.8230 loss_rec: 32.8230 acc_train: 0.5424 loss_val: 32.3781 acc_val: 0.5479 time: 0.0278s\n",
      "Epoch: 04176 loss_train: 23.7829 loss_rec: 23.7829 acc_train: 0.5486 loss_val: 23.4662 acc_val: 0.5550 time: 0.0268s\n",
      "Epoch: 04177 loss_train: 6.1804 loss_rec: 6.1804 acc_train: 0.5573 loss_val: 6.1479 acc_val: 0.5642 time: 0.0278s\n",
      "Epoch: 04178 loss_train: 19.8450 loss_rec: 19.8450 acc_train: 0.4875 loss_val: 20.1729 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04179 loss_train: 33.0856 loss_rec: 33.0856 acc_train: 0.4880 loss_val: 33.6151 acc_val: 0.4942 time: 0.0273s\n",
      "Epoch: 04180 loss_train: 33.6938 loss_rec: 33.6938 acc_train: 0.4885 loss_val: 34.2329 acc_val: 0.4946 time: 0.0258s\n",
      "Epoch: 04181 loss_train: 22.9004 loss_rec: 22.9004 acc_train: 0.4873 loss_val: 23.2733 acc_val: 0.4950 time: 0.0273s\n",
      "Test set results: loss= 1.7643 accuracy= 0.5432\n",
      "Epoch: 04182 loss_train: 2.0182 loss_rec: 2.0182 acc_train: 0.4866 loss_val: 2.0627 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04183 loss_train: 24.4058 loss_rec: 24.4058 acc_train: 0.5483 loss_val: 24.0790 acc_val: 0.5550 time: 0.0268s\n",
      "Epoch: 04184 loss_train: 39.1973 loss_rec: 39.1973 acc_train: 0.5410 loss_val: 38.6762 acc_val: 0.5483 time: 0.0263s\n",
      "Epoch: 04185 loss_train: 42.7461 loss_rec: 42.7461 acc_train: 0.5379 loss_val: 42.1828 acc_val: 0.5458 time: 0.0263s\n",
      "Epoch: 04186 loss_train: 36.1313 loss_rec: 36.1313 acc_train: 0.5433 loss_val: 35.6460 acc_val: 0.5483 time: 0.0278s\n",
      "Epoch: 04187 loss_train: 20.4956 loss_rec: 20.4956 acc_train: 0.5506 loss_val: 20.2322 acc_val: 0.5571 time: 0.0278s\n",
      "Epoch: 04188 loss_train: 2.4587 loss_rec: 2.4587 acc_train: 0.4866 loss_val: 2.5116 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04189 loss_train: 14.5263 loss_rec: 14.5263 acc_train: 0.4879 loss_val: 14.7717 acc_val: 0.4950 time: 0.0288s\n",
      "Epoch: 04190 loss_train: 14.2510 loss_rec: 14.2510 acc_train: 0.4879 loss_val: 14.4919 acc_val: 0.4950 time: 0.0258s\n",
      "Epoch: 04191 loss_train: 2.8770 loss_rec: 2.8770 acc_train: 0.4865 loss_val: 2.9374 acc_val: 0.4946 time: 0.0263s\n",
      "Test set results: loss= 18.2980 accuracy= 0.5098\n",
      "Epoch: 04192 loss_train: 16.1612 loss_rec: 16.1612 acc_train: 0.5492 loss_val: 15.9715 acc_val: 0.5558 time: 0.0283s\n",
      "Epoch: 04193 loss_train: 24.2855 loss_rec: 24.2855 acc_train: 0.5483 loss_val: 23.9601 acc_val: 0.5550 time: 0.0263s\n",
      "Epoch: 04194 loss_train: 22.0415 loss_rec: 22.0415 acc_train: 0.5492 loss_val: 21.7518 acc_val: 0.5546 time: 0.0273s\n",
      "Epoch: 04195 loss_train: 10.5414 loss_rec: 10.5414 acc_train: 0.5521 loss_val: 10.4455 acc_val: 0.5608 time: 0.0283s\n",
      "Epoch: 04196 loss_train: 9.1308 loss_rec: 9.1308 acc_train: 0.4879 loss_val: 9.2898 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 04197 loss_train: 17.2660 loss_rec: 17.2660 acc_train: 0.4877 loss_val: 17.5539 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04198 loss_train: 13.4508 loss_rec: 13.4508 acc_train: 0.4883 loss_val: 13.6788 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 04199 loss_train: 1.1595 loss_rec: 1.1595 acc_train: 0.6262 loss_val: 1.1841 acc_val: 0.6233 time: 0.0258s\n",
      "Epoch: 04200 loss_train: 7.3299 loss_rec: 7.3299 acc_train: 0.5543 loss_val: 7.2814 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 04201 loss_train: 4.6438 loss_rec: 4.6438 acc_train: 0.5634 loss_val: 4.6334 acc_val: 0.5704 time: 0.0273s\n",
      "Test set results: loss= 5.9640 accuracy= 0.5452\n",
      "Epoch: 04202 loss_train: 6.7043 loss_rec: 6.7043 acc_train: 0.4882 loss_val: 6.8251 acc_val: 0.4963 time: 0.0268s\n",
      "Epoch: 04203 loss_train: 6.7532 loss_rec: 6.7532 acc_train: 0.4882 loss_val: 6.8748 acc_val: 0.4963 time: 0.0283s\n",
      "Epoch: 04204 loss_train: 3.4303 loss_rec: 3.4303 acc_train: 0.5757 loss_val: 3.4356 acc_val: 0.5808 time: 0.0263s\n",
      "Epoch: 04205 loss_train: 4.0143 loss_rec: 4.0143 acc_train: 0.5679 loss_val: 4.0124 acc_val: 0.5742 time: 0.0273s\n",
      "Epoch: 04206 loss_train: 3.8798 loss_rec: 3.8798 acc_train: 0.4871 loss_val: 3.9567 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04207 loss_train: 1.0976 loss_rec: 1.0976 acc_train: 0.5555 loss_val: 1.1187 acc_val: 0.5479 time: 0.0253s\n",
      "Epoch: 04208 loss_train: 6.8914 loss_rec: 6.8914 acc_train: 0.5545 loss_val: 6.8485 acc_val: 0.5596 time: 0.0283s\n",
      "Epoch: 04209 loss_train: 5.3116 loss_rec: 5.3116 acc_train: 0.5603 loss_val: 5.2911 acc_val: 0.5692 time: 0.0263s\n",
      "Epoch: 04210 loss_train: 4.9079 loss_rec: 4.9079 acc_train: 0.4882 loss_val: 5.0010 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 04211 loss_train: 4.0838 loss_rec: 4.0838 acc_train: 0.4871 loss_val: 4.1640 acc_val: 0.4946 time: 0.0293s\n",
      "Test set results: loss= 7.2495 accuracy= 0.5210\n",
      "Epoch: 04212 loss_train: 6.4143 loss_rec: 6.4143 acc_train: 0.5553 loss_val: 6.3776 acc_val: 0.5617 time: 0.0258s\n",
      "Epoch: 04213 loss_train: 7.0645 loss_rec: 7.0645 acc_train: 0.5545 loss_val: 7.0194 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 04214 loss_train: 1.1138 loss_rec: 1.1138 acc_train: 0.5567 loss_val: 1.1353 acc_val: 0.5500 time: 0.0278s\n",
      "Epoch: 04215 loss_train: 2.7593 loss_rec: 2.7593 acc_train: 0.4865 loss_val: 2.8176 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04216 loss_train: 5.2479 loss_rec: 5.2479 acc_train: 0.5603 loss_val: 5.2282 acc_val: 0.5683 time: 0.0273s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04217 loss_train: 3.9434 loss_rec: 3.9434 acc_train: 0.5687 loss_val: 3.9424 acc_val: 0.5742 time: 0.0268s\n",
      "Epoch: 04218 loss_train: 5.9050 loss_rec: 5.9050 acc_train: 0.4882 loss_val: 6.0136 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 04219 loss_train: 4.5837 loss_rec: 4.5837 acc_train: 0.4877 loss_val: 4.6716 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04220 loss_train: 6.3922 loss_rec: 6.3922 acc_train: 0.5553 loss_val: 6.3554 acc_val: 0.5617 time: 0.0273s\n",
      "Epoch: 04221 loss_train: 7.4299 loss_rec: 7.4299 acc_train: 0.5543 loss_val: 7.3802 acc_val: 0.5596 time: 0.0273s\n",
      "Test set results: loss= 0.8191 accuracy= 0.6243\n",
      "Epoch: 04222 loss_train: 0.8687 loss_rec: 0.8687 acc_train: 0.6100 loss_val: 0.8863 acc_val: 0.6071 time: 0.0278s\n",
      "Epoch: 04223 loss_train: 7.6692 loss_rec: 7.6692 acc_train: 0.4879 loss_val: 7.8048 acc_val: 0.4963 time: 0.0268s\n",
      "Epoch: 04224 loss_train: 3.5470 loss_rec: 3.5470 acc_train: 0.4865 loss_val: 3.6184 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04225 loss_train: 9.5591 loss_rec: 9.5591 acc_train: 0.5495 loss_val: 9.4778 acc_val: 0.5583 time: 0.0288s\n",
      "Epoch: 04226 loss_train: 12.4377 loss_rec: 12.4377 acc_train: 0.5488 loss_val: 12.3120 acc_val: 0.5567 time: 0.0258s\n",
      "Epoch: 04227 loss_train: 5.7895 loss_rec: 5.7895 acc_train: 0.5583 loss_val: 5.7616 acc_val: 0.5658 time: 0.0268s\n",
      "Epoch: 04228 loss_train: 9.4025 loss_rec: 9.4025 acc_train: 0.4879 loss_val: 9.5658 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 04229 loss_train: 13.1001 loss_rec: 13.1001 acc_train: 0.4883 loss_val: 13.3224 acc_val: 0.4954 time: 0.0258s\n",
      "Epoch: 04230 loss_train: 5.4112 loss_rec: 5.4112 acc_train: 0.4882 loss_val: 5.5120 acc_val: 0.4963 time: 0.0268s\n",
      "Epoch: 04231 loss_train: 10.8119 loss_rec: 10.8119 acc_train: 0.5507 loss_val: 10.7120 acc_val: 0.5600 time: 0.0288s\n",
      "Test set results: loss= 18.5394 accuracy= 0.5096\n",
      "Epoch: 04232 loss_train: 16.3744 loss_rec: 16.3744 acc_train: 0.5494 loss_val: 16.1807 acc_val: 0.5563 time: 0.0273s\n",
      "Epoch: 04233 loss_train: 11.9951 loss_rec: 11.9951 acc_train: 0.5495 loss_val: 11.8767 acc_val: 0.5571 time: 0.0283s\n",
      "Epoch: 04234 loss_train: 0.9991 loss_rec: 0.9991 acc_train: 0.5537 loss_val: 1.0178 acc_val: 0.5463 time: 0.0263s\n",
      "Epoch: 04235 loss_train: 8.7529 loss_rec: 8.7529 acc_train: 0.4879 loss_val: 8.9058 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 04236 loss_train: 5.1120 loss_rec: 5.1120 acc_train: 0.4882 loss_val: 5.2082 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 04237 loss_train: 7.7674 loss_rec: 7.7674 acc_train: 0.5533 loss_val: 7.7137 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 04238 loss_train: 10.4354 loss_rec: 10.4354 acc_train: 0.5521 loss_val: 10.3409 acc_val: 0.5608 time: 0.0278s\n",
      "Epoch: 04239 loss_train: 3.8292 loss_rec: 3.8292 acc_train: 0.5711 loss_val: 3.8294 acc_val: 0.5767 time: 0.0258s\n",
      "Epoch: 04240 loss_train: 11.2860 loss_rec: 11.2860 acc_train: 0.4887 loss_val: 11.4797 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 04241 loss_train: 14.6681 loss_rec: 14.6681 acc_train: 0.4879 loss_val: 14.9156 acc_val: 0.4950 time: 0.0278s\n",
      "Test set results: loss= 5.9551 accuracy= 0.5452\n",
      "Epoch: 04242 loss_train: 6.6942 loss_rec: 6.6942 acc_train: 0.4882 loss_val: 6.8149 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 04243 loss_train: 9.8476 loss_rec: 9.8476 acc_train: 0.5523 loss_val: 9.7620 acc_val: 0.5621 time: 0.0258s\n",
      "Epoch: 04244 loss_train: 15.6281 loss_rec: 15.6281 acc_train: 0.5500 loss_val: 15.4471 acc_val: 0.5567 time: 0.0263s\n",
      "Epoch: 04245 loss_train: 11.4880 loss_rec: 11.4880 acc_train: 0.5505 loss_val: 11.3778 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 04246 loss_train: 1.1341 loss_rec: 1.1341 acc_train: 0.5567 loss_val: 1.1562 acc_val: 0.5500 time: 0.0275s\n",
      "Epoch: 04247 loss_train: 7.0096 loss_rec: 7.0096 acc_train: 0.4881 loss_val: 7.1351 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 04248 loss_train: 1.6671 loss_rec: 1.6671 acc_train: 0.4866 loss_val: 1.7036 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 04249 loss_train: 11.0668 loss_rec: 11.0668 acc_train: 0.5503 loss_val: 10.9632 acc_val: 0.5596 time: 0.0283s\n",
      "Epoch: 04250 loss_train: 13.6994 loss_rec: 13.6994 acc_train: 0.5481 loss_val: 13.5517 acc_val: 0.5554 time: 0.0258s\n",
      "Epoch: 04251 loss_train: 6.8047 loss_rec: 6.8047 acc_train: 0.5545 loss_val: 6.7626 acc_val: 0.5596 time: 0.0278s\n",
      "Test set results: loss= 7.5823 accuracy= 0.5450\n",
      "Epoch: 04252 loss_train: 8.5075 loss_rec: 8.5075 acc_train: 0.4879 loss_val: 8.6564 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04253 loss_train: 12.4840 loss_rec: 12.4840 acc_train: 0.4886 loss_val: 12.6966 acc_val: 0.4954 time: 0.0273s\n",
      "Epoch: 04254 loss_train: 5.0940 loss_rec: 5.0940 acc_train: 0.4877 loss_val: 5.1899 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 04255 loss_train: 10.8091 loss_rec: 10.8091 acc_train: 0.5510 loss_val: 10.7092 acc_val: 0.5596 time: 0.0258s\n",
      "Epoch: 04256 loss_train: 16.1380 loss_rec: 16.1380 acc_train: 0.5492 loss_val: 15.9484 acc_val: 0.5558 time: 0.0273s\n",
      "Epoch: 04257 loss_train: 11.6089 loss_rec: 11.6089 acc_train: 0.5507 loss_val: 11.4969 acc_val: 0.5596 time: 0.0283s\n",
      "Epoch: 04258 loss_train: 1.2978 loss_rec: 1.2978 acc_train: 0.4866 loss_val: 1.3245 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04259 loss_train: 6.2894 loss_rec: 6.2894 acc_train: 0.4882 loss_val: 6.4038 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 04260 loss_train: 0.8683 loss_rec: 0.8683 acc_train: 0.6118 loss_val: 0.8861 acc_val: 0.6050 time: 0.0273s\n",
      "Epoch: 04261 loss_train: 4.6478 loss_rec: 4.6478 acc_train: 0.5635 loss_val: 4.6373 acc_val: 0.5704 time: 0.0258s\n",
      "Test set results: loss= 1.3307 accuracy= 0.6003\n",
      "Epoch: 04262 loss_train: 1.2404 loss_rec: 1.2404 acc_train: 0.6185 loss_val: 1.2655 acc_val: 0.6167 time: 0.0278s\n",
      "Epoch: 04263 loss_train: 8.6127 loss_rec: 8.6127 acc_train: 0.4879 loss_val: 8.7634 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04264 loss_train: 6.5014 loss_rec: 6.5014 acc_train: 0.4882 loss_val: 6.6192 acc_val: 0.4963 time: 0.0278s\n",
      "Epoch: 04265 loss_train: 5.2430 loss_rec: 5.2430 acc_train: 0.5603 loss_val: 5.2235 acc_val: 0.5683 time: 0.0283s\n",
      "Epoch: 04266 loss_train: 6.9518 loss_rec: 6.9518 acc_train: 0.5545 loss_val: 6.9080 acc_val: 0.5596 time: 0.0258s\n",
      "Epoch: 04267 loss_train: 0.8807 loss_rec: 0.8807 acc_train: 0.6207 loss_val: 0.8996 acc_val: 0.6058 time: 0.0263s\n",
      "Epoch: 04268 loss_train: 8.4257 loss_rec: 8.4257 acc_train: 0.4879 loss_val: 8.5733 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04269 loss_train: 5.2631 loss_rec: 5.2631 acc_train: 0.4877 loss_val: 5.3617 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 04270 loss_train: 7.1922 loss_rec: 7.1922 acc_train: 0.5550 loss_val: 7.1455 acc_val: 0.5600 time: 0.0273s\n",
      "Epoch: 04271 loss_train: 9.5335 loss_rec: 9.5335 acc_train: 0.5495 loss_val: 9.4525 acc_val: 0.5583 time: 0.0263s\n",
      "Test set results: loss= 3.1560 accuracy= 0.5545\n",
      "Epoch: 04272 loss_train: 2.8199 loss_rec: 2.8199 acc_train: 0.5819 loss_val: 2.8341 acc_val: 0.5900 time: 0.0278s\n",
      "Epoch: 04273 loss_train: 12.1628 loss_rec: 12.1628 acc_train: 0.4886 loss_val: 12.3705 acc_val: 0.4958 time: 0.0298s\n",
      "Epoch: 04274 loss_train: 15.2804 loss_rec: 15.2804 acc_train: 0.4877 loss_val: 15.5376 acc_val: 0.4950 time: 0.0258s\n",
      "Epoch: 04275 loss_train: 7.1151 loss_rec: 7.1151 acc_train: 0.4881 loss_val: 7.2423 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 04276 loss_train: 9.5626 loss_rec: 9.5626 acc_train: 0.5495 loss_val: 9.4811 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 04277 loss_train: 15.5085 loss_rec: 15.5085 acc_train: 0.5495 loss_val: 15.3297 acc_val: 0.5567 time: 0.0263s\n",
      "Epoch: 04278 loss_train: 11.5587 loss_rec: 11.5587 acc_train: 0.5507 loss_val: 11.4475 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 04279 loss_train: 0.9882 loss_rec: 0.9882 acc_train: 0.5819 loss_val: 1.0066 acc_val: 0.5792 time: 0.0273s\n",
      "Epoch: 04280 loss_train: 8.5251 loss_rec: 8.5251 acc_train: 0.4879 loss_val: 8.6743 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 04281 loss_train: 4.7538 loss_rec: 4.7538 acc_train: 0.4870 loss_val: 4.8444 acc_val: 0.4946 time: 0.0283s\n",
      "Test set results: loss= 9.1874 accuracy= 0.5167\n",
      "Epoch: 04282 loss_train: 8.1175 loss_rec: 8.1175 acc_train: 0.5517 loss_val: 8.0591 acc_val: 0.5592 time: 0.0263s\n",
      "Epoch: 04283 loss_train: 10.8638 loss_rec: 10.8638 acc_train: 0.5503 loss_val: 10.7633 acc_val: 0.5596 time: 0.0263s\n",
      "Epoch: 04284 loss_train: 4.3137 loss_rec: 4.3137 acc_train: 0.5664 loss_val: 4.3077 acc_val: 0.5733 time: 0.0278s\n",
      "Epoch: 04285 loss_train: 10.7064 loss_rec: 10.7064 acc_train: 0.4877 loss_val: 10.8909 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04286 loss_train: 14.1580 loss_rec: 14.1580 acc_train: 0.4879 loss_val: 14.3974 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 04287 loss_train: 6.3341 loss_rec: 6.3341 acc_train: 0.4882 loss_val: 6.4492 acc_val: 0.4963 time: 0.0273s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04288 loss_train: 9.9825 loss_rec: 9.9825 acc_train: 0.5524 loss_val: 9.8947 acc_val: 0.5621 time: 0.0263s\n",
      "Epoch: 04289 loss_train: 15.6627 loss_rec: 15.6627 acc_train: 0.5495 loss_val: 15.4813 acc_val: 0.5567 time: 0.0273s\n",
      "Epoch: 04290 loss_train: 11.4870 loss_rec: 11.4870 acc_train: 0.5505 loss_val: 11.3769 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 04291 loss_train: 1.1200 loss_rec: 1.1200 acc_train: 0.5567 loss_val: 1.1417 acc_val: 0.5500 time: 0.0273s\n",
      "Test set results: loss= 6.3537 accuracy= 0.5451\n",
      "Epoch: 04292 loss_train: 7.1385 loss_rec: 7.1385 acc_train: 0.4881 loss_val: 7.2660 acc_val: 0.4963 time: 0.0273s\n",
      "Epoch: 04293 loss_train: 1.9724 loss_rec: 1.9724 acc_train: 0.4866 loss_val: 2.0158 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04294 loss_train: 11.0961 loss_rec: 11.0961 acc_train: 0.5503 loss_val: 10.9920 acc_val: 0.5596 time: 0.0263s\n",
      "Epoch: 04295 loss_train: 14.1094 loss_rec: 14.1094 acc_train: 0.5491 loss_val: 13.9547 acc_val: 0.5567 time: 0.0273s\n",
      "Epoch: 04296 loss_train: 7.5958 loss_rec: 7.5958 acc_train: 0.5543 loss_val: 7.5441 acc_val: 0.5596 time: 0.0293s\n",
      "Epoch: 04297 loss_train: 7.2090 loss_rec: 7.2090 acc_train: 0.4881 loss_val: 7.3377 acc_val: 0.4963 time: 0.0258s\n",
      "Epoch: 04298 loss_train: 10.8992 loss_rec: 10.8992 acc_train: 0.4877 loss_val: 11.0868 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04299 loss_train: 3.3500 loss_rec: 3.3500 acc_train: 0.4865 loss_val: 3.4183 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04300 loss_train: 12.4168 loss_rec: 12.4168 acc_train: 0.5488 loss_val: 12.2915 acc_val: 0.5567 time: 0.0258s\n",
      "Epoch: 04301 loss_train: 17.7735 loss_rec: 17.7735 acc_train: 0.5485 loss_val: 17.5562 acc_val: 0.5558 time: 0.0278s\n",
      "Test set results: loss= 15.0470 accuracy= 0.5096\n",
      "Epoch: 04302 loss_train: 13.2901 loss_rec: 13.2901 acc_train: 0.5481 loss_val: 13.1496 acc_val: 0.5558 time: 0.0273s\n",
      "Epoch: 04303 loss_train: 1.0464 loss_rec: 1.0464 acc_train: 0.6318 loss_val: 1.0696 acc_val: 0.6233 time: 0.0273s\n",
      "Epoch: 04304 loss_train: 16.6013 loss_rec: 16.6013 acc_train: 0.4877 loss_val: 16.8790 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 04305 loss_train: 21.3566 loss_rec: 21.3566 acc_train: 0.4875 loss_val: 21.7064 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04306 loss_train: 14.6997 loss_rec: 14.6997 acc_train: 0.4879 loss_val: 14.9479 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04307 loss_train: 1.8428 loss_rec: 1.8428 acc_train: 0.6019 loss_val: 1.8665 acc_val: 0.6017 time: 0.0263s\n",
      "Epoch: 04308 loss_train: 8.3202 loss_rec: 8.3202 acc_train: 0.5519 loss_val: 8.2589 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 04309 loss_train: 5.6606 loss_rec: 5.6606 acc_train: 0.5587 loss_val: 5.6346 acc_val: 0.5667 time: 0.0278s\n",
      "Epoch: 04310 loss_train: 5.4704 loss_rec: 5.4704 acc_train: 0.4877 loss_val: 5.5723 acc_val: 0.4950 time: 0.0258s\n",
      "Epoch: 04311 loss_train: 5.6761 loss_rec: 5.6761 acc_train: 0.4877 loss_val: 5.7812 acc_val: 0.4950 time: 0.0273s\n",
      "Test set results: loss= 4.5879 accuracy= 0.5347\n",
      "Epoch: 04312 loss_train: 4.0779 loss_rec: 4.0779 acc_train: 0.5679 loss_val: 4.0753 acc_val: 0.5742 time: 0.0273s\n",
      "Epoch: 04313 loss_train: 4.3395 loss_rec: 4.3395 acc_train: 0.5664 loss_val: 4.3333 acc_val: 0.5733 time: 0.0278s\n",
      "Epoch: 04314 loss_train: 3.7872 loss_rec: 3.7872 acc_train: 0.4865 loss_val: 3.8627 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04315 loss_train: 1.3254 loss_rec: 1.3254 acc_train: 0.4866 loss_val: 1.3529 acc_val: 0.4946 time: 0.0258s\n",
      "Epoch: 04316 loss_train: 7.7795 loss_rec: 7.7795 acc_train: 0.5533 loss_val: 7.7253 acc_val: 0.5592 time: 0.0263s\n",
      "Epoch: 04317 loss_train: 7.3765 loss_rec: 7.3765 acc_train: 0.5543 loss_val: 7.3276 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 04318 loss_train: 1.5342 loss_rec: 1.5342 acc_train: 0.4866 loss_val: 1.5673 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04319 loss_train: 1.6900 loss_rec: 1.6900 acc_train: 0.4866 loss_val: 1.7271 acc_val: 0.4946 time: 0.0293s\n",
      "Epoch: 04320 loss_train: 6.4143 loss_rec: 6.4143 acc_train: 0.5553 loss_val: 6.3774 acc_val: 0.5617 time: 0.0273s\n",
      "Epoch: 04321 loss_train: 5.2287 loss_rec: 5.2287 acc_train: 0.5603 loss_val: 5.2094 acc_val: 0.5683 time: 0.0258s\n",
      "Test set results: loss= 3.9065 accuracy= 0.5432\n",
      "Epoch: 04322 loss_train: 4.4102 loss_rec: 4.4102 acc_train: 0.4864 loss_val: 4.4956 acc_val: 0.4946 time: 0.0283s\n",
      "Epoch: 04323 loss_train: 3.2701 loss_rec: 3.2701 acc_train: 0.4865 loss_val: 3.3371 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04324 loss_train: 7.1991 loss_rec: 7.1991 acc_train: 0.5550 loss_val: 7.1524 acc_val: 0.5600 time: 0.0268s\n",
      "Epoch: 04325 loss_train: 7.9869 loss_rec: 7.9869 acc_train: 0.5533 loss_val: 7.9303 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 04326 loss_train: 0.9291 loss_rec: 0.9291 acc_train: 0.6170 loss_val: 0.9500 acc_val: 0.6092 time: 0.0273s\n",
      "Epoch: 04327 loss_train: 10.4208 loss_rec: 10.4208 acc_train: 0.4877 loss_val: 10.6009 acc_val: 0.4958 time: 0.0283s\n",
      "Epoch: 04328 loss_train: 9.3862 loss_rec: 9.3862 acc_train: 0.4879 loss_val: 9.5495 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04329 loss_train: 2.0062 loss_rec: 2.0062 acc_train: 0.5970 loss_val: 2.0289 acc_val: 0.6017 time: 0.0268s\n",
      "Epoch: 04330 loss_train: 4.2312 loss_rec: 4.2312 acc_train: 0.5663 loss_val: 4.2265 acc_val: 0.5733 time: 0.0263s\n",
      "Epoch: 04331 loss_train: 1.7209 loss_rec: 1.7209 acc_train: 0.4866 loss_val: 1.7588 acc_val: 0.4946 time: 0.0273s\n",
      "Test set results: loss= 1.6875 accuracy= 0.5907\n",
      "Epoch: 04332 loss_train: 1.5429 loss_rec: 1.5429 acc_train: 0.6102 loss_val: 1.5682 acc_val: 0.6100 time: 0.0263s\n",
      "Epoch: 04333 loss_train: 2.1873 loss_rec: 2.1873 acc_train: 0.4866 loss_val: 2.2352 acc_val: 0.4946 time: 0.0283s\n",
      "Epoch: 04334 loss_train: 3.7784 loss_rec: 3.7784 acc_train: 0.5711 loss_val: 3.7793 acc_val: 0.5767 time: 0.0273s\n",
      "Epoch: 04335 loss_train: 1.4605 loss_rec: 1.4605 acc_train: 0.6120 loss_val: 1.4860 acc_val: 0.6142 time: 0.0288s\n",
      "Epoch: 04336 loss_train: 7.8061 loss_rec: 7.8061 acc_train: 0.4879 loss_val: 7.9439 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 04337 loss_train: 5.4423 loss_rec: 5.4423 acc_train: 0.4877 loss_val: 5.5438 acc_val: 0.4950 time: 0.0258s\n",
      "Epoch: 04338 loss_train: 6.3034 loss_rec: 6.3034 acc_train: 0.5560 loss_val: 6.2678 acc_val: 0.5621 time: 0.0263s\n",
      "Epoch: 04339 loss_train: 8.1145 loss_rec: 8.1145 acc_train: 0.5517 loss_val: 8.0561 acc_val: 0.5592 time: 0.0283s\n",
      "Epoch: 04340 loss_train: 1.4625 loss_rec: 1.4625 acc_train: 0.6111 loss_val: 1.4879 acc_val: 0.6129 time: 0.0273s\n",
      "Epoch: 04341 loss_train: 12.2265 loss_rec: 12.2265 acc_train: 0.4877 loss_val: 12.4353 acc_val: 0.4958 time: 0.0273s\n",
      "Test set results: loss= 12.3482 accuracy= 0.5447\n",
      "Epoch: 04342 loss_train: 13.8137 loss_rec: 13.8137 acc_train: 0.4880 loss_val: 14.0476 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04343 loss_train: 4.4058 loss_rec: 4.4058 acc_train: 0.4864 loss_val: 4.4911 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04344 loss_train: 12.9999 loss_rec: 12.9999 acc_train: 0.5483 loss_val: 12.8644 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 04345 loss_train: 19.7924 loss_rec: 19.7924 acc_train: 0.5497 loss_val: 19.5406 acc_val: 0.5563 time: 0.0273s\n",
      "Epoch: 04346 loss_train: 16.6334 loss_rec: 16.6334 acc_train: 0.5489 loss_val: 16.4355 acc_val: 0.5563 time: 0.0273s\n",
      "Epoch: 04347 loss_train: 4.7341 loss_rec: 4.7341 acc_train: 0.5628 loss_val: 4.7222 acc_val: 0.5696 time: 0.0263s\n",
      "Epoch: 04348 loss_train: 15.4498 loss_rec: 15.4498 acc_train: 0.4877 loss_val: 15.7097 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04349 loss_train: 23.6886 loss_rec: 23.6886 acc_train: 0.4873 loss_val: 24.0725 acc_val: 0.4950 time: 0.0258s\n",
      "Epoch: 04350 loss_train: 20.1956 loss_rec: 20.1956 acc_train: 0.4875 loss_val: 20.5282 acc_val: 0.4950 time: 0.0283s\n",
      "Epoch: 04351 loss_train: 6.1978 loss_rec: 6.1978 acc_train: 0.4877 loss_val: 6.3110 acc_val: 0.4950 time: 0.0278s\n",
      "Test set results: loss= 17.1167 accuracy= 0.5098\n",
      "Epoch: 04352 loss_train: 15.1178 loss_rec: 15.1178 acc_train: 0.5500 loss_val: 14.9459 acc_val: 0.5579 time: 0.0278s\n",
      "Epoch: 04353 loss_train: 25.3330 loss_rec: 25.3330 acc_train: 0.5459 loss_val: 24.9944 acc_val: 0.5517 time: 0.0278s\n",
      "Epoch: 04354 loss_train: 25.2389 loss_rec: 25.2389 acc_train: 0.5464 loss_val: 24.9016 acc_val: 0.5517 time: 0.0263s\n",
      "Epoch: 04355 loss_train: 15.8623 loss_rec: 15.8623 acc_train: 0.5487 loss_val: 15.6778 acc_val: 0.5554 time: 0.0263s\n",
      "Epoch: 04356 loss_train: 1.2906 loss_rec: 1.2906 acc_train: 0.4866 loss_val: 1.3172 acc_val: 0.4946 time: 0.0283s\n",
      "Epoch: 04357 loss_train: 10.5720 loss_rec: 10.5720 acc_train: 0.4877 loss_val: 10.7546 acc_val: 0.4958 time: 0.0278s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04358 loss_train: 8.3085 loss_rec: 8.3085 acc_train: 0.4879 loss_val: 8.4545 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04359 loss_train: 3.6679 loss_rec: 3.6679 acc_train: 0.5715 loss_val: 3.6702 acc_val: 0.5767 time: 0.0278s\n",
      "Epoch: 04360 loss_train: 5.8130 loss_rec: 5.8130 acc_train: 0.5583 loss_val: 5.7847 acc_val: 0.5658 time: 0.0273s\n",
      "Epoch: 04361 loss_train: 0.9097 loss_rec: 0.9097 acc_train: 0.6008 loss_val: 0.9269 acc_val: 0.5929 time: 0.0263s\n",
      "Test set results: loss= 3.7659 accuracy= 0.5432\n",
      "Epoch: 04362 loss_train: 4.2534 loss_rec: 4.2534 acc_train: 0.4865 loss_val: 4.3363 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04363 loss_train: 2.4332 loss_rec: 2.4332 acc_train: 0.5887 loss_val: 2.4523 acc_val: 0.5929 time: 0.0278s\n",
      "Epoch: 04364 loss_train: 1.2296 loss_rec: 1.2296 acc_train: 0.6178 loss_val: 1.2549 acc_val: 0.6154 time: 0.0263s\n",
      "Epoch: 04365 loss_train: 6.1064 loss_rec: 6.1064 acc_train: 0.4877 loss_val: 6.2183 acc_val: 0.4950 time: 0.0268s\n",
      "Epoch: 04366 loss_train: 1.9630 loss_rec: 1.9630 acc_train: 0.4866 loss_val: 2.0063 acc_val: 0.4946 time: 0.0293s\n",
      "Epoch: 04367 loss_train: 10.1352 loss_rec: 10.1352 acc_train: 0.5519 loss_val: 10.0454 acc_val: 0.5621 time: 0.0273s\n",
      "Epoch: 04368 loss_train: 12.4021 loss_rec: 12.4021 acc_train: 0.5483 loss_val: 12.2773 acc_val: 0.5567 time: 0.0273s\n",
      "Epoch: 04369 loss_train: 5.4263 loss_rec: 5.4263 acc_train: 0.5598 loss_val: 5.4041 acc_val: 0.5683 time: 0.0273s\n",
      "Epoch: 04370 loss_train: 9.8733 loss_rec: 9.8733 acc_train: 0.4879 loss_val: 10.0447 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 04371 loss_train: 13.8825 loss_rec: 13.8825 acc_train: 0.4882 loss_val: 14.1177 acc_val: 0.4950 time: 0.0263s\n",
      "Test set results: loss= 5.9893 accuracy= 0.5452\n",
      "Epoch: 04372 loss_train: 6.7324 loss_rec: 6.7324 acc_train: 0.4882 loss_val: 6.8540 acc_val: 0.4963 time: 0.0268s\n",
      "Epoch: 04373 loss_train: 8.9218 loss_rec: 8.9218 acc_train: 0.5503 loss_val: 8.8509 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 04374 loss_train: 14.1506 loss_rec: 14.1506 acc_train: 0.5491 loss_val: 13.9956 acc_val: 0.5567 time: 0.0293s\n",
      "Epoch: 04375 loss_train: 9.7205 loss_rec: 9.7205 acc_train: 0.5530 loss_val: 9.6368 acc_val: 0.5633 time: 0.0273s\n",
      "Epoch: 04376 loss_train: 2.8343 loss_rec: 2.8343 acc_train: 0.4865 loss_val: 2.8941 acc_val: 0.4946 time: 0.0258s\n",
      "Epoch: 04377 loss_train: 4.9976 loss_rec: 4.9976 acc_train: 0.4870 loss_val: 5.0923 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04378 loss_train: 3.0619 loss_rec: 3.0619 acc_train: 0.5795 loss_val: 3.0729 acc_val: 0.5867 time: 0.0263s\n",
      "Epoch: 04379 loss_train: 2.3860 loss_rec: 2.3860 acc_train: 0.5885 loss_val: 2.4058 acc_val: 0.5925 time: 0.0273s\n",
      "Epoch: 04380 loss_train: 6.0786 loss_rec: 6.0786 acc_train: 0.4877 loss_val: 6.1901 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 04381 loss_train: 3.4443 loss_rec: 3.4443 acc_train: 0.4865 loss_val: 3.5144 acc_val: 0.4946 time: 0.0278s\n",
      "Test set results: loss= 9.2183 accuracy= 0.5157\n",
      "Epoch: 04382 loss_train: 8.1447 loss_rec: 8.1447 acc_train: 0.5513 loss_val: 8.0861 acc_val: 0.5592 time: 0.0278s\n",
      "Epoch: 04383 loss_train: 9.9898 loss_rec: 9.9898 acc_train: 0.5519 loss_val: 9.9021 acc_val: 0.5621 time: 0.0273s\n",
      "Epoch: 04384 loss_train: 2.9550 loss_rec: 2.9550 acc_train: 0.5792 loss_val: 2.9674 acc_val: 0.5867 time: 0.0273s\n",
      "Epoch: 04385 loss_train: 12.2101 loss_rec: 12.2101 acc_train: 0.4877 loss_val: 12.4187 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04386 loss_train: 15.7335 loss_rec: 15.7335 acc_train: 0.4877 loss_val: 15.9979 acc_val: 0.4950 time: 0.0263s\n",
      "Epoch: 04387 loss_train: 8.1521 loss_rec: 8.1521 acc_train: 0.4879 loss_val: 8.2955 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 04388 loss_train: 7.9571 loss_rec: 7.9571 acc_train: 0.5528 loss_val: 7.9008 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 04389 loss_train: 13.5609 loss_rec: 13.5609 acc_train: 0.5476 loss_val: 13.4158 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 04390 loss_train: 9.5271 loss_rec: 9.5271 acc_train: 0.5490 loss_val: 9.4463 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 04391 loss_train: 2.6568 loss_rec: 2.6568 acc_train: 0.4866 loss_val: 2.7133 acc_val: 0.4946 time: 0.0288s\n",
      "Test set results: loss= 4.0191 accuracy= 0.5432\n",
      "Epoch: 04392 loss_train: 4.5352 loss_rec: 4.5352 acc_train: 0.4864 loss_val: 4.6225 acc_val: 0.4946 time: 0.0258s\n",
      "Epoch: 04393 loss_train: 3.6027 loss_rec: 3.6027 acc_train: 0.5714 loss_val: 3.6053 acc_val: 0.5767 time: 0.0283s\n",
      "Epoch: 04394 loss_train: 2.8542 loss_rec: 2.8542 acc_train: 0.5820 loss_val: 2.8674 acc_val: 0.5904 time: 0.0273s\n",
      "Epoch: 04395 loss_train: 5.9023 loss_rec: 5.9023 acc_train: 0.4877 loss_val: 6.0109 acc_val: 0.4950 time: 0.0258s\n",
      "Epoch: 04396 loss_train: 3.7037 loss_rec: 3.7037 acc_train: 0.4865 loss_val: 3.7777 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04397 loss_train: 7.5402 loss_rec: 7.5402 acc_train: 0.5538 loss_val: 7.4888 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 04398 loss_train: 9.1205 loss_rec: 9.1205 acc_train: 0.5498 loss_val: 9.0455 acc_val: 0.5583 time: 0.0273s\n",
      "Epoch: 04399 loss_train: 2.0761 loss_rec: 2.0761 acc_train: 0.5956 loss_val: 2.0978 acc_val: 0.5975 time: 0.0273s\n",
      "Epoch: 04400 loss_train: 12.6760 loss_rec: 12.6760 acc_train: 0.4877 loss_val: 12.8918 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 04401 loss_train: 15.6402 loss_rec: 15.6402 acc_train: 0.4877 loss_val: 15.9031 acc_val: 0.4950 time: 0.0258s\n",
      "Test set results: loss= 6.7766 accuracy= 0.5450\n",
      "Epoch: 04402 loss_train: 7.6092 loss_rec: 7.6092 acc_train: 0.4880 loss_val: 7.7440 acc_val: 0.4963 time: 0.0263s\n",
      "Epoch: 04403 loss_train: 8.7906 loss_rec: 8.7906 acc_train: 0.5510 loss_val: 8.7212 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 04404 loss_train: 14.6713 loss_rec: 14.6713 acc_train: 0.5495 loss_val: 14.5064 acc_val: 0.5567 time: 0.0268s\n",
      "Epoch: 04405 loss_train: 10.9088 loss_rec: 10.9088 acc_train: 0.5497 loss_val: 10.8072 acc_val: 0.5596 time: 0.0283s\n",
      "Epoch: 04406 loss_train: 1.1262 loss_rec: 1.1262 acc_train: 0.5567 loss_val: 1.1481 acc_val: 0.5500 time: 0.0268s\n",
      "Epoch: 04407 loss_train: 6.6036 loss_rec: 6.6036 acc_train: 0.4877 loss_val: 6.7231 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 04408 loss_train: 1.3280 loss_rec: 1.3280 acc_train: 0.4866 loss_val: 1.3556 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 04409 loss_train: 10.0168 loss_rec: 10.0168 acc_train: 0.5519 loss_val: 9.9282 acc_val: 0.5621 time: 0.0278s\n",
      "Epoch: 04410 loss_train: 11.6393 loss_rec: 11.6393 acc_train: 0.5490 loss_val: 11.5267 acc_val: 0.5571 time: 0.0278s\n",
      "Epoch: 04411 loss_train: 4.2410 loss_rec: 4.2410 acc_train: 0.5659 loss_val: 4.2357 acc_val: 0.5733 time: 0.0263s\n",
      "Test set results: loss= 10.1647 accuracy= 0.5445\n",
      "Epoch: 04412 loss_train: 11.3822 loss_rec: 11.3822 acc_train: 0.4877 loss_val: 11.5778 acc_val: 0.4958 time: 0.0293s\n",
      "Epoch: 04413 loss_train: 15.6303 loss_rec: 15.6303 acc_train: 0.4877 loss_val: 15.8931 acc_val: 0.4950 time: 0.0258s\n",
      "Epoch: 04414 loss_train: 8.7698 loss_rec: 8.7698 acc_train: 0.4879 loss_val: 8.9232 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 04415 loss_train: 6.7753 loss_rec: 6.7753 acc_train: 0.5540 loss_val: 6.7332 acc_val: 0.5596 time: 0.0283s\n",
      "Epoch: 04416 loss_train: 11.8857 loss_rec: 11.8857 acc_train: 0.5490 loss_val: 11.7683 acc_val: 0.5571 time: 0.0273s\n",
      "Epoch: 04417 loss_train: 7.5534 loss_rec: 7.5534 acc_train: 0.5533 loss_val: 7.5016 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 04418 loss_train: 4.9173 loss_rec: 4.9173 acc_train: 0.4864 loss_val: 5.0106 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04419 loss_train: 6.7439 loss_rec: 6.7439 acc_train: 0.4877 loss_val: 6.8655 acc_val: 0.4950 time: 0.0268s\n",
      "Epoch: 04420 loss_train: 1.8759 loss_rec: 1.8759 acc_train: 0.6014 loss_val: 1.8986 acc_val: 0.6025 time: 0.0278s\n",
      "Epoch: 04421 loss_train: 2.2864 loss_rec: 2.2864 acc_train: 0.5903 loss_val: 2.3058 acc_val: 0.5925 time: 0.0273s\n",
      "Test set results: loss= 4.2465 accuracy= 0.5432\n",
      "Epoch: 04422 loss_train: 4.7882 loss_rec: 4.7882 acc_train: 0.4864 loss_val: 4.8793 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 04423 loss_train: 1.2063 loss_rec: 1.2063 acc_train: 0.5650 loss_val: 1.2304 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 04424 loss_train: 8.1017 loss_rec: 8.1017 acc_train: 0.5513 loss_val: 8.0424 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 04425 loss_train: 7.9681 loss_rec: 7.9681 acc_train: 0.5513 loss_val: 7.9106 acc_val: 0.5592 time: 0.0263s\n",
      "Epoch: 04426 loss_train: 0.9028 loss_rec: 0.9028 acc_train: 0.6015 loss_val: 0.9197 acc_val: 0.5929 time: 0.0278s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04427 loss_train: 6.4150 loss_rec: 6.4150 acc_train: 0.4877 loss_val: 6.5313 acc_val: 0.4950 time: 0.0288s\n",
      "Epoch: 04428 loss_train: 1.4173 loss_rec: 1.4173 acc_train: 0.4866 loss_val: 1.4473 acc_val: 0.4946 time: 0.0264s\n",
      "Epoch: 04429 loss_train: 10.0669 loss_rec: 10.0669 acc_train: 0.5510 loss_val: 9.9766 acc_val: 0.5600 time: 0.0272s\n",
      "Epoch: 04430 loss_train: 11.8499 loss_rec: 11.8499 acc_train: 0.5490 loss_val: 11.7323 acc_val: 0.5571 time: 0.0278s\n",
      "Epoch: 04431 loss_train: 4.6112 loss_rec: 4.6112 acc_train: 0.5631 loss_val: 4.6000 acc_val: 0.5704 time: 0.0263s\n",
      "Test set results: loss= 9.7107 accuracy= 0.5445\n",
      "Epoch: 04432 loss_train: 10.8759 loss_rec: 10.8759 acc_train: 0.4877 loss_val: 11.0631 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 04433 loss_train: 15.0791 loss_rec: 15.0791 acc_train: 0.4868 loss_val: 15.3330 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 04434 loss_train: 8.1993 loss_rec: 8.1993 acc_train: 0.4879 loss_val: 8.3432 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 04435 loss_train: 7.2644 loss_rec: 7.2644 acc_train: 0.5538 loss_val: 7.2158 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 04436 loss_train: 12.3651 loss_rec: 12.3651 acc_train: 0.5479 loss_val: 12.2388 acc_val: 0.5558 time: 0.0258s\n",
      "Epoch: 04437 loss_train: 8.0082 loss_rec: 8.0082 acc_train: 0.5513 loss_val: 7.9500 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 04438 loss_train: 4.4495 loss_rec: 4.4495 acc_train: 0.4864 loss_val: 4.5352 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04439 loss_train: 6.3389 loss_rec: 6.3389 acc_train: 0.4877 loss_val: 6.4540 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 04440 loss_train: 2.1108 loss_rec: 2.1108 acc_train: 0.5926 loss_val: 2.1315 acc_val: 0.5950 time: 0.0273s\n",
      "Epoch: 04441 loss_train: 2.2154 loss_rec: 2.2154 acc_train: 0.5915 loss_val: 2.2351 acc_val: 0.5942 time: 0.0253s\n",
      "Test set results: loss= 4.5685 accuracy= 0.5432\n",
      "Epoch: 04442 loss_train: 5.1471 loss_rec: 5.1471 acc_train: 0.4864 loss_val: 5.2437 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04443 loss_train: 1.7066 loss_rec: 1.7066 acc_train: 0.4866 loss_val: 1.7439 acc_val: 0.4946 time: 0.0288s\n",
      "Epoch: 04444 loss_train: 9.3003 loss_rec: 9.3003 acc_train: 0.5534 loss_val: 9.2215 acc_val: 0.5650 time: 0.0273s\n",
      "Epoch: 04445 loss_train: 10.7085 loss_rec: 10.7085 acc_train: 0.5498 loss_val: 10.6089 acc_val: 0.5596 time: 0.0263s\n",
      "Epoch: 04446 loss_train: 3.3014 loss_rec: 3.3014 acc_train: 0.5769 loss_val: 3.3072 acc_val: 0.5825 time: 0.0273s\n",
      "Epoch: 04447 loss_train: 12.2037 loss_rec: 12.2037 acc_train: 0.4877 loss_val: 12.4118 acc_val: 0.4958 time: 0.0268s\n",
      "Epoch: 04448 loss_train: 16.2435 loss_rec: 16.2435 acc_train: 0.4877 loss_val: 16.5154 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 04449 loss_train: 9.2529 loss_rec: 9.2529 acc_train: 0.4879 loss_val: 9.4138 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04450 loss_train: 6.3989 loss_rec: 6.3989 acc_train: 0.5552 loss_val: 6.3610 acc_val: 0.5617 time: 0.0268s\n",
      "Epoch: 04451 loss_train: 11.6179 loss_rec: 11.6179 acc_train: 0.5486 loss_val: 11.5040 acc_val: 0.5563 time: 0.0268s\n",
      "Test set results: loss= 8.4372 accuracy= 0.5174\n",
      "Epoch: 04452 loss_train: 7.4572 loss_rec: 7.4572 acc_train: 0.5533 loss_val: 7.4060 acc_val: 0.5596 time: 0.0258s\n",
      "Epoch: 04453 loss_train: 4.8065 loss_rec: 4.8065 acc_train: 0.4864 loss_val: 4.8978 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 04454 loss_train: 6.4977 loss_rec: 6.4977 acc_train: 0.4877 loss_val: 6.6152 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04455 loss_train: 2.1037 loss_rec: 2.1037 acc_train: 0.5928 loss_val: 2.1242 acc_val: 0.5950 time: 0.0273s\n",
      "Epoch: 04456 loss_train: 2.3281 loss_rec: 2.3281 acc_train: 0.5880 loss_val: 2.3465 acc_val: 0.5925 time: 0.0273s\n",
      "Epoch: 04457 loss_train: 4.9662 loss_rec: 4.9662 acc_train: 0.4864 loss_val: 5.0600 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04458 loss_train: 1.5456 loss_rec: 1.5456 acc_train: 0.4866 loss_val: 1.5789 acc_val: 0.4946 time: 0.0258s\n",
      "Epoch: 04459 loss_train: 9.0248 loss_rec: 9.0248 acc_train: 0.5489 loss_val: 8.9501 acc_val: 0.5583 time: 0.0283s\n",
      "Epoch: 04460 loss_train: 10.0726 loss_rec: 10.0726 acc_train: 0.5510 loss_val: 9.9821 acc_val: 0.5600 time: 0.0278s\n",
      "Epoch: 04461 loss_train: 2.4805 loss_rec: 2.4805 acc_train: 0.5856 loss_val: 2.4974 acc_val: 0.5904 time: 0.0278s\n",
      "Test set results: loss= 11.5761 accuracy= 0.5445\n",
      "Epoch: 04462 loss_train: 12.9524 loss_rec: 12.9524 acc_train: 0.4876 loss_val: 13.1723 acc_val: 0.4954 time: 0.0263s\n",
      "Epoch: 04463 loss_train: 16.7718 loss_rec: 16.7718 acc_train: 0.4877 loss_val: 17.0518 acc_val: 0.4950 time: 0.0268s\n",
      "Epoch: 04464 loss_train: 9.6036 loss_rec: 9.6036 acc_train: 0.4879 loss_val: 9.7702 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04465 loss_train: 6.2099 loss_rec: 6.2099 acc_train: 0.5555 loss_val: 6.1742 acc_val: 0.5621 time: 0.0273s\n",
      "Epoch: 04466 loss_train: 11.5685 loss_rec: 11.5685 acc_train: 0.5486 loss_val: 11.4552 acc_val: 0.5563 time: 0.0278s\n",
      "Epoch: 04467 loss_train: 7.5612 loss_rec: 7.5612 acc_train: 0.5527 loss_val: 7.5088 acc_val: 0.5592 time: 0.0288s\n",
      "Epoch: 04468 loss_train: 4.5319 loss_rec: 4.5319 acc_train: 0.4864 loss_val: 4.6189 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 04469 loss_train: 6.1170 loss_rec: 6.1170 acc_train: 0.4870 loss_val: 6.2286 acc_val: 0.4946 time: 0.0258s\n",
      "Epoch: 04470 loss_train: 2.4574 loss_rec: 2.4574 acc_train: 0.5862 loss_val: 2.4747 acc_val: 0.5904 time: 0.0278s\n",
      "Epoch: 04471 loss_train: 2.4705 loss_rec: 2.4705 acc_train: 0.5864 loss_val: 2.4876 acc_val: 0.5913 time: 0.0278s\n",
      "Test set results: loss= 4.5757 accuracy= 0.5432\n",
      "Epoch: 04472 loss_train: 5.1549 loss_rec: 5.1549 acc_train: 0.4864 loss_val: 5.2516 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04473 loss_train: 2.0353 loss_rec: 2.0353 acc_train: 0.4861 loss_val: 2.0798 acc_val: 0.4946 time: 0.0283s\n",
      "Epoch: 04474 loss_train: 9.1628 loss_rec: 9.1628 acc_train: 0.5534 loss_val: 9.0859 acc_val: 0.5650 time: 0.0283s\n",
      "Epoch: 04475 loss_train: 10.7943 loss_rec: 10.7943 acc_train: 0.5494 loss_val: 10.6930 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 04476 loss_train: 3.6100 loss_rec: 3.6100 acc_train: 0.5710 loss_val: 3.6116 acc_val: 0.5767 time: 0.0263s\n",
      "Epoch: 04477 loss_train: 11.6794 loss_rec: 11.6794 acc_train: 0.4877 loss_val: 11.8793 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 04478 loss_train: 15.6320 loss_rec: 15.6320 acc_train: 0.4868 loss_val: 15.8945 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04479 loss_train: 8.6250 loss_rec: 8.6250 acc_train: 0.4879 loss_val: 8.7758 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 04480 loss_train: 6.9182 loss_rec: 6.9182 acc_train: 0.5545 loss_val: 6.8734 acc_val: 0.5600 time: 0.0258s\n",
      "Epoch: 04481 loss_train: 12.1311 loss_rec: 12.1311 acc_train: 0.5479 loss_val: 12.0084 acc_val: 0.5558 time: 0.0283s\n",
      "Test set results: loss= 9.0098 accuracy= 0.5157\n",
      "Epoch: 04482 loss_train: 7.9608 loss_rec: 7.9608 acc_train: 0.5513 loss_val: 7.9030 acc_val: 0.5596 time: 0.0273s\n",
      "Epoch: 04483 loss_train: 4.2481 loss_rec: 4.2481 acc_train: 0.4865 loss_val: 4.3307 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04484 loss_train: 5.9850 loss_rec: 5.9850 acc_train: 0.4864 loss_val: 6.0947 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04485 loss_train: 2.4040 loss_rec: 2.4040 acc_train: 0.5883 loss_val: 2.4218 acc_val: 0.5929 time: 0.0268s\n",
      "Epoch: 04486 loss_train: 2.3469 loss_rec: 2.3469 acc_train: 0.5882 loss_val: 2.3654 acc_val: 0.5933 time: 0.0263s\n",
      "Epoch: 04487 loss_train: 5.2598 loss_rec: 5.2598 acc_train: 0.4864 loss_val: 5.3583 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04488 loss_train: 2.1196 loss_rec: 2.1196 acc_train: 0.4861 loss_val: 2.1659 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04489 loss_train: 9.1569 loss_rec: 9.1569 acc_train: 0.5536 loss_val: 9.0801 acc_val: 0.5642 time: 0.0288s\n",
      "Epoch: 04490 loss_train: 10.8994 loss_rec: 10.8994 acc_train: 0.5494 loss_val: 10.7967 acc_val: 0.5588 time: 0.0258s\n",
      "Epoch: 04491 loss_train: 3.8024 loss_rec: 3.8024 acc_train: 0.5702 loss_val: 3.8017 acc_val: 0.5763 time: 0.0263s\n",
      "Test set results: loss= 10.1753 accuracy= 0.5445\n",
      "Epoch: 04492 loss_train: 11.3928 loss_rec: 11.3928 acc_train: 0.4877 loss_val: 11.5882 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 04493 loss_train: 15.3230 loss_rec: 15.3230 acc_train: 0.4868 loss_val: 15.5807 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 04494 loss_train: 8.3189 loss_rec: 8.3189 acc_train: 0.4879 loss_val: 8.4648 acc_val: 0.4958 time: 0.0268s\n",
      "Epoch: 04495 loss_train: 7.1546 loss_rec: 7.1546 acc_train: 0.5538 loss_val: 7.1070 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 04496 loss_train: 12.3577 loss_rec: 12.3577 acc_train: 0.5479 loss_val: 12.2312 acc_val: 0.5558 time: 0.0278s\n",
      "Epoch: 04497 loss_train: 8.1859 loss_rec: 8.1859 acc_train: 0.5512 loss_val: 8.1248 acc_val: 0.5600 time: 0.0273s\n",
      "Epoch: 04498 loss_train: 3.9834 loss_rec: 3.9834 acc_train: 0.4860 loss_val: 4.0618 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04499 loss_train: 5.7682 loss_rec: 5.7682 acc_train: 0.4864 loss_val: 5.8745 acc_val: 0.4946 time: 0.0258s\n",
      "Epoch: 04500 loss_train: 2.5336 loss_rec: 2.5336 acc_train: 0.5848 loss_val: 2.5502 acc_val: 0.5900 time: 0.0278s\n",
      "Epoch: 04501 loss_train: 2.3675 loss_rec: 2.3675 acc_train: 0.5885 loss_val: 2.3860 acc_val: 0.5933 time: 0.0278s\n",
      "Test set results: loss= 4.7558 accuracy= 0.5421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04502 loss_train: 5.3560 loss_rec: 5.3560 acc_train: 0.4859 loss_val: 5.4561 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04503 loss_train: 2.3528 loss_rec: 2.3528 acc_train: 0.4861 loss_val: 2.4037 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04504 loss_train: 8.9889 loss_rec: 8.9889 acc_train: 0.5485 loss_val: 8.9150 acc_val: 0.5575 time: 0.0263s\n",
      "Epoch: 04505 loss_train: 10.8012 loss_rec: 10.8012 acc_train: 0.5494 loss_val: 10.7001 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 04506 loss_train: 3.8107 loss_rec: 3.8107 acc_train: 0.5702 loss_val: 3.8100 acc_val: 0.5763 time: 0.0273s\n",
      "Epoch: 04507 loss_train: 11.2549 loss_rec: 11.2549 acc_train: 0.4877 loss_val: 11.4482 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 04508 loss_train: 15.1215 loss_rec: 15.1215 acc_train: 0.4868 loss_val: 15.3761 acc_val: 0.4950 time: 0.0268s\n",
      "Epoch: 04509 loss_train: 8.0642 loss_rec: 8.0642 acc_train: 0.4879 loss_val: 8.2060 acc_val: 0.4958 time: 0.0268s\n",
      "Epoch: 04510 loss_train: 7.3943 loss_rec: 7.3943 acc_train: 0.5538 loss_val: 7.3441 acc_val: 0.5596 time: 0.0278s\n",
      "Epoch: 04511 loss_train: 12.6103 loss_rec: 12.6103 acc_train: 0.5479 loss_val: 12.4798 acc_val: 0.5558 time: 0.0293s\n",
      "Test set results: loss= 9.5910 accuracy= 0.5153\n",
      "Epoch: 04512 loss_train: 8.4731 loss_rec: 8.4731 acc_train: 0.5508 loss_val: 8.4079 acc_val: 0.5592 time: 0.0258s\n",
      "Epoch: 04513 loss_train: 3.6261 loss_rec: 3.6261 acc_train: 0.4860 loss_val: 3.6988 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 04514 loss_train: 5.4122 loss_rec: 5.4122 acc_train: 0.4859 loss_val: 5.5132 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04515 loss_train: 2.7707 loss_rec: 2.7707 acc_train: 0.5813 loss_val: 2.7846 acc_val: 0.5904 time: 0.0278s\n",
      "Epoch: 04516 loss_train: 2.4451 loss_rec: 2.4451 acc_train: 0.5866 loss_val: 2.4631 acc_val: 0.5913 time: 0.0273s\n",
      "Epoch: 04517 loss_train: 5.4929 loss_rec: 5.4929 acc_train: 0.4859 loss_val: 5.5950 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 04518 loss_train: 2.6882 loss_rec: 2.6882 acc_train: 0.4860 loss_val: 2.7452 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04519 loss_train: 8.6206 loss_rec: 8.6206 acc_train: 0.5506 loss_val: 8.5532 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 04520 loss_train: 10.4262 loss_rec: 10.4262 acc_train: 0.5501 loss_val: 10.3309 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 04521 loss_train: 3.5029 loss_rec: 3.5029 acc_train: 0.5733 loss_val: 3.5064 acc_val: 0.5792 time: 0.0273s\n",
      "Test set results: loss= 10.2202 accuracy= 0.5445\n",
      "Epoch: 04522 loss_train: 11.4432 loss_rec: 11.4432 acc_train: 0.4877 loss_val: 11.6397 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04523 loss_train: 15.1620 loss_rec: 15.1620 acc_train: 0.4868 loss_val: 15.4174 acc_val: 0.4950 time: 0.0263s\n",
      "Epoch: 04524 loss_train: 8.0236 loss_rec: 8.0236 acc_train: 0.4879 loss_val: 8.1649 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04525 loss_train: 7.4924 loss_rec: 7.4924 acc_train: 0.5528 loss_val: 7.4412 acc_val: 0.5588 time: 0.0263s\n",
      "Epoch: 04526 loss_train: 12.7724 loss_rec: 12.7724 acc_train: 0.5479 loss_val: 12.6390 acc_val: 0.5558 time: 0.0283s\n",
      "Epoch: 04527 loss_train: 8.7124 loss_rec: 8.7124 acc_train: 0.5506 loss_val: 8.6431 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 04528 loss_train: 3.2898 loss_rec: 3.2898 acc_train: 0.4860 loss_val: 3.3570 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04529 loss_train: 5.0593 loss_rec: 5.0593 acc_train: 0.4859 loss_val: 5.1547 acc_val: 0.4946 time: 0.0258s\n",
      "Epoch: 04530 loss_train: 3.0482 loss_rec: 3.0482 acc_train: 0.5791 loss_val: 3.0577 acc_val: 0.5871 time: 0.0278s\n",
      "Epoch: 04531 loss_train: 2.5724 loss_rec: 2.5724 acc_train: 0.5852 loss_val: 2.5886 acc_val: 0.5904 time: 0.0268s\n",
      "Test set results: loss= 4.9404 accuracy= 0.5421\n",
      "Epoch: 04532 loss_train: 5.5617 loss_rec: 5.5617 acc_train: 0.4859 loss_val: 5.6649 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04533 loss_train: 3.0102 loss_rec: 3.0102 acc_train: 0.4860 loss_val: 3.0727 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04534 loss_train: 8.1682 loss_rec: 8.1682 acc_train: 0.5510 loss_val: 8.1074 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 04535 loss_train: 9.9046 loss_rec: 9.9046 acc_train: 0.5515 loss_val: 9.8164 acc_val: 0.5613 time: 0.0258s\n",
      "Epoch: 04536 loss_train: 3.0065 loss_rec: 3.0065 acc_train: 0.5796 loss_val: 3.0165 acc_val: 0.5875 time: 0.0273s\n",
      "Epoch: 04537 loss_train: 11.8237 loss_rec: 11.8237 acc_train: 0.4877 loss_val: 12.0260 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04538 loss_train: 15.3603 loss_rec: 15.3603 acc_train: 0.4868 loss_val: 15.6187 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04539 loss_train: 8.0913 loss_rec: 8.0913 acc_train: 0.4869 loss_val: 8.2336 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04540 loss_train: 7.5113 loss_rec: 7.5113 acc_train: 0.5529 loss_val: 7.4594 acc_val: 0.5588 time: 0.0263s\n",
      "Epoch: 04541 loss_train: 12.8983 loss_rec: 12.8983 acc_train: 0.5477 loss_val: 12.7625 acc_val: 0.5550 time: 0.0263s\n",
      "Test set results: loss= 10.1239 accuracy= 0.5134\n",
      "Epoch: 04542 loss_train: 8.9432 loss_rec: 8.9432 acc_train: 0.5485 loss_val: 8.8699 acc_val: 0.5575 time: 0.0263s\n",
      "Epoch: 04543 loss_train: 2.9222 loss_rec: 2.9222 acc_train: 0.4860 loss_val: 2.9833 acc_val: 0.4946 time: 0.0268s\n",
      "Epoch: 04544 loss_train: 4.6872 loss_rec: 4.6872 acc_train: 0.4859 loss_val: 4.7767 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04545 loss_train: 3.3403 loss_rec: 3.3403 acc_train: 0.5752 loss_val: 3.3454 acc_val: 0.5808 time: 0.0258s\n",
      "Epoch: 04546 loss_train: 2.7412 loss_rec: 2.7412 acc_train: 0.5822 loss_val: 2.7552 acc_val: 0.5904 time: 0.0278s\n",
      "Epoch: 04547 loss_train: 5.5913 loss_rec: 5.5913 acc_train: 0.4859 loss_val: 5.6950 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04548 loss_train: 3.2716 loss_rec: 3.2716 acc_train: 0.4860 loss_val: 3.3386 acc_val: 0.4946 time: 0.0258s\n",
      "Epoch: 04549 loss_train: 7.7461 loss_rec: 7.7461 acc_train: 0.5524 loss_val: 7.6914 acc_val: 0.5588 time: 0.0293s\n",
      "Epoch: 04550 loss_train: 9.3685 loss_rec: 9.3685 acc_train: 0.5530 loss_val: 9.2887 acc_val: 0.5642 time: 0.0273s\n",
      "Epoch: 04551 loss_train: 2.4742 loss_rec: 2.4742 acc_train: 0.5863 loss_val: 2.4915 acc_val: 0.5913 time: 0.0258s\n",
      "Test set results: loss= 10.8388 accuracy= 0.5445\n",
      "Epoch: 04552 loss_train: 12.1315 loss_rec: 12.1315 acc_train: 0.4877 loss_val: 12.3388 acc_val: 0.4958 time: 0.0283s\n",
      "Epoch: 04553 loss_train: 15.3715 loss_rec: 15.3715 acc_train: 0.4868 loss_val: 15.6302 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04554 loss_train: 7.8726 loss_rec: 7.8726 acc_train: 0.4869 loss_val: 8.0115 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04555 loss_train: 7.8566 loss_rec: 7.8566 acc_train: 0.5524 loss_val: 7.8003 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 04556 loss_train: 13.4026 loss_rec: 13.4026 acc_train: 0.5472 loss_val: 13.2583 acc_val: 0.5546 time: 0.0258s\n",
      "Epoch: 04557 loss_train: 9.6001 loss_rec: 9.6001 acc_train: 0.5514 loss_val: 9.5167 acc_val: 0.5613 time: 0.0283s\n",
      "Epoch: 04558 loss_train: 2.0829 loss_rec: 2.0829 acc_train: 0.4861 loss_val: 2.1286 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04559 loss_train: 4.2031 loss_rec: 4.2031 acc_train: 0.4860 loss_val: 4.2852 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04560 loss_train: 3.4141 loss_rec: 3.4141 acc_train: 0.5739 loss_val: 3.4185 acc_val: 0.5800 time: 0.0273s\n",
      "Epoch: 04561 loss_train: 2.5313 loss_rec: 2.5313 acc_train: 0.5853 loss_val: 2.5481 acc_val: 0.5904 time: 0.0273s\n",
      "Test set results: loss= 5.3134 accuracy= 0.5421\n",
      "Epoch: 04562 loss_train: 5.9777 loss_rec: 5.9777 acc_train: 0.4859 loss_val: 6.0875 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 04563 loss_train: 3.8173 loss_rec: 3.8173 acc_train: 0.4860 loss_val: 3.8932 acc_val: 0.4946 time: 0.0283s\n",
      "Epoch: 04564 loss_train: 7.1594 loss_rec: 7.1594 acc_train: 0.5534 loss_val: 7.1120 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 04565 loss_train: 8.7410 loss_rec: 8.7410 acc_train: 0.5499 loss_val: 8.6713 acc_val: 0.5588 time: 0.0278s\n",
      "Epoch: 04566 loss_train: 1.9870 loss_rec: 1.9870 acc_train: 0.5950 loss_val: 2.0087 acc_val: 0.5975 time: 0.0263s\n",
      "Epoch: 04567 loss_train: 12.1829 loss_rec: 12.1829 acc_train: 0.4877 loss_val: 12.3910 acc_val: 0.4958 time: 0.0283s\n",
      "Epoch: 04568 loss_train: 14.9220 loss_rec: 14.9220 acc_train: 0.4868 loss_val: 15.1737 acc_val: 0.4950 time: 0.0263s\n",
      "Epoch: 04569 loss_train: 6.9777 loss_rec: 6.9777 acc_train: 0.4865 loss_val: 7.1029 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04570 loss_train: 8.9918 loss_rec: 8.9918 acc_train: 0.5485 loss_val: 8.9178 acc_val: 0.5575 time: 0.0273s\n",
      "Epoch: 04571 loss_train: 14.7829 loss_rec: 14.7829 acc_train: 0.5496 loss_val: 14.6150 acc_val: 0.5571 time: 0.0263s\n",
      "Test set results: loss= 12.6629 accuracy= 0.5114\n",
      "Epoch: 04572 loss_train: 11.1847 loss_rec: 11.1847 acc_train: 0.5495 loss_val: 11.0779 acc_val: 0.5588 time: 0.0273s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04573 loss_train: 0.8628 loss_rec: 0.8628 acc_train: 0.6036 loss_val: 0.8799 acc_val: 0.6025 time: 0.0278s\n",
      "Epoch: 04574 loss_train: 10.9922 loss_rec: 10.9922 acc_train: 0.4872 loss_val: 11.1815 acc_val: 0.4958 time: 0.0259s\n",
      "Epoch: 04575 loss_train: 10.4384 loss_rec: 10.4384 acc_train: 0.4872 loss_val: 10.6188 acc_val: 0.4958 time: 0.0277s\n",
      "Epoch: 04576 loss_train: 0.9106 loss_rec: 0.9106 acc_train: 0.6177 loss_val: 0.9308 acc_val: 0.6079 time: 0.0263s\n",
      "Epoch: 04577 loss_train: 6.4372 loss_rec: 6.4372 acc_train: 0.5543 loss_val: 6.3987 acc_val: 0.5596 time: 0.0288s\n",
      "Epoch: 04578 loss_train: 4.0765 loss_rec: 4.0765 acc_train: 0.5672 loss_val: 4.0725 acc_val: 0.5733 time: 0.0258s\n",
      "Epoch: 04579 loss_train: 6.3060 loss_rec: 6.3060 acc_train: 0.4859 loss_val: 6.4209 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 04580 loss_train: 6.1363 loss_rec: 6.1363 acc_train: 0.4859 loss_val: 6.2484 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04581 loss_train: 3.5366 loss_rec: 3.5366 acc_train: 0.5716 loss_val: 3.5392 acc_val: 0.5767 time: 0.0278s\n",
      "Test set results: loss= 4.7108 accuracy= 0.5311\n",
      "Epoch: 04582 loss_train: 4.1847 loss_rec: 4.1847 acc_train: 0.5655 loss_val: 4.1792 acc_val: 0.5725 time: 0.0263s\n",
      "Epoch: 04583 loss_train: 3.1139 loss_rec: 3.1139 acc_train: 0.4860 loss_val: 3.1782 acc_val: 0.4946 time: 0.0268s\n",
      "Epoch: 04584 loss_train: 0.8858 loss_rec: 0.8858 acc_train: 0.6033 loss_val: 0.9025 acc_val: 0.5963 time: 0.0263s\n",
      "Epoch: 04585 loss_train: 3.5947 loss_rec: 3.5947 acc_train: 0.5706 loss_val: 3.5965 acc_val: 0.5758 time: 0.0283s\n",
      "Epoch: 04586 loss_train: 0.8590 loss_rec: 0.8590 acc_train: 0.6078 loss_val: 0.8760 acc_val: 0.6067 time: 0.0273s\n",
      "Epoch: 04587 loss_train: 3.5529 loss_rec: 3.5529 acc_train: 0.4860 loss_val: 3.6245 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04588 loss_train: 2.9263 loss_rec: 2.9263 acc_train: 0.5794 loss_val: 2.9375 acc_val: 0.5875 time: 0.0263s\n",
      "Epoch: 04589 loss_train: 1.4896 loss_rec: 1.4896 acc_train: 0.6096 loss_val: 1.5140 acc_val: 0.6100 time: 0.0283s\n",
      "Epoch: 04590 loss_train: 6.5968 loss_rec: 6.5968 acc_train: 0.4859 loss_val: 6.7160 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04591 loss_train: 3.7122 loss_rec: 3.7122 acc_train: 0.4860 loss_val: 3.7863 acc_val: 0.4946 time: 0.0273s\n",
      "Test set results: loss= 8.8329 accuracy= 0.5159\n",
      "Epoch: 04592 loss_train: 7.8051 loss_rec: 7.8051 acc_train: 0.5524 loss_val: 7.7493 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 04593 loss_train: 9.8817 loss_rec: 9.8817 acc_train: 0.5515 loss_val: 9.7937 acc_val: 0.5613 time: 0.0278s\n",
      "Epoch: 04594 loss_train: 3.3208 loss_rec: 3.3208 acc_train: 0.5754 loss_val: 3.3260 acc_val: 0.5800 time: 0.0263s\n",
      "Epoch: 04595 loss_train: 11.1547 loss_rec: 11.1547 acc_train: 0.4872 loss_val: 11.3465 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04596 loss_train: 14.5308 loss_rec: 14.5308 acc_train: 0.4868 loss_val: 14.7763 acc_val: 0.4950 time: 0.0298s\n",
      "Epoch: 04597 loss_train: 7.1972 loss_rec: 7.1972 acc_train: 0.4864 loss_val: 7.3256 acc_val: 0.4946 time: 0.0268s\n",
      "Epoch: 04598 loss_train: 8.2874 loss_rec: 8.2874 acc_train: 0.5508 loss_val: 8.2247 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 04599 loss_train: 13.6774 loss_rec: 13.6774 acc_train: 0.5487 loss_val: 13.5280 acc_val: 0.5558 time: 0.0273s\n",
      "Epoch: 04600 loss_train: 9.7557 loss_rec: 9.7557 acc_train: 0.5515 loss_val: 9.6695 acc_val: 0.5613 time: 0.0263s\n",
      "Epoch: 04601 loss_train: 1.9838 loss_rec: 1.9838 acc_train: 0.4861 loss_val: 2.0275 acc_val: 0.4946 time: 0.0273s\n",
      "Test set results: loss= 3.8372 accuracy= 0.5422\n",
      "Epoch: 04602 loss_train: 4.3312 loss_rec: 4.3312 acc_train: 0.4859 loss_val: 4.4152 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 04603 loss_train: 3.1036 loss_rec: 3.1036 acc_train: 0.5781 loss_val: 3.1120 acc_val: 0.5833 time: 0.0293s\n",
      "Epoch: 04604 loss_train: 2.2112 loss_rec: 2.2112 acc_train: 0.5912 loss_val: 2.2309 acc_val: 0.5933 time: 0.0273s\n",
      "Epoch: 04605 loss_train: 6.1023 loss_rec: 6.1023 acc_train: 0.4859 loss_val: 6.2139 acc_val: 0.4946 time: 0.0258s\n",
      "Epoch: 04606 loss_train: 3.7129 loss_rec: 3.7129 acc_train: 0.4860 loss_val: 3.7871 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04607 loss_train: 7.3670 loss_rec: 7.3670 acc_train: 0.5534 loss_val: 7.3167 acc_val: 0.5588 time: 0.0263s\n",
      "Epoch: 04608 loss_train: 9.0969 loss_rec: 9.0969 acc_train: 0.5486 loss_val: 9.0209 acc_val: 0.5575 time: 0.0273s\n",
      "Epoch: 04609 loss_train: 2.3998 loss_rec: 2.3998 acc_train: 0.5879 loss_val: 2.4178 acc_val: 0.5921 time: 0.0273s\n",
      "Epoch: 04610 loss_train: 11.9246 loss_rec: 11.9246 acc_train: 0.4872 loss_val: 12.1286 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 04611 loss_train: 14.9823 loss_rec: 14.9823 acc_train: 0.4863 loss_val: 15.2349 acc_val: 0.4950 time: 0.0278s\n",
      "Test set results: loss= 6.5771 accuracy= 0.5434\n",
      "Epoch: 04612 loss_train: 7.3857 loss_rec: 7.3857 acc_train: 0.4870 loss_val: 7.5170 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 04613 loss_train: 8.3029 loss_rec: 8.3029 acc_train: 0.5508 loss_val: 8.2400 acc_val: 0.5592 time: 0.0278s\n",
      "Epoch: 04614 loss_train: 13.8870 loss_rec: 13.8870 acc_train: 0.5494 loss_val: 13.7341 acc_val: 0.5567 time: 0.0274s\n",
      "Epoch: 04615 loss_train: 10.1691 loss_rec: 10.1691 acc_train: 0.5506 loss_val: 10.0772 acc_val: 0.5592 time: 0.0263s\n",
      "Epoch: 04616 loss_train: 1.4149 loss_rec: 1.4149 acc_train: 0.4861 loss_val: 1.4450 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 04617 loss_train: 4.9523 loss_rec: 4.9523 acc_train: 0.4859 loss_val: 5.0462 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04618 loss_train: 1.7282 loss_rec: 1.7282 acc_train: 0.6029 loss_val: 1.7517 acc_val: 0.6033 time: 0.0288s\n",
      "Epoch: 04619 loss_train: 1.2570 loss_rec: 1.2570 acc_train: 0.6185 loss_val: 1.2816 acc_val: 0.6183 time: 0.0273s\n",
      "Epoch: 04620 loss_train: 5.0872 loss_rec: 5.0872 acc_train: 0.4859 loss_val: 5.1832 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04621 loss_train: 0.9498 loss_rec: 0.9498 acc_train: 0.5760 loss_val: 0.9674 acc_val: 0.5733 time: 0.0263s\n",
      "Test set results: loss= 7.0527 accuracy= 0.5201\n",
      "Epoch: 04622 loss_train: 6.2406 loss_rec: 6.2406 acc_train: 0.5553 loss_val: 6.2047 acc_val: 0.5613 time: 0.0278s\n",
      "Epoch: 04623 loss_train: 4.4537 loss_rec: 4.4537 acc_train: 0.5632 loss_val: 4.4444 acc_val: 0.5696 time: 0.0278s\n",
      "Epoch: 04624 loss_train: 5.3298 loss_rec: 5.3298 acc_train: 0.4859 loss_val: 5.4295 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 04625 loss_train: 4.7186 loss_rec: 4.7186 acc_train: 0.4859 loss_val: 4.8088 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04626 loss_train: 5.0497 loss_rec: 5.0497 acc_train: 0.5607 loss_val: 5.0314 acc_val: 0.5683 time: 0.0298s\n",
      "Epoch: 04627 loss_train: 5.6619 loss_rec: 5.6619 acc_train: 0.5574 loss_val: 5.6341 acc_val: 0.5650 time: 0.0253s\n",
      "Epoch: 04628 loss_train: 1.7524 loss_rec: 1.7524 acc_train: 0.4861 loss_val: 1.7910 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04629 loss_train: 0.8690 loss_rec: 0.8690 acc_train: 0.6047 loss_val: 0.8858 acc_val: 0.6025 time: 0.0273s\n",
      "Epoch: 04630 loss_train: 2.1277 loss_rec: 2.1277 acc_train: 0.5920 loss_val: 2.1481 acc_val: 0.5950 time: 0.0258s\n",
      "Epoch: 04631 loss_train: 2.6156 loss_rec: 2.6156 acc_train: 0.4860 loss_val: 2.6713 acc_val: 0.4946 time: 0.0278s\n",
      "Test set results: loss= 2.5622 accuracy= 0.5637\n",
      "Epoch: 04632 loss_train: 2.2988 loss_rec: 2.2988 acc_train: 0.5892 loss_val: 2.3177 acc_val: 0.5917 time: 0.0263s\n",
      "Epoch: 04633 loss_train: 0.8690 loss_rec: 0.8690 acc_train: 0.6029 loss_val: 0.8857 acc_val: 0.5958 time: 0.0263s\n",
      "Epoch: 04634 loss_train: 1.5312 loss_rec: 1.5312 acc_train: 0.4861 loss_val: 1.5645 acc_val: 0.4946 time: 0.0293s\n",
      "Epoch: 04635 loss_train: 4.9715 loss_rec: 4.9715 acc_train: 0.5607 loss_val: 4.9543 acc_val: 0.5683 time: 0.0273s\n",
      "Epoch: 04636 loss_train: 2.9779 loss_rec: 2.9779 acc_train: 0.5792 loss_val: 2.9881 acc_val: 0.5867 time: 0.0273s\n",
      "Epoch: 04637 loss_train: 6.8079 loss_rec: 6.8079 acc_train: 0.4858 loss_val: 6.9304 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04638 loss_train: 5.9483 loss_rec: 5.9483 acc_train: 0.4859 loss_val: 6.0576 acc_val: 0.4946 time: 0.0258s\n",
      "Epoch: 04639 loss_train: 4.1763 loss_rec: 4.1763 acc_train: 0.5655 loss_val: 4.1705 acc_val: 0.5725 time: 0.0263s\n",
      "Epoch: 04640 loss_train: 5.1251 loss_rec: 5.1251 acc_train: 0.5594 loss_val: 5.1055 acc_val: 0.5679 time: 0.0278s\n",
      "Epoch: 04641 loss_train: 1.9517 loss_rec: 1.9517 acc_train: 0.4861 loss_val: 1.9948 acc_val: 0.4946 time: 0.0278s\n",
      "Test set results: loss= 0.8810 accuracy= 0.6174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04642 loss_train: 0.8869 loss_rec: 0.8869 acc_train: 0.6191 loss_val: 0.9062 acc_val: 0.6083 time: 0.0288s\n",
      "Epoch: 04643 loss_train: 0.8794 loss_rec: 0.8794 acc_train: 0.6200 loss_val: 0.8984 acc_val: 0.6083 time: 0.0263s\n",
      "Epoch: 04644 loss_train: 1.6614 loss_rec: 1.6614 acc_train: 0.4861 loss_val: 1.6977 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04645 loss_train: 4.7048 loss_rec: 4.7048 acc_train: 0.5623 loss_val: 4.6916 acc_val: 0.5688 time: 0.0278s\n",
      "Epoch: 04646 loss_train: 2.6432 loss_rec: 2.6432 acc_train: 0.5835 loss_val: 2.6580 acc_val: 0.5892 time: 0.0273s\n",
      "Epoch: 04647 loss_train: 7.1094 loss_rec: 7.1094 acc_train: 0.4858 loss_val: 7.2364 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 04648 loss_train: 6.1430 loss_rec: 6.1430 acc_train: 0.4859 loss_val: 6.2552 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04649 loss_train: 4.1254 loss_rec: 4.1254 acc_train: 0.5655 loss_val: 4.1202 acc_val: 0.5725 time: 0.0263s\n",
      "Epoch: 04650 loss_train: 5.1631 loss_rec: 5.1631 acc_train: 0.5587 loss_val: 5.1426 acc_val: 0.5671 time: 0.0278s\n",
      "Epoch: 04651 loss_train: 1.8260 loss_rec: 1.8260 acc_train: 0.4861 loss_val: 1.8662 acc_val: 0.4946 time: 0.0288s\n",
      "Test set results: loss= 0.8819 accuracy= 0.6175\n",
      "Epoch: 04652 loss_train: 0.8869 loss_rec: 0.8869 acc_train: 0.6192 loss_val: 0.9061 acc_val: 0.6083 time: 0.0273s\n",
      "Epoch: 04653 loss_train: 0.8583 loss_rec: 0.8583 acc_train: 0.6219 loss_val: 0.8762 acc_val: 0.6083 time: 0.0273s\n",
      "Epoch: 04654 loss_train: 1.1672 loss_rec: 1.1672 acc_train: 0.5641 loss_val: 1.1903 acc_val: 0.5575 time: 0.0263s\n",
      "Epoch: 04655 loss_train: 3.8706 loss_rec: 3.8706 acc_train: 0.5677 loss_val: 3.8685 acc_val: 0.5733 time: 0.0258s\n",
      "Epoch: 04656 loss_train: 1.1476 loss_rec: 1.1476 acc_train: 0.6214 loss_val: 1.1711 acc_val: 0.6196 time: 0.0278s\n",
      "Epoch: 04657 loss_train: 7.3801 loss_rec: 7.3801 acc_train: 0.4857 loss_val: 7.5112 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04658 loss_train: 4.6756 loss_rec: 4.6756 acc_train: 0.4859 loss_val: 4.7649 acc_val: 0.4946 time: 0.0288s\n",
      "Epoch: 04659 loss_train: 6.7912 loss_rec: 6.7912 acc_train: 0.5541 loss_val: 6.7474 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 04660 loss_train: 8.8020 loss_rec: 8.8020 acc_train: 0.5540 loss_val: 8.7300 acc_val: 0.5654 time: 0.0258s\n",
      "Epoch: 04661 loss_train: 2.3837 loss_rec: 2.3837 acc_train: 0.5882 loss_val: 2.4011 acc_val: 0.5921 time: 0.0263s\n",
      "Test set results: loss= 10.4320 accuracy= 0.5436\n",
      "Epoch: 04662 loss_train: 11.6772 loss_rec: 11.6772 acc_train: 0.4872 loss_val: 11.8771 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 04663 loss_train: 14.4949 loss_rec: 14.4949 acc_train: 0.4863 loss_val: 14.7395 acc_val: 0.4950 time: 0.0269s\n",
      "Epoch: 04664 loss_train: 6.6726 loss_rec: 6.6726 acc_train: 0.4858 loss_val: 6.7927 acc_val: 0.4946 time: 0.0263s\n",
      "Epoch: 04665 loss_train: 9.1409 loss_rec: 9.1409 acc_train: 0.5530 loss_val: 9.0635 acc_val: 0.5642 time: 0.0278s\n",
      "Epoch: 04666 loss_train: 14.8331 loss_rec: 14.8331 acc_train: 0.5491 loss_val: 14.6629 acc_val: 0.5558 time: 0.0293s\n",
      "Epoch: 04667 loss_train: 11.1749 loss_rec: 11.1749 acc_train: 0.5493 loss_val: 11.0667 acc_val: 0.5567 time: 0.0318s\n",
      "Epoch: 04668 loss_train: 0.8574 loss_rec: 0.8574 acc_train: 0.6038 loss_val: 0.8739 acc_val: 0.6017 time: 0.0363s\n",
      "Epoch: 04669 loss_train: 10.7951 loss_rec: 10.7951 acc_train: 0.4872 loss_val: 10.9810 acc_val: 0.4958 time: 0.0422s\n",
      "Epoch: 04670 loss_train: 10.0862 loss_rec: 10.0862 acc_train: 0.4872 loss_val: 10.2607 acc_val: 0.4958 time: 0.0353s\n",
      "Epoch: 04671 loss_train: 1.0473 loss_rec: 1.0473 acc_train: 0.6273 loss_val: 1.0695 acc_val: 0.6188 time: 0.0387s\n",
      "Test set results: loss= 5.8910 accuracy= 0.5232\n",
      "Epoch: 04672 loss_train: 5.2188 loss_rec: 5.2188 acc_train: 0.5581 loss_val: 5.1970 acc_val: 0.5658 time: 0.0328s\n",
      "Epoch: 04673 loss_train: 1.7853 loss_rec: 1.7853 acc_train: 0.6011 loss_val: 1.8074 acc_val: 0.6008 time: 0.0278s\n",
      "Epoch: 04674 loss_train: 8.8290 loss_rec: 8.8290 acc_train: 0.4869 loss_val: 8.9832 acc_val: 0.4946 time: 0.0253s\n",
      "Epoch: 04675 loss_train: 8.3752 loss_rec: 8.3752 acc_train: 0.4869 loss_val: 8.5221 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04676 loss_train: 1.9384 loss_rec: 1.9384 acc_train: 0.5965 loss_val: 1.9594 acc_val: 0.6013 time: 0.0258s\n",
      "Epoch: 04677 loss_train: 3.7172 loss_rec: 3.7172 acc_train: 0.5703 loss_val: 3.7168 acc_val: 0.5758 time: 0.0279s\n",
      "Epoch: 04678 loss_train: 2.2204 loss_rec: 2.2204 acc_train: 0.4857 loss_val: 2.2688 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04679 loss_train: 1.4262 loss_rec: 1.4262 acc_train: 0.6098 loss_val: 1.4500 acc_val: 0.6125 time: 0.0273s\n",
      "Epoch: 04680 loss_train: 1.5172 loss_rec: 1.5172 acc_train: 0.4857 loss_val: 1.5499 acc_val: 0.4938 time: 0.0263s\n",
      "Epoch: 04681 loss_train: 3.6188 loss_rec: 3.6188 acc_train: 0.5707 loss_val: 3.6193 acc_val: 0.5763 time: 0.0308s\n",
      "Test set results: loss= 1.1959 accuracy= 0.6063\n",
      "Epoch: 04682 loss_train: 1.1246 loss_rec: 1.1246 acc_train: 0.6256 loss_val: 1.1477 acc_val: 0.6225 time: 0.0273s\n",
      "Epoch: 04683 loss_train: 7.0672 loss_rec: 7.0672 acc_train: 0.4858 loss_val: 7.1934 acc_val: 0.4946 time: 0.0278s\n",
      "Epoch: 04684 loss_train: 4.0554 loss_rec: 4.0554 acc_train: 0.4856 loss_val: 4.1350 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04685 loss_train: 7.5531 loss_rec: 7.5531 acc_train: 0.5524 loss_val: 7.4997 acc_val: 0.5583 time: 0.0259s\n",
      "Epoch: 04686 loss_train: 9.7369 loss_rec: 9.7369 acc_train: 0.5504 loss_val: 9.6500 acc_val: 0.5600 time: 0.0263s\n",
      "Epoch: 04687 loss_train: 3.3114 loss_rec: 3.3114 acc_train: 0.5742 loss_val: 3.3158 acc_val: 0.5800 time: 0.0278s\n",
      "Epoch: 04688 loss_train: 11.0065 loss_rec: 11.0065 acc_train: 0.4872 loss_val: 11.1957 acc_val: 0.4958 time: 0.0293s\n",
      "Epoch: 04689 loss_train: 14.2739 loss_rec: 14.2739 acc_train: 0.4865 loss_val: 14.5151 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04690 loss_train: 6.8994 loss_rec: 6.8994 acc_train: 0.4858 loss_val: 7.0231 acc_val: 0.4946 time: 0.0273s\n",
      "Epoch: 04691 loss_train: 8.5495 loss_rec: 8.5495 acc_train: 0.5497 loss_val: 8.4814 acc_val: 0.5588 time: 0.0263s\n",
      "Test set results: loss= 15.7798 accuracy= 0.5084\n",
      "Epoch: 04692 loss_train: 13.9373 loss_rec: 13.9373 acc_train: 0.5490 loss_val: 13.7819 acc_val: 0.5554 time: 0.0278s\n",
      "Epoch: 04693 loss_train: 10.0716 loss_rec: 10.0716 acc_train: 0.5501 loss_val: 9.9800 acc_val: 0.5588 time: 0.0268s\n",
      "Epoch: 04694 loss_train: 1.6179 loss_rec: 1.6179 acc_train: 0.4857 loss_val: 1.6532 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04695 loss_train: 4.6091 loss_rec: 4.6091 acc_train: 0.4855 loss_val: 4.6975 acc_val: 0.4938 time: 0.0298s\n",
      "Epoch: 04696 loss_train: 2.3586 loss_rec: 2.3586 acc_train: 0.5879 loss_val: 2.3762 acc_val: 0.5921 time: 0.0273s\n",
      "Epoch: 04697 loss_train: 1.5115 loss_rec: 1.5115 acc_train: 0.6089 loss_val: 1.5350 acc_val: 0.6092 time: 0.0259s\n",
      "Epoch: 04698 loss_train: 5.9359 loss_rec: 5.9359 acc_train: 0.4855 loss_val: 6.0448 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04699 loss_train: 2.5589 loss_rec: 2.5589 acc_train: 0.4856 loss_val: 2.6136 acc_val: 0.4938 time: 0.0258s\n",
      "Epoch: 04700 loss_train: 9.0063 loss_rec: 9.0063 acc_train: 0.5530 loss_val: 8.9306 acc_val: 0.5642 time: 0.0273s\n",
      "Epoch: 04701 loss_train: 11.1790 loss_rec: 11.1790 acc_train: 0.5493 loss_val: 11.0708 acc_val: 0.5567 time: 0.0278s\n",
      "Test set results: loss= 5.2054 accuracy= 0.5280\n",
      "Epoch: 04702 loss_train: 4.6167 loss_rec: 4.6167 acc_train: 0.5623 loss_val: 4.6040 acc_val: 0.5688 time: 0.0283s\n",
      "Epoch: 04703 loss_train: 9.9085 loss_rec: 9.9085 acc_train: 0.4868 loss_val: 10.0802 acc_val: 0.4946 time: 0.0253s\n",
      "Epoch: 04704 loss_train: 13.5977 loss_rec: 13.5977 acc_train: 0.4865 loss_val: 13.8281 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 04705 loss_train: 6.6265 loss_rec: 6.6265 acc_train: 0.4855 loss_val: 6.7461 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04706 loss_train: 8.4353 loss_rec: 8.4353 acc_train: 0.5497 loss_val: 8.3691 acc_val: 0.5588 time: 0.0278s\n",
      "Epoch: 04707 loss_train: 13.5293 loss_rec: 13.5293 acc_train: 0.5495 loss_val: 13.3811 acc_val: 0.5567 time: 0.0258s\n",
      "Epoch: 04708 loss_train: 9.4225 loss_rec: 9.4225 acc_train: 0.5520 loss_val: 9.3405 acc_val: 0.5625 time: 0.0278s\n",
      "Epoch: 04709 loss_train: 2.4642 loss_rec: 2.4642 acc_train: 0.4856 loss_val: 2.5171 acc_val: 0.4938 time: 0.0263s\n",
      "Epoch: 04710 loss_train: 4.5606 loss_rec: 4.5606 acc_train: 0.4855 loss_val: 4.6483 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04711 loss_train: 3.0448 loss_rec: 3.0448 acc_train: 0.5783 loss_val: 3.0532 acc_val: 0.5842 time: 0.0298s\n",
      "Test set results: loss= 2.5966 accuracy= 0.5626\n",
      "Epoch: 04712 loss_train: 2.3279 loss_rec: 2.3279 acc_train: 0.5880 loss_val: 2.3458 acc_val: 0.5925 time: 0.0258s\n",
      "Epoch: 04713 loss_train: 5.8268 loss_rec: 5.8268 acc_train: 0.4855 loss_val: 5.9341 acc_val: 0.4938 time: 0.0268s\n",
      "Epoch: 04714 loss_train: 3.4221 loss_rec: 3.4221 acc_train: 0.4856 loss_val: 3.4915 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04715 loss_train: 7.5806 loss_rec: 7.5806 acc_train: 0.5524 loss_val: 7.5271 acc_val: 0.5583 time: 0.0278s\n",
      "Epoch: 04716 loss_train: 9.3023 loss_rec: 9.3023 acc_train: 0.5531 loss_val: 9.2223 acc_val: 0.5642 time: 0.0273s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04717 loss_train: 2.5909 loss_rec: 2.5909 acc_train: 0.5837 loss_val: 2.6059 acc_val: 0.5892 time: 0.0278s\n",
      "Epoch: 04718 loss_train: 11.7610 loss_rec: 11.7610 acc_train: 0.4872 loss_val: 11.9625 acc_val: 0.4958 time: 0.0288s\n",
      "Epoch: 04719 loss_train: 14.9802 loss_rec: 14.9802 acc_train: 0.4863 loss_val: 15.2326 acc_val: 0.4950 time: 0.0253s\n",
      "Epoch: 04720 loss_train: 7.6279 loss_rec: 7.6279 acc_train: 0.4852 loss_val: 7.7630 acc_val: 0.4933 time: 0.0278s\n",
      "Epoch: 04721 loss_train: 7.8228 loss_rec: 7.8228 acc_train: 0.5506 loss_val: 7.7658 acc_val: 0.5592 time: 0.0273s\n",
      "Test set results: loss= 14.9849 accuracy= 0.5090\n",
      "Epoch: 04722 loss_train: 13.2351 loss_rec: 13.2351 acc_train: 0.5487 loss_val: 13.0922 acc_val: 0.5558 time: 0.0278s\n",
      "Epoch: 04723 loss_train: 9.4595 loss_rec: 9.4595 acc_train: 0.5515 loss_val: 9.3772 acc_val: 0.5613 time: 0.0273s\n",
      "Epoch: 04724 loss_train: 1.9360 loss_rec: 1.9360 acc_train: 0.4857 loss_val: 1.9787 acc_val: 0.4938 time: 0.0258s\n",
      "Epoch: 04725 loss_train: 4.0155 loss_rec: 4.0155 acc_train: 0.4855 loss_val: 4.0945 acc_val: 0.4938 time: 0.0303s\n",
      "Epoch: 04726 loss_train: 3.4791 loss_rec: 3.4791 acc_train: 0.5712 loss_val: 3.4816 acc_val: 0.5758 time: 0.0273s\n",
      "Epoch: 04727 loss_train: 2.5489 loss_rec: 2.5489 acc_train: 0.5844 loss_val: 2.5644 acc_val: 0.5896 time: 0.0258s\n",
      "Epoch: 04728 loss_train: 5.9428 loss_rec: 5.9428 acc_train: 0.4855 loss_val: 6.0518 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04729 loss_train: 3.8991 loss_rec: 3.8991 acc_train: 0.4856 loss_val: 3.9762 acc_val: 0.4938 time: 0.0258s\n",
      "Epoch: 04730 loss_train: 6.8751 loss_rec: 6.8751 acc_train: 0.5541 loss_val: 6.8304 acc_val: 0.5592 time: 0.0278s\n",
      "Epoch: 04731 loss_train: 8.3714 loss_rec: 8.3714 acc_train: 0.5506 loss_val: 8.3066 acc_val: 0.5588 time: 0.0278s\n",
      "Test set results: loss= 1.9122 accuracy= 0.5778\n",
      "Epoch: 04732 loss_train: 1.7335 loss_rec: 1.7335 acc_train: 0.6014 loss_val: 1.7562 acc_val: 0.6008 time: 0.0273s\n",
      "Epoch: 04733 loss_train: 12.0057 loss_rec: 12.0057 acc_train: 0.4872 loss_val: 12.2108 acc_val: 0.4954 time: 0.0308s\n",
      "Epoch: 04734 loss_train: 14.3892 loss_rec: 14.3892 acc_train: 0.4863 loss_val: 14.6324 acc_val: 0.4950 time: 0.0258s\n",
      "Epoch: 04735 loss_train: 6.3148 loss_rec: 6.3148 acc_train: 0.4855 loss_val: 6.4296 acc_val: 0.4938 time: 0.0263s\n",
      "Epoch: 04736 loss_train: 9.6127 loss_rec: 9.6127 acc_train: 0.5515 loss_val: 9.5282 acc_val: 0.5613 time: 0.0278s\n",
      "Epoch: 04737 loss_train: 15.4958 loss_rec: 15.4958 acc_train: 0.5490 loss_val: 15.3144 acc_val: 0.5558 time: 0.0278s\n",
      "Epoch: 04738 loss_train: 12.0759 loss_rec: 12.0759 acc_train: 0.5479 loss_val: 11.9533 acc_val: 0.5558 time: 0.0274s\n",
      "Epoch: 04739 loss_train: 1.1843 loss_rec: 1.1843 acc_train: 0.6173 loss_val: 1.2082 acc_val: 0.6154 time: 0.0272s\n",
      "Epoch: 04740 loss_train: 15.6386 loss_rec: 15.6386 acc_train: 0.4863 loss_val: 15.9011 acc_val: 0.4950 time: 0.0263s\n",
      "Epoch: 04741 loss_train: 20.4900 loss_rec: 20.4900 acc_train: 0.4861 loss_val: 20.8260 acc_val: 0.4950 time: 0.0263s\n",
      "Test set results: loss= 13.0514 accuracy= 0.5430\n",
      "Epoch: 04742 loss_train: 14.5932 loss_rec: 14.5932 acc_train: 0.4863 loss_val: 14.8396 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04743 loss_train: 1.0788 loss_rec: 1.0788 acc_train: 0.6279 loss_val: 1.1018 acc_val: 0.6217 time: 0.0273s\n",
      "Epoch: 04744 loss_train: 8.7718 loss_rec: 8.7718 acc_train: 0.5499 loss_val: 8.7006 acc_val: 0.5588 time: 0.0258s\n",
      "Epoch: 04745 loss_train: 8.0045 loss_rec: 8.0045 acc_train: 0.5510 loss_val: 7.9457 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 04746 loss_train: 0.9740 loss_rec: 0.9740 acc_train: 0.5524 loss_val: 0.9921 acc_val: 0.5450 time: 0.0268s\n",
      "Epoch: 04747 loss_train: 5.2015 loss_rec: 5.2015 acc_train: 0.4855 loss_val: 5.2993 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04748 loss_train: 1.0184 loss_rec: 1.0184 acc_train: 0.6304 loss_val: 1.0406 acc_val: 0.6221 time: 0.0278s\n",
      "Epoch: 04749 loss_train: 1.7508 loss_rec: 1.7508 acc_train: 0.6017 loss_val: 1.7736 acc_val: 0.6013 time: 0.0288s\n",
      "Epoch: 04750 loss_train: 3.8120 loss_rec: 3.8120 acc_train: 0.4856 loss_val: 3.8878 acc_val: 0.4938 time: 0.0258s\n",
      "Epoch: 04751 loss_train: 1.1369 loss_rec: 1.1369 acc_train: 0.6227 loss_val: 1.1604 acc_val: 0.6192 time: 0.0258s\n",
      "Test set results: loss= 0.8970 accuracy= 0.6164\n",
      "Epoch: 04752 loss_train: 0.8953 loss_rec: 0.8953 acc_train: 0.6173 loss_val: 0.9149 acc_val: 0.6067 time: 0.0268s\n",
      "Epoch: 04753 loss_train: 2.5749 loss_rec: 2.5749 acc_train: 0.4857 loss_val: 2.6300 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04754 loss_train: 3.8372 loss_rec: 3.8372 acc_train: 0.5677 loss_val: 3.8351 acc_val: 0.5733 time: 0.0273s\n",
      "Epoch: 04755 loss_train: 2.1194 loss_rec: 2.1194 acc_train: 0.5920 loss_val: 2.1391 acc_val: 0.5950 time: 0.0273s\n",
      "Epoch: 04756 loss_train: 6.9156 loss_rec: 6.9156 acc_train: 0.4854 loss_val: 7.0397 acc_val: 0.4938 time: 0.0283s\n",
      "Epoch: 04757 loss_train: 5.3106 loss_rec: 5.3106 acc_train: 0.4855 loss_val: 5.4100 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04758 loss_train: 5.2010 loss_rec: 5.2010 acc_train: 0.5582 loss_val: 5.1791 acc_val: 0.5663 time: 0.0258s\n",
      "Epoch: 04759 loss_train: 6.5152 loss_rec: 6.5152 acc_train: 0.5541 loss_val: 6.4746 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 04760 loss_train: 0.8465 loss_rec: 0.8465 acc_train: 0.6088 loss_val: 0.8633 acc_val: 0.6038 time: 0.0273s\n",
      "Epoch: 04761 loss_train: 6.8898 loss_rec: 6.8898 acc_train: 0.4854 loss_val: 7.0134 acc_val: 0.4938 time: 0.0258s\n",
      "Test set results: loss= 2.6186 accuracy= 0.5415\n",
      "Epoch: 04762 loss_train: 2.9704 loss_rec: 2.9704 acc_train: 0.4856 loss_val: 3.0322 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04763 loss_train: 9.0812 loss_rec: 9.0812 acc_train: 0.5530 loss_val: 9.0041 acc_val: 0.5642 time: 0.0278s\n",
      "Epoch: 04764 loss_train: 11.7615 loss_rec: 11.7615 acc_train: 0.5479 loss_val: 11.6434 acc_val: 0.5558 time: 0.0303s\n",
      "Epoch: 04765 loss_train: 5.6690 loss_rec: 5.6690 acc_train: 0.5570 loss_val: 5.6398 acc_val: 0.5646 time: 0.0273s\n",
      "Epoch: 04766 loss_train: 8.2368 loss_rec: 8.2368 acc_train: 0.4852 loss_val: 8.3816 acc_val: 0.4933 time: 0.0258s\n",
      "Epoch: 04767 loss_train: 11.6278 loss_rec: 11.6278 acc_train: 0.4872 loss_val: 11.8270 acc_val: 0.4958 time: 0.0258s\n",
      "Epoch: 04768 loss_train: 4.5414 loss_rec: 4.5414 acc_train: 0.4855 loss_val: 4.6286 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04769 loss_train: 10.3173 loss_rec: 10.3173 acc_train: 0.5502 loss_val: 10.2216 acc_val: 0.5588 time: 0.0278s\n",
      "Epoch: 04770 loss_train: 15.4316 loss_rec: 15.4316 acc_train: 0.5490 loss_val: 15.2504 acc_val: 0.5558 time: 0.0273s\n",
      "Epoch: 04771 loss_train: 11.3998 loss_rec: 11.3998 acc_train: 0.5486 loss_val: 11.2874 acc_val: 0.5563 time: 0.0273s\n",
      "Test set results: loss= 0.7958 accuracy= 0.6206\n",
      "Epoch: 04772 loss_train: 0.8515 loss_rec: 0.8515 acc_train: 0.6042 loss_val: 0.8680 acc_val: 0.6017 time: 0.0288s\n",
      "Epoch: 04773 loss_train: 11.1054 loss_rec: 11.1054 acc_train: 0.4872 loss_val: 11.2963 acc_val: 0.4958 time: 0.0278s\n",
      "Epoch: 04774 loss_train: 10.8687 loss_rec: 10.8687 acc_train: 0.4872 loss_val: 11.0559 acc_val: 0.4958 time: 0.0273s\n",
      "Epoch: 04775 loss_train: 0.9682 loss_rec: 0.9682 acc_train: 0.5524 loss_val: 0.9860 acc_val: 0.5450 time: 0.0258s\n",
      "Epoch: 04776 loss_train: 11.3099 loss_rec: 11.3099 acc_train: 0.5486 loss_val: 11.1991 acc_val: 0.5563 time: 0.0273s\n",
      "Epoch: 04777 loss_train: 14.0122 loss_rec: 14.0122 acc_train: 0.5491 loss_val: 13.8552 acc_val: 0.5558 time: 0.0278s\n",
      "Epoch: 04778 loss_train: 7.8734 loss_rec: 7.8734 acc_train: 0.5514 loss_val: 7.8154 acc_val: 0.5588 time: 0.0258s\n",
      "Epoch: 04779 loss_train: 5.8292 loss_rec: 5.8292 acc_train: 0.4855 loss_val: 5.9361 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04780 loss_train: 9.2664 loss_rec: 9.2664 acc_train: 0.4865 loss_val: 9.4275 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04781 loss_train: 2.3226 loss_rec: 2.3226 acc_train: 0.4856 loss_val: 2.3726 acc_val: 0.4938 time: 0.0278s\n",
      "Test set results: loss= 13.4331 accuracy= 0.5096\n",
      "Epoch: 04782 loss_train: 11.8654 loss_rec: 11.8654 acc_train: 0.5479 loss_val: 11.7437 acc_val: 0.5558 time: 0.0268s\n",
      "Epoch: 04783 loss_train: 16.5404 loss_rec: 16.5404 acc_train: 0.5493 loss_val: 16.3389 acc_val: 0.5554 time: 0.0258s\n",
      "Epoch: 04784 loss_train: 12.1613 loss_rec: 12.1613 acc_train: 0.5479 loss_val: 12.0349 acc_val: 0.5558 time: 0.0263s\n",
      "Epoch: 04785 loss_train: 0.8894 loss_rec: 0.8894 acc_train: 0.6172 loss_val: 0.9083 acc_val: 0.6088 time: 0.0278s\n",
      "Epoch: 04786 loss_train: 14.1975 loss_rec: 14.1975 acc_train: 0.4863 loss_val: 14.4371 acc_val: 0.4950 time: 0.0278s\n",
      "Epoch: 04787 loss_train: 17.2765 loss_rec: 17.2765 acc_train: 0.4863 loss_val: 17.5635 acc_val: 0.4950 time: 0.0273s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04788 loss_train: 9.9657 loss_rec: 9.9657 acc_train: 0.4863 loss_val: 10.1381 acc_val: 0.4938 time: 0.0303s\n",
      "Epoch: 04789 loss_train: 5.5261 loss_rec: 5.5261 acc_train: 0.5569 loss_val: 5.4982 acc_val: 0.5646 time: 0.0258s\n",
      "Epoch: 04790 loss_train: 11.0000 loss_rec: 11.0000 acc_train: 0.5493 loss_val: 10.8932 acc_val: 0.5567 time: 0.0258s\n",
      "Epoch: 04791 loss_train: 7.5790 loss_rec: 7.5790 acc_train: 0.5525 loss_val: 7.5242 acc_val: 0.5588 time: 0.0278s\n",
      "Test set results: loss= 3.0727 accuracy= 0.5415\n",
      "Epoch: 04792 loss_train: 3.4766 loss_rec: 3.4766 acc_train: 0.4856 loss_val: 3.5466 acc_val: 0.4938 time: 0.0263s\n",
      "Epoch: 04793 loss_train: 4.6959 loss_rec: 4.6959 acc_train: 0.4855 loss_val: 4.7852 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04794 loss_train: 3.4237 loss_rec: 3.4237 acc_train: 0.5712 loss_val: 3.4258 acc_val: 0.5758 time: 0.0263s\n",
      "Epoch: 04795 loss_train: 3.0974 loss_rec: 3.0974 acc_train: 0.5762 loss_val: 3.1040 acc_val: 0.5817 time: 0.0278s\n",
      "Epoch: 04796 loss_train: 4.7745 loss_rec: 4.7745 acc_train: 0.4855 loss_val: 4.8651 acc_val: 0.4938 time: 0.0308s\n",
      "Epoch: 04797 loss_train: 2.5531 loss_rec: 2.5531 acc_train: 0.4856 loss_val: 2.6075 acc_val: 0.4938 time: 0.0268s\n",
      "Epoch: 04798 loss_train: 7.8437 loss_rec: 7.8437 acc_train: 0.5507 loss_val: 7.7855 acc_val: 0.5592 time: 0.0268s\n",
      "Epoch: 04799 loss_train: 9.2441 loss_rec: 9.2441 acc_train: 0.5521 loss_val: 9.1636 acc_val: 0.5625 time: 0.0263s\n",
      "Epoch: 04800 loss_train: 2.4757 loss_rec: 2.4757 acc_train: 0.5850 loss_val: 2.4913 acc_val: 0.5892 time: 0.0268s\n",
      "Epoch: 04801 loss_train: 11.7433 loss_rec: 11.7433 acc_train: 0.4872 loss_val: 11.9440 acc_val: 0.4958 time: 0.0278s\n",
      "Test set results: loss= 13.4547 accuracy= 0.5428\n",
      "Epoch: 04802 loss_train: 15.0407 loss_rec: 15.0407 acc_train: 0.4863 loss_val: 15.2935 acc_val: 0.4950 time: 0.0258s\n",
      "Epoch: 04803 loss_train: 7.9857 loss_rec: 7.9857 acc_train: 0.4852 loss_val: 8.1263 acc_val: 0.4933 time: 0.0268s\n",
      "Epoch: 04804 loss_train: 7.0465 loss_rec: 7.0465 acc_train: 0.5534 loss_val: 6.9989 acc_val: 0.5588 time: 0.0303s\n",
      "Epoch: 04805 loss_train: 12.2672 loss_rec: 12.2672 acc_train: 0.5479 loss_val: 12.1394 acc_val: 0.5558 time: 0.0258s\n",
      "Epoch: 04806 loss_train: 8.5531 loss_rec: 8.5531 acc_train: 0.5540 loss_val: 8.4834 acc_val: 0.5654 time: 0.0278s\n",
      "Epoch: 04807 loss_train: 2.7234 loss_rec: 2.7234 acc_train: 0.4856 loss_val: 2.7807 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04808 loss_train: 4.3839 loss_rec: 4.3839 acc_train: 0.4855 loss_val: 4.4684 acc_val: 0.4938 time: 0.0263s\n",
      "Epoch: 04809 loss_train: 3.2971 loss_rec: 3.2971 acc_train: 0.5739 loss_val: 3.3008 acc_val: 0.5792 time: 0.0278s\n",
      "Epoch: 04810 loss_train: 2.7151 loss_rec: 2.7151 acc_train: 0.5797 loss_val: 2.7273 acc_val: 0.5887 time: 0.0278s\n",
      "Epoch: 04811 loss_train: 5.2911 loss_rec: 5.2911 acc_train: 0.4855 loss_val: 5.3897 acc_val: 0.4938 time: 0.0263s\n",
      "Test set results: loss= 2.7505 accuracy= 0.5415\n",
      "Epoch: 04812 loss_train: 3.1171 loss_rec: 3.1171 acc_train: 0.4856 loss_val: 3.1812 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04813 loss_train: 7.3679 loss_rec: 7.3679 acc_train: 0.5524 loss_val: 7.3158 acc_val: 0.5583 time: 0.0273s\n",
      "Epoch: 04814 loss_train: 8.8820 loss_rec: 8.8820 acc_train: 0.5530 loss_val: 8.8069 acc_val: 0.5642 time: 0.0278s\n",
      "Epoch: 04815 loss_train: 2.2946 loss_rec: 2.2946 acc_train: 0.5882 loss_val: 2.3118 acc_val: 0.5925 time: 0.0273s\n",
      "Epoch: 04816 loss_train: 11.6369 loss_rec: 11.6369 acc_train: 0.4872 loss_val: 11.8360 acc_val: 0.4958 time: 0.0263s\n",
      "Epoch: 04817 loss_train: 14.6822 loss_rec: 14.6822 acc_train: 0.4863 loss_val: 14.9295 acc_val: 0.4950 time: 0.0263s\n",
      "Epoch: 04818 loss_train: 7.4294 loss_rec: 7.4294 acc_train: 0.4852 loss_val: 7.5610 acc_val: 0.4933 time: 0.0278s\n",
      "Epoch: 04819 loss_train: 7.6659 loss_rec: 7.6659 acc_train: 0.5506 loss_val: 7.6100 acc_val: 0.5592 time: 0.0288s\n",
      "Epoch: 04820 loss_train: 13.0091 loss_rec: 13.0091 acc_train: 0.5494 loss_val: 12.8682 acc_val: 0.5567 time: 0.0268s\n",
      "Epoch: 04821 loss_train: 9.4215 loss_rec: 9.4215 acc_train: 0.5515 loss_val: 9.3383 acc_val: 0.5613 time: 0.0258s\n",
      "Test set results: loss= 1.5107 accuracy= 0.5415\n",
      "Epoch: 04822 loss_train: 1.7287 loss_rec: 1.7287 acc_train: 0.4857 loss_val: 1.7665 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04823 loss_train: 4.2071 loss_rec: 4.2071 acc_train: 0.4855 loss_val: 4.2887 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04824 loss_train: 2.7658 loss_rec: 2.7658 acc_train: 0.5787 loss_val: 2.7773 acc_val: 0.5867 time: 0.0273s\n",
      "Epoch: 04825 loss_train: 1.8904 loss_rec: 1.8904 acc_train: 0.5974 loss_val: 1.9111 acc_val: 0.6025 time: 0.0258s\n",
      "Epoch: 04826 loss_train: 5.8989 loss_rec: 5.8989 acc_train: 0.4855 loss_val: 6.0069 acc_val: 0.4938 time: 0.0303s\n",
      "Epoch: 04827 loss_train: 3.3409 loss_rec: 3.3409 acc_train: 0.4856 loss_val: 3.4087 acc_val: 0.4938 time: 0.0253s\n",
      "Epoch: 04828 loss_train: 7.4878 loss_rec: 7.4878 acc_train: 0.5524 loss_val: 7.4345 acc_val: 0.5588 time: 0.0278s\n",
      "Epoch: 04829 loss_train: 9.3014 loss_rec: 9.3014 acc_train: 0.5521 loss_val: 9.2200 acc_val: 0.5625 time: 0.0278s\n",
      "Epoch: 04830 loss_train: 2.8966 loss_rec: 2.8966 acc_train: 0.5792 loss_val: 2.9062 acc_val: 0.5867 time: 0.0273s\n",
      "Epoch: 04831 loss_train: 11.0243 loss_rec: 11.0243 acc_train: 0.4862 loss_val: 11.2137 acc_val: 0.4938 time: 0.0263s\n",
      "Test set results: loss= 12.7269 accuracy= 0.5430\n",
      "Epoch: 04832 loss_train: 14.2304 loss_rec: 14.2304 acc_train: 0.4863 loss_val: 14.4707 acc_val: 0.4950 time: 0.0283s\n",
      "Epoch: 04833 loss_train: 7.1654 loss_rec: 7.1654 acc_train: 0.4853 loss_val: 7.2929 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04834 loss_train: 7.7224 loss_rec: 7.7224 acc_train: 0.5507 loss_val: 7.6656 acc_val: 0.5592 time: 0.0278s\n",
      "Epoch: 04835 loss_train: 12.9257 loss_rec: 12.9257 acc_train: 0.5494 loss_val: 12.7862 acc_val: 0.5567 time: 0.0268s\n",
      "Epoch: 04836 loss_train: 9.2317 loss_rec: 9.2317 acc_train: 0.5521 loss_val: 9.1512 acc_val: 0.5625 time: 0.0278s\n",
      "Epoch: 04837 loss_train: 1.9750 loss_rec: 1.9750 acc_train: 0.4857 loss_val: 2.0182 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04838 loss_train: 4.1562 loss_rec: 4.1562 acc_train: 0.4855 loss_val: 4.2371 acc_val: 0.4938 time: 0.0258s\n",
      "Epoch: 04839 loss_train: 3.0212 loss_rec: 3.0212 acc_train: 0.5774 loss_val: 3.0288 acc_val: 0.5825 time: 0.0278s\n",
      "Epoch: 04840 loss_train: 2.1987 loss_rec: 2.1987 acc_train: 0.5898 loss_val: 2.2168 acc_val: 0.5917 time: 0.0263s\n",
      "Epoch: 04841 loss_train: 5.7565 loss_rec: 5.7565 acc_train: 0.4855 loss_val: 5.8623 acc_val: 0.4938 time: 0.0278s\n",
      "Test set results: loss= 3.0701 accuracy= 0.5415\n",
      "Epoch: 04842 loss_train: 3.4737 loss_rec: 3.4737 acc_train: 0.4856 loss_val: 3.5435 acc_val: 0.4938 time: 0.0298s\n",
      "Epoch: 04843 loss_train: 7.1297 loss_rec: 7.1297 acc_train: 0.5534 loss_val: 7.0808 acc_val: 0.5588 time: 0.0258s\n",
      "Epoch: 04844 loss_train: 8.7661 loss_rec: 8.7661 acc_train: 0.5529 loss_val: 8.6930 acc_val: 0.5642 time: 0.0263s\n",
      "Epoch: 04845 loss_train: 2.3509 loss_rec: 2.3509 acc_train: 0.5872 loss_val: 2.3676 acc_val: 0.5921 time: 0.0278s\n",
      "Epoch: 04846 loss_train: 11.3809 loss_rec: 11.3809 acc_train: 0.4867 loss_val: 11.5759 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04847 loss_train: 14.3337 loss_rec: 14.3337 acc_train: 0.4863 loss_val: 14.5755 acc_val: 0.4950 time: 0.0273s\n",
      "Epoch: 04848 loss_train: 7.0749 loss_rec: 7.0749 acc_train: 0.4853 loss_val: 7.2009 acc_val: 0.4938 time: 0.0258s\n",
      "Epoch: 04849 loss_train: 7.9339 loss_rec: 7.9339 acc_train: 0.5515 loss_val: 7.8741 acc_val: 0.5588 time: 0.0263s\n",
      "Epoch: 04850 loss_train: 13.2874 loss_rec: 13.2874 acc_train: 0.5494 loss_val: 13.1418 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 04851 loss_train: 9.7355 loss_rec: 9.7355 acc_train: 0.5501 loss_val: 9.6477 acc_val: 0.5588 time: 0.0298s\n",
      "Test set results: loss= 1.1984 accuracy= 0.5415\n",
      "Epoch: 04852 loss_train: 1.3725 loss_rec: 1.3725 acc_train: 0.4857 loss_val: 1.4011 acc_val: 0.4938 time: 0.0263s\n",
      "Epoch: 04853 loss_train: 4.8181 loss_rec: 4.8181 acc_train: 0.4855 loss_val: 4.9093 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04854 loss_train: 1.5651 loss_rec: 1.5651 acc_train: 0.6052 loss_val: 1.5877 acc_val: 0.6058 time: 0.0263s\n",
      "Epoch: 04855 loss_train: 1.2329 loss_rec: 1.2329 acc_train: 0.6178 loss_val: 1.2560 acc_val: 0.6175 time: 0.0273s\n",
      "Epoch: 04856 loss_train: 4.6653 loss_rec: 4.6653 acc_train: 0.4855 loss_val: 4.7542 acc_val: 0.4938 time: 0.0278s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04857 loss_train: 0.8505 loss_rec: 0.8505 acc_train: 0.6060 loss_val: 0.8665 acc_val: 0.6058 time: 0.0268s\n",
      "Epoch: 04858 loss_train: 4.1878 loss_rec: 4.1878 acc_train: 0.5641 loss_val: 4.1799 acc_val: 0.5704 time: 0.0298s\n",
      "Epoch: 04859 loss_train: 1.2718 loss_rec: 1.2718 acc_train: 0.6158 loss_val: 1.2950 acc_val: 0.6163 time: 0.0268s\n",
      "Epoch: 04860 loss_train: 7.6927 loss_rec: 7.6927 acc_train: 0.4852 loss_val: 7.8284 acc_val: 0.4933 time: 0.0258s\n",
      "Epoch: 04861 loss_train: 5.9502 loss_rec: 5.9502 acc_train: 0.4855 loss_val: 6.0589 acc_val: 0.4938 time: 0.0278s\n",
      "Test set results: loss= 5.0814 accuracy= 0.5282\n",
      "Epoch: 04862 loss_train: 4.5079 loss_rec: 4.5079 acc_train: 0.5624 loss_val: 4.4950 acc_val: 0.5692 time: 0.0273s\n",
      "Epoch: 04863 loss_train: 6.0030 loss_rec: 6.0030 acc_train: 0.5554 loss_val: 5.9676 acc_val: 0.5613 time: 0.0278s\n",
      "Epoch: 04864 loss_train: 0.8418 loss_rec: 0.8418 acc_train: 0.6087 loss_val: 0.8578 acc_val: 0.6071 time: 0.0273s\n",
      "Epoch: 04865 loss_train: 5.9610 loss_rec: 5.9610 acc_train: 0.4855 loss_val: 6.0699 acc_val: 0.4938 time: 0.0288s\n",
      "Epoch: 04866 loss_train: 1.5285 loss_rec: 1.5285 acc_train: 0.4857 loss_val: 1.5613 acc_val: 0.4938 time: 0.0258s\n",
      "Epoch: 04867 loss_train: 9.3611 loss_rec: 9.3611 acc_train: 0.5504 loss_val: 9.2780 acc_val: 0.5600 time: 0.0278s\n",
      "Epoch: 04868 loss_train: 11.2533 loss_rec: 11.2533 acc_train: 0.5486 loss_val: 11.1416 acc_val: 0.5563 time: 0.0278s\n",
      "Epoch: 04869 loss_train: 4.7682 loss_rec: 4.7682 acc_train: 0.5607 loss_val: 4.7513 acc_val: 0.5688 time: 0.0278s\n",
      "Epoch: 04870 loss_train: 9.3218 loss_rec: 9.3218 acc_train: 0.4858 loss_val: 9.4839 acc_val: 0.4933 time: 0.0273s\n",
      "Epoch: 04871 loss_train: 13.0582 loss_rec: 13.0582 acc_train: 0.4861 loss_val: 13.2797 acc_val: 0.4942 time: 0.0263s\n",
      "Test set results: loss= 5.8339 accuracy= 0.5412\n",
      "Epoch: 04872 loss_train: 6.5550 loss_rec: 6.5550 acc_train: 0.4854 loss_val: 6.6730 acc_val: 0.4938 time: 0.0303s\n",
      "Epoch: 04873 loss_train: 7.7472 loss_rec: 7.7472 acc_train: 0.5514 loss_val: 7.6895 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 04874 loss_train: 12.5358 loss_rec: 12.5358 acc_train: 0.5492 loss_val: 12.4024 acc_val: 0.5563 time: 0.0273s\n",
      "Epoch: 04875 loss_train: 8.5208 loss_rec: 8.5208 acc_train: 0.5534 loss_val: 8.4510 acc_val: 0.5654 time: 0.0258s\n",
      "Epoch: 04876 loss_train: 2.9241 loss_rec: 2.9241 acc_train: 0.4856 loss_val: 2.9847 acc_val: 0.4938 time: 0.0258s\n",
      "Epoch: 04877 loss_train: 4.8141 loss_rec: 4.8141 acc_train: 0.4855 loss_val: 4.9052 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04878 loss_train: 2.6607 loss_rec: 2.6607 acc_train: 0.5802 loss_val: 2.6732 acc_val: 0.5887 time: 0.0278s\n",
      "Epoch: 04879 loss_train: 2.2284 loss_rec: 2.2284 acc_train: 0.5892 loss_val: 2.2459 acc_val: 0.5921 time: 0.0273s\n",
      "Epoch: 04880 loss_train: 5.2721 loss_rec: 5.2721 acc_train: 0.4855 loss_val: 5.3703 acc_val: 0.4938 time: 0.0263s\n",
      "Epoch: 04881 loss_train: 2.6650 loss_rec: 2.6650 acc_train: 0.4856 loss_val: 2.7212 acc_val: 0.4938 time: 0.0278s\n",
      "Test set results: loss= 8.9827 accuracy= 0.5156\n",
      "Epoch: 04882 loss_train: 7.9366 loss_rec: 7.9366 acc_train: 0.5515 loss_val: 7.8765 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 04883 loss_train: 9.6409 loss_rec: 9.6409 acc_train: 0.5501 loss_val: 9.5538 acc_val: 0.5588 time: 0.0268s\n",
      "Epoch: 04884 loss_train: 3.1986 loss_rec: 3.1986 acc_train: 0.5759 loss_val: 3.2034 acc_val: 0.5813 time: 0.0273s\n",
      "Epoch: 04885 loss_train: 10.7656 loss_rec: 10.7656 acc_train: 0.4863 loss_val: 10.9508 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04886 loss_train: 14.1475 loss_rec: 14.1475 acc_train: 0.4859 loss_val: 14.3863 acc_val: 0.4942 time: 0.0273s\n",
      "Epoch: 04887 loss_train: 7.3613 loss_rec: 7.3613 acc_train: 0.4852 loss_val: 7.4918 acc_val: 0.4933 time: 0.0263s\n",
      "Epoch: 04888 loss_train: 7.2381 loss_rec: 7.2381 acc_train: 0.5529 loss_val: 7.1874 acc_val: 0.5588 time: 0.0293s\n",
      "Epoch: 04889 loss_train: 12.2707 loss_rec: 12.2707 acc_train: 0.5479 loss_val: 12.1425 acc_val: 0.5558 time: 0.0263s\n",
      "Epoch: 04890 loss_train: 8.5075 loss_rec: 8.5075 acc_train: 0.5540 loss_val: 8.4384 acc_val: 0.5654 time: 0.0273s\n",
      "Epoch: 04891 loss_train: 2.6527 loss_rec: 2.6527 acc_train: 0.4856 loss_val: 2.7087 acc_val: 0.4938 time: 0.0278s\n",
      "Test set results: loss= 3.9026 accuracy= 0.5415\n",
      "Epoch: 04892 loss_train: 4.4025 loss_rec: 4.4025 acc_train: 0.4855 loss_val: 4.4873 acc_val: 0.4938 time: 0.0263s\n",
      "Epoch: 04893 loss_train: 3.0862 loss_rec: 3.0862 acc_train: 0.5762 loss_val: 3.0930 acc_val: 0.5817 time: 0.0264s\n",
      "Epoch: 04894 loss_train: 2.5287 loss_rec: 2.5287 acc_train: 0.5847 loss_val: 2.5433 acc_val: 0.5900 time: 0.0277s\n",
      "Epoch: 04895 loss_train: 5.2300 loss_rec: 5.2300 acc_train: 0.4855 loss_val: 5.3276 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04896 loss_train: 2.9721 loss_rec: 2.9721 acc_train: 0.4856 loss_val: 3.0336 acc_val: 0.4938 time: 0.0288s\n",
      "Epoch: 04897 loss_train: 7.4037 loss_rec: 7.4037 acc_train: 0.5524 loss_val: 7.3508 acc_val: 0.5583 time: 0.0273s\n",
      "Epoch: 04898 loss_train: 8.9657 loss_rec: 8.9657 acc_train: 0.5530 loss_val: 8.8889 acc_val: 0.5642 time: 0.0258s\n",
      "Epoch: 04899 loss_train: 2.5184 loss_rec: 2.5184 acc_train: 0.5845 loss_val: 2.5331 acc_val: 0.5896 time: 0.0263s\n",
      "Epoch: 04900 loss_train: 11.2541 loss_rec: 11.2541 acc_train: 0.4862 loss_val: 11.4471 acc_val: 0.4938 time: 0.0283s\n",
      "Epoch: 04901 loss_train: 14.3595 loss_rec: 14.3595 acc_train: 0.4859 loss_val: 14.6017 acc_val: 0.4942 time: 0.0278s\n",
      "Test set results: loss= 6.5437 accuracy= 0.5411\n",
      "Epoch: 04902 loss_train: 7.3460 loss_rec: 7.3460 acc_train: 0.4852 loss_val: 7.4762 acc_val: 0.4933 time: 0.0278s\n",
      "Epoch: 04903 loss_train: 7.4180 loss_rec: 7.4180 acc_train: 0.5522 loss_val: 7.3650 acc_val: 0.5588 time: 0.0263s\n",
      "Epoch: 04904 loss_train: 12.5946 loss_rec: 12.5946 acc_train: 0.5487 loss_val: 12.4604 acc_val: 0.5558 time: 0.0293s\n",
      "Epoch: 04905 loss_train: 9.0076 loss_rec: 9.0076 acc_train: 0.5530 loss_val: 8.9300 acc_val: 0.5642 time: 0.0273s\n",
      "Epoch: 04906 loss_train: 1.9868 loss_rec: 1.9868 acc_train: 0.4857 loss_val: 2.0302 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04907 loss_train: 4.0603 loss_rec: 4.0603 acc_train: 0.4855 loss_val: 4.1396 acc_val: 0.4938 time: 0.0258s\n",
      "Epoch: 04908 loss_train: 3.0615 loss_rec: 3.0615 acc_train: 0.5767 loss_val: 3.0682 acc_val: 0.5813 time: 0.0278s\n",
      "Epoch: 04909 loss_train: 2.2757 loss_rec: 2.2757 acc_train: 0.5877 loss_val: 2.2929 acc_val: 0.5925 time: 0.0263s\n",
      "Epoch: 04910 loss_train: 5.5768 loss_rec: 5.5768 acc_train: 0.4855 loss_val: 5.6799 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04911 loss_train: 3.3757 loss_rec: 3.3757 acc_train: 0.4856 loss_val: 3.4440 acc_val: 0.4938 time: 0.0278s\n",
      "Test set results: loss= 7.9528 accuracy= 0.5175\n",
      "Epoch: 04912 loss_train: 7.0307 loss_rec: 7.0307 acc_train: 0.5534 loss_val: 6.9826 acc_val: 0.5588 time: 0.0278s\n",
      "Epoch: 04913 loss_train: 8.6071 loss_rec: 8.6071 acc_train: 0.5534 loss_val: 8.5362 acc_val: 0.5654 time: 0.0273s\n",
      "Epoch: 04914 loss_train: 2.2763 loss_rec: 2.2763 acc_train: 0.5877 loss_val: 2.2936 acc_val: 0.5925 time: 0.0263s\n",
      "Epoch: 04915 loss_train: 11.2453 loss_rec: 11.2453 acc_train: 0.4862 loss_val: 11.4382 acc_val: 0.4938 time: 0.0263s\n",
      "Epoch: 04916 loss_train: 14.1059 loss_rec: 14.1059 acc_train: 0.4859 loss_val: 14.3442 acc_val: 0.4942 time: 0.0273s\n",
      "Epoch: 04917 loss_train: 6.8979 loss_rec: 6.8979 acc_train: 0.4853 loss_val: 7.0213 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04918 loss_train: 7.9266 loss_rec: 7.9266 acc_train: 0.5515 loss_val: 7.8668 acc_val: 0.5588 time: 0.0273s\n",
      "Epoch: 04919 loss_train: 13.2273 loss_rec: 13.2273 acc_train: 0.5494 loss_val: 13.0825 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 04920 loss_train: 9.7460 loss_rec: 9.7460 acc_train: 0.5501 loss_val: 9.6578 acc_val: 0.5588 time: 0.0278s\n",
      "Epoch: 04921 loss_train: 1.2116 loss_rec: 1.2116 acc_train: 0.4857 loss_val: 1.2359 acc_val: 0.4938 time: 0.0258s\n",
      "Test set results: loss= 4.8162 accuracy= 0.5414\n",
      "Epoch: 04922 loss_train: 5.4213 loss_rec: 5.4213 acc_train: 0.4855 loss_val: 5.5219 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04923 loss_train: 0.8737 loss_rec: 0.8737 acc_train: 0.6179 loss_val: 0.8922 acc_val: 0.6071 time: 0.0278s\n",
      "Epoch: 04924 loss_train: 2.9005 loss_rec: 2.9005 acc_train: 0.5792 loss_val: 2.9098 acc_val: 0.5867 time: 0.0263s\n",
      "Epoch: 04925 loss_train: 1.5359 loss_rec: 1.5359 acc_train: 0.4857 loss_val: 1.5689 acc_val: 0.4938 time: 0.0258s\n",
      "Epoch: 04926 loss_train: 1.8595 loss_rec: 1.8595 acc_train: 0.5980 loss_val: 1.8800 acc_val: 0.6025 time: 0.0278s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04927 loss_train: 1.5648 loss_rec: 1.5648 acc_train: 0.4857 loss_val: 1.5985 acc_val: 0.4938 time: 0.0283s\n",
      "Epoch: 04928 loss_train: 2.8178 loss_rec: 2.8178 acc_train: 0.5797 loss_val: 2.8279 acc_val: 0.5875 time: 0.0268s\n",
      "Epoch: 04929 loss_train: 0.8382 loss_rec: 0.8382 acc_train: 0.6142 loss_val: 0.8546 acc_val: 0.6046 time: 0.0268s\n",
      "Epoch: 04930 loss_train: 3.1670 loss_rec: 3.1670 acc_train: 0.4856 loss_val: 3.2318 acc_val: 0.4938 time: 0.0258s\n",
      "Epoch: 04931 loss_train: 2.9434 loss_rec: 2.9434 acc_train: 0.5787 loss_val: 2.9513 acc_val: 0.5863 time: 0.0263s\n",
      "Test set results: loss= 1.6975 accuracy= 0.5859\n",
      "Epoch: 04932 loss_train: 1.5469 loss_rec: 1.5469 acc_train: 0.6064 loss_val: 1.5688 acc_val: 0.6075 time: 0.0263s\n",
      "Epoch: 04933 loss_train: 6.2753 loss_rec: 6.2753 acc_train: 0.4855 loss_val: 6.3889 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04934 loss_train: 3.8057 loss_rec: 3.8057 acc_train: 0.4856 loss_val: 3.8807 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04935 loss_train: 6.8623 loss_rec: 6.8623 acc_train: 0.5534 loss_val: 6.8157 acc_val: 0.5588 time: 0.0288s\n",
      "Epoch: 04936 loss_train: 8.6644 loss_rec: 8.6644 acc_train: 0.5529 loss_val: 8.5916 acc_val: 0.5642 time: 0.0253s\n",
      "Epoch: 04937 loss_train: 2.4936 loss_rec: 2.4936 acc_train: 0.5846 loss_val: 2.5076 acc_val: 0.5896 time: 0.0278s\n",
      "Epoch: 04938 loss_train: 10.9289 loss_rec: 10.9289 acc_train: 0.4862 loss_val: 11.1166 acc_val: 0.4938 time: 0.0263s\n",
      "Epoch: 04939 loss_train: 13.7951 loss_rec: 13.7951 acc_train: 0.4859 loss_val: 14.0282 acc_val: 0.4942 time: 0.0273s\n",
      "Epoch: 04940 loss_train: 6.6197 loss_rec: 6.6197 acc_train: 0.4854 loss_val: 6.7385 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04941 loss_train: 8.1490 loss_rec: 8.1490 acc_train: 0.5556 loss_val: 8.0848 acc_val: 0.5654 time: 0.0278s\n",
      "Test set results: loss= 15.1824 accuracy= 0.5084\n",
      "Epoch: 04942 loss_train: 13.4102 loss_rec: 13.4102 acc_train: 0.5490 loss_val: 13.2610 acc_val: 0.5554 time: 0.0268s\n",
      "Epoch: 04943 loss_train: 9.9085 loss_rec: 9.9085 acc_train: 0.5497 loss_val: 9.8170 acc_val: 0.5567 time: 0.0298s\n",
      "Epoch: 04944 loss_train: 1.0997 loss_rec: 1.0997 acc_train: 0.5581 loss_val: 1.1206 acc_val: 0.5533 time: 0.0273s\n",
      "Epoch: 04945 loss_train: 6.0571 loss_rec: 6.0571 acc_train: 0.4855 loss_val: 6.1674 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04946 loss_train: 1.2712 loss_rec: 1.2712 acc_train: 0.4857 loss_val: 1.2970 acc_val: 0.4938 time: 0.0258s\n",
      "Epoch: 04947 loss_train: 8.9502 loss_rec: 8.9502 acc_train: 0.5524 loss_val: 8.8728 acc_val: 0.5633 time: 0.0263s\n",
      "Epoch: 04948 loss_train: 10.3430 loss_rec: 10.3430 acc_train: 0.5489 loss_val: 10.2449 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 04949 loss_train: 3.6358 loss_rec: 3.6358 acc_train: 0.5703 loss_val: 3.6342 acc_val: 0.5758 time: 0.0278s\n",
      "Epoch: 04950 loss_train: 10.5253 loss_rec: 10.5253 acc_train: 0.4863 loss_val: 10.7066 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04951 loss_train: 14.3043 loss_rec: 14.3043 acc_train: 0.4859 loss_val: 14.5454 acc_val: 0.4942 time: 0.0273s\n",
      "Test set results: loss= 7.1170 accuracy= 0.5411\n",
      "Epoch: 04952 loss_train: 7.9841 loss_rec: 7.9841 acc_train: 0.4852 loss_val: 8.1244 acc_val: 0.4933 time: 0.0258s\n",
      "Epoch: 04953 loss_train: 6.2068 loss_rec: 6.2068 acc_train: 0.5545 loss_val: 6.1683 acc_val: 0.5600 time: 0.0278s\n",
      "Epoch: 04954 loss_train: 10.9088 loss_rec: 10.9088 acc_train: 0.5493 loss_val: 10.8021 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 04955 loss_train: 7.0690 loss_rec: 7.0690 acc_train: 0.5529 loss_val: 7.0197 acc_val: 0.5588 time: 0.0263s\n",
      "Epoch: 04956 loss_train: 4.1291 loss_rec: 4.1291 acc_train: 0.4855 loss_val: 4.2094 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04957 loss_train: 5.7217 loss_rec: 5.7217 acc_train: 0.4855 loss_val: 5.8268 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04958 loss_train: 2.0628 loss_rec: 2.0628 acc_train: 0.5920 loss_val: 2.0811 acc_val: 0.5950 time: 0.0258s\n",
      "Epoch: 04959 loss_train: 2.2157 loss_rec: 2.2157 acc_train: 0.5892 loss_val: 2.2326 acc_val: 0.5921 time: 0.0293s\n",
      "Epoch: 04960 loss_train: 4.4679 loss_rec: 4.4679 acc_train: 0.4855 loss_val: 4.5535 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04961 loss_train: 1.4114 loss_rec: 1.4114 acc_train: 0.4857 loss_val: 1.4412 acc_val: 0.4938 time: 0.0258s\n",
      "Test set results: loss= 8.9213 accuracy= 0.5152\n",
      "Epoch: 04962 loss_train: 7.8822 loss_rec: 7.8822 acc_train: 0.5506 loss_val: 7.8223 acc_val: 0.5588 time: 0.0274s\n",
      "Epoch: 04963 loss_train: 8.5526 loss_rec: 8.5526 acc_train: 0.5534 loss_val: 8.4818 acc_val: 0.5654 time: 0.0267s\n",
      "Epoch: 04964 loss_train: 1.6210 loss_rec: 1.6210 acc_train: 0.6034 loss_val: 1.6425 acc_val: 0.6038 time: 0.0268s\n",
      "Epoch: 04965 loss_train: 11.9213 loss_rec: 11.9213 acc_train: 0.4862 loss_val: 12.1246 acc_val: 0.4933 time: 0.0278s\n",
      "Epoch: 04966 loss_train: 14.6744 loss_rec: 14.6744 acc_train: 0.4859 loss_val: 14.9213 acc_val: 0.4942 time: 0.0273s\n",
      "Epoch: 04967 loss_train: 7.4617 loss_rec: 7.4617 acc_train: 0.4852 loss_val: 7.5937 acc_val: 0.4933 time: 0.0293s\n",
      "Epoch: 04968 loss_train: 7.3616 loss_rec: 7.3616 acc_train: 0.5528 loss_val: 7.3087 acc_val: 0.5583 time: 0.0273s\n",
      "Epoch: 04969 loss_train: 12.7043 loss_rec: 12.7043 acc_train: 0.5494 loss_val: 12.5674 acc_val: 0.5567 time: 0.0258s\n",
      "Epoch: 04970 loss_train: 9.3467 loss_rec: 9.3467 acc_train: 0.5504 loss_val: 9.2633 acc_val: 0.5600 time: 0.0263s\n",
      "Epoch: 04971 loss_train: 1.3806 loss_rec: 1.3806 acc_train: 0.4857 loss_val: 1.4095 acc_val: 0.4938 time: 0.0278s\n",
      "Test set results: loss= 4.0831 accuracy= 0.5415\n",
      "Epoch: 04972 loss_train: 4.6027 loss_rec: 4.6027 acc_train: 0.4855 loss_val: 4.6905 acc_val: 0.4938 time: 0.0258s\n",
      "Epoch: 04973 loss_train: 1.7036 loss_rec: 1.7036 acc_train: 0.6020 loss_val: 1.7247 acc_val: 0.6021 time: 0.0278s\n",
      "Epoch: 04974 loss_train: 1.2520 loss_rec: 1.2520 acc_train: 0.6171 loss_val: 1.2746 acc_val: 0.6167 time: 0.0273s\n",
      "Epoch: 04975 loss_train: 4.7807 loss_rec: 4.7807 acc_train: 0.4855 loss_val: 4.8713 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04976 loss_train: 0.9831 loss_rec: 0.9831 acc_train: 0.5537 loss_val: 1.0010 acc_val: 0.5467 time: 0.0273s\n",
      "Epoch: 04977 loss_train: 6.4171 loss_rec: 6.4171 acc_train: 0.5541 loss_val: 6.3758 acc_val: 0.5592 time: 0.0273s\n",
      "Epoch: 04978 loss_train: 5.3528 loss_rec: 5.3528 acc_train: 0.5573 loss_val: 5.3265 acc_val: 0.5650 time: 0.0268s\n",
      "Epoch: 04979 loss_train: 3.1827 loss_rec: 3.1827 acc_train: 0.4856 loss_val: 3.2477 acc_val: 0.4938 time: 0.0273s\n",
      "Epoch: 04980 loss_train: 2.3111 loss_rec: 2.3111 acc_train: 0.4857 loss_val: 2.3608 acc_val: 0.4938 time: 0.0258s\n",
      "Epoch: 04981 loss_train: 6.5561 loss_rec: 6.5561 acc_train: 0.5541 loss_val: 6.5132 acc_val: 0.5592 time: 0.0278s\n",
      "Test set results: loss= 7.8382 accuracy= 0.5175\n",
      "Epoch: 04982 loss_train: 6.9296 loss_rec: 6.9296 acc_train: 0.5534 loss_val: 6.8820 acc_val: 0.5588 time: 0.0278s\n",
      "Epoch: 04983 loss_train: 0.8375 loss_rec: 0.8375 acc_train: 0.6101 loss_val: 0.8535 acc_val: 0.6067 time: 0.0298s\n",
      "Epoch: 04984 loss_train: 7.0136 loss_rec: 7.0136 acc_train: 0.4853 loss_val: 7.1386 acc_val: 0.4938 time: 0.0278s\n",
      "Epoch: 04985 loss_train: 3.6575 loss_rec: 3.6575 acc_train: 0.4856 loss_val: 3.7303 acc_val: 0.4938 time: 0.0258s\n",
      "Epoch: 04986 loss_train: 7.6111 loss_rec: 7.6111 acc_train: 0.5523 loss_val: 7.5549 acc_val: 0.5592 time: 0.0263s\n",
      "Epoch: 04987 loss_train: 10.0291 loss_rec: 10.0291 acc_train: 0.5497 loss_val: 9.9357 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 04988 loss_train: 4.2645 loss_rec: 4.2645 acc_train: 0.5633 loss_val: 4.2545 acc_val: 0.5696 time: 0.0273s\n",
      "Epoch: 04989 loss_train: 8.9398 loss_rec: 8.9398 acc_train: 0.4852 loss_val: 9.0957 acc_val: 0.4933 time: 0.0278s\n",
      "Epoch: 04990 loss_train: 12.0187 loss_rec: 12.0187 acc_train: 0.4860 loss_val: 12.2235 acc_val: 0.4933 time: 0.0273s\n",
      "Epoch: 04991 loss_train: 5.1598 loss_rec: 5.1598 acc_train: 0.4855 loss_val: 5.2562 acc_val: 0.4938 time: 0.0268s\n",
      "Test set results: loss= 10.3286 accuracy= 0.5148\n",
      "Epoch: 04992 loss_train: 9.1240 loss_rec: 9.1240 acc_train: 0.5521 loss_val: 9.0439 acc_val: 0.5629 time: 0.0278s\n",
      "Epoch: 04993 loss_train: 14.0998 loss_rec: 14.0998 acc_train: 0.5490 loss_val: 13.9388 acc_val: 0.5554 time: 0.0273s\n",
      "Epoch: 04994 loss_train: 10.4102 loss_rec: 10.4102 acc_train: 0.5489 loss_val: 10.3113 acc_val: 0.5567 time: 0.0278s\n",
      "Epoch: 04995 loss_train: 0.9002 loss_rec: 0.9002 acc_train: 0.6033 loss_val: 0.9162 acc_val: 0.5929 time: 0.0263s\n",
      "Epoch: 04996 loss_train: 8.5703 loss_rec: 8.5703 acc_train: 0.4852 loss_val: 8.7203 acc_val: 0.4933 time: 0.0278s\n",
      "Epoch: 04997 loss_train: 6.3163 loss_rec: 6.3163 acc_train: 0.4855 loss_val: 6.4306 acc_val: 0.4938 time: 0.0263s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04998 loss_train: 4.3768 loss_rec: 4.3768 acc_train: 0.5629 loss_val: 4.3654 acc_val: 0.5696 time: 0.0278s\n",
      "Epoch: 04999 loss_train: 6.2449 loss_rec: 6.2449 acc_train: 0.5545 loss_val: 6.2059 acc_val: 0.5600 time: 0.0293s\n",
      "Epoch: 05000 loss_train: 1.0056 loss_rec: 1.0056 acc_train: 0.6298 loss_val: 1.0265 acc_val: 0.6200 time: 0.0273s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 149.0380s\n",
      "Test set results: loss= 8.1703 accuracy= 0.5407\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t_total = time.time()\n",
    "for epoch in range(5000):\n",
    "    train(epoch)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        output=test(epoch)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        save_model(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "output=test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_train[(labels==1)[idx_train]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(output, 'GraphSage_Upsample_output.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = output.max(1)[1].type_as(labels)\n",
    "cout=preds.cpu().detach().numpy()\n",
    "clabels=labels.cpu().detach().numpy()\n",
    "cidx_test=idx_test.cpu().detach().numpy()\n",
    "cidx_valid=idx_val.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 427.9555555555555, 'Predicted label')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "cnf_matrix_n = metrics.confusion_matrix(clabels[cidx_test], cout[cidx_test])\n",
    "class_names=[\"Not_Related\", \"Related\"]\n",
    "#class_names=[\"Not_Related\", \"1\",\"2\",\"3\"] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "sns.heatmap(cnf_matrix_n, annot=True, cmap=\"YlGnBu\" ,fmt='d', xticklabels=class_names, yticklabels=class_names)\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Graph Sage Upsample')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix_n[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not_Related       0.14      0.01      0.01      4203\n",
      "     Related       0.55      0.96      0.70      5304\n",
      "\n",
      "    accuracy                           0.54      9507\n",
      "   macro avg       0.35      0.49      0.36      9507\n",
      "weighted avg       0.37      0.54      0.40      9507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(clabels[cidx_test], cout[cidx_test], target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  32, 4171],\n",
       "       [ 196, 5108]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 427.9555555555555, 'Predicted label')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "cnf_matrix_n_valid = metrics.confusion_matrix(clabels[cidx_valid], cout[cidx_valid])\n",
    "class_names=[\"Not_Related\", \"Related\"]\n",
    "#class_names=[\"Not_Related\", \"1\",\"2\",\"3\"] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "sns.heatmap(cnf_matrix_n, annot=True, cmap=\"YlGnBu\" ,fmt='d', xticklabels=class_names, yticklabels=class_names)\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Graph Sage Upsample')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_test=cnf_matrix_n[0][1]\n",
    "true_positive_test=cnf_matrix_n[1][1]\n",
    "true_negative_test=cnf_matrix_n[0][0]\n",
    "false_negative_test=cnf_matrix_n[1][0]\n",
    "\n",
    "false_positive_valid=cnf_matrix_n_valid[0][1]\n",
    "true_positive_valid=cnf_matrix_n_valid[1][1]\n",
    "true_negative_valid=cnf_matrix_n_valid[0][0]\n",
    "false_negative_valid=cnf_matrix_n_valid[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(true_negative,false_positive):\n",
    "    return str(true_negative/(true_negative+false_positive))\n",
    "def sensitivity(true_positive,false_negative):\n",
    "    return str(true_positive/(true_positive+false_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "Specificity: 0.0076136093266714255\n",
      "Sensitivity: 0.9630467571644042\n"
     ]
    }
   ],
   "source": [
    "print('test')\n",
    "print(\"Specificity: \"+ specificity(true_negative_test,false_positive_test))\n",
    "print(\"Sensitivity: \"+ sensitivity(true_positive_test,false_negative_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n",
      "Specificity: 0.009166666666666667\n",
      "Sensitivity: 0.9775\n"
     ]
    }
   ],
   "source": [
    "print('validation')\n",
    "print(\"Specificity: \"+ specificity(true_negative_valid,false_positive_valid))\n",
    "print(\"Sensitivity: \"+sensitivity(true_positive_valid,false_negative_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels=clabels[cidx_test]\n",
    "test_output=cout[cidx_test]\n",
    "valid_labels=clabels[cidx_valid]\n",
    "valid_output=cout[cidx_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_enc=np.eye(4)[test_labels]\n",
    "test_output_enc=np.eye(4)[test_output]\n",
    "valid_labels_enc=np.eye(4)[valid_labels]\n",
    "valid_output_enc=np.eye(4)[valid_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48533018324553784"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(clabels[cidx_test], cout[cidx_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49333333333333335"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(clabels[cidx_valid], cout[cidx_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_out=output.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output=c_out[cidx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_labels=pd.get_dummies(clabels[cidx_test]).values\n",
    "dummy_preds=pd.get_dummies(cout[cidx_test]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "fpr=[]\n",
    "tpr=[]\n",
    "roc_auc=[]\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(1,2):\n",
    "        fpr_, tpr_, _ = metrics.roc_curve(dummy_labels[:, i], test_output[:, i])\n",
    "        fpr.append(fpr_)\n",
    "        tpr.append(tpr_)\n",
    "        roc_auc.append(roc_auc_score(dummy_labels[:, i], test_output[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Receiver operating characteristic')\n",
    "labels=['Genes Not Associated to ASD',\"Genes Associated to ASD\"]\n",
    "for i in range(1):\n",
    "    ax.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % (roc_auc[i]))\n",
    "ax.legend(loc=\"best\")\n",
    "ax.grid(alpha=.4)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
